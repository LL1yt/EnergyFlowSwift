#!/usr/bin/env python3
"""
–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏
========================================================

–ú–æ–¥—É–ª–∏ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –º–µ–∂–¥—É Teacher –º–æ–¥–µ–ª—å—é –∏ 3D –∫—É–±–æ–º.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, Dict, Any

from .interfaces import EmbeddingProcessor
from ...utils.logging import get_logger, LogContext
from ...utils.device_manager import get_device_manager
from ...config.simple_config import SimpleProjectConfig

logger = get_logger(__name__)


class EmbeddingTransformer(nn.Module, EmbeddingProcessor):
    """
    –ë–∞–∑–æ–≤—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏ teacher –º–æ–¥–µ–ª–∏ (768D) 
    –∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –∫—É–±–∞ (37√ó37 = 1369D) —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    """
    
    def __init__(self, config: SimpleProjectConfig):
        super().__init__()
        self.config = config
        self.logger = get_logger(__name__)
        self.device_manager = get_device_manager()
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.teacher_dim = config.embedding.teacher_embedding_dim
        self.cube_dim = config.embedding.cube_embedding_dim
        self.surface_dim = config.embedding.cube_surface_dim
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        expected_cube_dim = self.surface_dim ** 2
        if self.cube_dim != expected_cube_dim:
            raise ValueError(f"Cube dimension mismatch: {self.cube_dim} != {expected_cube_dim}")
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        transformation_type = config.embedding.transformation_type
        
        if transformation_type == "linear":
            self._build_linear_transformers()
        elif transformation_type == "hierarchical":
            self._build_hierarchical_transformers()
        elif transformation_type == "attention":
            self._build_attention_transformers()
        else:
            raise ValueError(f"Unknown transformation type: {transformation_type}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if config.embedding.use_layer_norm:
            self.layer_norm_to = nn.LayerNorm(self.cube_dim)
            self.layer_norm_from = nn.LayerNorm(self.teacher_dim)
        
        self.dropout = nn.Dropout(config.embedding.dropout_rate)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –∫—É–±–∞
        self.positional_encoding = nn.Parameter(
            torch.randn(self.surface_dim, self.surface_dim) * 0.1
        )
        
        self.logger.info(f"üîÑ EmbeddingTransformer initialized: {self.teacher_dim}D ‚Üî {self.cube_dim}D")
    
    def _build_linear_transformers(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        # Teacher ‚Üí Cube
        self.to_cube = nn.Sequential(
            nn.Linear(self.teacher_dim, self.cube_dim),
            nn.GELU(),
            nn.Linear(self.cube_dim, self.cube_dim)
        )
        
        # Cube ‚Üí Teacher  
        self.from_cube = nn.Sequential(
            nn.Linear(self.cube_dim, self.teacher_dim),
            nn.GELU(),
            nn.Linear(self.teacher_dim, self.teacher_dim)
        )
        
        # Residual connections –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã
        if self.config.embedding.use_residual_connections:
            self.residual_to = nn.Linear(self.teacher_dim, self.cube_dim)
            self.residual_from = nn.Linear(self.cube_dim, self.teacher_dim)
    
    def _build_hierarchical_transformers(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª–µ–π (—É–ª—É—á—à–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)"""
        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: 768 ‚Üí 512 ‚Üí 1024 ‚Üí 1369
        self.to_cube = nn.Sequential(
            nn.Linear(self.teacher_dim, 512),
            nn.GELU(),
            nn.Dropout(self.config.embedding.dropout_rate),
            nn.Linear(512, 1024),
            nn.GELU(),
            nn.Dropout(self.config.embedding.dropout_rate),
            nn.Linear(1024, self.cube_dim)
        )
        
        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Å–∂–∞—Ç–∏–µ: 1369 ‚Üí 1024 ‚Üí 512 ‚Üí 768
        self.from_cube = nn.Sequential(
            nn.Linear(self.cube_dim, 1024),
            nn.GELU(),
            nn.Dropout(self.config.embedding.dropout_rate),
            nn.Linear(1024, 512),
            nn.GELU(),
            nn.Dropout(self.config.embedding.dropout_rate),
            nn.Linear(512, self.teacher_dim)
        )
        
        # Residual connections —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if self.config.embedding.use_residual_connections:
            self.residual_to_1 = nn.Linear(self.teacher_dim, 1024)
            self.residual_to_2 = nn.Linear(1024, self.cube_dim)
            self.residual_from_1 = nn.Linear(self.cube_dim, 1024)
            self.residual_from_2 = nn.Linear(1024, self.teacher_dim)
    
    def _build_attention_transformers(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ attention-based –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        # –ü–æ–∫–∞ —Ä–µ–∞–ª–∏–∑—É–µ–º –∫–∞–∫ linear, attention –¥–æ–±–∞–≤–∏–º –ø–æ–∑–∂–µ
        self._build_linear_transformers()
        
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å attention –º–µ—Ö–∞–Ω–∏–∑–º
        self.logger.info("‚ö†Ô∏è  Attention transformers not fully implemented yet, using linear")
    
    def transform_to_cube(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Teacher embeddings ‚Üí Cube surface embeddings
        
        Args:
            embeddings: Tensor —Ä–∞–∑–º–µ—Ä–∞ [batch, teacher_dim]
        
        Returns:
            Tensor —Ä–∞–∑–º–µ—Ä–∞ [batch, surface_dim, surface_dim]
        """
        with LogContext("embedding_transform", direction="to_cube"):
            batch_size = embeddings.size(0)
            
            # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            cube_flat = self.to_cube(embeddings)
            
            # Residual connection –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if (hasattr(self, 'residual_to') and 
                self.config.embedding.use_residual_connections):
                cube_flat = cube_flat + self.residual_to(embeddings)
            
            # Layer normalization
            if hasattr(self, 'layer_norm_to'):
                cube_flat = self.layer_norm_to(cube_flat)
            
            # Dropout
            cube_flat = self.dropout(cube_flat)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ 2D –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å
            cube_2d = cube_flat.view(batch_size, self.surface_dim, self.surface_dim)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            cube_2d = cube_2d + self.positional_encoding.unsqueeze(0)
            
            return cube_2d
    
    def transform_from_cube(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Cube surface embeddings ‚Üí Teacher embeddings
        
        Args:
            embeddings: Tensor —Ä–∞–∑–º–µ—Ä–∞ [batch, surface_dim, surface_dim]
        
        Returns:
            Tensor —Ä–∞–∑–º–µ—Ä–∞ [batch, teacher_dim]
        """
        with LogContext("embedding_transform", direction="from_cube"):
            batch_size = embeddings.size(0)
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            cube_2d = embeddings - self.positional_encoding.unsqueeze(0)
            
            # –ü–ª–æ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
            cube_flat = cube_2d.view(batch_size, self.cube_dim)
            
            # –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            teacher_emb = self.from_cube(cube_flat)
            
            # Residual connection –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if (hasattr(self, 'residual_from') and 
                self.config.embedding.use_residual_connections):
                teacher_emb = teacher_emb + self.residual_from(cube_flat)
            
            # Layer normalization
            if hasattr(self, 'layer_norm_from'):
                teacher_emb = self.layer_norm_from(teacher_emb)
            
            return teacher_emb
    
    def get_compression_ratio(self) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è/—Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        return self.cube_dim / self.teacher_dim
    
    def get_parameter_count(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º"""
        counts = {}
        for name, module in self.named_children():
            counts[name] = sum(p.numel() for p in module.parameters())
        counts['total'] = sum(p.numel() for p in self.parameters())
        return counts


class HierarchicalEmbeddingTransformer(EmbeddingTransformer):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ residual connections
    """
    
    def __init__(self, config: SimpleProjectConfig):
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Ç–∏–ø
        config.embedding.transformation_type = "hierarchical"
        super().__init__(config)
    
    def transform_to_cube(self, embeddings: torch.Tensor) -> torch.Tensor:
        """–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å multiple residual connections"""
        with LogContext("hierarchical_transform", direction="to_cube"):
            batch_size = embeddings.size(0)
            
            # –ü–æ—ç—Ç–∞–ø–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å residual connections
            h1 = F.gelu(self.to_cube[0](embeddings))  # 768 ‚Üí 512
            h1 = self.to_cube[1](h1)  # dropout
            
            h2 = F.gelu(self.to_cube[3](h1))  # 512 ‚Üí 1024
            h2 = self.to_cube[4](h2)  # dropout
            
            # Residual –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–º —É—Ä–æ–≤–Ω–µ
            if hasattr(self, 'residual_to_1'):
                h2 = h2 + self.residual_to_1(embeddings)
            
            h3 = self.to_cube[6](h2)  # 1024 ‚Üí 1369
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π residual
            if hasattr(self, 'residual_to_2'):
                h3 = h3 + self.residual_to_2(h2)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ reshape
            if hasattr(self, 'layer_norm_to'):
                h3 = self.layer_norm_to(h3)
            
            h3 = self.dropout(h3)
            cube_2d = h3.view(batch_size, self.surface_dim, self.surface_dim)
            cube_2d = cube_2d + self.positional_encoding.unsqueeze(0)
            
            return cube_2d


def create_embedding_transformer(config: SimpleProjectConfig) -> EmbeddingTransformer:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤"""
    transformation_type = config.embedding.transformation_type
    
    if transformation_type == "hierarchical":
        return HierarchicalEmbeddingTransformer(config)
    else:
        return EmbeddingTransformer(config)


# === UTILITY FUNCTIONS ===

def test_embedding_transformer(config: SimpleProjectConfig, batch_size: int = 4):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤"""
    logger.info("üß™ Testing EmbeddingTransformer...")
    
    transformer = create_embedding_transformer(config)
    device_manager = get_device_manager()
    transformer = device_manager.transfer_module(transformer)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    teacher_embeddings = torch.randn(batch_size, config.embedding.teacher_embedding_dim)
    teacher_embeddings = device_manager.transfer_tensor(teacher_embeddings)
    
    # –ü—Ä—è–º–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    cube_embeddings = transformer.transform_to_cube(teacher_embeddings)
    logger.info(f"  Teacher ‚Üí Cube: {teacher_embeddings.shape} ‚Üí {cube_embeddings.shape}")
    
    # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    reconstructed = transformer.transform_from_cube(cube_embeddings)
    logger.info(f"  Cube ‚Üí Teacher: {cube_embeddings.shape} ‚Üí {reconstructed.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    assert reconstructed.shape == teacher_embeddings.shape
    
    # –í—ã—á–∏—Å–ª—è–µ–º reconstruction loss
    mse_loss = F.mse_loss(reconstructed, teacher_embeddings)
    cosine_sim = F.cosine_similarity(reconstructed, teacher_embeddings).mean()
    
    logger.info(f"  üìä Reconstruction MSE: {mse_loss:.6f}")
    logger.info(f"  üìä Cosine Similarity: {cosine_sim:.6f}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    param_counts = transformer.get_parameter_count()
    logger.info(f"  üîß Total parameters: {param_counts['total']:,}")
    
    logger.info("‚úÖ EmbeddingTransformer test completed!")
    
    return transformer, {
        'mse_loss': mse_loss.item(),
        'cosine_similarity': cosine_sim.item(),
        'parameter_count': param_counts['total']
    }