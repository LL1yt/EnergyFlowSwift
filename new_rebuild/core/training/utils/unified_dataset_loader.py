#!/usr/bin/env python3
"""
Unified Dataset Loader –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 3D Cellular Neural Network
======================================================================

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:
- dialogue datasets (cache/dialogue_dataset/)
- prepared embeddings (data/embeddings/)
- cache embeddings (cache/llm_*.pt)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ new_rebuild.config
"""

import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
import random
from dataclasses import dataclass

from ....config import SimpleProjectConfig
from ....utils.logging import get_logger

logger = get_logger(__name__)


@dataclass 
class DatasetStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    total_samples: int
    embedding_dim: int
    source_distribution: Dict[str, int]
    type_distribution: Dict[str, int]


class UnifiedEmbeddingDataset(Dataset):
    """
    Unified Dataset –¥–ª—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    """
    
    def __init__(self, config: SimpleProjectConfig, max_total_samples: Optional[int] = None):
        self.config = config
        self.max_total_samples = max_total_samples  # –û–±—â–∏–π –ª–∏–º–∏—Ç –Ω–∞ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        self.embeddings: List[torch.Tensor] = []
        self.metadata: List[Dict] = []
        
        logger.info("üîÑ Initializing UnifiedEmbeddingDataset with central config...")
        if self.max_total_samples is not None:
            logger.info(f"üìä Total sample limit: {self.max_total_samples}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self._load_all_sources()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç –µ—Å–ª–∏ –∑–∞–¥–∞–Ω
        self._apply_total_limit()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º
        self._filter_and_validate()
        
        logger.info(f"‚úÖ Dataset ready: {len(self.embeddings)} samples")
    
    def _load_all_sources(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        # –í—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        self._load_dialogue_cache()
        self._load_prepared_embeddings()
        self._load_cache_embeddings()
    
    def _apply_total_limit(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤"""
        if self.max_total_samples is None:
            return
            
        current_total = len(self.embeddings)
        logger.info(f"üìä Before limit: {current_total} samples")
        
        if current_total > self.max_total_samples:
            # –°–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤—ã–±–∏—Ä–∞–µ–º —Å—ç–º–ø–ª—ã, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
            indices = list(range(current_total))
            random.shuffle(indices)
            selected_indices = indices[:self.max_total_samples]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä
            self.embeddings = [self.embeddings[i] for i in selected_indices]
            self.metadata = [self.metadata[i] for i in selected_indices]
            
            logger.info(f"üìä Applied total limit: {len(self.embeddings)} samples (reduced from {current_total})")
    
    def _load_dialogue_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º dialogue datasets –∏–∑ cache/dialogue_dataset/"""
        cache_dir = Path("cache/dialogue_dataset")
        files = list(cache_dir.glob("*.pt"))
        
        logger.info(f"üìÇ Loading dialogue cache: {len(files)} files")
        
        loaded_count = 0
        for file in files:
            try:
                data = torch.load(file, map_location='cpu')
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ –∞–Ω–∞–ª–∏–∑–∞: questions [4, 768], answers [4, 768]
                embeddings = []
                
                if 'questions' in data and isinstance(data['questions'], torch.Tensor):
                    questions = data['questions']
                    if questions.dim() == 2:  # [batch, 768]
                        embeddings.extend([questions[i] for i in range(questions.size(0))])
                
                if 'answers' in data and isinstance(data['answers'], torch.Tensor):
                    answers = data['answers'] 
                    if answers.dim() == 2:  # [batch, 768]
                        embeddings.extend([answers[i] for i in range(answers.size(0))])
                
                for emb in embeddings:
                    if self._is_valid_embedding(emb):
                        self.embeddings.append(emb)
                        self.metadata.append({
                            "source": "dialogue_cache",
                            "file": file.name,
                            "type": "dialogue"
                        })
                        loaded_count += 1
                            
            except Exception as e:
                logger.warning(f"Failed to load dialogue file {file}: {e}")
                
        logger.info(f"‚úÖ Loaded {loaded_count} embeddings from dialogue cache")
    
    def _load_prepared_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º prepared embeddings –∏–∑ data/embeddings/"""
        embeddings_dir = Path("data/embeddings")
        files = list(embeddings_dir.glob("*.pt"))
        
        logger.info(f"üìÇ Loading prepared embeddings: {len(files)} files")
        
        loaded_count = 0
        for file in files:
            try:
                data = torch.load(file, map_location='cpu')
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ –∞–Ω–∞–ª–∏–∑–∞: question_embeddings, answer_embeddings
                embeddings = []
                
                if isinstance(data, dict):
                    for key in ['question_embeddings', 'answer_embeddings']:
                        if key in data:
                            emb_data = data[key]
                            if isinstance(emb_data, torch.Tensor) and emb_data.dim() == 2:
                                # [num_samples, 768] -> list of [768] tensors
                                embeddings.extend([emb_data[i] for i in range(emb_data.size(0))])
                
                elif isinstance(data, torch.Tensor):
                    if data.dim() == 2:  # [batch, dim]
                        embeddings.extend([data[i] for i in range(data.size(0))])
                    elif data.dim() == 1:  # [dim]
                        embeddings.append(data)
                
                for emb in embeddings:
                    if self._is_valid_embedding(emb):
                        self.embeddings.append(emb)
                        self.metadata.append({
                            "source": "prepared_embeddings",
                            "file": file.name,
                            "type": "prepared"
                        })
                        loaded_count += 1
                    
            except Exception as e:
                logger.warning(f"Failed to load prepared embedding {file}: {e}")
                
        logger.info(f"‚úÖ Loaded {loaded_count} embeddings from prepared files")
    
    def _load_cache_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º cache —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ cache/llm_*.pt"""
        cache_files = list(Path("cache").glob("llm_*.pt"))
        
        logger.info(f"üìÇ Loading cache embeddings: {len(cache_files)} files")
        
        loaded_count = 0
        for file in cache_files:
            try:
                data = torch.load(file, map_location='cpu')
                
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞: [94, 4096] torch.float16
                if isinstance(data, torch.Tensor):
                    # –≠—Ç–∏ —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 4096, –Ω–∞–º –Ω—É–∂–Ω–∞ 768
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ö –ø–æ–∫–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–µ–∫—Ü–∏—é
                    if data.shape[-1] != self.config.embedding.input_dim:
                        logger.debug(f"Skipping {file.name}: wrong dimension {data.shape[-1]} != {self.config.embedding.input_dim}")
                        continue
                    
                    if data.dim() == 2:  # [batch, dim]
                        for i in range(min(data.size(0), 100)):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                            emb = data[i].float()  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ float16
                            if self._is_valid_embedding(emb):
                                self.embeddings.append(emb)
                                self.metadata.append({
                                    "source": "cache_embeddings",
                                    "file": file.name,
                                    "type": "cache"
                                })
                                loaded_count += 1
                    elif data.dim() == 1:  # [dim]
                        emb = data.float()
                        if self._is_valid_embedding(emb):
                            self.embeddings.append(emb)
                            self.metadata.append({
                                "source": "cache_embeddings",
                                "file": file.name,
                                "type": "cache"
                            })
                            loaded_count += 1
                    
            except Exception as e:
                logger.warning(f"Failed to load cache embedding {file}: {e}")
                
        logger.info(f"‚úÖ Loaded {loaded_count} embeddings from cache files")
    
    def _is_valid_embedding(self, emb: torch.Tensor) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ñ–∏–≥"""
        if not isinstance(emb, torch.Tensor):
            return False
            
        if emb.dim() != 1:
            return False
            
        if emb.size(0) != self.config.embedding.input_dim:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º—É (–±–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥–µ–ª—ã)
        norm = torch.norm(emb).item()
        if norm < 0.1 or norm > 100.0:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/Inf
        if torch.isnan(emb).any() or torch.isinf(emb).any():
            return False
            
        return True
    
    def _filter_and_validate(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç Filtering and validating dataset...")
        
        # Shuffle
        combined = list(zip(self.embeddings, self.metadata))
        random.shuffle(combined)
        self.embeddings, self.metadata = zip(*combined)
        self.embeddings = list(self.embeddings)
        self.metadata = list(self.metadata)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ç–∏–ø
        self.embeddings = [emb.float() for emb in self.embeddings]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        source_stats = {}
        for meta in self.metadata:
            source = meta['source']
            source_stats[source] = source_stats.get(source, 0) + 1
            
        logger.info("üìä Dataset statistics:")
        for source, count in source_stats.items():
            logger.info(f"  {source}: {count} samples")
    
    def __len__(self) -> int:
        return len(self.embeddings)
    
    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, Dict]]:
        return {
            'embedding': self.embeddings[idx],
            'metadata': self.metadata[idx]
        }
    
    def get_stats(self) -> DatasetStats:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
        source_stats = {}
        type_stats = {}
        
        for meta in self.metadata:
            source = meta['source']
            type_name = meta.get('type', 'unknown')
            
            source_stats[source] = source_stats.get(source, 0) + 1
            type_stats[type_name] = type_stats.get(type_name, 0) + 1
        
        return DatasetStats(
            total_samples=len(self.embeddings),
            embedding_dim=self.config.embedding.input_dim,
            source_distribution=source_stats,
            type_distribution=type_stats
        )


def create_training_dataloader(
    config: SimpleProjectConfig,
    max_total_samples: Optional[int] = None,
    shuffle: bool = True,
    num_workers: int = 0
) -> Tuple[DataLoader, DatasetStats]:
    """
    –°–æ–∑–¥–∞–µ—Ç DataLoader –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    
    Args:
        config: –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        max_total_samples: –û–±—â–∏–π –ª–∏–º–∏—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)
        shuffle: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
        num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        
    Returns:
        Tuple[DataLoader, DatasetStats]: DataLoader –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω
    if config.training_embedding.max_total_samples is not None:
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: max_total_samples –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        effective_max_samples = config.training_embedding.max_total_samples
        logger.info(f"üìä Using max_total_samples from config: {effective_max_samples}")
    else:
        # Fallback –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏
        effective_max_samples = max_total_samples
        if effective_max_samples is not None:
            logger.info(f"üìä Using max_total_samples parameter: {effective_max_samples}")
        else:
            logger.info("üìä No sample limit - using full dataset")
    
    dataset = UnifiedEmbeddingDataset(config, effective_max_samples)
    stats = dataset.get_stats()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º batch_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    batch_size = config.training_embedding.embedding_batch_size
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        drop_last=True  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π
    )
    
    logger.info(f"üöÄ DataLoader created: {len(dataset)} samples, batch_size={batch_size}")
    
    return dataloader, stats


def main():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º unified dataset loader"""
    from ....utils.logging import get_logger
    logger = get_logger(__name__)
    logger.info("üß™ TESTING UNIFIED DATASET LOADER")
    logger.info("=" * 50)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    from ....config import SimpleProjectConfig
    config = SimpleProjectConfig()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    max_samples = 50
    
    # –°–æ–∑–¥–∞–µ–º DataLoader
    dataloader, stats = create_training_dataloader(
        config=config,
        max_total_samples=max_samples,
        shuffle=True
    )
    
    logger.info(f"\nüìä DATASET STATISTICS:")
    logger.info(f"Total samples: {stats.total_samples}")
    logger.info(f"Embedding dim: {stats.embedding_dim}")
    logger.info(f"Source distribution: {stats.source_distribution}")
    logger.info(f"Type distribution: {stats.type_distribution}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–µ–π
    logger.info(f"\nüîÑ TESTING BATCH LOADING:")
    for i, batch in enumerate(dataloader):
        embeddings = batch['embedding']  # [batch_size, embedding_dim]
        metadata = batch['metadata']  # List of dicts
        
        logger.info(f"Batch {i+1}:")
        logger.info(f"  Embeddings shape: {embeddings.shape}")
        logger.info(f"  Embeddings dtype: {embeddings.dtype}")
        logger.info(f"  Metadata samples: {len(metadata)}")
        
        if i >= 2:  # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –±–∞—Ç—á–∞
            break
    
    logger.info(f"\n‚úÖ Unified Dataset Loader test completed!")
    logger.info(f"üìà Ready for real training with {stats.total_samples} samples")


if __name__ == "__main__":
    main()