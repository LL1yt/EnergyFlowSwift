#!/usr/bin/env python3
"""
Unified Dataset Loader –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 3D Cellular Neural Network
======================================================================

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:
- dialogue datasets (cache/dialogue_dataset/)
- prepared embeddings (data/embeddings/)
- cache embeddings (cache/llm_*.pt)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ new_rebuild.config
"""

import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
import random
from dataclasses import dataclass
import gc

from ....config import SimpleProjectConfig
from ....utils.logging import get_logger
from ....utils.device_manager import get_device_manager

logger = get_logger(__name__)


class GPUMemoryEstimator:
    """–û—Ü–µ–Ω–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ GPU –ø–∞–º—è—Ç—å—é –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    def __init__(self):
        self.device_manager = get_device_manager()
        self.embedding_size_mb = 768 * 4 / (1024**2)  # float32, 768 dim
        self.use_gpu_only = False  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ dataset
    
    def estimate_dataset_memory_mb(self, num_samples: int) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ MB"""
        return num_samples * self.embedding_size_mb
    
    def get_safe_sample_limit(self, reserve_for_training_gb: float = 20.0) -> Optional[int]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ª–∏–º–∏—Ç —Å—ç–º–ø–ª–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è –ø–∞–º—è—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            reserve_for_training_gb: –°–∫–æ–ª—å–∫–æ GB –æ—Å—Ç–∞–≤–∏—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.device_manager.is_cuda():
            if self.use_gpu_only:
                raise RuntimeError("üö® GPU memory estimator needs CUDA but GPU not available!")
            return None  # CPU —Ä–µ–∂–∏–º - –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            
        total_memory_gb = self.device_manager.get_available_memory_gb()
        available_for_dataset_gb = total_memory_gb - reserve_for_training_gb
        
        if available_for_dataset_gb <= 0:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞. Total: {total_memory_gb:.1f}GB, Reserved: {reserve_for_training_gb}GB")
            return 100  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback
            
        available_for_dataset_mb = available_for_dataset_gb * 1024
        safe_samples = int(available_for_dataset_mb / self.embedding_size_mb * 0.8)  # 80% –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
        
        logger.info(f"üßÆ GPU Memory Planning:")
        logger.info(f"  Total GPU: {total_memory_gb:.1f}GB")
        logger.info(f"  Reserved for training: {reserve_for_training_gb}GB")
        logger.info(f"  Available for dataset: {available_for_dataset_gb:.1f}GB")
        logger.info(f"  Safe sample limit: {safe_samples:,}")
        
        return safe_samples


@dataclass 
class DatasetStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    total_samples: int
    embedding_dim: int
    source_distribution: Dict[str, int]
    type_distribution: Dict[str, int]


class UnifiedEmbeddingDataset(Dataset):
    """
    Unified Dataset –¥–ª—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    """
    
    def __init__(self, config: SimpleProjectConfig, max_total_samples: Optional[int] = None):
        self.config = config
        self.device_manager = get_device_manager()
        self.memory_estimator = GPUMemoryEstimator()
        
        # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ GPU-only —Ä–µ–∂–∏–º–∞
        if not config.device.fallback_cpu and not self.device_manager.is_cuda():
            raise RuntimeError(
                "üö® GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ fallback_cpu=False –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏! "
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ GPU –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ config.device.fallback_cpu=True"
            )
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU –∏–ª–∏ –æ—à–∏–±–∫–∞
        self.use_gpu_only = not config.device.fallback_cpu
        
        # –£–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        self.max_total_samples = self._plan_memory_usage(max_total_samples)
        
        self.embeddings: List[torch.Tensor] = []
        self.metadata: List[Dict] = []
        
        logger.info("üîÑ Initializing STRICT GPU-only UnifiedEmbeddingDataset...")
        logger.info(f"‚ö° GPU-only mode: {'‚úÖ ENFORCED' if self.use_gpu_only else '‚ö†Ô∏è Fallback allowed'}")
        if self.max_total_samples is not None:
            logger.info(f"üìä Smart memory limit: {self.max_total_samples:,} samples")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self._load_all_sources()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç –µ—Å–ª–∏ –∑–∞–¥–∞–Ω
        self._apply_total_limit()
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU –≤–∞–ª–∏–¥–∞—Ü–∏—é
        self._gpu_filter_and_validate()
        
        logger.info(f"‚úÖ GPU Dataset ready: {len(self.embeddings)} samples")
        self._log_memory_usage()
    
    def _plan_memory_usage(self, requested_limit: Optional[int]) -> Optional[int]:
        """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑–µ—Ä–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        training_reserve_gb = getattr(self.config.training_embedding, 'gpu_memory_reserve_gb', 20.0)
        
        # –ü–µ—Ä–µ–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU-only —Ä–µ–∂–∏–º–µ –≤ estimator
        self.memory_estimator.use_gpu_only = self.use_gpu_only
        
        # –í—ã—á–∏—Å–ª—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ª–∏–º–∏—Ç –∏—Å—Ö–æ–¥—è –∏–∑ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
        safe_limit = self.memory_estimator.get_safe_sample_limit(training_reserve_gb)
        
        if requested_limit is None:
            return safe_limit  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ª–∏–º–∏—Ç
        
        if safe_limit is None:
            return requested_limit  # CPU —Ä–µ–∂–∏–º - –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º—É–º –∏–∑ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ
        final_limit = min(requested_limit, safe_limit)
        
        if final_limit < requested_limit:
            logger.warning(f"‚ö†Ô∏è Requested {requested_limit:,} samples, but GPU memory allows only {final_limit:,}")
            
        return final_limit
    
    def _log_memory_usage(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        if self.device_manager.is_cuda():
            stats = self.device_manager.get_memory_stats()
            estimated_mb = self.memory_estimator.estimate_dataset_memory_mb(len(self.embeddings))
            
            mode_prefix = "‚ö° GPU-ONLY" if self.use_gpu_only else "üîÑ GPU/CPU"
            logger.info(f"üìä {mode_prefix} Memory Usage:")
            logger.info(f"  Dataset estimated: {estimated_mb:.1f}MB")
            logger.info(f"  GPU allocated: {stats.get('allocated_mb', 0):.1f}MB")
            logger.info(f"  GPU available: {self.device_manager.get_available_memory_gb():.1f}GB")
    
    def _load_all_sources(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å early stopping"""
        
        logger.info(f"üîÑ Loading sources with limit: {self.max_total_samples}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ª–∏–º–∏—Ç–∞
        self._load_dialogue_cache()
        if self._check_early_stop():
            return
            
        self._load_prepared_embeddings() 
        if self._check_early_stop():
            return
            
        self._load_cache_embeddings()
    
    def _check_early_stop(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º—ã –ª–∏–º–∏—Ç–∞ —Å—ç–º–ø–ª–æ–≤"""
        if self.max_total_samples is None:
            return False
            
        current_count = len(self.embeddings)
        if current_count >= self.max_total_samples:
            logger.info(f"‚ö° Early stop: reached {current_count} samples (limit: {self.max_total_samples})")
            return True
        return False
    
    def _apply_total_limit(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤"""
        if self.max_total_samples is None:
            return
            
        current_total = len(self.embeddings)
        logger.info(f"üìä Before limit: {current_total} samples")
        
        if current_total > self.max_total_samples:
            # –°–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤—ã–±–∏—Ä–∞–µ–º —Å—ç–º–ø–ª—ã, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
            indices = list(range(current_total))
            random.shuffle(indices)
            selected_indices = indices[:self.max_total_samples]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä
            self.embeddings = [self.embeddings[i] for i in selected_indices]
            self.metadata = [self.metadata[i] for i in selected_indices]
            
            logger.info(f"üìä Applied total limit: {len(self.embeddings)} samples (reduced from {current_total})")
    
    def _load_dialogue_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º dialogue datasets –∏–∑ cache/dialogue_dataset/"""
        cache_dir = Path("cache/dialogue_dataset")
        files = list(cache_dir.glob("*.pt"))
        
        logger.info(f"üìÇ Loading dialogue cache: {len(files)} files")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU
        if self.use_gpu_only:
            map_location = 'cuda'
            logger.info("‚ö° GPU-ONLY: Loading directly to CUDA")
        else:
            map_location = 'cuda' if self.device_manager.is_cuda() else 'cpu'
        
        loaded_count = 0
        for file in files:
            try:
                data = torch.load(file, map_location=map_location)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ –∞–Ω–∞–ª–∏–∑–∞: questions [4, 768], answers [4, 768]
                embeddings = []
                
                if 'questions' in data and isinstance(data['questions'], torch.Tensor):
                    questions = data['questions']
                    if questions.dim() == 2:  # [batch, 768]
                        embeddings.extend([questions[i] for i in range(questions.size(0))])
                
                if 'answers' in data and isinstance(data['answers'], torch.Tensor):
                    answers = data['answers'] 
                    if answers.dim() == 2:  # [batch, 768]
                        embeddings.extend([answers[i] for i in range(answers.size(0))])
                
                for emb in embeddings:
                    if self._is_valid_embedding(emb):
                        self.embeddings.append(emb)
                        self.metadata.append({
                            "source": "dialogue_cache",
                            "file": file.name,
                            "type": "dialogue"
                        })
                        loaded_count += 1
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
                        if self.max_total_samples and len(self.embeddings) >= self.max_total_samples:
                            logger.info(f"‚ö° Reached limit in dialogue_cache: {len(self.embeddings)} samples")
                            return
                            
            except Exception as e:
                logger.warning(f"Failed to load dialogue file {file}: {e}")
                
        logger.info(f"‚úÖ Loaded {loaded_count} embeddings from dialogue cache")
    
    def _load_prepared_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º prepared embeddings –∏–∑ data/embeddings/"""
        embeddings_dir = Path("data/embeddings")
        files = list(embeddings_dir.glob("*.pt"))
        
        logger.info(f"üìÇ Loading prepared embeddings: {len(files)} files")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU
        if self.use_gpu_only:
            map_location = 'cuda'
        else:
            map_location = 'cuda' if self.device_manager.is_cuda() else 'cpu'
        
        loaded_count = 0
        for file in files:
            try:
                data = torch.load(file, map_location=map_location)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ –∞–Ω–∞–ª–∏–∑–∞: question_embeddings, answer_embeddings
                embeddings = []
                
                if isinstance(data, dict):
                    for key in ['question_embeddings', 'answer_embeddings']:
                        if key in data:
                            emb_data = data[key]
                            if isinstance(emb_data, torch.Tensor) and emb_data.dim() == 2:
                                # [num_samples, 768] -> list of [768] tensors
                                embeddings.extend([emb_data[i] for i in range(emb_data.size(0))])
                
                elif isinstance(data, torch.Tensor):
                    if data.dim() == 2:  # [batch, dim]
                        embeddings.extend([data[i] for i in range(data.size(0))])
                    elif data.dim() == 1:  # [dim]
                        embeddings.append(data)
                
                for emb in embeddings:
                    if self._is_valid_embedding(emb):
                        self.embeddings.append(emb)
                        self.metadata.append({
                            "source": "prepared_embeddings",
                            "file": file.name,
                            "type": "prepared"
                        })
                        loaded_count += 1
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
                        if self.max_total_samples and len(self.embeddings) >= self.max_total_samples:
                            logger.info(f"‚ö° Reached limit in prepared_embeddings: {len(self.embeddings)} samples")
                            return
                    
            except Exception as e:
                logger.warning(f"Failed to load prepared embedding {file}: {e}")
                
        logger.info(f"‚úÖ Loaded {loaded_count} embeddings from prepared files")
    
    def _load_cache_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º cache —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ cache/llm_*.pt"""
        cache_files = list(Path("cache").glob("llm_*.pt"))
        
        logger.info(f"üìÇ Loading cache embeddings: {len(cache_files)} files")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU
        if self.use_gpu_only:
            map_location = 'cuda'
        else:
            map_location = 'cuda' if self.device_manager.is_cuda() else 'cpu'
        
        loaded_count = 0
        for file in cache_files:
            try:
                data = torch.load(file, map_location=map_location)
                
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞: [94, 4096] torch.float16
                if isinstance(data, torch.Tensor):
                    # –≠—Ç–∏ —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 4096, –Ω–∞–º –Ω—É–∂–Ω–∞ 768
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ö –ø–æ–∫–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–µ–∫—Ü–∏—é
                    if data.shape[-1] != self.config.embedding.input_dim:
                        logger.debug(f"Skipping {file.name}: wrong dimension {data.shape[-1]} != {self.config.embedding.input_dim}")
                        continue
                    
                    if data.dim() == 2:  # [batch, dim]
                        for i in range(min(data.size(0), 100)):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                            emb = data[i].float()  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ float16
                            if self._is_valid_embedding(emb):
                                self.embeddings.append(emb)
                                self.metadata.append({
                                    "source": "cache_embeddings",
                                    "file": file.name,
                                    "type": "cache"
                                })
                                loaded_count += 1
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
                                if self.max_total_samples and len(self.embeddings) >= self.max_total_samples:
                                    logger.info(f"‚ö° Reached limit in cache_embeddings: {len(self.embeddings)} samples")
                                    return
                                    
                    elif data.dim() == 1:  # [dim]
                        emb = data.float()
                        if self._is_valid_embedding(emb):
                            self.embeddings.append(emb)
                            self.metadata.append({
                                "source": "cache_embeddings",
                                "file": file.name,
                                "type": "cache"
                            })
                            loaded_count += 1
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
                            if self.max_total_samples and len(self.embeddings) >= self.max_total_samples:
                                logger.info(f"‚ö° Reached limit in cache_embeddings: {len(self.embeddings)} samples")
                                return
                    
            except Exception as e:
                logger.warning(f"Failed to load cache embedding {file}: {e}")
                
        logger.info(f"‚úÖ Loaded {loaded_count} embeddings from cache files")
    
    def _is_valid_embedding(self, emb: torch.Tensor) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ñ–∏–≥"""
        if not isinstance(emb, torch.Tensor):
            return False
            
        if emb.dim() != 1:
            return False
            
        if emb.size(0) != self.config.embedding.input_dim:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º—É (–±–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥–µ–ª—ã)
        norm = torch.norm(emb).item()
        if norm < 0.1 or norm > 100.0:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/Inf
        if torch.isnan(emb).any() or torch.isinf(emb).any():
            return False
            
        return True
    
    def _filter_and_validate(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç Filtering and validating dataset...")
        
        # Shuffle
        combined = list(zip(self.embeddings, self.metadata))
        random.shuffle(combined)
        self.embeddings, self.metadata = zip(*combined)
        self.embeddings = list(self.embeddings)
        self.metadata = list(self.metadata)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ç–∏–ø
        self.embeddings = [emb.float() for emb in self.embeddings]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        source_stats = {}
        for meta in self.metadata:
            source = meta['source']
            source_stats[source] = source_stats.get(source, 0) + 1
            
        logger.info("üìä Dataset statistics:")
        for source, count in source_stats.items():
            logger.info(f"  {source}: {count} samples")
    
    def _gpu_filter_and_validate(self):
        """GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üöÄ GPU-accelerated filtering and validation...")
        
        if not self.embeddings:
            return
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ batch tensor –¥–ª—è GPU –æ–±—Ä–∞–±–æ—Ç–∫–∏
        try:
            # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU
            embeddings_batch = torch.stack([emb.float() for emb in self.embeddings])
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ GPU
            valid_mask = self._gpu_validate_batch(embeddings_batch)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
            valid_embeddings_batch = embeddings_batch[valid_mask]
            valid_metadata = [meta for i, meta in enumerate(self.metadata) if valid_mask[i]]
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫
            self.embeddings = [valid_embeddings_batch[i] for i in range(valid_embeddings_batch.size(0))]
            self.metadata = valid_metadata
            
            # Shuffle –Ω–∞ GPU
            if len(self.embeddings) > 0:
                indices = torch.randperm(len(self.embeddings), device=embeddings_batch.device)
                self.embeddings = [self.embeddings[i] for i in indices.cpu()]
                self.metadata = [self.metadata[i] for i in indices.cpu()]
            
            logger.info(f"‚úÖ GPU validation completed: {len(self.embeddings)} valid samples")
            
        except Exception as e:
            if self.use_gpu_only:
                raise RuntimeError(f"üö® GPU validation failed in GPU-ONLY mode: {e}")
            else:
                logger.warning(f"‚ö†Ô∏è GPU validation failed, falling back to CPU: {e}")
                self._filter_and_validate()
                return
            
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        source_stats = {}
        for meta in self.metadata:
            source = meta['source']
            source_stats[source] = source_stats.get(source, 0) + 1
            
        logger.info("üìä GPU Dataset statistics:")
        for source, count in source_stats.items():
            logger.info(f"  {source}: {count} samples")
            
        # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
        if 'embeddings_batch' in locals():
            del embeddings_batch
        if 'valid_embeddings_batch' in locals():
            del valid_embeddings_batch
        gc.collect()
        torch.cuda.empty_cache()
    
    def _gpu_validate_batch(self, embeddings_batch: torch.Tensor) -> torch.Tensor:
        """
        –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –±–∞—Ç—á–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ GPU
        
        Args:
            embeddings_batch: [N, 768] tensor on GPU
            
        Returns:
            valid_mask: [N] boolean tensor
        """
        N = embeddings_batch.size(0)
        device = embeddings_batch.device
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (–≤—Å–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 768)
        dim_valid = embeddings_batch.size(1) == self.config.embedding.input_dim
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º
        norms = torch.norm(embeddings_batch, dim=1)  # [N]
        norm_valid = (norms > 0.1) & (norms < 100.0)  # [N]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
        nan_valid = ~torch.isnan(embeddings_batch).any(dim=1)  # [N]
        inf_valid = ~torch.isinf(embeddings_batch).any(dim=1)  # [N]
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ —É—Å–ª–æ–≤–∏—è
        valid_mask = norm_valid & nan_valid & inf_valid
        
        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è, –æ—Ç–∫–ª–æ–Ω—è–µ–º –≤—Å–µ
        if not dim_valid:
            valid_mask = torch.zeros(N, dtype=torch.bool, device=device)
            
        logger.info(f"üîç GPU validation: {valid_mask.sum().item()}/{N} samples passed")
        
        return valid_mask
    
    def __len__(self) -> int:
        return len(self.embeddings)
    
    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, Dict]]:
        return {
            'embedding': self.embeddings[idx],
            'metadata': self.metadata[idx]
        }
    
    def get_stats(self) -> DatasetStats:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
        source_stats = {}
        type_stats = {}
        
        for meta in self.metadata:
            source = meta['source']
            type_name = meta.get('type', 'unknown')
            
            source_stats[source] = source_stats.get(source, 0) + 1
            type_stats[type_name] = type_stats.get(type_name, 0) + 1
        
        return DatasetStats(
            total_samples=len(self.embeddings),
            embedding_dim=self.config.embedding.input_dim,
            source_distribution=source_stats,
            type_distribution=type_stats
        )


def create_training_dataloader(
    config: SimpleProjectConfig,
    max_total_samples: Optional[int] = None,
    shuffle: bool = True,
    num_workers: int = 0
) -> Tuple[DataLoader, DatasetStats]:
    """
    –°–æ–∑–¥–∞–µ—Ç DataLoader –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    
    Args:
        config: –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        max_total_samples: –û–±—â–∏–π –ª–∏–º–∏—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç config)
        shuffle: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
        num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        
    Returns:
        Tuple[DataLoader, DatasetStats]: DataLoader –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω
    if config.training_embedding.max_total_samples is not None:
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: max_total_samples –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        effective_max_samples = config.training_embedding.max_total_samples
        logger.info(f"üìä Using max_total_samples from config: {effective_max_samples}")
    else:
        # Fallback –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏
        effective_max_samples = max_total_samples
        if effective_max_samples is not None:
            logger.info(f"üìä Using max_total_samples parameter: {effective_max_samples}")
        else:
            logger.info("üìä No sample limit - using full dataset")
    
    dataset = UnifiedEmbeddingDataset(config, effective_max_samples)
    stats = dataset.get_stats()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º batch_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    batch_size = config.training_embedding.embedding_batch_size
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 5090
    device_manager = get_device_manager()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU-only –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if not config.device.fallback_cpu and not device_manager.is_cuda():
        raise RuntimeError("üö® GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è DataLoader –≤ GPU-ONLY —Ä–µ–∂–∏–º–µ!")
    
    is_cuda = device_manager.is_cuda()
    use_gpu_only = not config.device.fallback_cpu
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 5090
    if use_gpu_only:
        # Windows multiprocessing fix: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
        optimal_num_workers = num_workers if num_workers is not None else 0
        prefetch_factor = 6 if optimal_num_workers > 0 else 2
        # –í–ê–ñ–ù–û: pin_memory=False –¥–ª—è GPU —Ç–µ–Ω–∑–æ—Ä–æ–≤ (–æ–Ω–∏ —É–∂–µ –Ω–∞ GPU)
        pin_memory = False
        logger.info(f"‚ö° GPU-ONLY DataLoader optimizations enabled (workers: {optimal_num_workers})")
        logger.info("üîß pin_memory=False (tensors already on GPU)")
    else:
        optimal_num_workers = 8 if is_cuda else num_workers
        prefetch_factor = 4 if is_cuda else 2
        pin_memory = is_cuda
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=optimal_num_workers,
        pin_memory=pin_memory,
        drop_last=True,  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π
        persistent_workers=True if optimal_num_workers > 0 else False,  # PyTorch 2.x –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        prefetch_factor=prefetch_factor if optimal_num_workers > 0 else None
    )
    
    mode_info = "‚ö° GPU-ONLY" if use_gpu_only else "üîÑ GPU/CPU Hybrid"
    logger.info(f"üöÄ {mode_info} DataLoader for RTX 5090:")
    logger.info(f"  Samples: {len(dataset):,}")
    logger.info(f"  Batch size: {batch_size}")
    logger.info(f"  Workers: {optimal_num_workers}")
    logger.info(f"  Pin memory: {pin_memory}")
    logger.info(f"  Persistent workers: {optimal_num_workers > 0}")
    logger.info(f"  Prefetch factor: {prefetch_factor}")
    
    if use_gpu_only:
        logger.info(f"‚ö° GPU-ONLY mode enforced - no CPU fallbacks")
    
    logger.info(f"üöÄ DataLoader created: {len(dataset)} samples, batch_size={batch_size}")
    
    return dataloader, stats


def main():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º unified dataset loader"""
    from ....utils.logging import get_logger
    logger = get_logger(__name__)
    logger.info("üß™ TESTING UNIFIED DATASET LOADER")
    logger.info("=" * 50)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    from ....config import SimpleProjectConfig
    config = SimpleProjectConfig()
    
    
    # –°–æ–∑–¥–∞–µ–º DataLoader
    dataloader, stats = create_training_dataloader(
        config=config,
        max_total_samples=config.training_embedding.max_total_samples,
        shuffle=True
    )
    
    logger.info(f"\nüìä DATASET STATISTICS:")
    logger.info(f"Total samples: {stats.total_samples}")
    logger.info(f"Embedding dim: {stats.embedding_dim}")
    logger.info(f"Source distribution: {stats.source_distribution}")
    logger.info(f"Type distribution: {stats.type_distribution}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–µ–π
    logger.info(f"\nüîÑ TESTING BATCH LOADING:")
    for i, batch in enumerate(dataloader):
        embeddings = batch['embedding']  # [batch_size, embedding_dim]
        metadata = batch['metadata']  # List of dicts
        
        logger.info(f"Batch {i+1}:")
        logger.info(f"  Embeddings shape: {embeddings.shape}")
        logger.info(f"  Embeddings dtype: {embeddings.dtype}")
        logger.info(f"  Metadata samples: {len(metadata)}")
        
        if i >= 2:  # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –±–∞—Ç—á–∞
            break
    
    logger.info(f"\n‚úÖ Unified Dataset Loader test completed!")
    logger.info(f"üìà Ready for real training with {stats.total_samples} samples")


if __name__ == "__main__":
    main()