#!/usr/bin/env python3
"""
–î–µ–∫–æ–¥–µ—Ä —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç
=========================

–ü—Ä–æ—Å—Ç–æ–π –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç.
–í–∫–ª—é—á–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å joint training —Å –∫—É–±–æ–º.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Dict, Any, Tuple
import hashlib
import json
from pathlib import Path

from ...utils.logging import get_logger, LogContext
from ...utils.device_manager import get_device_manager
from ...config.simple_config import SimpleProjectConfig

logger = get_logger(__name__)


class EmbeddingTextCache:
    """
    –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞—Ä —ç–º–±–µ–¥–∏–Ω–≥-—Ç–µ–∫—Å—Ç
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    –∏ LRU —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º.
    """
    
    def __init__(self, max_size: int = 10000, similarity_threshold: float = 0.95):
        self.max_size = max_size
        self.similarity_threshold = similarity_threshold
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        self.embedding_to_text = {}  # hash -> text
        self.text_to_embedding = {}  # text -> (hash, embedding)
        self.embeddings_store = {}   # hash -> embedding tensor
        
        # LRU tracking
        self.access_order = []  # –ü–æ—Ä—è–¥–æ–∫ –¥–æ—Å—Ç—É–ø–∞ –∫ —ç–ª–µ–º–µ–Ω—Ç–∞–º
        self.access_count = {}  # –°—á–µ—Ç—á–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        self.logger = get_logger(__name__)
        
    def _hash_embedding(self, embedding: torch.Tensor) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö—ç—à –¥–ª—è —ç–º–±–µ–¥–∏–Ω–≥–∞"""
        # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 4 –∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        rounded = torch.round(embedding * 10000) / 10000
        embedding_bytes = rounded.detach().cpu().numpy().tobytes()
        return hashlib.md5(embedding_bytes).hexdigest()[:16]
    
    def _hash_text(self, text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö—ç—à –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]
    
    def get(self, embedding: torch.Tensor) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —ç–º–±–µ–¥–∏–Ω–≥—É"""
        emb_hash = self._hash_embedding(embedding)
        
        # –ü—Ä—è–º–æ–µ –ø–æ–ø–∞–¥–∞–Ω–∏–µ
        if emb_hash in self.embedding_to_text:
            self._update_access(emb_hash)
            return self.embedding_to_text[emb_hash]
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        best_text = self._find_similar_embedding(embedding)
        return best_text
    
    def _find_similar_embedding(self, embedding: torch.Tensor) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ —ç–º–±–µ–¥–∏–Ω–≥–∞ –≤ –∫—ç—à–µ"""
        if not self.embeddings_store:
            return None
            
        best_similarity = -1
        best_text = None
        
        for cached_hash, cached_embedding in self.embeddings_store.items():
            similarity = F.cosine_similarity(
                embedding.unsqueeze(0), 
                cached_embedding.unsqueeze(0)
            ).item()
            
            if similarity > best_similarity and similarity > self.similarity_threshold:
                best_similarity = similarity
                best_text = self.embedding_to_text.get(cached_hash)
        
        if best_text:
            self.logger.debug(f"Found similar embedding with similarity {best_similarity:.3f}")
            
        return best_text
    
    def put(self, embedding: torch.Tensor, text: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä—ã —ç–º–±–µ–¥–∏–Ω–≥-—Ç–µ–∫—Å—Ç"""
        emb_hash = self._hash_embedding(embedding)
        text_hash = self._hash_text(text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞
        if len(self.embedding_to_text) >= self.max_size:
            self._evict_lru()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self.embedding_to_text[emb_hash] = text
        self.text_to_embedding[text] = (emb_hash, embedding.clone().detach())
        self.embeddings_store[emb_hash] = embedding.clone().detach()
        
        self._update_access(emb_hash)
    
    def _update_access(self, emb_hash: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è LRU"""
        if emb_hash in self.access_order:
            self.access_order.remove(emb_hash)
        self.access_order.append(emb_hash)
        self.access_count[emb_hash] = self.access_count.get(emb_hash, 0) + 1
    
    def _evict_lru(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        if not self.access_order:
            return
            
        lru_hash = self.access_order.pop(0)
        
        # –£–¥–∞–ª—è–µ–º –∏–∑ –≤—Å–µ—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â
        text = self.embedding_to_text.pop(lru_hash, None)
        if text:
            self.text_to_embedding.pop(text, None)
        self.embeddings_store.pop(lru_hash, None)
        self.access_count.pop(lru_hash, None)
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        return {
            'size': len(self.embedding_to_text),
            'max_size': self.max_size,
            'hit_rate': sum(self.access_count.values()) / max(len(self.access_count), 1),
            'similarity_threshold': self.similarity_threshold
        }
    
    def save(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –Ω–∞ –¥–∏—Å–∫"""
        cache_data = {
            'embedding_to_text': self.embedding_to_text,
            'text_to_embedding': {k: (v[0], v[1].tolist()) for k, v in self.text_to_embedding.items()},
            'embeddings_store': {k: v.tolist() for k, v in self.embeddings_store.items()},
            'access_count': self.access_count
        }
        
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'w') as f:
            json.dump(cache_data, f)
        
        self.logger.info(f"Cache saved to {path} ({len(self.embedding_to_text)} items)")
    
    def load(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —Å –¥–∏—Å–∫–∞"""
        if not Path(path).exists():
            return
            
        with open(path, 'r') as f:
            cache_data = json.load(f)
        
        self.embedding_to_text = cache_data['embedding_to_text']
        self.text_to_embedding = {
            k: (v[0], torch.tensor(v[1])) 
            for k, v in cache_data['text_to_embedding'].items()
        }
        self.embeddings_store = {
            k: torch.tensor(v) 
            for k, v in cache_data['embeddings_store'].items()
        }
        self.access_count = cache_data['access_count']
        
        self.logger.info(f"Cache loaded from {path} ({len(self.embedding_to_text)} items)")


class SimpleTextDecoder(nn.Module):
    """
    –ü—Ä–æ—Å—Ç–æ–π –¥–µ–∫–æ–¥–µ—Ä —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, DistilBERT) –¥–ª—è 
    –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç.
    """
    
    def __init__(self, config: SimpleProjectConfig):
        super().__init__()
        self.config = config
        self.logger = get_logger(__name__)
        self.device_manager = get_device_manager()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.teacher_dim = config.embedding.teacher_embedding_dim
        self.decoder_model_name = config.embedding.decoder_model
        self.max_length = config.embedding.max_decode_length
        
        # –ö—ç—à
        self.cache_enabled = config.embedding.decoder_cache_enabled
        if self.cache_enabled:
            cache_path = Path(config.embedding.cache_dir) / "text_decoder_cache.json"
            self.cache = EmbeddingTextCache()
            self.cache.load(str(cache_path))
            self.cache_path = cache_path
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–µ–∫–æ–¥–µ—Ä–∞ –±—É–¥–µ—Ç lazy
        self._decoder_model = None
        self._tokenizer = None
        
        self.logger.info(f"üî§ SimpleTextDecoder initialized (cache: {self.cache_enabled})")
    
    def _init_decoder_model(self):
        """Lazy –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–µ–∫–æ–¥–µ—Ä–∞"""
        if self._decoder_model is not None:
            return
            
        try:
            from transformers import AutoTokenizer, AutoModel
            
            self._tokenizer = AutoTokenizer.from_pretrained(self.decoder_model_name)
            self._decoder_model = AutoModel.from_pretrained(self.decoder_model_name)
            
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self._decoder_model = self.device_manager.transfer_module(self._decoder_model)
            self._decoder_model.eval()  # –î–ª—è inference
            
            self.logger.info(f"Decoder model {self.decoder_model_name} loaded")
            
        except ImportError:
            self.logger.warning("transformers not available, using dummy decoder")
            self._decoder_model = "dummy"
            self._tokenizer = "dummy"
    
    def decode_embeddings(self, embeddings: torch.Tensor, use_cache: bool = True) -> List[str]:
        """
        –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            embeddings: Tensor —Ä–∞–∑–º–µ—Ä–∞ [batch, embedding_dim]
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫—ç—à
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        """
        with LogContext("text_decoding", batch_size=embeddings.size(0)):
            batch_size = embeddings.size(0)
            results = []
            cache_hits = 0
            
            for i in range(batch_size):
                embedding = embeddings[i]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                if use_cache and self.cache_enabled:
                    cached_text = self.cache.get(embedding)
                    if cached_text:
                        results.append(cached_text)
                        cache_hits += 1
                        continue
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
                decoded_text = self._decode_single(embedding)
                results.append(decoded_text)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if use_cache and self.cache_enabled:
                    self.cache.put(embedding, decoded_text)
            
            self.logger.info(f"Decoded {batch_size} embeddings (cache hits: {cache_hits})")
            return results
    
    def _decode_single(self, embedding: torch.Tensor) -> str:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–∏–Ω–≥–∞"""
        self._init_decoder_model()
        
        if self._decoder_model == "dummy":
            # Dummy –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            emb_hash = hashlib.md5(embedding.detach().cpu().numpy().tobytes()).hexdigest()[:8]
            return f"[Generated from embedding {emb_hash}]"
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–∏–∂–∞–π—à–∏–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–∏–Ω–≥–∞–º –º–æ–¥–µ–ª–∏
            # –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø–æ–∑–∂–µ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –∏—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –ø–æ —ç–º–±–µ–¥–∏–Ω–≥—É
            with torch.no_grad():
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
                candidates = [
                    "This is a sample text.",
                    "Hello world example.",
                    "Generated response text.",
                    "Sample decoded message.",
                    "Embedding reconstruction."
                ]
                
                best_text = candidates[0]
                best_similarity = -1
                
                for candidate in candidates:
                    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–∏–Ω–≥ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                    inputs = self._tokenizer(candidate, return_tensors="pt", 
                                           padding=True, truncation=True)
                    inputs = {k: self.device_manager.ensure_device(v) for k, v in inputs.items()}
                    
                    outputs = self._decoder_model(**inputs)
                    candidate_embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                    similarity = F.cosine_similarity(
                        embedding.unsqueeze(0), 
                        candidate_embedding.unsqueeze(0)
                    ).item()
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_text = candidate
                
                return f"{best_text} (sim: {best_similarity:.3f})"
                
        except Exception as e:
            self.logger.warning(f"Decoding error: {e}")
            return f"[Decoding error: {str(e)[:50]}]"
    
    def update_training_pairs(self, embeddings: torch.Tensor, texts: List[str]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ –ø–∞—Ä–∞–º–∏ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not self.cache_enabled:
            return
            
        for i, text in enumerate(texts):
            if i < embeddings.size(0):
                self.cache.put(embeddings[i], text)
        
        self.logger.info(f"Updated cache with {len(texts)} training pairs")
    
    def save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞"""
        if self.cache_enabled and hasattr(self, 'cache_path'):
            self.cache.save(str(self.cache_path))
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        if self.cache_enabled:
            return self.cache.get_stats()
        return {'cache_enabled': False}


class JointTextDecoder(nn.Module):
    """
    –î–µ–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –≤–º–µ—Å—Ç–µ —Å –∫—É–±–æ–º
    
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è joint training —Å –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é.
    """
    
    def __init__(self, config: SimpleProjectConfig):
        super().__init__()
        self.config = config
        self.logger = get_logger(__name__)
        
        # –ë–∞–∑–æ–≤—ã–π –¥–µ–∫–æ–¥–µ—Ä
        self.base_decoder = SimpleTextDecoder(config)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—É—á–∞–µ–º–∞—è —á–∞—Å—Ç—å
        self.embedding_dim = config.embedding.teacher_embedding_dim
        self.vocab_size = 50000  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        
        # –ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è joint learning
        self.projection = nn.Sequential(
            nn.Linear(self.embedding_dim, self.embedding_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.embedding_dim * 2, self.vocab_size)
        )
        
        self.training_mode = True  # –í–∫–ª—é—á–µ–Ω–æ –ª–∏ –æ–±—É—á–µ–Ω–∏–µ –¥–µ–∫–æ–¥–µ—Ä–∞
        
    def forward(self, embeddings: torch.Tensor, target_texts: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Forward pass –¥–ª—è joint training
        
        Args:
            embeddings: –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
            target_texts: –¶–µ–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ loss
        """
        results = {
            'decoded_texts': [],
            'logits': None,
            'loss': None
        }
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if not self.training:
            # –í —Ä–µ–∂–∏–º–µ inference –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –¥–µ–∫–æ–¥–µ—Ä
            results['decoded_texts'] = self.base_decoder.decode_embeddings(embeddings)
        else:
            # –í —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–∞–µ–º—É—é —á–∞—Å—Ç—å
            logits = self.projection(embeddings)
            results['logits'] = logits
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ü–µ–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã, –≤—ã—á–∏—Å–ª—è–µ–º loss
            if target_texts and self.training_mode:
                # –ó–¥–µ—Å—å –Ω—É–∂–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
                # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–∫–∞
                target_tokens = self._tokenize_texts(target_texts)
                if target_tokens is not None:
                    loss = F.cross_entropy(
                        logits.view(-1, self.vocab_size),
                        target_tokens.view(-1),
                        ignore_index=-100
                    )
                    results['loss'] = loss
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            results['decoded_texts'] = self._decode_from_logits(logits)
        
        return results
    
    def _tokenize_texts(self, texts: List[str]) -> Optional[torch.Tensor]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        return None
    
    def _decode_from_logits(self, logits: torch.Tensor) -> List[str]:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ logits"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ - –±–µ—Ä–µ–º —Ç–æ–ø —Ç–æ–∫–µ–Ω—ã
        top_tokens = torch.argmax(logits, dim=-1)
        
        results = []
        for i in range(logits.size(0)):
            # –ó–∞–≥–ª—É—à–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            results.append(f"[Joint decoded: tokens {top_tokens[i][:5].tolist()}]")
        
        return results
    
    def set_training_mode(self, enabled: bool):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞"""
        self.training_mode = enabled
        self.logger.info(f"Decoder training mode: {enabled}")


def create_text_decoder(config: SimpleProjectConfig, joint_training: bool = False) -> nn.Module:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞"""
    if joint_training:
        return JointTextDecoder(config)
    else:
        return SimpleTextDecoder(config)