#!/usr/bin/env python3
"""
GPU Enhanced CNF - –£–ª—É—á—à–µ–Ω–Ω—ã–π CNF —Å GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
===================================================================

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è LightweightCNF —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π GPU Optimized Euler Solver:
- Batch processing –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö connections
- Vectorized Neural ODE operations
- Adaptive Lipschitz-based integration
- Memory-efficient batch operations
- Real-time performance monitoring

–ö–õ–Æ–ß–ï–í–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:
1. Batch CNF processing - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
2. GPU-accelerated Neural ODE —Å vectorized operations
3. Adaptive step size –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π Lipschitz –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
4. Memory pooling –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏
5. Enhanced stability analysis –∏ error control

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: 2.0.0 (2024-12-27)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple, Any, List, Union
from enum import Enum
import time

try:
    from ...config import get_project_config
    from ...utils.logging import get_logger, log_cell_init, log_cell_forward
    from ...utils.device_manager import get_device_manager
    from .gpu_optimized_euler_solver import (
        GPUOptimizedEulerSolver,
        AdaptiveMethod,
        create_gpu_optimized_euler_solver,
    )

    # from .lightweight_cnf import ConnectionType  # –ò–º–ø–æ—Ä—Ç –∏–∑ legacy –≤–µ—Ä—Å–∏–∏
except ImportError:
    # Fallback –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    import sys
    import os

    sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
    from config import get_project_config
    from utils.logging import get_logger, log_cell_init, log_cell_forward
    from utils.device_manager import get_device_manager

logger = get_logger(__name__)


class BatchProcessingMode(Enum):
    """–†–µ–∂–∏–º—ã batch processing –¥–ª—è CNF"""

    SINGLE = "single"  # –û–¥–Ω–∞ —Å–≤—è–∑—å –∑–∞ —Ä–∞–∑ (legacy)
    CONNECTION_BATCH = "batch"  # Batch –ø–æ —Å–≤—è–∑—è–º
    ADAPTIVE_BATCH = "adaptive"  # Adaptive batch size –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏


class ConnectionType(Enum):
    """–¢–∏–ø—ã —Å–≤—è–∑–µ–π –¥–ª—è CNF –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

    FUNCTIONAL = "functional"  # 60% —Å–≤—è–∑–µ–π - —Å—Ä–µ–¥–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    DISTANT = "distant"  # 30% —Å–≤—è–∑–µ–π - –¥–∞–ª—å–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è


class VectorizedNeuralODE(nn.Module):
    """
    Vectorized Neural ODE –¥–ª—è GPU-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç batch processing –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö connections
    —Å shared parameters –Ω–æ —Ä–∞–∑–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    """

    def __init__(
        self,
        state_size: int,
        connection_type: ConnectionType,
        hidden_dim: Optional[int] = None,
        batch_size: int = 100,
    ):
        super().__init__()

        self.state_size = state_size
        self.connection_type = connection_type
        self.hidden_dim = hidden_dim or max(16, state_size // 2)
        self.max_batch_size = batch_size

        # Device management
        self.device_manager = get_device_manager()
        self.device = self.device_manager.get_device()

        # –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ + –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ—Å–µ–¥–∏
        input_size = state_size * 2

        # –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –Ω–æ –º–æ—â–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è vectorized operations
        self.ode_network = nn.Sequential(
            nn.Linear(input_size, self.hidden_dim, bias=True),
            nn.GELU(),
            nn.Dropout(0.1),  # –ù–µ–±–æ–ª—å—à–∞—è regularization
            nn.Linear(self.hidden_dim, self.hidden_dim // 2, bias=True),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 2, state_size, bias=True),
        )

        # Learnable damping –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        self.damping_strength = nn.Parameter(torch.tensor(0.1))

        # Time embedding –¥–ª—è time-dependent dynamics
        self.time_embedding = nn.Linear(1, self.hidden_dim // 4, bias=False)

        # Normalization layers
        self.input_norm = nn.LayerNorm(input_size)
        self.output_norm = nn.LayerNorm(state_size)

        total_params = sum(p.numel() for p in self.parameters())
        log_cell_init(
            cell_type="VectorizedNeuralODE",
            total_params=total_params,
            target_params=3000,  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è GPU efficiency
            state_size=state_size,
            connection_type=connection_type.value,
            hidden_dim=self.hidden_dim,
            input_size=input_size,
            max_batch_size=batch_size,
        )
        self.to(self.device)

    def forward(
        self,
        t: torch.Tensor,
        current_states: torch.Tensor,
        neighbor_influences: torch.Tensor,
    ) -> torch.Tensor:
        """
        Vectorized –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π dx/dt –¥–ª—è batch connections

        Args:
            t: –≤—Ä–µ–º—è [batch] –∏–ª–∏ scalar
            current_states: [batch, state_size] - —Ç–µ–∫—É—â–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            neighbor_influences: [batch, state_size] - –≤–ª–∏—è–Ω–∏–µ —Å–æ—Å–µ–¥–µ–π

        Returns:
            derivatives: [batch, state_size] - dx/dt –¥–ª—è –∫–∞–∂–¥–æ–π —Å–≤—è–∑–∏
        """
        batch_size = current_states.shape[0]

        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        current_states = self.device_manager.ensure_device(current_states)
        neighbor_influences = self.device_manager.ensure_device(neighbor_influences)

        if not torch.is_tensor(t):
            t = torch.tensor(t, device=self.device, dtype=current_states.dtype)
        elif t.device != self.device:
            t = t.to(self.device)

        # Ensure t has batch dimension
        if t.dim() == 0:
            t = t.expand(batch_size)
        elif t.shape[0] != batch_size:
            t = t.expand(batch_size)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Ö–æ–¥—ã
        combined_input = torch.cat([current_states, neighbor_influences], dim=-1)

        # Input normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        normalized_input = self.input_norm(combined_input)

        # Time embedding
        time_features = self.time_embedding(t.unsqueeze(-1))  # [batch, time_features]

        # –û—Å–Ω–æ–≤–Ω–∞—è ODE network
        ode_output = self.ode_network(normalized_input)

        # –î–æ–±–∞–≤–ª—è–µ–º time features —á–µ—Ä–µ–∑ residual connection
        if time_features.shape[-1] == ode_output.shape[-1]:
            ode_output = ode_output + time_features
        elif time_features.shape[-1] < ode_output.shape[-1]:
            # Pad time features
            padding = ode_output.shape[-1] - time_features.shape[-1]
            time_features_padded = F.pad(time_features, (0, padding))
            ode_output = ode_output + time_features_padded

        # Output normalization
        normalized_output = self.output_norm(ode_output)

        # Damping term –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏: -Œª * x
        damping_term = -self.damping_strength.abs() * current_states

        # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
        derivatives = normalized_output + damping_term

        return derivatives


class GPUEnhancedCNF(nn.Module):
    """
    GPU Enhanced CNF - –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è LightweightCNF

    –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
    - Batch processing –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö connections
    - GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Lipschitz-based adaptive stepping
    - Vectorized Neural ODE operations
    - Memory-efficient batch operations
    - Real-time performance monitoring
    """

    def __init__(
        self,
        state_size: int,
        connection_type: ConnectionType = ConnectionType.DISTANT,
        integration_steps: int = 3,
        batch_processing_mode: BatchProcessingMode = BatchProcessingMode.ADAPTIVE_BATCH,
        max_batch_size: int = 100,
        adaptive_method: AdaptiveMethod = AdaptiveMethod.LIPSCHITZ_BASED,
    ):
        super().__init__()

        self.state_size = state_size
        self.connection_type = connection_type
        self.integration_steps = integration_steps
        self.batch_processing_mode = batch_processing_mode
        self.max_batch_size = max_batch_size

        # Device management
        self.device_manager = get_device_manager()
        self.device = self.device_manager.get_device()

        # Vectorized Neural ODE
        self.neural_ode = VectorizedNeuralODE(
            state_size=state_size,
            connection_type=connection_type,
            batch_size=max_batch_size,
        )

        # GPU Optimized Euler Solver
        euler_cfg = get_project_config().euler
        self.solver = GPUOptimizedEulerSolver(config=euler_cfg)

        # Performance tracking
        self.performance_stats = {
            "total_forward_passes": 0,
            "total_connections_processed": 0,
            "avg_batch_size": 0.0,
            "avg_processing_time_ms": 0.0,
            "batch_efficiency": 0.0,
            "gpu_memory_usage_mb": 0.0,
        }

        total_params = sum(p.numel() for p in self.parameters())

        log_cell_init(
            cell_type="GPUEnhancedCNF",
            total_params=total_params,
            target_params=get_project_config().cnf.target_params_per_connection,
            state_size=state_size,
            connection_type=(
                connection_type.value
                if isinstance(connection_type, Enum)
                else connection_type
            ),
            integration_steps=integration_steps,
            batch_mode=(
                batch_processing_mode.value
                if isinstance(batch_processing_mode, Enum)
                else batch_processing_mode
            ),
            max_batch_size=max_batch_size,
            adaptive_method=(
                adaptive_method.value
                if isinstance(adaptive_method, Enum)
                else adaptive_method
            ),
        )

    def _create_derivative_function(self, neighbor_influences: torch.Tensor):
        """–°–æ–∑–¥–∞–µ—Ç derivative function –¥–ª—è solver'–∞"""

        def derivative_fn(
            t: torch.Tensor, states: torch.Tensor, *args, **kwargs
        ) -> torch.Tensor:
            return self.neural_ode(t, states, neighbor_influences)

        return derivative_fn

    def _process_single_connection(
        self, current_state: torch.Tensor, neighbor_states: torch.Tensor
    ) -> torch.Tensor:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å–≤—è–∑–∏ (–æ–¥–∏–Ω-–∫–æ-–º–Ω–æ–≥–∏–º)"""
        start_time = time.time()

        # 1. –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤–ª–∏—è–Ω–∏–µ —Å–æ—Å–µ–¥–µ–π
        if neighbor_states.dim() == 3:
            # Batched input: [batch, num_neighbors, state_size] -> [batch, state_size]
            aggregated_influence = torch.mean(neighbor_states, dim=1)
        else:
            # Non-batched: [num_neighbors, state_size] -> [state_size]
            aggregated_influence = torch.mean(neighbor_states, dim=0)

        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä–∞
        if current_state.dim() == 1:
            initial_state = current_state.unsqueeze(0)  # [1, state_size]
        else:
            initial_state = current_state  # Already [batch, state_size]

        if aggregated_influence.dim() == 1:
            aggregated_influence = aggregated_influence.unsqueeze(0)  # Ensure batch dim

        # 3. –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π —Å –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–ª–∏—è–Ω–∏–µ–º —Å–æ—Å–µ–¥–µ–π
        derivative_fn = self._create_derivative_function(
            aggregated_influence  # Shape: [batch, state_size]
        )

        # 4. –í—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
        result = self.solver.batch_integrate(
            derivative_fn,
            initial_state,
            t_span=(0.0, 1.0),
            num_steps=self.integration_steps,
            return_trajectory=True,
            adaptive_method=self.solver.config.adaptive_method,
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        final_state = result.trajectory[-1]

        processing_time_ms = (time.time() - start_time) * 1000
        logger.debug(
            f"Processed single connection in {processing_time_ms:.2f}ms. "
            f"Input shape: {current_state.shape}, Neighbor shape: {neighbor_states.shape}"
        )

        return final_state

    def _process_connection_batch(
        self, current_states: torch.Tensor, neighbor_states_list: List[torch.Tensor]
    ) -> torch.Tensor:
        """Batch processing –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö connections"""
        batch_size = len(neighbor_states_list)

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º neighbor influences –¥–ª—è –≤—Å–µ–≥–æ batch'–∞
        neighbor_influences = []
        for neighbor_states in neighbor_states_list:
            if neighbor_states.shape[0] > 0:
                neighbor_influence = neighbor_states.mean(dim=0)
            else:
                neighbor_influence = torch.zeros(self.state_size, device=self.device)
            neighbor_influences.append(neighbor_influence)

        neighbor_influences = torch.stack(neighbor_influences)  # [batch, state_size]

        # –°–æ–∑–¥–∞–µ–º derivative function –¥–ª—è batch
        derivative_fn = self._create_derivative_function(neighbor_influences)

        # Batch –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        result = self.solver.batch_integrate(
            derivative_fn,
            current_states,
            t_span=(0.0, 1.0),
            num_steps=self.integration_steps,
        )

        return result

    def _determine_optimal_batch_size(self, total_connections: int) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä batch'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        if self.batch_processing_mode == BatchProcessingMode.SINGLE:
            return 1
        elif self.batch_processing_mode == BatchProcessingMode.CONNECTION_BATCH:
            return min(total_connections, self.max_batch_size)
        else:  # ADAPTIVE_BATCH
            # –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
            device_stats = self.device_manager.get_memory_stats()
            available_mb = device_stats.get("available_mb", 1000)

            # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ memory per connection
            memory_per_connection = (
                self.state_size * 4 * 10 / (1024**2)
            )  # –æ—á–µ–Ω—å –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞

            max_affordable = int(
                available_mb * 0.5 / memory_per_connection
            )  # 50% –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
            optimal_batch_size = min(
                total_connections, self.max_batch_size, max_affordable
            )

            return max(1, optimal_batch_size)

    def forward(
        self,
        current_state: torch.Tensor,
        neighbor_states: torch.Tensor,
        cell_idx: Optional[int] = None,
        neighbor_indices: Optional[List[int]] = None,
        **kwargs,
    ) -> Dict[str, torch.Tensor]:
        """
        Enhanced forward pass —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö batch modes

        Args:
            current_state: [1, state_size] –∏–ª–∏ [batch, state_size] - —Ç–µ–∫—É—â–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            neighbor_states: [num_neighbors, state_size] –∏–ª–∏ List[Tensor] –¥–ª—è batch mode
            cell_idx: –∏–Ω–¥–µ–∫—Å –∫–ª–µ—Ç–∫–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            neighbor_indices: –∏–Ω–¥–µ–∫—Å—ã —Å–æ—Å–µ–¥–µ–π (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)

        Returns:
            dict —Å new_state –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        start_time = time.time()

        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ inputs –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        current_state = self.device_manager.ensure_device(current_state)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if isinstance(neighbor_states, list) or (
            isinstance(neighbor_states, torch.Tensor) and neighbor_states.dim() == 3
        ):

            if isinstance(neighbor_states, torch.Tensor):
                # –î–ª—è single-mode, –Ω–æ —Å batched-like tensor
                batch_size = neighbor_states.shape[0]
            else:
                # –î–ª—è batch mode
                batch_size = len(neighbor_states)

            if current_state.dim() == 1:
                current_state = current_state.unsqueeze(0)

            if current_state.shape[0] == 1 and batch_size > 1:
                current_state = current_state.expand(batch_size, -1)

            optimal_batch_size = self._determine_optimal_batch_size(batch_size)

            if optimal_batch_size >= batch_size:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–µ—Å—å batch —Å—Ä–∞–∑—É
                integration_result = self._process_connection_batch(
                    current_state, neighbor_states
                )
                new_states = integration_result.final_state

            else:
                # Chunked processing
                new_states = []
                for i in range(0, batch_size, optimal_batch_size):
                    end_idx = min(i + optimal_batch_size, batch_size)
                    batch_current = current_state[i:end_idx]
                    batch_neighbors = (
                        neighbor_states[i:end_idx]
                        if isinstance(neighbor_states, list)
                        else neighbor_states[:, i:end_idx]
                    )

                    batch_result = self._process_connection_batch(
                        batch_current, batch_neighbors
                    )
                    new_states.append(batch_result.final_state)

                new_states = torch.cat(new_states, dim=0)

            # –î–ª—è batch mode –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å batch
            result_state = new_states

        else:
            # Single connection mode (legacy compatibility)
            neighbor_states = self.device_manager.ensure_device(neighbor_states)
            result_state = self._process_single_connection(
                current_state, neighbor_states
            )

        processing_time_ms = (time.time() - start_time) * 1000

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._update_performance_stats(processing_time_ms, current_state.shape[0])

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if hasattr(self, "_log_forward_passes") and self._log_forward_passes:
            log_cell_forward(
                cell_type="GPUEnhancedCNF",
                input_shape=current_state.shape,
                output_shape=result_state.shape,
                processing_time_ms=processing_time_ms,
                connection_type=self.connection_type.value,
                batch_size=current_state.shape[0],
            )

        return {
            "new_state": result_state,
            "processing_time_ms": processing_time_ms,
            "connection_type": self.connection_type.value,
            "batch_size": current_state.shape[0],
        }

    def _update_performance_stats(self, processing_time_ms: float, batch_size: int):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.performance_stats["total_forward_passes"] += 1
        self.performance_stats["total_connections_processed"] += batch_size

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        total_passes = self.performance_stats["total_forward_passes"]
        old_avg_time = self.performance_stats["avg_processing_time_ms"]
        new_avg_time = (
            old_avg_time * (total_passes - 1) + processing_time_ms
        ) / total_passes
        self.performance_stats["avg_processing_time_ms"] = new_avg_time

        # –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä batch'–∞
        total_connections = self.performance_stats["total_connections_processed"]
        self.performance_stats["avg_batch_size"] = total_connections / total_passes

        # Batch efficiency (connections per second)
        self.performance_stats["batch_efficiency"] = batch_size / (
            processing_time_ms / 1000
        )

        # GPU memory usage
        device_stats = self.device_manager.get_memory_stats()
        self.performance_stats["gpu_memory_usage_mb"] = device_stats.get(
            "allocated_mb", 0
        )

    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        solver_stats = self.solver.get_comprehensive_stats()

        return {
            "cnf_config": {
                "state_size": self.state_size,
                "connection_type": self.connection_type.value,
                "integration_steps": self.integration_steps,
                "batch_mode": self.batch_processing_mode.value,
                "max_batch_size": self.max_batch_size,
            },
            "cnf_performance": self.performance_stats.copy(),
            "solver_stats": solver_stats,
            "neural_ode": {
                "total_params": sum(p.numel() for p in self.neural_ode.parameters()),
                "hidden_dim": self.neural_ode.hidden_dim,
                "max_batch_size": self.neural_ode.max_batch_size,
            },
        }

    def optimize_performance(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU Enhanced CNF")

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º solver
        self.solver.optimize_performance()

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.performance_stats["gpu_memory_usage_mb"] = 0.0

        logger.info("‚úÖ CNF –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def cleanup(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        logger.info("üõë Cleanup GPU Enhanced CNF")
        self.solver.cleanup()


# === FACTORY FUNCTIONS ===


def create_gpu_enhanced_cnf(
    state_size: int,
    connection_type: ConnectionType = ConnectionType.DISTANT,
    batch_processing_mode: BatchProcessingMode = BatchProcessingMode.ADAPTIVE_BATCH,
    max_batch_size: int = 100,
    adaptive_method: AdaptiveMethod = AdaptiveMethod.LIPSCHITZ_BASED,
) -> GPUEnhancedCNF:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è GPU Enhanced CNF

    Args:
        state_size: —Ä–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è
        connection_type: —Ç–∏–ø —Å–≤—è–∑–µ–π
        batch_processing_mode: —Ä–µ–∂–∏–º batch processing
        max_batch_size: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä batch'–∞
        adaptive_method: –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ solver'–∞

    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π GPUEnhancedCNF
    """
    config = get_project_config()

    return GPUEnhancedCNF(
        state_size=state_size,
        connection_type=connection_type,
        integration_steps=config.cnf_integration_steps,
        batch_processing_mode=batch_processing_mode,
        max_batch_size=max_batch_size,
        adaptive_method=adaptive_method,
    )


def benchmark_cnf_performance(
    state_sizes: List[int] = [16, 32, 64],
    batch_sizes: List[int] = [1, 10, 50, 100],
    num_trials: int = 5,
) -> Dict[str, Any]:
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GPU Enhanced CNF

    Returns:
        –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    results = {}

    for state_size in state_sizes:
        for batch_size in batch_sizes:
            logger.info(
                f"üß™ –ë–µ–Ω—á–º–∞—Ä–∫ CNF: state_size={state_size}, batch_size={batch_size}"
            )

            trial_results = []

            for trial in range(num_trials):
                # –°–æ–∑–¥–∞–µ–º CNF
                cnf = create_gpu_enhanced_cnf(
                    state_size=state_size,
                    batch_processing_mode=BatchProcessingMode.CONNECTION_BATCH,
                    max_batch_size=max(100, batch_size),
                )

                # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                current_states = torch.randn(batch_size, state_size)
                neighbor_states_list = [
                    torch.randn(torch.randint(5, 20, (1,)).item(), state_size)
                    for _ in range(batch_size)
                ]

                # –í—ã–ø–æ–ª–Ω—è–µ–º forward pass
                start_time = time.time()
                result = cnf(current_states, neighbor_states_list)
                wall_time = time.time() - start_time

                trial_results.append(
                    {
                        "wall_time_s": wall_time,
                        "processing_time_ms": result["processing_time_ms"],
                        "batch_size": result["batch_size"],
                    }
                )

                cnf.cleanup()

            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            avg_wall_time = sum(r["wall_time_s"] for r in trial_results) / num_trials
            avg_processing_time = (
                sum(r["processing_time_ms"] for r in trial_results) / num_trials
            )

            key = f"state_{state_size}_batch_{batch_size}"
            results[key] = {
                "state_size": state_size,
                "batch_size": batch_size,
                "avg_wall_time_s": avg_wall_time,
                "avg_processing_time_ms": avg_processing_time,
                "throughput_connections_per_second": batch_size / avg_wall_time,
            }

    return results
