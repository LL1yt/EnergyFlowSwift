#!/usr/bin/env python3
"""
Parallel Spatial Processor - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
============================================================================

ParallelSpatialProcessor –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""

import torch
import threading
import time
from typing import Dict, List, Optional, Callable, Any
from concurrent.futures import ThreadPoolExecutor, Future
from ....config.project_config import ChunkInfo
from ....config.project_config import get_project_config
from .chunker import LatticeChunker
from .hierarchical_index import HierarchicalSpatialIndex
from .memory_manager import MemoryPoolManager
from ....utils.logging import get_logger

logger = get_logger(__name__)


class ParallelSpatialProcessor:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç chunking, memory management –∏ parallel processing
    –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ä–µ—à–µ—Ç–∫–∞—Ö.
    """

    def __init__(
        self,
        chunker: LatticeChunker,
        spatial_index: HierarchicalSpatialIndex,
        memory_manager: MemoryPoolManager,
        config: dict = None,
    ):
        self.chunker = chunker
        self.spatial_index = spatial_index
        self.memory_manager = memory_manager
        self.config = config or get_project_config().get_spatial_optim_config()

        # Thread pool –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        self._init_thread_pool()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_stats = {
            "total_batches_processed": 0,
            "avg_batch_time_ms": 0.0,
            "parallel_efficiency": 1.0,
            "memory_efficiency": 1.0,
        }

        # –õ–æ–∫ –¥–ª—è thread-safe –æ–ø–µ—Ä–∞—Ü–∏–π
        self._stats_lock = threading.Lock()

        logger.info(
            f"‚öôÔ∏è ParallelSpatialProcessor –≥–æ—Ç–æ–≤ —Å {self.config['num_worker_threads']} –ø–æ—Ç–æ–∫–∞–º–∏"
        )

    def _init_thread_pool(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç thread pool –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if self.config["enable_async_processing"]:
            self.thread_pool = ThreadPoolExecutor(
                max_workers=self.config["num_worker_threads"],
                thread_name_prefix="SpatialProcessor",
            )

    def process_lattice_parallel(
        self, states: torch.Tensor, neighbor_processor_fn: Callable
    ) -> torch.Tensor:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–π —Ä–µ—à–µ—Ç–∫–∏

        Args:
            states: [num_cells, state_size] - —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–ª–µ—Ç–æ–∫
            neighbor_processor_fn: —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Å–µ–¥–µ–π

        Returns:
            new_states: [num_cells, state_size] - –Ω–æ–≤—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        """
        start_time = time.time()
        num_cells = states.shape[0]

        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ {num_cells:,} –∫–ª–µ—Ç–æ–∫")

        # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–æ–≤
        processing_schedule = self.chunker.get_processing_schedule()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        output_states = states.clone()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch'–∏ chunk'–æ–≤
        total_batches = len(processing_schedule)
        processed_batches = 0

        for batch_idx, chunk_ids in enumerate(processing_schedule):
            logger.debug(
                f"   üîÑ Batch {batch_idx + 1}/{total_batches}: chunk'—ã {chunk_ids}"
            )

            if self.config["enable_async_processing"] and len(chunk_ids) > 1:
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö chunk'–æ–≤
                batch_results = self._process_batch_async(
                    chunk_ids, states, neighbor_processor_fn
                )
            else:
                # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö chunk'–æ–≤
                batch_results = self._process_batch_sync(
                    chunk_ids, states, neighbor_processor_fn
                )

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã batch'–∞
            for chunk_id, chunk_output in batch_results.items():
                self._merge_chunk_output(output_states, chunk_id, chunk_output)

            processed_batches += 1

            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if processed_batches % 3 == 0:
                self.memory_manager.garbage_collect()

        processing_time = time.time() - start_time
        self._update_processing_stats(total_batches, processing_time)

        logger.info(f"‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.3f}s")

        return output_states

    def _process_batch_async(
        self,
        chunk_ids: List[int],
        states: torch.Tensor,
        neighbor_processor_fn: Callable,
    ) -> Dict[int, torch.Tensor]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ batch'–∞ chunk'–æ–≤"""
        futures: Dict[int, Future] = {}

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∂–¥–æ–≥–æ chunk'–∞
        for chunk_id in chunk_ids:
            future = self.thread_pool.submit(
                self._process_chunk_async, chunk_id, states, neighbor_processor_fn
            )
            futures[chunk_id] = future

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {}
        for chunk_id, future in futures.items():
            try:
                results[chunk_id] = future.result(timeout=60.0)  # 60 —Å–µ–∫—É–Ω–¥ timeout
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ async –æ–±—Ä–∞–±–æ—Ç–∫–µ chunk {chunk_id}: {e}")
                # Fallback –∫ sync –æ–±—Ä–∞–±–æ—Ç–∫–µ
                results[chunk_id] = self._process_chunk_sync(
                    chunk_id, states, neighbor_processor_fn
                )

        return results

    def _process_batch_sync(
        self,
        chunk_ids: List[int],
        states: torch.Tensor,
        neighbor_processor_fn: Callable,
    ) -> Dict[int, torch.Tensor]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ batch'–∞ chunk'–æ–≤"""
        results = {}

        for chunk_id in chunk_ids:
            results[chunk_id] = self._process_chunk_sync(
                chunk_id, states, neighbor_processor_fn
            )

        return results

    def _process_chunk_async(
        self, chunk_id: int, states: torch.Tensor, neighbor_processor_fn: Callable
    ) -> torch.Tensor:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ chunk'–∞"""
        return self._process_chunk_sync(chunk_id, states, neighbor_processor_fn)

    def _process_chunk_sync(
        self, chunk_id: int, states: torch.Tensor, neighbor_processor_fn: Callable
    ) -> torch.Tensor:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ chunk'–∞"""
        chunk = self.chunker.chunks[chunk_id]
        chunk_cells = chunk.cell_indices

        # –ü–æ–ª—É—á–∞–µ–º tensor –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π chunk'–∞
        chunk_size = len(chunk_cells)
        state_size = states.shape[1]
        chunk_output = self.memory_manager.get_tensor((chunk_size, state_size))

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –∫–ª–µ—Ç–∫—É –≤ chunk'–µ
        for i, cell_idx in enumerate(chunk_cells):
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Å–µ–¥–µ–π –¥–ª—è –∫–ª–µ—Ç–∫–∏
            chunk_neighbors = self._get_chunk_neighbors(chunk, states)

            if chunk_neighbors.shape[0] > 0:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Å–µ–¥–µ–π
                new_state = neighbor_processor_fn(
                    states[cell_idx], chunk_neighbors, cell_idx, chunk_cells
                )
                chunk_output[i] = new_state
            else:
                # –ï—Å–ª–∏ —Å–æ—Å–µ–¥–µ–π –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                chunk_output[i] = states[cell_idx]

        return chunk_output

    def _get_chunk_neighbors(
        self, chunk: ChunkInfo, states: torch.Tensor
    ) -> torch.Tensor:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ—Å–µ–¥–µ–π –¥–ª—è chunk'–∞ –∏–∑ spatial index"""
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–µ—Ç–æ–∫ –∫–∞–∫ —Å–æ—Å–µ–¥–µ–π
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π spatial query
        max_neighbors = min(10, len(chunk.cell_indices))
        if max_neighbors > 0:
            neighbor_indices = chunk.cell_indices[:max_neighbors]
            return states[neighbor_indices]
        else:
            return torch.empty(0, states.shape[1], device=states.device)

    def _merge_chunk_output(
        self, output_states: torch.Tensor, chunk_id: int, chunk_output: torch.Tensor
    ):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã chunk'–∞ —Å –æ–±—â–∏–º–∏ –≤—ã—Ö–æ–¥–Ω—ã–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏"""
        chunk = self.chunker.chunks[chunk_id]
        chunk_cells = chunk.cell_indices

        # –ö–æ–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã chunk'–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π tensor
        for i, cell_idx in enumerate(chunk_cells):
            if i < chunk_output.shape[0]:
                output_states[cell_idx] = chunk_output[i]

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º chunk_output –≤ memory pool
        self.memory_manager.return_tensor(chunk_output)

    def _update_processing_stats(self, num_batches: int, total_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        with self._stats_lock:
            self.performance_stats["total_batches_processed"] += num_batches

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è batch'–∞
            avg_batch_time_ms = (total_time * 1000) / max(num_batches, 1)
            current_avg = self.performance_stats["avg_batch_time_ms"]
            self.performance_stats["avg_batch_time_ms"] = (
                current_avg * 0.8 + avg_batch_time_ms * 0.2
            )

            # –í—ã—á–∏—Å–ª—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
            expected_time = avg_batch_time_ms * num_batches
            actual_time = total_time * 1000
            if actual_time > 0:
                parallel_efficiency = min(1.0, expected_time / actual_time)
                self.performance_stats["parallel_efficiency"] = (
                    self.performance_stats["parallel_efficiency"] * 0.9
                    + parallel_efficiency * 0.1
                )

    def get_performance_stats(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        with self._stats_lock:
            stats = self.performance_stats.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É memory manager'–∞
        memory_stats = self.memory_manager.get_memory_stats()
        stats.update({f"memory_{key}": value for key, value in memory_stats.items()})

        return stats

    def shutdown(self):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –∏ –æ—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
            self.thread_pool = None

        self.memory_manager.cleanup()

        logger.info("üîí ParallelSpatialProcessor –∑–∞–≤–µ—Ä—à–µ–Ω")
