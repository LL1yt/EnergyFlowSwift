#!/usr/bin/env python3
"""
Unified Spatial Optimizer - –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
==============================================================================

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å SpatialOptimizer –∏ MoESpatialOptimizer –≤ –µ–¥–∏–Ω—É—é
–≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å –ø–æ–ª–Ω–æ–π GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π.

–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ï–¥–∏–Ω—ã–π API –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- –ü–æ–ª–Ω–∞—è GPU-acceleration —Å fallback –Ω–∞ CPU
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- GPUMortonEncoder –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ spatial indexing
- Adaptive chunking –∏ memory management
- Real-time performance monitoring

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: 3.0.0 (2024-12-27)
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Any, Tuple, Union, Set, Callable
import time
import numpy as np
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

from ....config import get_project_config
from ....utils.logging import get_logger
from ....utils.device_manager import get_device_manager
from ..position import Position3D
from .gpu_spatial_processor import GPUSpatialProcessor
from .adaptive_chunker import AdaptiveGPUChunker
from ..gpu_spatial_hashing import AdaptiveGPUSpatialHash, GPUMortonEncoder

logger = get_logger(__name__)

Coordinates3D = Tuple[int, int, int]


class OptimizationMode(Enum):
    """–†–µ–∂–∏–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""

    GPU_ONLY = "gpu_only"


class ConnectionType(Enum):
    """–¢–∏–ø—ã —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""

    LOCAL = "local"
    FUNCTIONAL = "functional"
    DISTANT = "distant"


@dataclass
class OptimizationConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è UnifiedSpatialOptimizer"""

    enable_moe: bool = True
    enable_morton_encoding: bool = True
    enable_adaptive_chunking: bool = True
    max_memory_gb: float = 8.0
    target_performance_ms: float = 10.0


@dataclass
class SpatialOptimizationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""

    new_states: torch.Tensor
    processing_time_ms: float
    memory_usage_mb: float
    neighbors_found: int
    gpu_utilization: float
    mode_used: OptimizationMode
    cache_hit_rate: float = 0.0
    chunks_processed: int = 0


class BaseSpatialProcessor(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö spatial processors"""

    @abstractmethod
    def find_neighbors(
        self, coords: Union[Coordinates3D, torch.Tensor], radius: float
    ) -> List[int]:
        """–ù–∞–π—Ç–∏ —Å–æ—Å–µ–¥–µ–π –≤ —Ä–∞–¥–∏—É—Å–µ"""
        pass

    @abstractmethod
    def process_lattice(
        self, states: torch.Tensor, processor_fn: Callable
    ) -> torch.Tensor:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ—à–µ—Ç–∫—É"""
        pass

    @abstractmethod
    def get_performance_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        pass


class GPUSpatialProcessorWrapper(BaseSpatialProcessor):
    """Wrapper –¥–ª—è GPU Spatial Processor"""

    def __init__(self, dimensions: Coordinates3D, config: dict):
        self.dimensions = dimensions
        self.config = config
        self.device_manager = get_device_manager()
        self.device = self.device_manager.get_device()

        # –°–æ–∑–¥–∞–µ–º GPU –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.gpu_processor = GPUSpatialProcessor(dimensions, config)
        self.pos_helper = Position3D(dimensions)

        # GPU-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.morton_encoder = GPUMortonEncoder(dimensions)
        self.adaptive_hash = AdaptiveGPUSpatialHash(
            dimensions, config.get("memory_pool_size_gb", 8.0) * 1024 * 0.6
        )

        logger.info(f"üöÄ GPU Spatial Processor –≥–æ—Ç–æ–≤ –Ω–∞ {self.device}")

    def find_neighbors(
        self, coords: Union[Coordinates3D, torch.Tensor], radius: float
    ) -> List[int]:
        """GPU-accelerated –ø–æ–∏—Å–∫ —Å–æ—Å–µ–¥–µ–π"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ tensor
        if isinstance(coords, (tuple, list)):
            coords_tensor = torch.tensor(
                [list(coords)], dtype=torch.float32, device=self.device
            )
        else:
            coords_tensor = self.device_manager.ensure_device(coords)
            if coords_tensor.dim() == 1:
                coords_tensor = coords_tensor.unsqueeze(0)

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ GPU processor
            result = self.gpu_processor.query_neighbors_sync(
                coords_tensor, radius, timeout=10.0
            )

            if result and result.neighbor_lists:
                neighbors = result.neighbor_lists[0].cpu().tolist()
                return neighbors
            else:
                return []

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è GPU –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è: {e}, fallback –Ω–∞ adaptive hash")

            # Fallback –Ω–∞ adaptive hash
            try:
                neighbor_lists = self.adaptive_hash.query_radius_batch(
                    coords_tensor, radius
                )
                if neighbor_lists:
                    return neighbor_lists[0].cpu().tolist()
                return []
            except Exception as e2:
                logger.error(f"‚ùå Adaptive hash —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª—Å—è: {e2}")
                return []

    def process_lattice(
        self, states: torch.Tensor, processor_fn: Callable
    ) -> torch.Tensor:
        """GPU –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—à–µ—Ç–∫–∏ —Å chunking"""
        states = self.device_manager.ensure_device(states)
        num_cells = states.shape[0]
        output_states = states.clone()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º adaptive chunker –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–µ—à–µ—Ç–æ–∫
        cfg = get_project_config().unified_optimizer
        max_batch_size = cfg.max_test_batches * cfg.batch_size_multiplier

        if num_cells > max_batch_size:
            # Chunked processing
            for batch_start in range(0, num_cells, max_batch_size):
                batch_end = min(batch_start + max_batch_size, num_cells)
                batch_indices = list(range(batch_start, batch_end))

                batch_output = self._process_batch_gpu(
                    states, batch_indices, processor_fn
                )
                output_states[batch_start:batch_end] = batch_output
        else:
            # Single batch processing
            batch_indices = list(range(num_cells))
            output_states = self._process_batch_gpu(states, batch_indices, processor_fn)

        return output_states

    def _process_batch_gpu(
        self, states: torch.Tensor, batch_indices: List[int], processor_fn: Callable
    ) -> torch.Tensor:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ batch –Ω–∞ GPU"""
        batch_size = len(batch_indices)
        output_batch = torch.zeros_like(states[batch_indices])

        cfg = get_project_config().unified_optimizer
        for i, cell_idx in enumerate(batch_indices):
            coords = self.pos_helper.to_3d_coordinates(cell_idx)
            neighbors = self.find_neighbors(
                coords, self.config.get("max_search_radius", cfg.default_search_radius)
            )

            if neighbors:
                neighbor_states = states[neighbors]
                new_state = processor_fn(
                    states[cell_idx].unsqueeze(0), neighbor_states, cell_idx, neighbors
                )
                output_batch[i] = new_state.squeeze(0)
            else:
                output_batch[i] = states[cell_idx]

        return output_batch

    def get_performance_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GPU processor"""
        gpu_stats = self.gpu_processor.get_performance_stats()
        device_stats = self.device_manager.get_memory_stats()

        return {
            "mode": "gpu_accelerated",
            "gpu_processor": gpu_stats,
            "device": device_stats,
            "morton_encoder": {
                "enabled": True,
                "dimensions": self.dimensions,
                "device": str(self.device),
            },
        }


class UnifiedSpatialOptimizer:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (GPU-only).
    """

    def __init__(
        self,
        dimensions: Coordinates3D,
        config: Optional[OptimizationConfig] = None,
        moe_processor: Optional[nn.Module] = None,
    ):
        self.config = config or get_project_config().unified_optimizer
        self.dimensions = dimensions
        self.device_manager = get_device_manager()
        self.pos_helper = Position3D(dimensions)
        self.moe_processor = moe_processor
        self.mode = self._determine_optimal_mode()

        if not self.device_manager.is_cuda():
            raise RuntimeError("UnifiedSpatialOptimizer requires a CUDA-enabled GPU.")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ GPU –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.gpu_processor = GPUSpatialProcessor(self.dimensions, self.config)
        self.chunker = (
            AdaptiveGPUChunker(self.dimensions, self.config)
            if self.config.enable_adaptive_chunking
            else None
        )

        self.perf_history = []
        self._setup_moe_integration()

        logger.info(
            f"üöÄ UnifiedSpatialOptimizer initialized in GPU_ONLY mode for dimensions {dimensions}"
        )

    def _determine_optimal_mode(self) -> OptimizationMode:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º. –¢–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ GPU_ONLY –∏–ª–∏ –æ—à–∏–±–∫–∞."""
        if not self.device_manager.is_cuda():
            raise RuntimeError(
                "Cannot initialize UnifiedSpatialOptimizer: CUDA device not available."
            )
        return OptimizationMode.GPU_ONLY

    def _setup_moe_integration(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ MoE –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å."""
        if self.moe_processor:
            self.moe_processor = self.device_manager.transfer_module(self.moe_processor)
            logger.info("‚úÖ MoE Processor integrated with UnifiedSpatialOptimizer.")

    def find_neighbors_optimized(
        self, coords: Union[Coordinates3D, torch.Tensor], radius: float
    ) -> List[int]:
        """GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å–æ—Å–µ–¥–µ–π."""
        return self.gpu_processor.find_neighbors(coords, radius)

    def optimize_lattice_forward(
        self, states: torch.Tensor, processor_fn: Optional[Callable] = None
    ) -> SpatialOptimizationResult:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ forward pass —á–µ—Ä–µ–∑ —Ä–µ—à–µ—Ç–∫—É, –∏—Å–ø–æ–ª—å–∑—É—è GPU.
        """
        start_time = time.time()
        num_cells = states.shape[0]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if processor_fn is None:
            if self.moe_processor:
                processor_fn = self._create_moe_processor_fn()
            else:
                processor_fn = self._create_default_processor_fn()

        mem_before = self.device_manager.get_memory_stats().get("allocated_mb", 0)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–¥–∞ —á–µ—Ä–µ–∑ GPU
        new_states = self.gpu_processor.process_lattice(
            states, processor_fn, self.chunker
        )

        self.device_manager.synchronize()
        processing_time_ms = (time.time() - start_time) * 1000

        result = self._create_optimization_result(
            new_states, processing_time_ms, self.mode, num_cells, mem_before
        )
        self._record_performance(processing_time_ms, self.mode, num_cells)

        return result

    def _create_moe_processor_fn(self) -> Callable:
        """–°–æ–∑–¥–∞–µ—Ç processor function –¥–ª—è MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""

        def moe_processor(current_state, neighbor_states, cell_idx, neighbor_indices):
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥—ã –¥–ª—è MoE
                if current_state.dim() == 1:
                    current_state = current_state.unsqueeze(0)

                if len(neighbor_indices) == 0:
                    empty_neighbors = torch.empty(
                        1, 0, current_state.shape[-1], device=current_state.device
                    )
                else:
                    if neighbor_states.dim() == 1:
                        neighbor_states = neighbor_states.unsqueeze(0).unsqueeze(0)
                    elif neighbor_states.dim() == 2:
                        neighbor_states = neighbor_states.unsqueeze(0)
                    empty_neighbors = neighbor_states

                # –í—ã–∑—ã–≤–∞–µ–º MoE processor
                result = self.moe_processor(
                    current_state=current_state,
                    neighbor_states=empty_neighbors,
                    cell_idx=cell_idx,
                    neighbor_indices=neighbor_indices,
                    spatial_optimizer=self,
                )

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if isinstance(result, dict) and "new_state" in result:
                    return result["new_state"].squeeze(0)
                elif isinstance(result, torch.Tensor):
                    return result.squeeze(0)
                else:
                    return current_state.squeeze(0)

            except Exception as e:
                import traceback
                logger.error(f"‚ö†Ô∏è MoE processor error: {e}")
                logger.error(f"üìç Full traceback:\n{traceback.format_exc()}")
                logger.error(f"üîç Context: cell_idx={cell_idx}, current_state.shape={getattr(current_state, 'shape', 'N/A')}")
                return (
                    current_state.squeeze(0)
                    if current_state.dim() > 1
                    else current_state
                )

        return moe_processor

    def _create_default_processor_fn(self) -> Callable:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—É—é processor function"""

        def default_processor(
            current_state, neighbor_states, cell_idx, neighbor_indices
        ):
            if len(neighbor_indices) == 0:
                return current_state

            # –ü—Ä–æ—Å—Ç–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º
            if neighbor_states.dim() == 1:
                neighbor_states = neighbor_states.unsqueeze(0)

            mean_neighbor = neighbor_states.mean(dim=0)

            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ: 70% —Ç–µ–∫—É—â–µ–µ, 30% —Å–æ—Å–µ–¥–∏
            new_state = 0.7 * current_state + 0.3 * mean_neighbor

            return new_state

        return default_processor

    def _create_optimization_result(
        self,
        new_states: torch.Tensor,
        processing_time_ms: float,
        mode: OptimizationMode,
        num_cells: int,
        mem_before: float,
    ) -> SpatialOptimizationResult:
        cfg = get_project_config().unified_optimizer

        # Memory usage
        memory_usage_mb = (
            self.device_manager.get_memory_stats().get("allocated_mb", 0) - mem_before
        )

        # GPU utilization
        gpu_utilization = 1.0

        # Cache hit rate (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
        cache_hit_rate = 0.0
        if hasattr(self.gpu_processor, "get_performance_stats"):
            gpu_stats = self.gpu_processor.get_performance_stats()
            cache_hit_rate = (
                gpu_stats.get("gpu_processor", {})
                .get("processor", {})
                .get("cache_hit_rate", 0.0)
            )

        return SpatialOptimizationResult(
            new_states=new_states,
            processing_time_ms=processing_time_ms,
            memory_usage_mb=memory_usage_mb,
            neighbors_found=num_cells * cfg.neighbors_found_factor,
            gpu_utilization=gpu_utilization,
            mode_used=mode,
            cache_hit_rate=cache_hit_rate,
            chunks_processed=max(1, num_cells // cfg.chunks_processed_div),
        )

    def _record_performance(
        self, time_ms: float, mode: OptimizationMode, data_size: int
    ):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        self.perf_history.append(time_ms)

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π
        if len(self.perf_history) > 100:
            self.perf_history = self.perf_history[-100:]

    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—É."""
        stats = {
            "mode": self.mode.value,
            "performance_history_ms": [round(t, 2) for t in self.perf_history[-100:]],
            "avg_perf_ms": np.mean(self.perf_history) if self.perf_history else 0,
            "gpu_processor": self.gpu_processor.get_performance_stats(),
        }

        if self.chunker:
            stats["chunker"] = self.chunker.get_comprehensive_stats()

        if self.moe_processor and hasattr(self.moe_processor, "get_usage_stats"):
            stats["moe_processor"] = self.moe_processor.get_usage_stats()

        return stats

    def optimize_performance(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        logger.info("üîß –ó–∞–ø—É—Å–∫ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ UnifiedSpatialOptimizer")

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º GPU –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.gpu_processor.optimize_performance()

        # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.perf_history = self.perf_history[-20:]  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        self.device_manager.cleanup()

        logger.info("‚úÖ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def cleanup(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã UnifiedSpatialOptimizer")

        self.gpu_processor.shutdown()

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        self.device_manager.cleanup()

        logger.info("‚úÖ UnifiedSpatialOptimizer –∑–∞–≤–µ—Ä—à–µ–Ω")


# === FACTORY FUNCTIONS ===


def create_unified_spatial_optimizer(
    dimensions: Coordinates3D,
    config: Optional[OptimizationConfig] = None,
    moe_processor: Optional[nn.Module] = None,
) -> UnifiedSpatialOptimizer:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞

    Args:
        dimensions: –†–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        moe_processor: MoE processor –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π UnifiedSpatialOptimizer
    """
    if config is None:
        config = OptimizationConfig()

    logger.info(f"üè≠ –°–æ–∑–¥–∞–Ω–∏–µ UnifiedSpatialOptimizer –¥–ª—è {dimensions}")

    return UnifiedSpatialOptimizer(
        dimensions=dimensions, config=config, moe_processor=moe_processor
    )


def estimate_unified_memory_requirements(
    dimensions: Coordinates3D, config: Optional[OptimizationConfig] = None
) -> Dict[str, float]:
    """
    –û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã

    Args:
        dimensions: –†–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –ø–∞–º—è—Ç–∏ –≤ GB
    """
    cfg = get_project_config().unified_optimizer
    if config is None:
        config = OptimizationConfig()

    total_cells = np.prod(dimensions)

    # –ë–∞–∑–æ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    cell_states_gb = (
        total_cells * cfg.moe_expert_state_size * 4 / (1024**3)
    )  # float32 —Å–æ—Å—Ç–æ—è–Ω–∏—è

    # GPU –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã)
    gpu_requirements = {
        "gpu_spatial_hash_gb": total_cells * cfg.gpu_spatial_hash_bytes / (1024**3),
        "gpu_morton_encoder_gb": total_cells * cfg.gpu_morton_encoder_bytes / (1024**3),
        "gpu_chunker_gb": config.max_memory_gb * cfg.gpu_chunker_memory_fraction,
        "gpu_tensor_overhead_gb": cell_states_gb * cfg.gpu_tensor_overhead_fraction,
    }

    # MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã)
    moe_requirements = {}
    if config.enable_moe:
        moe_requirements = {
            "moe_expert_states_gb": total_cells
            * cfg.moe_expert_state_size
            * 4
            * cfg.moe_expert_count
            / (1024**3),
            "moe_connection_classification_gb": total_cells
            * cfg.moe_connection_neighbors
            * 4
            / (1024**3),
        }

    # –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    base_memory = cell_states_gb
    gpu_memory = sum(gpu_requirements.values())
    moe_memory = sum(moe_requirements.values())

    total_memory_gb = base_memory + gpu_memory + moe_memory

    result = {
        "cell_states_gb": base_memory,
        **gpu_requirements,
        **moe_requirements,
        "total_memory_gb": total_memory_gb,
        "recommended_gpu_memory_gb": total_memory_gb
        * cfg.recommended_gpu_memory_fraction,
        "recommended_system_memory_gb": gpu_memory
        * cfg.recommended_system_memory_fraction,
    }

    return result
