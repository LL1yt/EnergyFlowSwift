#!/usr/bin/env python3
"""
Adaptive GPU Chunker - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ä–µ—à–µ—Ç–æ–∫ —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
========================================================================

AdaptiveGPUChunker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ä–µ—à–µ—Ç–∫–∏ –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏
—Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ–π GPU –ø–∞–º—è—Ç–∏, –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–æ—Å—Ç—É–ø–∞ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- Dynamic chunk sizing –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
- GPU memory monitoring –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- Adaptive load balancing –º–µ–∂–¥—É chunk'–∞–º–∏
- Intelligent prefetching –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: 2.0.0 (2024-12-27)
"""

import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set, Any
from dataclasses import dataclass, field
import time
import threading
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor, Future

from ....config import get_project_config, AdaptiveChunkerConfig
from ..position import Position3D
from ....utils.logging import get_logger
from ....utils.device_manager import get_device_manager
from .memory_manager import get_memory_pool_manager

logger = get_logger(__name__)

Coordinates3D = Tuple[int, int, int]


@dataclass
class AdaptiveChunkInfo:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ chunk'–µ —Å adaptive —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏"""

    chunk_id: int
    start_coords: Coordinates3D
    end_coords: Coordinates3D
    cell_indices: List[int]
    neighbor_chunks: List[int] = field(default_factory=list)
    memory_size_mb: float = 0.0
    # GPU —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
    gpu_memory_usage_mb: float = 0.0
    last_access_time: float = field(default_factory=time.time)
    access_frequency: int = 0
    processing_priority: int = 0
    # Adaptive —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    optimal_batch_size: int = 1000
    preferred_device: str = "cuda"
    memory_pressure_level: float = 0.0  # 0.0 = –Ω–∏–∑–∫–æ–µ, 1.0 = –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ
    # Performance metrics
    avg_processing_time_ms: float = 0.0
    cache_hit_rate: float = 0.0
    neighbor_access_pattern: Dict[int, int] = field(default_factory=dict)
    prefetched_data: Optional[torch.Tensor] = None


@dataclass
class ChunkProcessingTask:
    """–ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–∞"""

    chunk_id: int
    operation_type: str  # "load", "process", "unload", "prefetch"
    priority: int = 0
    estimated_memory_mb: float = 0.0
    dependencies: List[int] = field(default_factory=list)
    callback: Optional[callable] = None


class AdaptiveMemoryPredictor:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ chunking'–∞"""

    def __init__(self):
        self.device_manager = get_device_manager()
        self.historical_usage = []
        cfg = get_project_config().adaptive_chunker
        self.max_history = cfg.max_history
        self.memory_per_cell_base = cfg.memory_per_cell_base
        self.memory_overhead_factor = cfg.memory_overhead_factor
        self.min_available_memory_mb = cfg.min_available_memory_mb
        self.cuda_fallback_available_mb = cfg.cuda_fallback_available_mb
        self.cpu_fallback_available_mb = cfg.cpu_fallback_available_mb
        self.safe_memory_buffer = cfg.safe_memory_buffer

    def predict_chunk_memory(self, chunk_info: AdaptiveChunkInfo) -> float:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è chunk'–∞

        Args:
            chunk_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ chunk'–µ

        Returns:
            –û–∂–∏–¥–∞–µ–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ MB
        """
        num_cells = len(chunk_info.cell_indices)

        # –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        base_memory = num_cells * self.memory_per_cell_base

        # –ù–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        overhead_memory = base_memory * self.memory_overhead_factor

        # –£—á–∏—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
        if self.historical_usage:
            avg_historical = np.mean(
                self.historical_usage[-10:]
            )  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π
            predicted_memory = 0.7 * overhead_memory + 0.3 * avg_historical
        else:
            predicted_memory = overhead_memory

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ MB
        predicted_memory_mb = predicted_memory / (1024**2)

        return predicted_memory_mb

    def update_actual_usage(self, chunk_id: int, actual_memory_mb: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        self.historical_usage.append(actual_memory_mb)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.historical_usage) > self.max_history:
            self.historical_usage = self.historical_usage[-self.max_history :]

    def get_available_memory_mb(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—É—é GPU –ø–∞–º—è—Ç—å"""
        device_stats = self.device_manager.get_memory_stats()
        cfg = get_project_config().adaptive_chunker
        if self.device_manager.is_cuda():
            available_mb = device_stats.get(
                "available_mb", cfg.cuda_fallback_available_mb
            )
        else:
            available_mb = device_stats.get(
                "available_mb", cfg.cpu_fallback_available_mb
            )
        safe_available_mb = available_mb * cfg.safe_memory_buffer
        return max(cfg.min_available_memory_mb, safe_available_mb)


class ChunkScheduler:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""

    def __init__(self, max_concurrent_chunks: int = None):
        cfg = get_project_config().adaptive_chunker
        self.max_concurrent_chunks = max_concurrent_chunks or cfg.max_concurrent_chunks
        self.task_queue = Queue()
        self.active_chunks: Set[int] = set()
        self.chunk_locks = {}
        self.memory_predictor = AdaptiveMemoryPredictor()

        self.device_manager = get_device_manager()
        if self.device_manager.is_cuda():
            self.main_stream = torch.cuda.Stream()
            self.prefetch_stream = torch.cuda.Stream()
        else:
            self.main_stream = None
            self.prefetch_stream = None

        # Thread pool –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent_chunks)
        self.running = True

        # –ó–∞–ø—É—Å–∫–∞–µ–º scheduler thread
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop)
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()

        logger.info(
            f"üìÖ ChunkScheduler –∑–∞–ø—É—â–µ–Ω: max_concurrent={max_concurrent_chunks}"
        )

    def schedule_task(self, task: ChunkProcessingTask) -> Future:
        """–ü–ª–∞–Ω–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–∞"""
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å
        if hasattr(task, "chunk_info"):
            task.estimated_memory_mb = self.memory_predictor.predict_chunk_memory(
                task.chunk_info
            )

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
        future = Future()
        self.task_queue.put((task, future))

        return future

    def _scheduler_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        while self.running:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
                available_memory = self.memory_predictor.get_available_memory_mb()

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
                if len(self.active_chunks) < self.max_concurrent_chunks:
                    try:
                        task, future = self.task_queue.get(timeout=1.0)

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É
                        if task.estimated_memory_mb <= available_memory:
                            self._execute_task(task, future)
                        else:
                            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–¥–∞—á—É –≤ –æ—á–µ—Ä–µ–¥—å —Å –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
                            task.priority -= 1
                            self.task_queue.put((task, future))

                    except Empty:
                        continue

                time.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ scheduler loop: {e}")

    def _execute_task(self, task: ChunkProcessingTask, future: Future):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–∞"""
        self.active_chunks.add(task.chunk_id)

        def task_wrapper():
            try:
                start_time = time.time()

                # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É
                if task.callback:
                    result = task.callback(task)
                else:
                    result = f"Processed chunk {task.chunk_id}"

                processing_time = (time.time() - start_time) * 1000  # ms

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                logger.debug(
                    f"‚úÖ Chunk {task.chunk_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.1f}ms"
                )

                future.set_result(result)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk {task.chunk_id}: {e}")
                future.set_exception(e)
            finally:
                self.active_chunks.discard(task.chunk_id)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        self.executor.submit(task_wrapper)

    def shutdown(self):
        """–ó–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        self.running = False
        self.executor.shutdown(wait=True)
        if self.scheduler_thread.is_alive():
            self.scheduler_thread.join()


class AdaptiveGPUChunker:
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ä–µ—à–µ—Ç–∫–∏ –Ω–∞ chunk'–∏
    """

    def __init__(self, dimensions: Coordinates3D, config: dict = None):
        self.config = config or get_project_config().adaptive_chunker
        self.dimensions = dimensions
        self.pos_helper = Position3D(dimensions)
        self.device_manager = get_device_manager()
        self.memory_predictor = AdaptiveMemoryPredictor()
        self.scheduler = ChunkScheduler(self.config.get("max_concurrent_chunks"))

        if self.device_manager.is_cuda():
            self.prefetch_events: Dict[int, torch.cuda.Event] = {}

        self._chunks: List[AdaptiveChunkInfo] = self._create_adaptive_chunks()
        self._neighbor_map: Dict[int, List[int]] = {}
        self._compute_neighbor_chunks()

        # Device management
        self.device_manager = get_device_manager()
        self.device = self.device_manager.get_device()

        # Memory management
        self.memory_manager = get_memory_pool_manager(self.config)

        # Performance monitoring
        self.performance_stats = {
            "total_chunks": 0,
            "memory_efficiency": 0.0,
            "avg_chunk_processing_time_ms": 0.0,
            "memory_pressure_events": 0,
            "adaptive_rebalancing_events": 0,
        }

        logger.info(
            f"üéØ AdaptiveGPUChunker —Å–æ–∑–¥–∞–Ω: {len(self._chunks)} chunks –Ω–∞ {self.device}"
        )

    @property
    def chunks(self) -> List[AdaptiveChunkInfo]:
        """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç adaptive_chunks –∫–∞–∫ chunks"""
        return self._chunks

    def _create_adaptive_chunks(self) -> List[AdaptiveChunkInfo]:
        """–°–æ–∑–¥–∞–µ—Ç adaptive chunk'–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º"""
        available_memory_mb = self.memory_predictor.get_available_memory_mb()
        optimal_chunk_size = self._calculate_optimal_chunk_size(available_memory_mb)
        x_dim, y_dim, z_dim = self.dimensions

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ chunk'–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –æ—Å–∏
        x_chunks = max(1, (x_dim + optimal_chunk_size - 1) // optimal_chunk_size)
        y_chunks = max(1, (y_dim + optimal_chunk_size - 1) // optimal_chunk_size)
        z_chunks = max(1, (z_dim + optimal_chunk_size - 1) // optimal_chunk_size)

        chunk_id = 0
        chunks = []

        for z_idx in range(z_chunks):
            for y_idx in range(y_chunks):
                for x_idx in range(x_chunks):
                    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã chunk'–∞
                    start_x = x_idx * optimal_chunk_size
                    start_y = y_idx * optimal_chunk_size
                    start_z = z_idx * optimal_chunk_size

                    end_x = min(start_x + optimal_chunk_size, x_dim)
                    end_y = min(start_y + optimal_chunk_size, y_dim)
                    end_z = min(start_z + optimal_chunk_size, z_dim)

                    # –°–æ–∑–¥–∞–µ–º adaptive chunk info
                    chunk_info = self._create_adaptive_chunk_info(
                        chunk_id,
                        (start_x, start_y, start_z),
                        (end_x, end_y, end_z),
                        available_memory_mb,
                    )

                    chunks.append(chunk_info)
                    chunk_id += 1

        # –í—ã—á–∏—Å–ª—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ chunk'–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
        self._compute_neighbor_chunks()
        self._optimize_chunk_parameters()

        self.performance_stats["total_chunks"] = len(chunks)

        return chunks

    def _calculate_optimal_chunk_size(self, available_memory_mb: float) -> int:
        cfg = get_project_config().adaptive_chunker
        total_cells = np.prod(self.dimensions)
        target_memory_per_chunk_mb = (
            available_memory_mb * 0.75 / self.config.get("max_chunks_in_memory", 4)
        )
        memory_per_cell_bytes = cfg.memory_per_cell_base
        cells_per_chunk = int(
            target_memory_per_chunk_mb * 1024**2 / memory_per_cell_bytes
        )
        if cells_per_chunk <= 0:
            chunk_size = max(self.dimensions) // cfg.chunk_size_fallback_div
        else:
            chunk_size = max(cfg.min_chunk_size, int(cells_per_chunk ** (1 / 3)))
        max_chunk_size = max(self.dimensions) // 2
        min_chunk_size = cfg.min_chunk_size
        optimal_size = max(min_chunk_size, min(chunk_size, max_chunk_size))

        logger.debug(
            f"üìè Optimal chunk size: {optimal_size} "
            f"(available_memory={available_memory_mb:.1f}MB, cells_per_chunk={cells_per_chunk})"
        )

        return optimal_size

    def _create_adaptive_chunk_info(
        self,
        chunk_id: int,
        start: Coordinates3D,
        end: Coordinates3D,
        available_memory_mb: float,
    ) -> AdaptiveChunkInfo:
        cfg = get_project_config().adaptive_chunker

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–ª–µ—Ç–∫–∏ –≤ chunk'–µ
        cell_indices = []
        for z in range(start[2], end[2]):
            for y in range(start[1], end[1]):
                for x in range(start[0], end[0]):
                    cell_idx = self.pos_helper.to_linear_index((x, y, z))
                    cell_indices.append(cell_idx)

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        num_cells = len(cell_indices)
        memory_size_mb = num_cells * 64 / (1024**2)  # –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

        # –°–æ–∑–¥–∞–µ–º adaptive chunk info
        chunk_info = AdaptiveChunkInfo(
            chunk_id=chunk_id,
            start_coords=start,
            end_coords=end,
            cell_indices=cell_indices,
            neighbor_chunks=[],  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
            memory_size_mb=memory_size_mb,
            gpu_memory_usage_mb=0.0,
            last_access_time=time.time(),
            access_frequency=0,
            processing_priority=self._calculate_initial_priority(start, end),
            optimal_batch_size=min(cfg.optimal_batch_size, num_cells),
            preferred_device=cfg.preferred_device,
            memory_pressure_level=min(1.0, memory_size_mb / available_memory_mb),
        )

        # –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è chunk_info, –µ—Å–ª–∏ –º—ã –Ω–∞ GPU, —Å–æ–∑–¥–∞–µ–º prefetch event
        if self.device_manager.is_cuda():
            self.prefetch_events[chunk_id] = torch.cuda.Event()

        return chunk_info

    def _calculate_initial_priority(
        self, start: Coordinates3D, end: Coordinates3D
    ) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç chunk'–∞"""
        # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ chunk'–∏ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        center = tuple(d // 2 for d in self.dimensions)
        chunk_center = tuple((s + e) // 2 for s, e in zip(start, end))

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç —Ü–µ–Ω—Ç—Ä–∞ —Ä–µ—à–µ—Ç–∫–∏
        distance = sum((c1 - c2) ** 2 for c1, c2 in zip(center, chunk_center)) ** 0.5
        max_distance = sum(d**2 for d in self.dimensions) ** 0.5

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç 1 –¥–æ 100 (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ - –≤—ã—à–µ)
        priority = int(100 * (1 - distance / max_distance))

        return max(1, priority)

    def _compute_neighbor_chunks(self):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ chunk'–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ chunk'–∞"""
        overlap = self.config.get("chunk_overlap", 8)

        for chunk in self._chunks:
            neighbor_chunk_ids = []

            for other_chunk in self._chunks:
                if chunk.chunk_id != other_chunk.chunk_id:
                    if self._are_chunks_neighbors(chunk, other_chunk, overlap):
                        neighbor_chunk_ids.append(other_chunk.chunk_id)

            chunk.neighbor_chunks = neighbor_chunk_ids
            self._neighbor_map[chunk.chunk_id] = neighbor_chunk_ids

    def _are_chunks_neighbors(
        self, chunk1: AdaptiveChunkInfo, chunk2: AdaptiveChunkInfo, overlap: int
    ) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è—é—Ç—Å—è –ª–∏ chunk'–∏ —Å–æ—Å–µ–¥–Ω–∏–º–∏"""
        # –†–∞—Å—à–∏—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã chunk1 –Ω–∞ overlap
        start1 = tuple(max(0, s - overlap) for s in chunk1.start_coords)
        end1 = tuple(
            min(d, e + overlap) for d, e in zip(self.dimensions, chunk1.end_coords)
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å chunk2
        return all(
            s1 < chunk2.end_coords[i] and e1 > chunk2.start_coords[i]
            for i, (s1, e1) in enumerate(zip(start1, end1))
        )

    def _optimize_chunk_parameters(self):
        cfg = get_project_config().adaptive_chunker
        for chunk in self._chunks:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º batch size –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ chunk'–∞
            num_cells = len(chunk.cell_indices)

            if num_cells < cfg.optimal_batch_size_small:
                chunk.optimal_batch_size = num_cells
            elif num_cells < cfg.optimal_batch_size_medium:
                chunk.optimal_batch_size = num_cells // 4
            else:
                chunk.optimal_batch_size = cfg.optimal_batch_size_large

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ memory pressure
            if chunk.memory_pressure_level > cfg.memory_pressure_high:
                chunk.processing_priority = max(
                    1, chunk.processing_priority - cfg.processing_priority_low_delta
                )
            elif chunk.memory_pressure_level < cfg.memory_pressure_low:
                chunk.processing_priority = min(
                    100, chunk.processing_priority + cfg.processing_priority_high_delta
                )

    def get_chunk_by_coords(self, coords: Coordinates3D) -> AdaptiveChunkInfo:
        """–ù–∞—Ö–æ–¥–∏—Ç chunk –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ—Å—Ç—É–ø–∞"""
        for chunk in self._chunks:
            if all(
                chunk.start_coords[i] <= coords[i] < chunk.end_coords[i]
                for i in range(3)
            ):
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ—Å—Ç—É–ø–∞
                chunk.last_access_time = time.time()
                chunk.access_frequency += 1

                return chunk

        raise ValueError(f"Chunk –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç {coords}")

    def get_adaptive_processing_schedule(self) -> List[List[int]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç adaptive —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–æ–≤

        –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        """
        available_memory = self.memory_predictor.get_available_memory_mb()
        max_concurrent = self.config.get("max_chunks_in_memory", 4)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º chunk'–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ memory pressure
        sorted_chunks = sorted(
            self._chunks,
            key=lambda c: (c.processing_priority, -c.memory_pressure_level),
            reverse=True,
        )

        schedule = []
        remaining_chunks = set(c.chunk_id for c in sorted_chunks)

        while remaining_chunks:
            current_batch = []
            current_memory_usage = 0.0
            used_neighbors = set()

            for chunk in sorted_chunks:
                if chunk.chunk_id not in remaining_chunks:
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                estimated_memory = chunk.memory_size_mb
                conflicts = set(chunk.neighbor_chunks) & used_neighbors

                can_add = (
                    not conflicts
                    and len(current_batch) < max_concurrent
                    and current_memory_usage + estimated_memory
                    <= available_memory * 0.9
                )

                if can_add:
                    current_batch.append(chunk.chunk_id)
                    current_memory_usage += estimated_memory
                    used_neighbors.add(chunk.chunk_id)
                    used_neighbors.update(chunk.neighbor_chunks)

            # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ chunk'–∏
            for chunk_id in current_batch:
                remaining_chunks.remove(chunk_id)

            if current_batch:
                schedule.append(current_batch)
            else:
                # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ chunk'–∞, –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
                if remaining_chunks:
                    first_chunk = next(iter(remaining_chunks))
                    schedule.append([first_chunk])
                    remaining_chunks.remove(first_chunk)

        logger.debug(
            f"üìÖ Adaptive schedule —Å–æ–∑–¥–∞–Ω–æ: {len(schedule)} batches, "
            f"avg_batch_size={np.mean([len(b) for b in schedule]):.1f}"
        )

        return schedule

    def _prefetch_chunk_data(self, chunk_id: int, all_states: torch.Tensor):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è chunk'–∞."""
        if not self.device_manager.is_cuda():
            return

        chunk_info = self._chunks[chunk_id]

        with torch.cuda.stream(self.scheduler.prefetch_stream):
            indices = torch.tensor(
                chunk_info.cell_indices, device="cpu", dtype=torch.long
            )
            chunk_info.prefetched_data = all_states[indices].to(
                self.device_manager.get_device(), non_blocking=True
            )
            self.prefetch_events[chunk_id].record()

    def process_chunk_async(
        self,
        chunk_id: int,
        operation: str,
        callback: Optional[callable] = None,
        all_states: Optional[torch.Tensor] = None,
    ) -> Future:
        if (
            operation == "process"
            and all_states is not None
            and self.device_manager.is_cuda()
        ):
            self._prefetch_chunk_data(chunk_id, all_states)

            for neighbor_id in self._neighbor_map.get(chunk_id, []):
                self._prefetch_chunk_data(neighbor_id, all_states)

        def processing_wrapper(task):
            if self.device_manager.is_cuda():
                self.scheduler.main_stream.wait_event(self.prefetch_events[chunk_id])

            with torch.cuda.stream(self.scheduler.main_stream):
                if callback:
                    return callback(chunk_id, self._chunks[chunk_id])
            return None

        task = ChunkProcessingTask(
            chunk_id=chunk_id,
            operation_type=operation,
            callback=processing_wrapper,
        )
        return self.scheduler.schedule_task(task)

    def rebalance_chunks(self):
        """–ü–µ—Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ chunk'–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        current_memory = self.memory_predictor.get_available_memory_mb()

        # –ù–∞—Ö–æ–¥–∏–º chunk'–∏ —Å –≤—ã—Å–æ–∫–∏–º memory pressure
        high_pressure_chunks = [
            c for c in self._chunks if c.memory_pressure_level > 0.8
        ]

        if high_pressure_chunks:
            logger.info(
                f"üîÑ Rebalancing {len(high_pressure_chunks)} high-pressure chunks"
            )

            # –ü–æ–Ω–∏–∂–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç chunk'–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º –¥–∞–≤–ª–µ–Ω–∏–µ–º
            for chunk in high_pressure_chunks:
                chunk.processing_priority = max(1, chunk.processing_priority - 10)
                chunk.optimal_batch_size = max(100, chunk.optimal_batch_size // 2)

            self.performance_stats["adaptive_rebalancing_events"] += 1

    def get_memory_stats(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        total_chunks_memory = sum(c.memory_size_mb for c in self._chunks)
        active_chunks_memory = sum(
            c.gpu_memory_usage_mb for c in self._chunks if c.gpu_memory_usage_mb > 0
        )

        device_stats = self.device_manager.get_memory_stats()

        return {
            "total_chunks_memory_mb": total_chunks_memory,
            "active_chunks_memory_mb": active_chunks_memory,
            "memory_efficiency": active_chunks_memory / max(1, total_chunks_memory),
            "available_memory_mb": self.memory_predictor.get_available_memory_mb(),
            "device_stats": device_stats,
        }

    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É adaptive chunker'–∞"""
        memory_stats = self.get_memory_stats()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ chunk'–∞–º
        chunk_stats = {
            "total_chunks": len(self._chunks),
            "avg_chunk_size": np.mean([len(c.cell_indices) for c in self._chunks]),
            "avg_memory_usage_mb": np.mean([c.memory_size_mb for c in self._chunks]),
            "avg_access_frequency": np.mean([c.access_frequency for c in self._chunks]),
            "high_pressure_chunks": len(
                [c for c in self._chunks if c.memory_pressure_level > 0.8]
            ),
        }

        return {
            "performance": self.performance_stats,
            "memory": memory_stats,
            "chunks": chunk_stats,
            "scheduler": {
                "active_chunks": len(self.scheduler.active_chunks),
                "queue_size": self.scheduler.task_queue.qsize(),
            },
        }

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.scheduler.shutdown()
        self.memory_manager.cleanup()

        logger.info(" AdaptiveGPUChunker –æ—á–∏—â–µ–Ω")
