#!/usr/bin/env python3
"""
Adaptive GPU Chunker - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Ä–µ—à–µ—Ç–æ–∫ —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
========================================================================

AdaptiveGPUChunker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ä–µ—à–µ—Ç–∫–∏ –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏
—Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ–π GPU –ø–∞–º—è—Ç–∏, –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–æ—Å—Ç—É–ø–∞ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- Dynamic chunk sizing –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
- GPU memory monitoring –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- Adaptive load balancing –º–µ–∂–¥—É chunk'–∞–º–∏
- Intelligent prefetching –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: 2.0.0 (2024-12-27)
"""

import torch
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass, field
import time
import threading
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor, Future

try:
    from ....config.project_config import ChunkInfo, get_project_config
    from ..spatial_hashing import Coordinates3D
    from ..position import Position3D
    from ....utils.logging import get_logger
    from ....utils.device_manager import get_device_manager
    from .memory_manager import MemoryPoolManager
except ImportError:
    # Fallback –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    import sys
    import os

    sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
    from config.project_config import ChunkInfo, get_project_config
    from core.lattice.spatial_hashing import Coordinates3D
    from core.lattice.position import Position3D
    from utils.logging import get_logger
    from utils.device_manager import get_device_manager
    from core.lattice.spatial_optimization.memory_manager import MemoryPoolManager

logger = get_logger(__name__)


@dataclass
class AdaptiveChunkInfo(ChunkInfo):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ chunk'–µ —Å adaptive —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏"""

    # GPU —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
    gpu_memory_usage_mb: float = 0.0
    last_access_time: float = field(default_factory=time.time)
    access_frequency: int = 0
    processing_priority: int = 0

    # Adaptive —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    optimal_batch_size: int = 1000
    preferred_device: str = "cuda"
    memory_pressure_level: float = 0.0  # 0.0 = –Ω–∏–∑–∫–æ–µ, 1.0 = –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ

    # Performance metrics
    avg_processing_time_ms: float = 0.0
    cache_hit_rate: float = 0.0
    neighbor_access_pattern: Dict[int, int] = field(default_factory=dict)


@dataclass
class ChunkProcessingTask:
    """–ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–∞"""

    chunk_id: int
    operation_type: str  # "load", "process", "unload", "prefetch"
    priority: int = 0
    estimated_memory_mb: float = 0.0
    dependencies: List[int] = field(default_factory=list)
    callback: Optional[callable] = None


class AdaptiveMemoryPredictor:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ chunking'–∞"""

    def __init__(self):
        self.device_manager = get_device_manager()
        self.historical_usage = []
        self.max_history = 1000

        # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ chunk'–∞
        self.memory_per_cell_base = 64  # –±–∞–∑–æ–≤—ã–µ –±–∞–π—Ç—ã –Ω–∞ –∫–ª–µ—Ç–∫—É
        self.memory_overhead_factor = 1.3  # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤

    def predict_chunk_memory(self, chunk_info: AdaptiveChunkInfo) -> float:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è chunk'–∞

        Args:
            chunk_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ chunk'–µ

        Returns:
            –û–∂–∏–¥–∞–µ–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ MB
        """
        num_cells = len(chunk_info.cell_indices)

        # –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        base_memory = num_cells * self.memory_per_cell_base

        # –ù–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        overhead_memory = base_memory * self.memory_overhead_factor

        # –£—á–∏—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
        if self.historical_usage:
            avg_historical = np.mean(
                self.historical_usage[-10:]
            )  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π
            predicted_memory = 0.7 * overhead_memory + 0.3 * avg_historical
        else:
            predicted_memory = overhead_memory

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ MB
        predicted_memory_mb = predicted_memory / (1024**2)

        return predicted_memory_mb

    def update_actual_usage(self, chunk_id: int, actual_memory_mb: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        self.historical_usage.append(actual_memory_mb)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.historical_usage) > self.max_history:
            self.historical_usage = self.historical_usage[-self.max_history :]

    def get_available_memory_mb(self) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—É—é GPU –ø–∞–º—è—Ç—å"""
        device_stats = self.device_manager.get_memory_stats()

        if self.device_manager.is_cuda():
            # –î–ª—è CUDA –∏—Å–ø–æ–ª—å–∑—É–µ–º torch —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            available_mb = device_stats.get("available_mb", 8000.0)  # fallback
        else:
            # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –ø–∞–º—è—Ç—å
            available_mb = device_stats.get("available_mb", 16000.0)  # fallback

        # –û—Å—Ç–∞–≤–ª—è–µ–º 20% –±—É—Ñ–µ—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        safe_available_mb = available_mb * 0.8

        return max(500.0, safe_available_mb)  # –º–∏–Ω–∏–º—É–º 500MB


class ChunkScheduler:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""

    def __init__(self, max_concurrent_chunks: int = 4):
        self.max_concurrent_chunks = max_concurrent_chunks
        self.task_queue = Queue()
        self.active_chunks: Set[int] = set()
        self.chunk_locks = {}
        self.memory_predictor = AdaptiveMemoryPredictor()

        # Thread pool –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent_chunks)
        self.running = True

        # –ó–∞–ø—É—Å–∫–∞–µ–º scheduler thread
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop)
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()

        logger.info(
            f"üìÖ ChunkScheduler –∑–∞–ø—É—â–µ–Ω: max_concurrent={max_concurrent_chunks}"
        )

    def schedule_task(self, task: ChunkProcessingTask) -> Future:
        """–ü–ª–∞–Ω–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–∞"""
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å
        if hasattr(task, "chunk_info"):
            task.estimated_memory_mb = self.memory_predictor.predict_chunk_memory(
                task.chunk_info
            )

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
        future = Future()
        self.task_queue.put((task, future))

        return future

    def _scheduler_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        while self.running:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
                available_memory = self.memory_predictor.get_available_memory_mb()

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
                if len(self.active_chunks) < self.max_concurrent_chunks:
                    try:
                        task, future = self.task_queue.get(timeout=1.0)

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É
                        if task.estimated_memory_mb <= available_memory:
                            self._execute_task(task, future)
                        else:
                            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–¥–∞—á—É –≤ –æ—á–µ—Ä–µ–¥—å —Å –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
                            task.priority -= 1
                            self.task_queue.put((task, future))

                    except Empty:
                        continue

                time.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ scheduler loop: {e}")

    def _execute_task(self, task: ChunkProcessingTask, future: Future):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–∞"""
        self.active_chunks.add(task.chunk_id)

        def task_wrapper():
            try:
                start_time = time.time()

                # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É
                if task.callback:
                    result = task.callback(task)
                else:
                    result = f"Processed chunk {task.chunk_id}"

                processing_time = (time.time() - start_time) * 1000  # ms

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                logger.debug(
                    f"‚úÖ Chunk {task.chunk_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.1f}ms"
                )

                future.set_result(result)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk {task.chunk_id}: {e}")
                future.set_exception(e)
            finally:
                self.active_chunks.discard(task.chunk_id)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        self.executor.submit(task_wrapper)

    def shutdown(self):
        """–ó–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        self.running = False
        self.executor.shutdown(wait=True)
        if self.scheduler_thread.is_alive():
            self.scheduler_thread.join()


class AdaptiveGPUChunker:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π chunker —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –∏ adaptive –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π

    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä—ã chunk'–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
    –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
    """

    def __init__(self, dimensions: Coordinates3D, config: dict = None):
        self.dimensions = dimensions
        self.config = config or get_project_config().get_spatial_optim_config()

        # Device management
        self.device_manager = get_device_manager()
        self.device = self.device_manager.get_device()

        # Position helper
        self.pos_helper = Position3D(dimensions)

        # Memory management
        self.memory_manager = MemoryPoolManager(self.config)
        self.memory_predictor = AdaptiveMemoryPredictor()

        # Chunk management
        self.adaptive_chunks: List[AdaptiveChunkInfo] = []
        self.chunk_scheduler = ChunkScheduler(
            self.config.get("max_chunks_in_memory", 4)
        )

        # Performance monitoring
        self.performance_stats = {
            "total_chunks": 0,
            "memory_efficiency": 0.0,
            "avg_chunk_processing_time_ms": 0.0,
            "memory_pressure_events": 0,
            "adaptive_rebalancing_events": 0,
        }

        # –°–æ–∑–¥–∞–µ–º adaptive chunk'–∏
        self._create_adaptive_chunks()

        logger.info(
            f"üéØ AdaptiveGPUChunker —Å–æ–∑–¥–∞–Ω: {len(self.adaptive_chunks)} chunks –Ω–∞ {self.device}"
        )

    def _create_adaptive_chunks(self):
        """–°–æ–∑–¥–∞–µ—Ç adaptive chunk'–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º"""
        available_memory_mb = self.memory_predictor.get_available_memory_mb()

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ chunk'–∞
        optimal_chunk_size = self._calculate_optimal_chunk_size(available_memory_mb)

        x_dim, y_dim, z_dim = self.dimensions

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ chunk'–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –æ—Å–∏
        x_chunks = max(1, (x_dim + optimal_chunk_size - 1) // optimal_chunk_size)
        y_chunks = max(1, (y_dim + optimal_chunk_size - 1) // optimal_chunk_size)
        z_chunks = max(1, (z_dim + optimal_chunk_size - 1) // optimal_chunk_size)

        chunk_id = 0

        for z_idx in range(z_chunks):
            for y_idx in range(y_chunks):
                for x_idx in range(x_chunks):
                    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã chunk'–∞
                    start_x = x_idx * optimal_chunk_size
                    start_y = y_idx * optimal_chunk_size
                    start_z = z_idx * optimal_chunk_size

                    end_x = min(start_x + optimal_chunk_size, x_dim)
                    end_y = min(start_y + optimal_chunk_size, y_dim)
                    end_z = min(start_z + optimal_chunk_size, z_dim)

                    # –°–æ–∑–¥–∞–µ–º adaptive chunk info
                    chunk_info = self._create_adaptive_chunk_info(
                        chunk_id,
                        (start_x, start_y, start_z),
                        (end_x, end_y, end_z),
                        available_memory_mb,
                    )

                    self.adaptive_chunks.append(chunk_info)
                    chunk_id += 1

        # –í—ã—á–∏—Å–ª—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ chunk'–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
        self._compute_neighbor_chunks()
        self._optimize_chunk_parameters()

        self.performance_stats["total_chunks"] = len(self.adaptive_chunks)

    def _calculate_optimal_chunk_size(self, available_memory_mb: float) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä chunk'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        total_cells = np.prod(self.dimensions)

        # –¶–µ–ª–µ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ chunk (75% –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π)
        target_memory_per_chunk_mb = (
            available_memory_mb * 0.75 / self.config.get("max_chunks_in_memory", 4)
        )

        # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –Ω–∞ –∫–ª–µ—Ç–∫—É
        memory_per_cell_bytes = 64  # —Å–æ—Å—Ç–æ—è–Ω–∏–µ + —Å–æ—Å–µ–¥–∏ + –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã
        cells_per_chunk = int(
            target_memory_per_chunk_mb * 1024**2 / memory_per_cell_bytes
        )

        # –ö—É–±–∏—á–µ—Å–∫–∏–π –∫–æ—Ä–µ–Ω—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ chunk'–∞
        if cells_per_chunk <= 0:
            chunk_size = max(self.dimensions) // 8  # fallback
        else:
            chunk_size = max(8, int(cells_per_chunk ** (1 / 3)))

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
        max_chunk_size = max(self.dimensions) // 2
        min_chunk_size = 8

        optimal_size = max(min_chunk_size, min(chunk_size, max_chunk_size))

        logger.debug(
            f"üìè Optimal chunk size: {optimal_size} "
            f"(available_memory={available_memory_mb:.1f}MB, cells_per_chunk={cells_per_chunk})"
        )

        return optimal_size

    def _create_adaptive_chunk_info(
        self,
        chunk_id: int,
        start: Coordinates3D,
        end: Coordinates3D,
        available_memory_mb: float,
    ) -> AdaptiveChunkInfo:
        """–°–æ–∑–¥–∞–µ—Ç adaptive chunk info —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–ª–µ—Ç–∫–∏ –≤ chunk'–µ
        cell_indices = []
        for z in range(start[2], end[2]):
            for y in range(start[1], end[1]):
                for x in range(start[0], end[0]):
                    cell_idx = self.pos_helper.to_linear_index((x, y, z))
                    cell_indices.append(cell_idx)

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        num_cells = len(cell_indices)
        memory_size_mb = num_cells * 64 / (1024**2)  # –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

        # –°–æ–∑–¥–∞–µ–º adaptive chunk info
        chunk_info = AdaptiveChunkInfo(
            chunk_id=chunk_id,
            start_coords=start,
            end_coords=end,
            cell_indices=cell_indices,
            neighbor_chunks=[],  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
            memory_size_mb=memory_size_mb,
            # Adaptive –ø–æ–ª—è
            gpu_memory_usage_mb=0.0,
            last_access_time=time.time(),
            access_frequency=0,
            processing_priority=self._calculate_initial_priority(start, end),
            optimal_batch_size=min(1000, num_cells),
            preferred_device=self.device.type,
            memory_pressure_level=min(1.0, memory_size_mb / available_memory_mb),
        )

        return chunk_info

    def _calculate_initial_priority(
        self, start: Coordinates3D, end: Coordinates3D
    ) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç chunk'–∞"""
        # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ chunk'–∏ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        center = tuple(d // 2 for d in self.dimensions)
        chunk_center = tuple((s + e) // 2 for s, e in zip(start, end))

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç —Ü–µ–Ω—Ç—Ä–∞ —Ä–µ—à–µ—Ç–∫–∏
        distance = sum((c1 - c2) ** 2 for c1, c2 in zip(center, chunk_center)) ** 0.5
        max_distance = sum(d**2 for d in self.dimensions) ** 0.5

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç 1 –¥–æ 100 (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ - –≤—ã—à–µ)
        priority = int(100 * (1 - distance / max_distance))

        return max(1, priority)

    def _compute_neighbor_chunks(self):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ chunk'–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ chunk'–∞"""
        overlap = self.config.get("chunk_overlap", 8)

        for chunk in self.adaptive_chunks:
            neighbor_chunk_ids = []

            for other_chunk in self.adaptive_chunks:
                if chunk.chunk_id != other_chunk.chunk_id:
                    if self._are_chunks_neighbors(chunk, other_chunk, overlap):
                        neighbor_chunk_ids.append(other_chunk.chunk_id)

            chunk.neighbor_chunks = neighbor_chunk_ids

    def _are_chunks_neighbors(
        self, chunk1: AdaptiveChunkInfo, chunk2: AdaptiveChunkInfo, overlap: int
    ) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è—é—Ç—Å—è –ª–∏ chunk'–∏ —Å–æ—Å–µ–¥–Ω–∏–º–∏"""
        # –†–∞—Å—à–∏—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã chunk1 –Ω–∞ overlap
        start1 = tuple(max(0, s - overlap) for s in chunk1.start_coords)
        end1 = tuple(
            min(d, e + overlap) for d, e in zip(self.dimensions, chunk1.end_coords)
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å chunk2
        return all(
            s1 < chunk2.end_coords[i] and e1 > chunk2.start_coords[i]
            for i, (s1, e1) in enumerate(zip(start1, end1))
        )

    def _optimize_chunk_parameters(self):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunk'–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        for chunk in self.adaptive_chunks:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º batch size –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ chunk'–∞
            num_cells = len(chunk.cell_indices)

            if num_cells < 100:
                chunk.optimal_batch_size = num_cells
            elif num_cells < 10000:
                chunk.optimal_batch_size = num_cells // 4
            else:
                chunk.optimal_batch_size = 2500  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—É–º–Ω—ã–π —Ä–∞–∑–º–µ—Ä

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ memory pressure
            if chunk.memory_pressure_level > 0.8:
                chunk.processing_priority = max(1, chunk.processing_priority - 20)
            elif chunk.memory_pressure_level < 0.3:
                chunk.processing_priority = min(100, chunk.processing_priority + 10)

    def get_chunk_by_coords(self, coords: Coordinates3D) -> AdaptiveChunkInfo:
        """–ù–∞—Ö–æ–¥–∏—Ç chunk –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ—Å—Ç—É–ø–∞"""
        for chunk in self.adaptive_chunks:
            if all(
                chunk.start_coords[i] <= coords[i] < chunk.end_coords[i]
                for i in range(3)
            ):
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ—Å—Ç—É–ø–∞
                chunk.last_access_time = time.time()
                chunk.access_frequency += 1

                return chunk

        raise ValueError(f"Chunk –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç {coords}")

    def get_adaptive_processing_schedule(self) -> List[List[int]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç adaptive —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk'–æ–≤

        –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        """
        available_memory = self.memory_predictor.get_available_memory_mb()
        max_concurrent = self.config.get("max_chunks_in_memory", 4)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º chunk'–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ memory pressure
        sorted_chunks = sorted(
            self.adaptive_chunks,
            key=lambda c: (c.processing_priority, -c.memory_pressure_level),
            reverse=True,
        )

        schedule = []
        remaining_chunks = set(c.chunk_id for c in sorted_chunks)

        while remaining_chunks:
            current_batch = []
            current_memory_usage = 0.0
            used_neighbors = set()

            for chunk in sorted_chunks:
                if chunk.chunk_id not in remaining_chunks:
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                estimated_memory = chunk.memory_size_mb
                conflicts = set(chunk.neighbor_chunks) & used_neighbors

                can_add = (
                    not conflicts
                    and len(current_batch) < max_concurrent
                    and current_memory_usage + estimated_memory
                    <= available_memory * 0.9
                )

                if can_add:
                    current_batch.append(chunk.chunk_id)
                    current_memory_usage += estimated_memory
                    used_neighbors.add(chunk.chunk_id)
                    used_neighbors.update(chunk.neighbor_chunks)

            # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ chunk'–∏
            for chunk_id in current_batch:
                remaining_chunks.remove(chunk_id)

            if current_batch:
                schedule.append(current_batch)
            else:
                # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ chunk'–∞, –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
                if remaining_chunks:
                    first_chunk = next(iter(remaining_chunks))
                    schedule.append([first_chunk])
                    remaining_chunks.remove(first_chunk)

        logger.debug(
            f"üìÖ Adaptive schedule —Å–æ–∑–¥–∞–Ω–æ: {len(schedule)} batches, "
            f"avg_batch_size={np.mean([len(b) for b in schedule]):.1f}"
        )

        return schedule

    def process_chunk_async(
        self, chunk_id: int, operation: str, callback: Optional[callable] = None
    ) -> Future:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ chunk'–∞ —á–µ—Ä–µ–∑ scheduler"""
        chunk_info = self.adaptive_chunks[chunk_id]

        task = ChunkProcessingTask(
            chunk_id=chunk_id,
            operation_type=operation,
            priority=chunk_info.processing_priority,
            estimated_memory_mb=chunk_info.memory_size_mb,
            dependencies=chunk_info.neighbor_chunks[
                :2
            ],  # –ø–µ—Ä–≤—ã–µ 2 —Å–æ—Å–µ–¥–∞ –∫–∞–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            callback=callback,
        )

        return self.chunk_scheduler.schedule_task(task)

    def rebalance_chunks(self):
        """–ü–µ—Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ chunk'–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        current_memory = self.memory_predictor.get_available_memory_mb()

        # –ù–∞—Ö–æ–¥–∏–º chunk'–∏ —Å –≤—ã—Å–æ–∫–∏–º memory pressure
        high_pressure_chunks = [
            c for c in self.adaptive_chunks if c.memory_pressure_level > 0.8
        ]

        if high_pressure_chunks:
            logger.info(
                f"üîÑ Rebalancing {len(high_pressure_chunks)} high-pressure chunks"
            )

            # –ü–æ–Ω–∏–∂–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç chunk'–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º –¥–∞–≤–ª–µ–Ω–∏–µ–º
            for chunk in high_pressure_chunks:
                chunk.processing_priority = max(1, chunk.processing_priority - 10)
                chunk.optimal_batch_size = max(100, chunk.optimal_batch_size // 2)

            self.performance_stats["adaptive_rebalancing_events"] += 1

    def get_memory_stats(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        total_chunks_memory = sum(c.memory_size_mb for c in self.adaptive_chunks)
        active_chunks_memory = sum(
            c.gpu_memory_usage_mb
            for c in self.adaptive_chunks
            if c.gpu_memory_usage_mb > 0
        )

        device_stats = self.device_manager.get_memory_stats()

        return {
            "total_chunks_memory_mb": total_chunks_memory,
            "active_chunks_memory_mb": active_chunks_memory,
            "memory_efficiency": active_chunks_memory / max(1, total_chunks_memory),
            "available_memory_mb": self.memory_predictor.get_available_memory_mb(),
            "device_stats": device_stats,
        }

    def get_comprehensive_stats(self) -> Dict[str, any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É adaptive chunker'–∞"""
        memory_stats = self.get_memory_stats()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ chunk'–∞–º
        chunk_stats = {
            "total_chunks": len(self.adaptive_chunks),
            "avg_chunk_size": np.mean(
                [len(c.cell_indices) for c in self.adaptive_chunks]
            ),
            "avg_memory_usage_mb": np.mean(
                [c.memory_size_mb for c in self.adaptive_chunks]
            ),
            "avg_access_frequency": np.mean(
                [c.access_frequency for c in self.adaptive_chunks]
            ),
            "high_pressure_chunks": len(
                [c for c in self.adaptive_chunks if c.memory_pressure_level > 0.8]
            ),
        }

        return {
            "performance": self.performance_stats,
            "memory": memory_stats,
            "chunks": chunk_stats,
            "scheduler": {
                "active_chunks": len(self.chunk_scheduler.active_chunks),
                "queue_size": self.chunk_scheduler.task_queue.qsize(),
            },
        }

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.chunk_scheduler.shutdown()
        self.memory_manager.cleanup()

        logger.info("üßπ AdaptiveGPUChunker –æ—á–∏—â–µ–Ω")
