#!/usr/bin/env python3
"""
–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏ –∏ –ø–∞–º—è—Ç—å—é
===============================================

DeviceManager –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
1. –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU/CPU –≤–æ –≤—Å–µ—Ö –º–æ–¥—É–ª—è—Ö
2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ memory leaks
4. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏
"""

import torch
import gc
from .logging import get_logger
from typing import Tuple, Optional, Dict, Any, Union
from pathlib import Path
import psutil

logger = get_logger(__name__)


class MemoryMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –ø–∞–º—è—Ç–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU/CPU –ø–∞–º—è—Ç–∏"""

    def __init__(self, device: torch.device):
        self.device = device
        self.allocation_count = 0
        self.peak_memory_mb = 0.0
        self.cleanup_threshold = 100  # GC –∫–∞–∂–¥—ã–µ 100 –æ–ø–µ—Ä–∞—Ü–∏–π

    def can_allocate(self, required_memory_bytes: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏

        Args:
            required_memory_bytes: –¢—Ä–µ–±—É–µ–º—ã–π –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏ –≤ –±–∞–π—Ç–∞—Ö

        Returns:
            True –µ—Å–ª–∏ –ø–∞–º—è—Ç—å –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å
        """
        if self.device.type == "cuda":
            available_memory = torch.cuda.get_device_properties(
                self.device
            ).total_memory
            allocated_memory = torch.cuda.memory_allocated(self.device)
            free_memory = available_memory - allocated_memory

            # –û—Å—Ç–∞–≤–ª—è–µ–º 15% –±—É—Ñ–µ—Ä
            safe_memory = free_memory * 0.85
            return required_memory_bytes < safe_memory
        else:
            # CPU memory check
            available_memory = psutil.virtual_memory().available
            safe_memory = available_memory * 0.85
            return required_memory_bytes < safe_memory

    def cleanup(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        gc.collect()

        if self.device.type == "cuda" and torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            except Exception:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã
                logger.debug_memory(
                    f"‚ö†Ô∏è –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã"
                )   
                pass

        logger.debug_memory(f"üßπ Memory cleanup –≤—ã–ø–æ–ª–Ω–µ–Ω –¥–ª—è {self.device}")

    def get_memory_stats(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        stats = {
            "device": str(self.device),
            "allocation_count": self.allocation_count,
            "peak_memory_mb": self.peak_memory_mb,
        }

        if self.device.type == "cuda":
            stats.update(
                {
                    "allocated_mb": torch.cuda.memory_allocated(self.device)
                    / (1024**2),
                    "reserved_mb": torch.cuda.memory_reserved(self.device) / (1024**2),
                    "max_allocated_mb": torch.cuda.max_memory_allocated(self.device)
                    / (1024**2),
                }
            )
        else:
            memory_info = psutil.virtual_memory()
            stats.update(
                {
                    "total_mb": memory_info.total / (1024**2),
                    "available_mb": memory_info.available / (1024**2),
                    "used_mb": memory_info.used / (1024**2),
                    "percent": memory_info.percent,
                }
            )

        return stats


class DeviceManager:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏ –∏ –ø–∞–º—è—Ç—å—é"""

    def __init__(self, prefer_cuda: bool = True, debug_mode: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeviceManager

        Args:
            prefer_cuda: –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å CUDA –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            debug_mode: –í–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –∏–∑ config.logging.debug_mode)
        """
        self.prefer_cuda = prefer_cuda
        self.debug_mode = debug_mode
        self.device = self._detect_optimal_device(prefer_cuda)
        self.memory_monitor = MemoryMonitor(self.device)

        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.tensor_transfers = 0
        self.allocations = 0

        if debug_mode:
            self._log_device_info()

    def _detect_optimal_device(self, prefer_cuda: bool) -> torch.device:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
        –°–¢–†–û–ì–ê–Ø –ü–û–õ–ò–¢–ò–ö–ê: –ë–µ–∑ fallback –Ω–∞ CPU –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏ GPU

        Args:
            prefer_cuda: –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å CUDA

        Returns:
            –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ torch.device
            
        Raises:
            RuntimeError: –ü—Ä–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏ GPU –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö
        """
        if prefer_cuda:
            if not torch.cuda.is_available():
                raise RuntimeError(
                    "‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ó–∞–ø—Ä–æ—à–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CUDA, –Ω–æ CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. "
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA –∏–ª–∏ —è–≤–Ω–æ —É–∫–∞–∂–∏—Ç–µ prefer_cuda=False –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"
                )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            device_count = torch.cuda.device_count()
            if device_count == 0:
                raise RuntimeError(
                    "‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: CUDA –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ GPU"
                )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å GPU
            device_properties = torch.cuda.get_device_properties(0)
            gpu_memory_gb = device_properties.total_memory / (1024**3)

            if gpu_memory_gb < 8.0:  # –ú–∏–Ω–∏–º—É–º 8GB –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã
                raise RuntimeError(
                    f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU "
                    f"({gpu_memory_gb:.1f}GB < 8GB –º–∏–Ω–∏–º—É–º). "
                    f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –ø–∞–º—è—Ç–∏ –∏–ª–∏ —è–≤–Ω–æ —É–∫–∞–∂–∏—Ç–µ prefer_cuda=False"
                )
            
            device = torch.device("cuda:0")
            if self.debug_mode:
                logger.info(
                    f"‚úÖ CUDA device selected: {torch.cuda.get_device_name(0)} ({gpu_memory_gb:.1f}GB)"
                )
            return device
        else:
            # CPU —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
            if self.debug_mode:
                logger.info("üíª CPU –≤—ã–±—Ä–∞–Ω –ø–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return torch.device("cpu")

    def _log_device_info(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ"""
        logger.info(f"üñ•Ô∏è DeviceManager initialized:")
        logger.info(f"   Device: {self.device}")

        if self.device.type == "cuda":
            try:
                props = torch.cuda.get_device_properties(self.device)
                logger.info(f"   GPU: {torch.cuda.get_device_name(self.device)}")
                logger.info(f"   Memory: {props.total_memory / (1024**3):.1f}GB")
                logger.info(f"   Compute Capability: {props.major}.{props.minor}")
            except (RuntimeError, AssertionError) as e:
                logger.warning(f"   ‚ö†Ô∏è Error getting GPU info: {str(e)}")
        else:
            try:
                memory_info = psutil.virtual_memory()
                logger.info(
                    f"   CPU Memory: {memory_info.total / (1024**3):.1f}GB total"
                )
                logger.info(f"   Available: {memory_info.available / (1024**3):.1f}GB")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ CPU: {str(e)}")

    def get_available_memory_gb(self) -> float:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å –≤ –ì–ë –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.

        Returns:
            –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å –≤ –ì–ë.
        """
        if self.device.type == "cuda":
            try:
                total_memory = torch.cuda.get_device_properties(
                    self.device
                ).total_memory
                reserved_memory = torch.cuda.memory_reserved(self.device)
                available_memory_bytes = total_memory - reserved_memory
                return available_memory_bytes / (1024**3)
            except (RuntimeError, AssertionError) as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–∞–º—è—Ç—å GPU: {e}")
                return 0.0
        else:
            # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º psutil
            try:
                return psutil.virtual_memory().available / (1024**3)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–∞–º—è—Ç—å CPU: {e}")
                return 0.0

    def ensure_device(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç tensor –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

        Args:
            tensor: –ò—Å—Ö–æ–¥–Ω—ã–π tensor

        Returns:
            Tensor –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        """
        if tensor.device != self.device:
            self.tensor_transfers += 1
            if self.debug_mode and self.tensor_transfers <= 5:
                logger.debug_memory(
                    f"üîÑ –ü–µ—Ä–µ–Ω–æ—Å tensor {tensor.shape} —Å {tensor.device} –Ω–∞ {self.device}"
                )

            return tensor.to(self.device, non_blocking=True)

        return tensor

    def allocate_tensor(
        self,
        shape: Tuple[int, ...],
        dtype: torch.dtype = torch.float32,
        requires_grad: bool = False,
    ) -> torch.Tensor:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ tensor'–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–∞–º—è—Ç–∏

        Args:
            shape: –§–æ—Ä–º–∞ tensor'–∞
            dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö
            requires_grad: –¢—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç

        Returns:
            –ù–æ–≤—ã–π tensor –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        """
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç—Ä–µ–±—É–µ–º—É—é –ø–∞–º—è—Ç—å
        element_size = torch.tensor([], dtype=dtype).element_size()
        required_memory = torch.tensor(shape).prod().item() * element_size

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏
        if not self.memory_monitor.can_allocate(required_memory):
            logger.warning(
                f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è tensor {shape}, –≤—ã–ø–æ–ª–Ω—è–µ–º cleanup"
            )
            self.memory_monitor.cleanup()

            # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ cleanup
            if not self.memory_monitor.can_allocate(required_memory):
                raise RuntimeError(
                    f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å {required_memory / (1024**2):.1f}MB –ø–∞–º—è—Ç–∏ –¥–ª—è tensor {shape}"
                )

        # –°–æ–∑–¥–∞–µ–º tensor
        tensor = torch.zeros(
            shape, device=self.device, dtype=dtype, requires_grad=requires_grad
        )

        self.allocations += 1
        self.memory_monitor.allocation_count += 1

        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π cleanup
        if (
            self.memory_monitor.allocation_count % self.memory_monitor.cleanup_threshold
            == 0
        ):
            self.memory_monitor.cleanup()

        return tensor

    def allocate_like(self, reference_tensor: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        –°–æ–∑–¥–∞–µ—Ç tensor –ø–æ—Ö–æ–∂–∏–π –Ω–∞ reference_tensor –Ω–æ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ

        Args:
            reference_tensor: –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π tensor
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (dtype, requires_grad, etc.)

        Returns:
            –ù–æ–≤—ã–π tensor –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        """
        dtype = kwargs.get("dtype", reference_tensor.dtype)
        requires_grad = kwargs.get("requires_grad", reference_tensor.requires_grad)

        return self.allocate_tensor(
            shape=reference_tensor.shape, dtype=dtype, requires_grad=requires_grad
        )

    def transfer_module(self, module: torch.nn.Module) -> torch.nn.Module:
        """
        –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

        Args:
            module: PyTorch –º–æ–¥–µ–ª—å

        Returns:
            –ú–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        """
        if self.debug_mode:
            param_count = sum(p.numel() for p in module.parameters())
            logger.info(
                f"üîÑ –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ {self.device} ({param_count:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)"
            )

        return module.to(self.device)

    def get_device(self) -> torch.device:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        return self.device

    def get_device_str(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        return str(self.device)

    def is_cuda(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ CUDA."""
        return self.device.type == "cuda"

    def synchronize(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (–≤–∞–∂–Ω–æ –¥–ª—è CUDA)"""
        if self.device.type == "cuda":
            torch.cuda.synchronize(self.device)

    def get_memory_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        stats = self.memory_monitor.get_memory_stats()
        stats.update(
            {
                "tensor_transfers": self.tensor_transfers,
                "total_allocations": self.allocations,
                "device_type": self.device.type,
                "prefer_cuda": self.prefer_cuda,
            }
        )

        return stats

    def cleanup(self):
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        try:
            if hasattr(self, "memory_monitor") and self.memory_monitor:
                self.memory_monitor.cleanup()

            if hasattr(self, "debug_mode") and self.debug_mode:
                stats = self.get_memory_stats()
                logger.info(
                    f"üßπ DeviceManager cleanup: {stats['tensor_transfers']} –ø–µ—Ä–µ–Ω–æ—Å–æ–≤, {stats['total_allocations']} –≤—ã–¥–µ–ª–µ–Ω–∏–π"
                )
        except Exception:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã
            pass

    def __del__(self):
        """Cleanup –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞"""
        if hasattr(self, "memory_monitor"):
            self.cleanup()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä DeviceManager (singleton pattern)
_global_device_manager: Optional[DeviceManager] = None


def get_device_manager(
    prefer_cuda: bool = True, debug_mode: bool = True
) -> DeviceManager:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π instance DeviceManager (singleton pattern)

    Args:
        prefer_cuda: –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å CUDA –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        debug_mode: –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ (–ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –∏–∑ config.logging.debug_mode)

    Returns:
        DeviceManager instance
    """
    global _global_device_manager

    if _global_device_manager is None:
        _global_device_manager = DeviceManager(
            prefer_cuda=prefer_cuda, debug_mode=debug_mode
        )

    return _global_device_manager


def reset_device_manager():
    """–°–±—Ä–æ—Å –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ DeviceManager (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)"""
    global _global_device_manager

    if _global_device_manager is not None:
        _global_device_manager.cleanup()
        _global_device_manager = None


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
def ensure_device(tensor: torch.Tensor) -> torch.Tensor:
    """–ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ ensure_device"""
    return get_device_manager().ensure_device(tensor)


def allocate_tensor(shape: Tuple[int, ...], **kwargs) -> torch.Tensor:
    """–ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ allocate_tensor"""
    return get_device_manager().allocate_tensor(shape, **kwargs)


def transfer_module(module: torch.nn.Module) -> torch.nn.Module:
    """–ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ transfer_module"""
    return get_device_manager().transfer_module(module)


def get_optimal_device() -> torch.device:
    """–ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É"""
    return get_device_manager().get_device()


def cleanup_memory():
    """–ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ cleanup"""
    get_device_manager().cleanup()
