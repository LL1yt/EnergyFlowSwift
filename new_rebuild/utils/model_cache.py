#!/usr/bin/env python3
"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ª–æ–∫–∞–ª—å–Ω–æ
=======================================

–£–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 5090 —Å –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π —Å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞.
"""

import os
import shutil
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
import hashlib
import json

from .logging import get_logger
from ..config.simple_config import SimpleProjectConfig

logger = get_logger(__name__)


class ModelCacheManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π

    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±—ã—Å—Ç—Ä—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π —Å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞
    –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
    """

    def __init__(self, config: SimpleProjectConfig):
        self.config = config
        self.logger = get_logger(__name__)

        # –ü—É—Ç–∏
        self.local_models_dir = Path(config.embedding.local_models_dir)
        self.auto_download = config.embedding.auto_download_models
        self.prefer_local = config.embedding.prefer_local_models

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.local_models_dir.mkdir(parents=True, exist_ok=True)

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∞
        self.cache_metadata_file = self.local_models_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()

        self.logger.info(f"üóÑÔ∏è ModelCacheManager initialized: {self.local_models_dir}")

    def _load_metadata(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞"""
        if self.cache_metadata_file.exists():
            try:
                with open(self.cache_metadata_file, "r") as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"Failed to load cache metadata: {e}")

        return {"models": {}, "created": None, "version": "1.0"}

    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞"""
        try:
            with open(self.cache_metadata_file, "w") as f:
                json.dump(self.metadata, f, indent=2)
        except Exception as e:
            self.logger.warning(f"Failed to save cache metadata: {e}")

    def _get_model_path(self, model_name: str) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        # –û—á–∏—â–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—É—Ç–∏
        clean_name = model_name.replace("/", "_").replace("-", "_")
        return self.local_models_dir / clean_name

    def is_model_cached(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏ –≤ –∫—ç—à–µ"""
        model_path = self._get_model_path(model_name)

        self.logger.debug(f"üîç Checking cache for '{model_name}' at: {model_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        required_files = ["config.json"]

        if model_name.startswith("distilbert"):
            required_files.extend(["tokenizer.json"])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –≤ –ª—é–±–æ–º –∏–∑ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            model_files = ["pytorch_model.bin", "model.safetensors"]

        missing_files = []
        for file in required_files:
            file_path = model_path / file
            if not file_path.exists():
                missing_files.append(file)
                self.logger.debug(f"  ‚ùå Missing: {file}")
            else:
                self.logger.debug(f"  ‚úÖ Found: {file}")

        # –î–ª—è distilbert –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏ –≤ –ª—é–±–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        if model_name.startswith("distilbert"):
            model_file_found = False
            for model_file in model_files:
                model_file_path = model_path / model_file
                if model_file_path.exists():
                    self.logger.debug(f"  ‚úÖ Found model file: {model_file}")
                    model_file_found = True
                    break
                else:
                    self.logger.debug(f"  ‚ùå Missing model file: {model_file}")

            if not model_file_found:
                missing_files.append(
                    "model file (pytorch_model.bin or model.safetensors)"
                )

        is_cached = len(missing_files) == 0

        if is_cached:
            self.logger.debug(f"‚úÖ Model '{model_name}' is cached at: {model_path}")
        else:
            self.logger.debug(
                f"‚ùå Model '{model_name}' not cached. Missing files: {missing_files}"
            )

        return is_cached

    def get_model_path(self, model_name: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è.

        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'distilbert-base-uncased')

        Returns:
            –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        """
        self.logger.debug(f"üîç get_model_path called for '{model_name}'")
        self.logger.debug(f"  prefer_local: {self.prefer_local}")
        self.logger.debug(f"  auto_download: {self.auto_download}")

        local_path = self._get_model_path(model_name)
        is_cached = self.is_model_cached(model_name)

        self.logger.debug(f"  local_path: {local_path}")
        self.logger.debug(f"  is_cached: {is_cached}")

        # 1. –ï—Å–ª–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
        if self.prefer_local:
            if is_cached:
                self.logger.info(f"üìÅ Using cached model: {local_path}")
                return str(local_path)

            # –õ–æ–∫–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞, –Ω–æ –µ—ë –Ω–µ—Ç. –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–∞—á–∞—Ç—å.
            if self.auto_download:
                self.logger.info(
                    f"üîÑ Model '{model_name}' not in cache, attempting to download."
                )
                if self._download_model(model_name):
                    self.logger.info(
                        f"‚úÖ Download successful. Now using cached model: {local_path}"
                    )
                    return str(local_path)
                else:
                    self.logger.warning(
                        f"‚ö†Ô∏è Failed to download '{model_name}'. Falling back to online version."
                    )
            else:
                self.logger.warning(
                    f"Local model preferred but not found, and auto-download is off. Falling back to online."
                )

        # 2. –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –Ω–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞ –∏–ª–∏ —Å–∫–∞—á–∞—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–Ω–ª–∞–π–Ω
        self.logger.info(f"üåê Using online model: {model_name}")
        return model_name

    def _download_model(self, model_name: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à

        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        try:
            from transformers import AutoModel, AutoTokenizer

            local_path = self._get_model_path(model_name)
            local_path.mkdir(parents=True, exist_ok=True)

            self.logger.info(f"üì• Downloading {model_name} to {local_path}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.logger.info("  Downloading tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer.save_pretrained(str(local_path))

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.logger.info("  Downloading model...")
            model = AutoModel.from_pretrained(model_name)
            model.save_pretrained(str(local_path))

            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.metadata["models"][model_name] = {
                "path": str(local_path),
                "downloaded_at": None,  # timestamp
                "size_mb": self._get_directory_size_mb(local_path),
            }
            self._save_metadata()

            self.logger.info(f"‚úÖ Successfully cached {model_name}")
            return True

        except ImportError:
            self.logger.error("transformers library not available for model download")
            return False
        except Exception as e:
            self.logger.error(f"Failed to download {model_name}: {e}")
            return False

    def _get_directory_size_mb(self, path: Path) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ MB"""
        total_size = 0
        try:
            for file in path.rglob("*"):
                if file.is_file():
                    total_size += file.stat().st_size
            return total_size / (1024 * 1024)
        except Exception:
            return 0.0

    def download_distilbert(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ DistilBERT (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å legacy)"""
        return self._download_model("distilbert-base-uncased")

    def clear_cache(self, model_name: Optional[str] = None):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        if model_name:
            model_path = self._get_model_path(model_name)
            if model_path.exists():
                shutil.rmtree(model_path)
                self.metadata["models"].pop(model_name, None)
                self._save_metadata()
                self.logger.info(f"üóëÔ∏è Cleared cache for {model_name}")
        else:
            # –û—á–∏—â–∞–µ–º –≤–µ—Å—å –∫—ç—à
            for path in self.local_models_dir.iterdir():
                if path.is_dir() and path.name != "__pycache__":
                    shutil.rmtree(path)
            self.metadata["models"] = {}
            self._save_metadata()
            self.logger.info("üóëÔ∏è Cleared all model cache")

    def get_cache_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ –º–æ–¥–µ–ª–µ–π"""
        total_size = 0
        model_count = 0

        for model_name, info in self.metadata["models"].items():
            if "size_mb" in info:
                total_size += info["size_mb"]
                model_count += 1

        return {
            "models_count": model_count,
            "total_size_mb": total_size,
            "cache_dir": str(self.local_models_dir),
            "models": list(self.metadata["models"].keys()),
        }

    def verify_model_integrity(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        if not self.is_model_cached(model_name):
            return False

        model_path = self._get_model_path(model_name)

        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            from transformers import AutoModel, AutoTokenizer

            tokenizer = AutoTokenizer.from_pretrained(str(model_path))
            model = AutoModel.from_pretrained(str(model_path))

            self.logger.info(f"‚úÖ Model {model_name} integrity verified")
            return True

        except Exception as e:
            self.logger.warning(f"‚ùå Model {model_name} integrity check failed: {e}")
            return False


# === –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–† ===

_global_cache_manager: Optional[ModelCacheManager] = None


def get_model_cache_manager(
    config: Optional[SimpleProjectConfig] = None,
) -> ModelCacheManager:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π"""
    global _global_cache_manager

    if _global_cache_manager is None:
        if config is None:
            from ..config.simple_config import get_project_config

            config = get_project_config()
        logger.debug("üîß Creating new global ModelCacheManager")
        _global_cache_manager = ModelCacheManager(config)
    else:
        logger.debug("‚ôªÔ∏è Reusing existing global ModelCacheManager")

    return _global_cache_manager


# === LEGACY COMPATIBILITY ===


def download_distilbert() -> Optional[str]:
    """Legacy —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ DistilBERT"""
    manager = get_model_cache_manager()
    if manager.download_distilbert():
        return str(manager._get_model_path("distilbert-base-uncased"))
    return None


def get_distilbert_path() -> Optional[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ DistilBERT (–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏)"""
    manager = get_model_cache_manager()
    return manager.get_model_path("distilbert-base-uncased")


# === UTILITY FUNCTIONS ===


def setup_model_cache(models: list = None) -> Dict[str, bool]:
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π

    Args:
        models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ['distilbert-base-uncased']

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏
    """
    if models is None:
        models = ["distilbert-base-uncased"]

    manager = get_model_cache_manager()
    results = {}

    for model in models:
        logger.info(f"üîÑ Setting up cache for {model}")
        results[model] = manager._download_model(model)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    cache_info = manager.get_cache_info()
    logger.info(f"üìä Cache setup complete:")
    logger.info(f"  Models: {cache_info['models_count']}")
    logger.info(f"  Total size: {cache_info['total_size_mb']:.1f} MB")

    return results


def check_model_cache_status() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π"""
    manager = get_model_cache_manager()
    info = manager.get_cache_info()

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
    info["model_status"] = {}
    for model in ["distilbert-base-uncased"]:
        info["model_status"][model] = {
            "cached": manager.is_model_cached(model),
            "path": manager.get_model_path(model),
        }

    return info
