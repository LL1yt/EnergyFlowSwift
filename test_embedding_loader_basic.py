#!/usr/bin/env python3
"""
–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç –º–æ–¥—É–ª—è embedding_loader
–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.
"""

import os
import torch
import numpy as np
from pathlib import Path

# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
def create_test_embeddings():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    test_dir = Path("./data/embeddings/test/")
    test_dir.mkdir(parents=True, exist_ok=True)
    
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤...")
    
    # 1. –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π GloVe-like —Ñ–∞–π–ª
    glove_path = test_dir / "test_glove.txt"
    with open(glove_path, 'w', encoding='utf-8') as f:
        # –°–æ–∑–¥–∞–µ–º 10 –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª–æ–≤ —Å 5-–º–µ—Ä–Ω—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏
        words = ['the', 'cat', 'dog', 'run', 'jump', 'happy', 'sad', 'big', 'small', 'good']
        for i, word in enumerate(words):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ, –Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            np.random.seed(i)
            vector = np.random.normal(0, 1, 5)
            vector_str = ' '.join([f"{v:.6f}" for v in vector])
            f.write(f"{word} {vector_str}\n")
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π GloVe —Ñ–∞–π–ª: {glove_path}")
    
    # 2. –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π Word2Vec-like —Ñ–∞–π–ª (—Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º)
    w2v_path = test_dir / "test_word2vec.txt"
    with open(w2v_path, 'w', encoding='utf-8') as f:
        f.write("10 5\n")  # vocab_size embedding_dim
        for i, word in enumerate(words):
            np.random.seed(i + 100)  # –î—Ä—É–≥–∏–µ —Å–µ–º–µ–Ω–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            vector = np.random.normal(0, 1, 5)
            vector_str = ' '.join([f"{v:.6f}" for v in vector])
            f.write(f"{word} {vector_str}\n")
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π Word2Vec —Ñ–∞–π–ª: {w2v_path}")
    
    # 3. –°–æ–∑–¥–∞–µ–º BERT-like PyTorch —Ñ–∞–π–ª
    bert_path = test_dir / "test_bert.pt"
    # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º [10, 8] (10 —Ç–æ–∫–µ–Ω–æ–≤, 8-–º–µ—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã)
    np.random.seed(42)
    bert_embeddings = torch.randn(10, 8)
    torch.save(bert_embeddings, bert_path)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π BERT —Ñ–∞–π–ª: {bert_path}")
    
    return {
        'glove': str(glove_path),
        'word2vec': str(w2v_path),
        'bert': str(bert_path)
    }

def test_embedding_loader():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç –º–æ–¥—É–ª—è embedding_loader."""
    
    print("="*60)
    print("–¢–ï–°–¢ –ú–û–î–£–õ–Ø EMBEDDING_LOADER")
    print("="*60)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_files = create_test_embeddings()
    
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à –º–æ–¥—É–ª—å
        from data.embedding_loader import EmbeddingLoader, EmbeddingPreprocessor
        
        print("\n‚úÖ –ú–æ–¥—É–ª—å —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
        loader = EmbeddingLoader(cache_dir="./data/cache/test/")
        print("‚úÖ EmbeddingLoader –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ GloVe
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 1: –ó–∞–≥—Ä—É–∑–∫–∞ GloVe")
        print("-"*40)
        
        glove_embeddings = loader.load_embeddings(
            path=test_files['glove'],
            format_type="glove",
            preprocess=True
        )
        
        print(f"‚úÖ GloVe –∑–∞–≥—Ä—É–∂–µ–Ω: {glove_embeddings.shape}")
        print(f"   –¢–∏–ø: {glove_embeddings.dtype}")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {glove_embeddings.device}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
        assert glove_embeddings.shape == (10, 5), f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {glove_embeddings.shape}"
        print("‚úÖ –†–∞–∑–º–µ—Ä—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
        
        # –¢–µ—Å—Ç 2: –ó–∞–≥—Ä—É–∑–∫–∞ Word2Vec
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 2: –ó–∞–≥—Ä—É–∑–∫–∞ Word2Vec")
        print("-"*40)
        
        w2v_embeddings = loader.load_embeddings(
            path=test_files['word2vec'],
            format_type="word2vec",
            preprocess=True
        )
        
        print(f"‚úÖ Word2Vec –∑–∞–≥—Ä—É–∂–µ–Ω: {w2v_embeddings.shape}")
        assert w2v_embeddings.shape == (10, 5), f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {w2v_embeddings.shape}"
        print("‚úÖ –†–∞–∑–º–µ—Ä—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
        
        # –¢–µ—Å—Ç 3: –ó–∞–≥—Ä—É–∑–∫–∞ BERT
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 3: –ó–∞–≥—Ä—É–∑–∫–∞ BERT")
        print("-"*40)
        
        bert_embeddings = loader.load_embeddings(
            path=test_files['bert'],
            format_type="bert",
            preprocess=True
        )
        
        print(f"‚úÖ BERT –∑–∞–≥—Ä—É–∂–µ–Ω: {bert_embeddings.shape}")
        assert bert_embeddings.shape == (10, 8), f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {bert_embeddings.shape}"
        print("‚úÖ –†–∞–∑–º–µ—Ä—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
        
        # –¢–µ—Å—Ç 4: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 4: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")
        print("-"*40)
        
        preprocessor = EmbeddingPreprocessor()
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        normalized = preprocessor.preprocess(
            glove_embeddings.clone(),
            normalize=True,
            center=False,
            clip_outliers=False
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
        norms = torch.norm(normalized, dim=1)
        assert torch.allclose(norms, torch.ones_like(norms), atol=1e-6), "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
        print("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
        centered = preprocessor.preprocess(
            glove_embeddings.clone(),
            normalize=False,
            center=True,
            clip_outliers=False
        )
        
        mean = centered.mean(dim=0)
        assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-6), "–¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
        print("‚úÖ –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –¢–µ—Å—Ç 5: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 5: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ")
        print("-"*40)
        
        # –ö—ç—à–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        cache_key = "test_glove_cache"
        loader.cache_embeddings(glove_embeddings, cache_key)
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω—ã")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫—ç—à–∞
        cached_embeddings = loader.load_from_cache(cache_key)
        
        if cached_embeddings is not None:
            assert torch.equal(glove_embeddings, cached_embeddings), "–ö—ç—à –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ"
            print("‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print("‚ö†Ô∏è  –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω (–≤–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ —Å –¥–∏—Å–∫–æ–º)")
        
        # –¢–µ—Å—Ç 6: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 6: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
        print("-"*40)
        
        info = loader.get_embedding_info(glove_embeddings)
        print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—É—á–µ–Ω—ã:")
        print(f"   –§–æ—Ä–º–∞: {info['shape']}")
        print(f"   –ü–∞–º—è—Ç—å: {info['memory_mb']:.2f} MB")
        print(f"   –ú–∏–Ω: {info['min_value']:.4f}")
        print(f"   –ú–∞–∫—Å: {info['max_value']:.4f}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {info['mean_value']:.4f}")
        
        # –¢–µ—Å—Ç 7: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        print("\n" + "-"*40)
        print("–¢–ï–°–¢ 7: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã")
        print("-"*40)
        
        formats = loader.get_supported_formats()
        expected_formats = ['word2vec', 'glove', 'bert']
        
        for fmt in expected_formats:
            assert fmt in formats, f"–§–æ—Ä–º–∞—Ç {fmt} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
        
        print(f"‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {formats}")
        
        print("\n" + "="*60)
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –í –¢–ï–°–¢–ê–•: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_embedding_loader()
    exit(0 if success else 1) 