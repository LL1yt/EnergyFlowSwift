#!/usr/bin/env python3
"""
ğŸ¯ System Readiness Validation
Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

ĞŸĞ ĞĞ’Ğ•Ğ Ğ¯Ğ•Ğ¢:
1. LLaMA-3-8B Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
2. 3D Cube processing Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ  
3. End-to-end pipeline Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
4. Mini training Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ learning capability
"""

import sys
import os
sys.path.insert(0, os.path.abspath('.'))

import torch
import time
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_llama_availability():
    """Test LLaMA-3-8B basic functionality"""
    logger.info("ğŸ” Testing LLaMA-3-8B availability...")
    
    try:
        from utils.llm_handler import create_llm_handler
        
        llm = create_llm_handler('llama3-8b-local')  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ„Ğ°Ğ±Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
        embedding = llm.generate_embedding("Test text for validation")
        
        if embedding.shape[-1] == 4096:
            logger.info(f"âœ… LLaMA-3-8B working: {embedding.shape}")
            return True
        else:
            logger.error(f"âŒ Wrong embedding dimension: {embedding.shape}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ LLaMA-3-8B test failed: {e}")
        return False

def test_cube_processing():
    """Test 3D Cube basic initialization"""
    logger.info("ğŸ² Testing 3D Cube initialization...")
    
    try:
        from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
            EmergentCubeTrainer, EmergentTrainingConfig
        )
        
        config = EmergentTrainingConfig()
        config.cube_dimensions = (8, 8, 3)  # Smaller Ğ´Ğ»Ñ validation
        config.enable_nca = True
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        trainer = EmergentCubeTrainer(config, device=device)
        
        # Simple initialization check
        system_info = trainer.get_system_info()
        
        if system_info and 'total_system_params' in system_info:
            params = system_info['total_system_params']
            logger.info(f"âœ… Cube initialization working: {params:,} parameters")
            return True
        else:
            logger.error("âŒ Cube initialization failed - no system info")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Cube initialization test failed: {e}")
        return False

def test_end_to_end_pipeline():
    """Test complete pipeline"""
    logger.info("ğŸ”„ Testing end-to-end pipeline...")
    
    try:
        from utils.llm_handler import create_llm_handler
        from core.universal_adapter.universal_embedding_adapter import UniversalEmbeddingAdapter
        
        # Text â†’ LLaMA
        llm = create_llm_handler('llama3-8b-local')  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ„Ğ°Ğ±Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
        embedding = llm.generate_embedding("What is AI?")
        
        # LLaMA â†’ Adapter â†’ Surface
        adapter = UniversalEmbeddingAdapter(
            input_dim=4096,
            output_dim=225,  # 15*15 = 225
            strategy='hierarchical'
        )
        surface = adapter.forward(embedding.unsqueeze(0))
        
        logger.info(f"âœ… Pipeline working: {embedding.shape} â†’ {surface.shape}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ End-to-end pipeline test failed: {e}")
        return False

def test_mini_training():
    """Test mini training session"""
    logger.info("ğŸš€ Testing mini training session...")
    
    try:
        from training.embedding_trainer.dialogue_dataset import create_dialogue_dataset
        from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
            EmergentCubeTrainer, EmergentTrainingConfig
        )
        from core.universal_adapter.universal_embedding_adapter import UniversalEmbeddingAdapter
        import numpy as np
        
        # Mini dataset
        pairs = [
            {"question": "What is AI?", "answer": "AI is artificial intelligence."},
            {"question": "What is ML?", "answer": "ML is machine learning."}
        ]
        
        dataset = create_dialogue_dataset(
            pairs, teacher_model="llama3-8b-local", 
            cache_embeddings=False, validation_split=0.0
        )
        
        # Mini training
        config = EmergentTrainingConfig()
        # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ - ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ
        config.cube_dimensions = (8, 8, 3)  
        config.epochs = 3
        config.batch_size = 1
        config.learning_rate = 0.001
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        trainer = EmergentCubeTrainer(config, device=device)
        
        # Surface Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²
        surface_size = 8 * 8  # 64
        adapter = UniversalEmbeddingAdapter(4096, surface_size, 'hierarchical')
        adapter = adapter.to(trainer.device)  # ĞŸĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼ adapter Ğ½Ğ° Ñ‚Ğ¾ Ğ¶Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
        
        logger.info(f"ğŸ§ª Test config: cube={config.cube_dimensions}, surface={surface_size}, device={trainer.device}")
        
        losses = []
        for epoch in range(3):
            sample = dataset[0]
            if isinstance(sample, tuple):
                q_emb, a_emb = sample
                
                # ĞŸĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼ ÑĞ¼Ğ±ĞµĞ´Ğ¸Ğ½Ğ³Ğ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ trainer
                q_emb = q_emb.to(trainer.device)
                a_emb = a_emb.to(trainer.device)
                
                q_surface = adapter.forward(q_emb.unsqueeze(0))
                a_surface = adapter.forward(a_emb.unsqueeze(0))
                
                metrics = trainer.train_step(q_surface, a_surface)
                loss_key = 'total_loss' if 'total_loss' in metrics else 'loss'
                losses.append(metrics.get(loss_key, 0.0))
        
        if len(losses) >= 2:
            improvement = (losses[0] - losses[-1]) / losses[0] if losses[0] > 0 else 0
            logger.info(f"âœ… Mini training working: {improvement:.1%} improvement")
            return improvement > 0.05  # 5% minimum improvement
        else:
            logger.error("âŒ Mini training failed")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Mini training test failed: {e}")
        return False

def main():
    """Run all validation tests"""
    logger.info("=" * 60)
    logger.info("ğŸ¯ SYSTEM READINESS VALIDATION")
    logger.info("=" * 60)
    
    tests = [
        ("LLaMA-3-8B Availability", test_llama_availability),
        ("3D Cube Initialization", test_cube_processing), 
        ("End-to-End Pipeline", test_end_to_end_pipeline),
        ("Mini Training", test_mini_training)
    ]
    
    results = {}
    for test_name, test_func in tests:
        logger.info(f"\nğŸ§ª Running: {test_name}")
        start_time = time.time()
        
        success = test_func()
        duration = time.time() - start_time
        
        results[test_name] = {'success': success, 'duration': duration}
        
        status = "âœ… PASSED" if success else "âŒ FAILED"
        logger.info(f"{status} ({duration:.1f}s)")
    
    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š VALIDATION SUMMARY")
    logger.info("=" * 60)
    
    passed = sum(1 for r in results.values() if r['success'])
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ…" if result['success'] else "âŒ"
        logger.info(f"{status} {test_name}: {result['duration']:.1f}s")
    
    logger.info(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("ğŸ‰ SYSTEM READY FOR REAL TRAINING!")
        logger.info("ğŸš€ You can now run: python real_llama_training_production.py")
        return True
    else:
        logger.error("âŒ System not ready - fix failing tests first")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 