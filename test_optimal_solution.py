#!/usr/bin/env python3
"""
üéØ –¢–ï–°–¢ –û–ü–¢–ò–ú–ê–õ–¨–ù–û–ì–û –†–ï–®–ï–ù–ò–Ø: Smart State Management

–ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É:
‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (–Ω–µ—Ç tensor version errors)
‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ (emergent specialization, memory continuity)
‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è overhead)
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import logging
from training.embedding_trainer.emergent_training_stage_3_1_4_1 import EmergentCubeTrainer, EmergentTrainingConfig

# –í–∫–ª—é—á–∞–µ–º anomaly detection –¥–ª—è —Ç–æ—á–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
torch.autograd.set_detect_anomaly(True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,  # INFO level –¥–ª—è production-like testing
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('optimal_solution_test.log', mode='w', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def test_optimal_stability():
    """–¢–µ—Å—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π"""
    
    print("üéØ === OPTIMAL SOLUTION STABILITY TEST ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    config = EmergentTrainingConfig()
    config.cube_dimensions = (5, 5, 5)  # –ö–æ–º–ø—Ä–æ–º–∏—Å—Å: –±—ã—Å—Ç—Ä–æ, –Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ
    config.mixed_precision = False  # –£–ø—Ä–æ—â–∞–µ–º –¥–ª—è –Ω–∞—á–∞–ª–∞
    config.gradient_checkpointing = False
    
    # –°–æ–∑–¥–∞–µ–º trainer
    trainer = EmergentCubeTrainer(config=config, device="cpu")
    print(f"‚úÖ Trainer created with {trainer.get_system_info()['total_system_params']} parameters")
    
    # –°–æ–∑–¥–∞–µ–º test data (–∞–¥–∞–ø—Ç–∏–≤–Ω–æ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –∫—É–±–∞)
    batch_size = 4
    surface_size = config.cube_dimensions[0] * config.cube_dimensions[1]  # 5√ó5 = 25
    question_embeddings = torch.randn(batch_size, surface_size, requires_grad=True)  # [4, 25]
    answer_embeddings = torch.randn(batch_size, 4096, requires_grad=True)  # [4, 4096]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º emergent specialization tracking
    initial_specialization_scores = []
    for i, cell in enumerate(trainer.gmlp_cells[:3]):
        score = cell.get_specialization_score()
        initial_specialization_scores.append(score)
        print(f"üß† Cell {i} initial specialization score: {score:.6f}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º 15 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö training steps
    successful_steps = 0
    specialization_evolution = []
    
    for step in range(15):
        try:
            print(f"\nüîÑ === TRAINING STEP {step + 1}/15 ===")
            
            # Training step
            loss_metrics = trainer.train_step(question_embeddings, answer_embeddings)
            
            print(f"‚úÖ Step {step + 1} SUCCESS:")
            print(f"   Total loss: {loss_metrics['total_loss']:.6f}")
            print(f"   Cosine similarity: {loss_metrics['cosine_similarity']:.6f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–≤–æ–ª—é—Ü–∏—é specialization
            current_scores = []
            for i, cell in enumerate(trainer.gmlp_cells[:3]):
                score = cell.get_specialization_score()
                current_scores.append(score)
                if step % 5 == 4:  # –ö–∞–∂–¥—ã–µ 5 —à–∞–≥–æ–≤
                    print(f"üß† Cell {i} specialization score: {score:.6f}")
            
            specialization_evolution.append(current_scores)
            successful_steps += 1
            
        except Exception as e:
            print(f"‚ùå Step {step + 1} FAILED: {e}")
            break
    
    print(f"\nüéØ === –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —à–∞–≥–æ–≤: {successful_steps}/15")
    
    # –ê–Ω–∞–ª–∏–∑ emergent behavior
    if len(specialization_evolution) > 5:
        print("\nüß† === –ê–ù–ê–õ–ò–ó EMERGENT SPECIALIZATION ===")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–≤–∏—Ç–∏–µ specialization
        final_scores = specialization_evolution[-1]
        for i in range(len(initial_specialization_scores)):
            initial = initial_specialization_scores[i]
            final = final_scores[i]
            improvement = final - initial
            
            print(f"Cell {i}: {initial:.6f} ‚Üí {final:.6f} (Œî{improvement:+.6f})")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ specialization —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è
            if improvement > 0.001:
                print(f"   ‚úÖ Cell {i}: Emergent specialization —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è!")
            else:
                print(f"   ‚ö†Ô∏è Cell {i}: –°–ª–∞–±–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ specialization")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º memory continuity
    print("\nüß† === –ê–ù–ê–õ–ò–ó MEMORY CONTINUITY ===")
    memory_preserved_cells = 0
    for i, cell in enumerate(trainer.gmlp_cells[:5]):
        if hasattr(cell.base_gmlp, 'memory_state') and cell.base_gmlp.memory_state is not None:
            memory_preserved_cells += 1
            
    print(f"Cells —Å preserved memory: {memory_preserved_cells}/5")
    if memory_preserved_cells >= 3:
        print("‚úÖ Memory continuity —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è!")
    else:
        print("‚ö†Ô∏è Memory continuity –Ω–∞—Ä—É—à–µ–Ω–∞")
    
    return successful_steps >= 10  # 10+ —É—Å–ø–µ—à–Ω—ã—Ö —à–∞–≥–æ–≤ = —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

def test_performance_overhead():
    """–¢–µ—Å—Ç overhead –æ—Ç smart state management"""
    
    print("\n‚ö° === PERFORMANCE OVERHEAD TEST ===")
    
    import time
    
    config = EmergentTrainingConfig()
    config.cube_dimensions = (5, 5, 5)  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä
    config.mixed_precision = False
    
    trainer = EmergentCubeTrainer(config=config, device="cpu")
    
    batch_size = 4
    surface_size = config.cube_dimensions[0] * config.cube_dimensions[1]  # 5√ó5 = 25
    question_embeddings = torch.randn(batch_size, surface_size)  # [4, 25]
    answer_embeddings = torch.randn(batch_size, 4096)
    
    # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è 10 training steps
    start_time = time.time()
    
    for step in range(10):
        try:
            trainer.train_step(question_embeddings, answer_embeddings)
        except:
            break
            
    end_time = time.time()
    total_time = end_time - start_time
    avg_time_per_step = total_time / 10
    
    print(f"Average time per training step: {avg_time_per_step:.3f}s")
    
    if avg_time_per_step < 2.0:  # –ú–µ–Ω–µ–µ 2 —Å–µ–∫—É–Ω–¥ –Ω–∞ —à–∞–≥
        print("‚úÖ Performance overhead –ø—Ä–∏–µ–º–ª–µ–º—ã–π")
        return True
    else:
        print("‚ö†Ô∏è Performance overhead –≤—ã—Å–æ–∫–∏–π")
        return False

def test_adaptive_cleanup_strategy():
    """–¢–µ—Å—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—á–∏—Å—Ç–∫–∏"""
    
    print("\nüß† === ADAPTIVE CLEANUP STRATEGY TEST ===")
    
    config = EmergentTrainingConfig()
    config.cube_dimensions = (5, 5, 5)  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä
    config.mixed_precision = False
    
    trainer = EmergentCubeTrainer(config=config, device="cpu")
    
    batch_size = 4
    surface_size = config.cube_dimensions[0] * config.cube_dimensions[1]  # 5√ó5 = 25
    question_embeddings = torch.randn(batch_size, surface_size)  # [4, 25]
    answer_embeddings = torch.randn(batch_size, 4096)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ full reset –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —à–∞–≥–∞—Ö
    full_reset_steps = []
    lightweight_steps = []
    
    for step in range(25):
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        needs_full_reset = (
            step == 0 or  # –ü–µ—Ä–≤—ã–π —à–∞–≥
            step % 10 == 0 or  # –ö–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
            (hasattr(trainer, '_last_error_step') and step - trainer._last_error_step < 3)
        )
        
        if needs_full_reset:
            full_reset_steps.append(step)
        else:
            lightweight_steps.append(step)
    
    print(f"Full reset steps: {full_reset_steps}")
    print(f"Lightweight cleanup steps: {len(lightweight_steps)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    full_reset_ratio = len(full_reset_steps) / 25
    print(f"Full reset ratio: {full_reset_ratio:.2%}")
    
    if 0.1 <= full_reset_ratio <= 0.3:  # 10-30% —à–∞–≥–æ–≤ —Å full reset
        print("‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∞")
        return True
    else:
        print("‚ö†Ô∏è –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        return False

if __name__ == "__main__":
    print("üöÄ Starting optimal solution comprehensive test...")
    
    # –¢–µ—Å—Ç 1: –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
    stability_success = test_optimal_stability()
    
    # –¢–µ—Å—Ç 2: Performance
    performance_success = test_performance_overhead()
    
    # –¢–µ—Å—Ç 3: Adaptive strategy
    strategy_success = test_adaptive_cleanup_strategy()
    
    print(f"\nüéØ === FINAL RESULTS ===")
    print(f"Stability: {'‚úÖ PASS' if stability_success else '‚ùå FAIL'}")
    print(f"Performance: {'‚úÖ PASS' if performance_success else '‚ùå FAIL'}")
    print(f"Strategy: {'‚úÖ PASS' if strategy_success else '‚ùå FAIL'}")
    
    if stability_success and performance_success and strategy_success:
        print("\nüéâ OPTIMAL SOLUTION READY FOR PRODUCTION!")
    else:
        print("\n‚ö†Ô∏è Some issues detected - check logs for details") 