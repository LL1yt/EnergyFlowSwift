#!/usr/bin/env python3
"""
Final GPU Optimization Test
==========================

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º:

1. ‚úÖ Memory leaks –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã (tensor reuse)
2. ‚úÖ Blocking synchronization –æ—Ç–∫–ª—é—á–µ–Ω
3. ‚úÖ CPU tensor operations —É–±—Ä–∞–Ω—ã (.cpu() calls)
4. ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (—É–±—Ä–∞–Ω—ã list comprehensions)
5. ‚úÖ GPU-direct data loading (map_location='cuda')
6. ‚úÖ Strict GPU-only mode –¥–æ–±–∞–≤–ª–µ–Ω
7. ‚úÖ In-place operations –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã (checkpoint compatibility)

–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
- GPU —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è: 60-90% (–±—ã–ª–æ 20-25%)
- GPU –ø–∞–º—è—Ç—å: 50-80% (–±—ã–ª–æ 10%)
- –ù–µ—Ç in-place operation errors
- –°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –±–µ–∑ memory leaks
"""

import torch
import time
import sys
import subprocess
import psutil
import threading
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append("/mnt/c/Users/n0n4a/projects/AA")

from new_rebuild.config.simple_config import SimpleProjectConfig
from new_rebuild.config.config_components import ConfigMode, ModeSettings
from new_rebuild.utils.device_manager import get_device_manager
from new_rebuild.utils.logging import get_logger

logger = get_logger(__name__)


class GPUMemoryMonitor:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –∏ CPU –ø–∞–º—è—Ç–∏"""

    def __init__(self):
        self.monitoring = False
        self.stats = {
            "gpu_utilization": [],
            "gpu_memory_usage": [],
            "cpu_memory_usage": [],
            "timestamps": [],
        }
        self.thread = None

    def start_monitoring(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.monitoring = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.daemon = True
        self.thread.start()
        logger.info("üîç GPU/CPU memory monitoring started")

    def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.monitoring = False
        if self.thread:
            self.thread.join()
        logger.info("‚èπÔ∏è GPU/CPU memory monitoring stopped")

    def _monitor_loop(self):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º CPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º"""
        while self.monitoring:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyTorch –¥–ª—è GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–±–µ–∑ subprocess)
                if torch.cuda.is_available():
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å GPU utilization —á–µ—Ä–µ–∑ PyTorch
                        import pynvml
                        pynvml.nvmlInit()
                        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        
                        gpu_util = util.gpu
                        gpu_mem_used = (mem_info.used / mem_info.total) * 100
                        
                        self.stats["gpu_utilization"].append(gpu_util)
                        self.stats["gpu_memory_usage"].append(gpu_mem_used)
                    except:
                        # Fallback –µ—Å–ª–∏ pynvml –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                        self.stats["gpu_utilization"].append(0)
                        self.stats["gpu_memory_usage"].append(0)
                else:
                    self.stats["gpu_utilization"].append(0)
                    self.stats["gpu_memory_usage"].append(0)

                # CPU –ø–∞–º—è—Ç—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º overhead
                try:
                    import psutil
                    cpu_memory = psutil.virtual_memory()
                    self.stats["cpu_memory_usage"].append(cpu_memory.percent)
                except:
                    self.stats["cpu_memory_usage"].append(0)
                    
                self.stats["timestamps"].append(time.time())

            except Exception:
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                pass

            time.sleep(0.5)  # –£–º–µ–Ω—å—à–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏

    def get_comprehensive_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if not self.stats["gpu_utilization"]:
            return {
                "avg_gpu_util": 0.0,
                "max_gpu_util": 0.0,
                "avg_gpu_memory": 0.0,
                "max_gpu_memory": 0.0,
                "avg_cpu_memory": 0.0,
                "max_cpu_memory": 0.0,
                "samples": 0,
            }

        return {
            "avg_gpu_util": sum(self.stats["gpu_utilization"])
            / len(self.stats["gpu_utilization"]),
            "max_gpu_util": max(self.stats["gpu_utilization"]),
            "avg_gpu_memory": sum(self.stats["gpu_memory_usage"])
            / len(self.stats["gpu_memory_usage"]),
            "max_gpu_memory": max(self.stats["gpu_memory_usage"]),
            "avg_cpu_memory": sum(self.stats["cpu_memory_usage"])
            / len(self.stats["cpu_memory_usage"]),
            "max_cpu_memory": max(self.stats["cpu_memory_usage"]),
            "samples": len(self.stats["gpu_utilization"]),
        }


def test_gpu_optimizations_comprehensive():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π"""

    logger.info("üöÄ Testing Final GPU Optimizations")
    logger.info("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
    device_manager = get_device_manager()
    if not device_manager.is_cuda():
        logger.error("‚ùå CUDA not available - cannot test GPU optimizations")
        return None

    logger.info(f"üìä GPU Device: {device_manager.get_device()}")

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ OPTIMIZED —Ä–µ–∂–∏–º–µ
    config = SimpleProjectConfig(mode=ModeSettings(mode=ConfigMode.OPTIMIZED))

    # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º strict GPU mode
    device_manager.enable_strict_gpu_mode()

    logger.info("üîß Optimization settings:")
    logger.info(f"  - Mode: {config.mode.mode}")
    logger.info(f"  - Training batch size: {config.training.batch_size}")
    logger.info(
        f"  - Embedding batch size: {config.training_embedding.embedding_batch_size}"
    )
    logger.info(f"  - Debug mode: {config.logging.debug_mode}")
    logger.info(f"  - CPU fallback: {config.device.fallback_cpu}")
    logger.info(f"  - Strict GPU mode: {device_manager.strict_gpu}")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    monitor = GPUMemoryMonitor()
    monitor.start_monitoring()

    success = False
    error_info = None

    try:
        logger.info("üî• Starting optimized forward pass test...")

        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
        sys.path.append("/mnt/c/Users/n0n4a/projects/AA")

        # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ forward pass
        start_time = time.time()

        # –≠–º—É–ª–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π forward pass
        from new_rebuild.core.lattice.lattice import Lattice3D
        from new_rebuild.core.moe.moe_processor import MoEConnectionProcessor

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ä–µ—à–µ—Ç–∫—É
        lattice = Lattice3D()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ GPU
        with torch.no_grad():
            initial_states = torch.randn(
                3375, config.model.state_size, device=device_manager.get_device()
            )
            lattice.states = initial_states

            # –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ forward passes
            for i in range(3):
                logger.info(f"--- Forward pass {i+1}/3 ---")
                lattice.forward()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞–ª–∏—Å—å –Ω–∞ GPU
                if lattice.states.device.type != "cuda":
                    raise RuntimeError(f"States moved to CPU: {lattice.states.device}")

            logger.info("‚úÖ Forward passes completed successfully")

        end_time = time.time()
        processing_time = end_time - start_time

        logger.info(f"‚è±Ô∏è Total processing time: {processing_time:.2f} seconds")
        success = True

    except Exception as e:
        logger.error(f"‚ùå Test failed with error: {e}")
        error_info = str(e)
        success = False

    finally:
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        monitor.stop_monitoring()

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    stats = monitor.get_comprehensive_stats()

    logger.info("\n" + "=" * 50)
    logger.info("üìà FINAL OPTIMIZATION RESULTS:")
    logger.info("=" * 50)

    logger.info(f"  - Average GPU Utilization: {stats['avg_gpu_util']:.1f}%")
    logger.info(f"  - Maximum GPU Utilization: {stats['max_gpu_util']:.1f}%")
    logger.info(f"  - Average GPU Memory: {stats['avg_gpu_memory']:.1f}%")
    logger.info(f"  - Maximum GPU Memory: {stats['max_gpu_memory']:.1f}%")
    logger.info(f"  - Average CPU Memory: {stats['avg_cpu_memory']:.1f}%")
    logger.info(f"  - Maximum CPU Memory: {stats['max_cpu_memory']:.1f}%")
    logger.info(f"  - Monitoring samples: {stats['samples']}")
    logger.info(f"  - Test success: {'‚úÖ' if success else '‚ùå'}")

    # –û—Ü–µ–Ω–∫–∞ GPU —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏
    if stats["avg_gpu_util"] >= 80:
        logger.info("üéâ EXCELLENT: GPU utilization dramatically improved!")
        result_level = "EXCELLENT"
    elif stats["avg_gpu_util"] >= 60:
        logger.info("‚úÖ SUCCESS: GPU utilization significantly improved!")
        result_level = "SUCCESS"
    elif stats["avg_gpu_util"] >= 40:
        logger.info("‚ö†Ô∏è PARTIAL: GPU utilization improved but not optimal")
        result_level = "PARTIAL"
    else:
        logger.info("‚ùå POOR: GPU utilization still low")
        result_level = "POOR"

    # –û—Ü–µ–Ω–∫–∞ GPU –ø–∞–º—è—Ç–∏
    if stats["avg_gpu_memory"] >= 70:
        logger.info("‚úÖ GPU memory well utilized")
    elif stats["avg_gpu_memory"] >= 30:
        logger.info("‚ö†Ô∏è GPU memory moderately utilized")
    else:
        logger.info("‚ùå GPU memory underutilized - data may still be on CPU")

    # –û—Ü–µ–Ω–∫–∞ CPU –ø–∞–º—è—Ç–∏
    if stats["avg_cpu_memory"] <= 30:
        logger.info("‚úÖ CPU memory usage optimized")
    elif stats["avg_cpu_memory"] <= 50:
        logger.info("‚ö†Ô∏è CPU memory usage moderate")
    else:
        logger.info("‚ùå CPU memory usage high - potential memory leaks")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    if success and result_level in ["EXCELLENT", "SUCCESS"]:
        logger.info("üéØ OPTIMIZATION COMPLETE: GPU utilization optimized successfully!")
    elif success and result_level == "PARTIAL":
        logger.info("üîß OPTIMIZATION PARTIAL: Some improvements needed")
    else:
        logger.info("‚ùó OPTIMIZATION FAILED: Requires further investigation")

    if error_info:
        logger.info(f"üêõ Error details: {error_info}")

    return {
        "success": success,
        "result_level": result_level,
        "stats": stats,
        "error": error_info,
    }


if __name__ == "__main__":
    result = test_gpu_optimizations_comprehensive()

    if result:
        print(f"\nüéØ FINAL RESULT: {result['result_level']}")
        print(f"üìä GPU Utilization: {result['stats']['avg_gpu_util']:.1f}%")
        print(f"üíæ GPU Memory: {result['stats']['avg_gpu_memory']:.1f}%")
        print(f"üß† CPU Memory: {result['stats']['avg_cpu_memory']:.1f}%")
        print(f"‚úÖ Success: {result['success']}")
    else:
        print("\n‚ùå TEST FAILED: Could not complete optimization test")
