{
  "dataPath": "/Users/id/Documents/GitHub/EnergyFlowSwift/EnergyFlowSwift/data/2k_snli_tokens.efb",
  "epochs": 2,
  "batchSize": 8,
  "maxLength": 128,
  "maxBatches": 10,
  "lr": 0.0003,
  "weightDecay": 0.01,
  "alphaCos": 1.0,
  "betaMSE": 1.0,
  "accumSteps": 1,
  "warmupSteps": 500,
  "cosineDecaySteps": 5000,
  "minLR": 0.00001,
  "microBatch": 8,
  "clipNorm": 1.0,
  "valFraction": 0.1,
  "saveCheckpoint": "Checkpoints/proj_best.bin",
  "loadCheckpoint": "",
  "saveOptState": "Checkpoints/opt_best.efop",
  "loadOptState": "",
  "unfreezeLastTCN": true,
  "_comments": {
    "dataPath": "Path to dataset file (.jsonl or .efb). Required.",
    "epochs": "Number of epochs to train.",
    "batchSize": "Batch size for high-level iteration (will be split into micro-batches).",
    "maxLength": "Max tokens per sample; inputs are padded/truncated to this length.",
    "maxBatches": "Limit the number of batches per epoch (0 or negative to disable).",
    "lr": "Base learning rate for AdamW (will be scheduled with warmup+cosine).",
    "weightDecay": "AdamW weight decay for L2 regularization.",
    "alphaCos": "Weight for (1 - cosine) component in the loss.",
    "betaMSE": "Weight for MSE component in the loss.",
    "accumSteps": "Gradient accumulation steps before optimizer step.",
    "warmupSteps": "Number of steps for linear warmup of LR.",
    "cosineDecaySteps": "Number of steps for cosine LR decay after warmup.",
    "minLR": "Minimum LR floor during cosine decay.",
    "microBatch": "Micro-batch size used to split batch for memory constraints.",
    "clipNorm": "Global gradient norm clipping value (0 to disable).",
    "valFraction": "Fraction of dataset reserved for validation (0..1).",
    "saveCheckpoint": "Path to save best projection weights (optional).",
    "loadCheckpoint": "Path to load initial projection weights (optional).",
    "saveOptState": "Path to save optimizer state for projector (optional).",
    "loadOptState": "Path to load optimizer state for projector (optional).",
    "unfreezeLastTCN": "If true, also train the last TCN block parameters."
  }
}
