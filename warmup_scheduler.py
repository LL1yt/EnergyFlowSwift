#!/usr/bin/env python3
"""
Warm-up Learning Rate Scheduler
–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç learning rate –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è
"""

import torch
import math
from typing import Optional


class WarmupScheduler:
    """–ü—Ä–æ—Å—Ç–æ–π warm-up scheduler –¥–ª—è learning rate"""

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        warmup_epochs: int = 3,
        base_lr: Optional[float] = None,
        warmup_start_factor: float = 0.1,
    ):
        """
        Args:
            optimizer: PyTorch optimizer
            warmup_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è warm-up
            base_lr: –ë–∞–∑–æ–≤—ã–π learning rate (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ—Ä–µ—Ç—Å—è –∏–∑ optimizer)
            warmup_start_factor: –ù–∞—á–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (0.1 = –Ω–∞—á–∏–Ω–∞–µ–º —Å 10% –æ—Ç base_lr)
        """
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.warmup_start_factor = warmup_start_factor

        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π learning rate
        if base_lr is None:
            self.base_lr = optimizer.param_groups[0]["lr"]
        else:
            self.base_lr = base_lr

        self.current_epoch = 0

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ learning rates –¥–ª—è –≤—Å–µ—Ö param_groups
        self.base_lrs = [group["lr"] for group in optimizer.param_groups]

    def step(self, epoch: Optional[int] = None):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç learning rate –Ω–∞ –Ω–æ–≤–æ–π —ç–ø–æ—Ö–µ

        Args:
            epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å—á–µ—Ç—á–∏–∫)
        """
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏
        if self.current_epoch <= self.warmup_epochs:
            # Warm-up —Ñ–∞–∑–∞: –ª–∏–Ω–µ–π–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ—Ç start_factor –¥–æ 1.0
            warmup_factor = self.warmup_start_factor + (
                1.0 - self.warmup_start_factor
            ) * (self.current_epoch / self.warmup_epochs)
        else:
            # –ü–æ—Å–ª–µ warm-up: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π learning rate
            warmup_factor = 1.0

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ –≤—Å–µ–º param_groups
        for i, param_group in enumerate(self.optimizer.param_groups):
            param_group["lr"] = self.base_lrs[i] * warmup_factor

        return warmup_factor

    def get_current_lr(self) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π learning rate"""
        return self.optimizer.param_groups[0]["lr"]

    def get_warmup_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ warm-up"""
        current_lr = self.get_current_lr()
        warmup_factor = current_lr / self.base_lr if self.base_lr > 0 else 0

        return {
            "epoch": self.current_epoch,
            "warmup_epochs": self.warmup_epochs,
            "base_lr": self.base_lr,
            "current_lr": current_lr,
            "warmup_factor": warmup_factor,
            "is_warmup_phase": self.current_epoch <= self.warmup_epochs,
        }


def create_warmup_scheduler(
    optimizer: torch.optim.Optimizer, is_resume: bool = False, warmup_epochs: int = 3
) -> Optional[WarmupScheduler]:
    """
    –°–æ–∑–¥–∞–µ—Ç warm-up scheduler –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

    Args:
        optimizer: PyTorch optimizer
        is_resume: –ï—Å–ª–∏ True, —Å–æ–∑–¥–∞–µ—Ç warm-up scheduler
        warmup_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è warm-up

    Returns:
        WarmupScheduler –∏–ª–∏ None
    """
    if is_resume:
        return WarmupScheduler(
            optimizer=optimizer,
            warmup_epochs=warmup_epochs,
            warmup_start_factor=0.2,  # –ù–∞—á–∏–Ω–∞–µ–º —Å 20% –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ LR
        )
    else:
        return None


def test_warmup_scheduler():
    """–¢–µ—Å—Ç warm-up scheduler"""
    print("üß™ Testing WarmupScheduler")

    # –°–æ–∑–¥–∞–µ–º dummy optimizer
    import torch.nn as nn

    model = nn.Linear(10, 1)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)

    # –°–æ–∑–¥–∞–µ–º scheduler
    scheduler = WarmupScheduler(optimizer, warmup_epochs=5, warmup_start_factor=0.1)

    print(f"\nBase LR: {scheduler.base_lr:.6f}")
    print(f"Warmup epochs: {scheduler.warmup_epochs}")
    print(f"Start factor: {scheduler.warmup_start_factor}")
    print("\nWarm-up progression:")

    for epoch in range(1, 8):
        factor = scheduler.step(epoch)
        info = scheduler.get_warmup_info()

        print(
            f"Epoch {epoch:2d}: LR={info['current_lr']:.6f} (factor={factor:.3f}) {'[WARMUP]' if info['is_warmup_phase'] else '[NORMAL]'}"
        )


if __name__ == "__main__":
    test_warmup_scheduler()
