#!/usr/bin/env python3
"""
OVERNIGHT TRAINING —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
"""

import torch
import torch.nn as nn
import time
import logging
import json
from datetime import datetime
from pathlib import Path
import sys
import os
import signal

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.append(str(Path(__file__).parent))

from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
    EmergentCubeTrainer,
)
from utils.config_manager.config_manager import ConfigManager
from precomputed_embedding_loader import (
    PrecomputedEmbeddingLoader,
    create_precomputed_dataset,
)
from model_weights_manager import ModelWeightsManager
from config_converter import convert_config_dict_to_object

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Unicode –ø—Ä–æ–±–ª–µ–º—ã –≤ Windows
import re


class EmojiFilter(logging.Filter):
    """–§–∏–ª—å—Ç—Ä –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —ç–º–æ–¥–∑–∏ –∏–∑ –ª–æ–≥–æ–≤ –Ω–∞ Windows"""

    def filter(self, record):
        if sys.platform == "win32":
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —ç–º–æ–¥–∑–∏ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
            emoji_pattern = re.compile(
                "["
                "\U0001f600-\U0001f64f"  # emoticons
                "\U0001f300-\U0001f5ff"  # symbols & pictographs
                "\U0001f680-\U0001f6ff"  # transport & map symbols
                "\U0001f1e0-\U0001f1ff"  # flags (iOS)
                "\U00002702-\U000027b0"  # dingbats
                "\U000024c2-\U0001f251"
                "\U0001f900-\U0001f9ff"  # supplemental symbols
                "\U00002600-\U000026ff"  # miscellaneous symbols
                "\U00002700-\U000027bf"  # dingbats
                "]+",
                flags=re.UNICODE,
            )

            # –ó–∞–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã
            emoji_replacements = {
                "üöÄ": "[START]",
                "‚öôÔ∏è": "[SETUP]",
                "üìö": "[DATA]",
                "‚úÖ": "[OK]",
                "üéØ": "[TARGET]",
                "üèÜ": "[BEST]",
                "üèÅ": "[DONE]",
                "‚ùå": "[ERROR]",
                "üìä": "[STATS]",
                "üíæ": "[SAVE]",
                "üîß": "[DEBUG]",
                "üìà": "[PROGRESS]",
                "‚è∞": "[TIME]",
                "üß™": "[TEST]",
                "üìÇ": "[LOAD]",
            }

            message = record.getMessage()
            for emoji, replacement in emoji_replacements.items():
                message = message.replace(emoji, replacement)

            # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–º–æ–¥–∑–∏
            message = emoji_pattern.sub("", message)
            record.msg = message
            record.args = ()

        return True


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
console_handler = logging.StreamHandler()
file_handler = logging.FileHandler(
    "logs/overnight_training_precomputed.log", encoding="utf-8"
)

# –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –∫ –∫–æ–Ω—Å–æ–ª—å–Ω–æ–º—É –æ–±—Ä–∞–±–æ—Ç—á–∏–∫—É –Ω–∞ Windows
if sys.platform == "win32":
    console_handler.addFilter(EmojiFilter())

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[file_handler, console_handler],
)
logger = logging.getLogger(__name__)


class PrecomputedOvernightTrainer:
    """Overnight Trainer —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""

    def __init__(self, embeddings_file: str = None):
        self.trainer = None
        self.dataset = None
        self.config = None
        self.weights_manager = ModelWeightsManager()
        self.embedding_loader = PrecomputedEmbeddingLoader()
        self.embeddings_file = embeddings_file
        self.should_stop = False
        self.best_similarity = 0.0
        self.training_log = []

        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

        logger.info("üöÄ PrecomputedOvernightTrainer initialized")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –∫ –≤—Å–µ–º –ª–æ–≥–≥–µ—Ä–∞–º –≤–∫–ª—é—á–∞—è trainer –ª–æ–≥–≥–µ—Ä—ã
        if sys.platform == "win32":
            self._apply_emoji_filter_to_all_loggers()

    def _apply_emoji_filter_to_all_loggers(self):
        """–ü—Ä–∏–º–µ–Ω–∏—Ç—å —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –∫–æ –≤—Å–µ–º –ª–æ–≥–≥–µ—Ä–∞–º"""
        emoji_filter = EmojiFilter()

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ª–æ–≥–≥–µ—Ä—ã
        all_loggers = [
            logging.getLogger(name) for name in logging.root.manager.loggerDict
        ]
        all_loggers.append(logging.root)

        for logger_obj in all_loggers:
            for handler in logger_obj.handlers:
                if isinstance(handler, logging.StreamHandler):
                    handler.addFilter(emoji_filter)

    def _signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful –æ—Å—Ç–∞–Ω–æ–≤–∫–∏"""
        logger.info(f"üì° Received signal {signum}, stopping training gracefully...")
        self.should_stop = True

    def auto_select_dataset(self) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Å–∞–º–æ–≥–æ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        logger.info("üîç Auto-selecting dataset...")

        datasets = self.embedding_loader.list_available_datasets()

        if not datasets:
            raise FileNotFoundError(
                "No precomputed datasets found! "
                "Please run generate_large_embedding_dataset.py first."
            )

        # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π –∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç
        latest_dataset = datasets[0]  # –£–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏

        logger.info(f"üìÇ Selected dataset: {latest_dataset['filename']}")
        logger.info(f"   Size: {latest_dataset['size']:,} pairs")
        logger.info(f"   Teacher model: {latest_dataset['teacher_model']}")
        logger.info(f"   File size: {latest_dataset['file_size_mb']:.1f} MB")

        return latest_dataset["file_path"]

    def setup_training(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("‚öôÔ∏è Setting up training components...")

        # 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if self.embeddings_file is None:
            self.embeddings_file = self.auto_select_dataset()

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_manager = ConfigManager()
        config_dict = config_manager.get_config()
        self.config = convert_config_dict_to_object(config_dict)

        # 3. –°–æ–∑–¥–∞–µ–º trainer
        self.trainer = EmergentCubeTrainer(self.config)
        self.trainer.to("cuda" if torch.cuda.is_available() else "cpu")

        # –ü–æ–≤—Ç–æ—Ä–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è trainer
        if sys.platform == "win32":
            self._apply_emoji_filter_to_all_loggers()

        # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        logger.info("üìÇ Loading precomputed embeddings...")
        self.dataset = self.embedding_loader.load_dataset(self.embeddings_file)

        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        sample = self.dataset[0]
        q_emb, a_emb = sample
        logger.info(f"‚úÖ Dataset loaded successfully:")
        logger.info(f"   Question embedding norm: {q_emb.norm().item():.6f}")
        logger.info(f"   Answer embedding norm: {a_emb.norm().item():.6f}")
        logger.info(f"   Dataset size: {len(self.dataset):,}")

        if q_emb.norm().item() < 0.1 or a_emb.norm().item() < 0.1:
            raise ValueError("Dataset contains zero embeddings!")

        logger.info("‚úÖ Training setup completed successfully")

    def run_training(self, max_epochs: int = 999999, batch_size: int = 2048):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –≥–æ—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        logger.info(f"üéØ Starting overnight training with precomputed embeddings:")
        logger.info(f"   Max epochs: {max_epochs}")
        logger.info(f"   Batch size: {batch_size}")
        logger.info(f"   Dataset size: {len(self.dataset):,}")
        logger.info(f"   Device: {next(self.trainer.parameters()).device}")

        # –°–æ–∑–¥–∞–µ–º DataLoader - –±–æ–ª—å—à–∏–π batch_size –≤–æ–∑–º–æ–∂–µ–Ω —Å –≥–æ—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        from torch.utils.data import DataLoader

        dataloader = DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,  # 0 –¥–ª—è Windows —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            pin_memory=True if torch.cuda.is_available() else False,
        )

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = torch.optim.AdamW(self.trainer.parameters(), lr=0.0001)

        epoch = 0
        start_time = time.time()

        try:
            while epoch < max_epochs and not self.should_stop:
                epoch_start_time = time.time()

                # Training epoch
                total_loss = 0.0
                total_similarity = 0.0
                num_batches = 0

                for batch_idx, (question_emb, answer_emb) in enumerate(dataloader):
                    if self.should_stop:
                        break

                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ device
                    device = next(self.trainer.parameters()).device
                    question_emb = question_emb.to(device)
                    answer_emb = answer_emb.to(device)

                    # Forward pass
                    optimizer.zero_grad()
                    outputs = self.trainer.forward(question_emb)

                    # –ö–†–ò–¢–ò–ß–ù–û: –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º target embedding –∫ 225D —á–µ—Ä–µ–∑ —Ç–æ—Ç –∂–µ –∞–¥–∞–ø—Ç–µ—Ä
                    with torch.no_grad():
                        adapted_target = self.trainer.base_trainer.adapter(answer_emb)

                    # Targets
                    targets = {
                        "target_embedding": adapted_target,
                        "target_surface": outputs["input_surface"],
                    }

                    # Loss computation
                    losses = self.trainer.compute_loss(outputs, targets)

                    # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ loss'–æ–≤ –≤ —Å–∫–∞–ª—è—Ä
                    total_loss_tensor = torch.tensor(
                        0.0, device=device, requires_grad=True
                    )
                    for loss_name, loss_value in losses.items():
                        if torch.is_tensor(loss_value) and loss_value.requires_grad:
                            if loss_value.dim() > 0:
                                loss_value = loss_value.mean()
                            total_loss_tensor = total_loss_tensor + loss_value

                    # Backward pass
                    total_loss_tensor.backward()

                    # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    torch.nn.utils.clip_grad_norm_(
                        self.trainer.parameters(), max_norm=1.0
                    )

                    optimizer.step()

                    # Metrics - –∏—Å–ø–æ–ª—å–∑—É–µ–º adapted target –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                    with torch.no_grad():
                        similarity = (
                            torch.cosine_similarity(
                                outputs["final_output"], adapted_target, dim=-1
                            )
                            .mean()
                            .item()
                        )

                    total_loss += total_loss_tensor.item()
                    total_similarity += similarity
                    num_batches += 1

                # Epoch metrics
                avg_loss = total_loss / max(num_batches, 1)
                avg_similarity = total_similarity / max(num_batches, 1)
                epoch_time = time.time() - epoch_start_time

                # Logging
                epoch += 1

                # –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö –∏–ª–∏ –µ—Å–ª–∏ –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                if epoch % 5 == 0 or epoch <= 10 or avg_similarity > 0.4:
                    logger.info(
                        f"Epoch {epoch:4d} | "
                        f"Loss: {avg_loss:.6f} | "
                        f"Similarity: {avg_similarity:.4f} | "
                        f"Time: {epoch_time:.1f}s | "
                        f"Batches: {num_batches}"
                    )

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                log_entry = {
                    "epoch": epoch,
                    "loss": avg_loss,
                    "similarity": avg_similarity,
                    "time": epoch_time,
                    "batches": num_batches,
                    "timestamp": datetime.now().isoformat(),
                }
                self.training_log.append(log_entry)

                # Best model tracking
                if avg_similarity > self.best_similarity:
                    self.best_similarity = avg_similarity
                    logger.info(f"üèÜ New best similarity: {avg_similarity:.4f}")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    self.weights_manager.save_latest_weights(
                        self.trainer,
                        self.config.to_dict(),
                        metadata={
                            "epoch": epoch,
                            "loss": avg_loss,
                            "similarity": avg_similarity,
                            "training_type": "overnight_precomputed",
                            "dataset_size": len(self.dataset),
                            "embeddings_file": Path(self.embeddings_file).name,
                        },
                    )

                # Checkpoint –∫–∞–∂–¥—ã–µ 25 —ç–ø–æ—Ö
                if epoch % 25 == 0:
                    self.weights_manager.create_training_checkpoint(
                        self.trainer,
                        self.config.to_dict(),
                        epoch,
                        avg_loss,
                        avg_similarity,
                        metadata={
                            "training_type": "overnight_precomputed",
                            "dataset_size": len(self.dataset),
                            "embeddings_file": Path(self.embeddings_file).name,
                        },
                    )

                # –û—Å–æ–±—ã–µ –æ—Ç–º–µ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                if avg_similarity > 0.7:
                    logger.info(f"üéâ OUTSTANDING RESULTS! Similarity > 70%")
                elif avg_similarity > 0.5:
                    logger.info(f"üéØ EXCELLENT PROGRESS! Similarity > 50%")
                elif avg_similarity > 0.3:
                    logger.info(f"üìà GOOD PROGRESS! Similarity > 30%")

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö
                if epoch % 50 == 0:
                    self._save_training_log()

        except KeyboardInterrupt:
            logger.info("Training interrupted by user")
        except Exception as e:
            logger.error(f"Training error: {e}")
            raise
        finally:
            self._finalize_training(epoch, time.time() - start_time)

    def _save_training_log(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        log_path = f"logs/overnight_training_precomputed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(log_path, "w") as f:
            json.dump(self.training_log, f, indent=2)

    def _finalize_training(self, final_epoch: int, total_time: float):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üèÅ Training completed:")
        logger.info(f"   Final epoch: {final_epoch}")
        logger.info(f"   Total time: {total_time/3600:.1f} hours")
        logger.info(f"   Best similarity: {self.best_similarity:.4f}")
        logger.info(f"   Dataset used: {Path(self.embeddings_file).name}")
        logger.info(f"   Dataset size: {len(self.dataset):,} pairs")

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_training_log()

        # –°–æ–∑–¥–∞–µ–º milestone checkpoint
        self.weights_manager.create_milestone_checkpoint(
            self.trainer,
            self.config.to_dict(),
            f"overnight_precomputed_final_{final_epoch}",
            {
                "final_epoch": final_epoch,
                "total_time_hours": total_time / 3600,
                "best_similarity": self.best_similarity,
                "total_batches": len(self.training_log),
                "dataset_size": len(self.dataset),
                "embeddings_file": Path(self.embeddings_file).name,
            },
        )

        logger.info("‚úÖ Training finalization completed")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üåô OVERNIGHT TRAINING –° –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–û –í–´–ß–ò–°–õ–ï–ù–ù–´–ú–ò –≠–ú–ë–ï–î–î–ò–ù–ì–ê–ú–ò")
    print("=" * 70)
    print("Auto-selects latest dataset or specify with --dataset argument")
    print("Larger batch sizes possible with precomputed embeddings")
    print("Optimal batch_size 2048+ for RTX 5090")
    print("=" * 70)

    # –ü—Ä–æ—Å—Ç–æ–π argument parsing
    embeddings_file = None
    if len(sys.argv) > 1 and sys.argv[1] == "--dataset" and len(sys.argv) > 2:
        embeddings_file = sys.argv[2]
        print(f"Using specified dataset: {embeddings_file}")

    trainer = PrecomputedOvernightTrainer(embeddings_file)

    try:
        trainer.setup_training()
        trainer.run_training(
            max_epochs=999999,  # Unlimited
            batch_size=2048,  # Larger batch possible with precomputed embeddings
        )
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
