#!/usr/bin/env python3
"""
–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ Loss Function - –ø–æ—á–µ–º—É loss = 0.0000 –≤–º–µ—Å—Ç–æ 5-10
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.append(str(Path(__file__).parent))

from training.embedding_trainer.emergent_cube_trainer import EmergentCubeTrainer
from training.embedding_trainer.dialogue_dataset import create_dialogue_dataset
from config.config_manager import ConfigManager

class LossFunctionDiagnostics:
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ—á–µ–º—É loss = 0.0000"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"[CONFIG] –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ Loss Function (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device})")
    
    def run_diagnostics(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ loss function"""
        print("\n" + "="*60)
        print("[MAGNIFY] –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê LOSS FUNCTION")
        print("="*60)
        
        # 1. –°–æ–∑–¥–∞–µ–º trainer
        trainer = self._create_trainer()
        
        # 2. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_data = self._create_test_data()
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç loss –æ—Ç–¥–µ–ª—å–Ω–æ
        self._test_individual_loss_components(trainer, test_data)
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω—ã–π loss
        self._test_full_loss_computation(trainer, test_data)
        
        # 5. –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ scenarios
        self._test_different_scenarios(trainer)
        
        print("\n" + "="*60)
        print("[OK] –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print("="*60)
    
    def _create_trainer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ trainer –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        config_manager = ConfigManager()
        config = config_manager.get_full_config()
        
        trainer = EmergentCubeTrainer(config)
        trainer.to(self.device)
        trainer.eval()  # Evaluation mode –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        
        return trainer
    
    def _create_test_data(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –†–∞–∑–Ω—ã–µ scenarios –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_cases = [
            {"question": "What is AI?", "answer": "AI is artificial intelligence."},
            {"question": "How does learning work?", "answer": "Learning involves acquiring knowledge."},
            {"question": "Explain neural networks.", "answer": "Neural networks process information."}
        ]
        
        dataset = create_dialogue_dataset(
            test_cases,
            teacher_model="distilbert-base-uncased",
            cache_embeddings=False,
            validation_split=0.0,
            normalize_embeddings=True
        )
        
        return dataset
    
    def _test_individual_loss_components(self, trainer, dataset):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ loss –æ—Ç–¥–µ–ª—å–Ω–æ"""
        print("\nüß© –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–û–í LOSS:")
        
        # –ü–æ–ª—É—á–∞–µ–º sample
        sample = dataset[0]
        if isinstance(sample, tuple):
            question_emb, answer_emb = sample
            question_emb = question_emb.unsqueeze(0).to(self.device)
            answer_emb = answer_emb.unsqueeze(0).to(self.device)
        else:
            question_emb = sample['question_embedding'].unsqueeze(0).to(self.device)
            answer_emb = sample['answer_embedding'].unsqueeze(0).to(self.device)
        
        # Forward pass
        with torch.no_grad():
            outputs = trainer.forward(question_emb)
        
        # Targets
        targets = {
            'target_embedding': answer_emb,
            'target_surface': outputs['input_surface']
        }
        
        print(f"   Input shapes:")
        print(f"      question_emb: {question_emb.shape}")
        print(f"      answer_emb: {answer_emb.shape}")
        print(f"      final_output: {outputs['final_output'].shape}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º dialogue_similarity_loss
        self._test_dialogue_similarity_loss(trainer, outputs, targets)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º surface_consistency_loss  
        self._test_surface_consistency_loss(trainer, outputs, targets)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º internal_dynamics_loss
        self._test_internal_dynamics_loss(trainer, outputs, targets)
    
    def _test_dialogue_similarity_loss(self, trainer, outputs, targets):
        """–î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ dialogue similarity loss"""
        print(f"\n   [DATA] DIALOGUE SIMILARITY LOSS:")
        
        final_output = outputs['final_output']
        target_embedding = targets['target_embedding']
        
        print(f"      final_output: shape={final_output.shape}, norm={final_output.norm().item():.6f}")
        print(f"      target_embedding: shape={target_embedding.shape}, norm={target_embedding.norm().item():.6f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º projection –∫ surface —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if hasattr(trainer.loss_function, 'embedding_to_surface'):
            projected_target = trainer.loss_function.embedding_to_surface(target_embedding)
            print(f"      projected_target: shape={projected_target.shape}, norm={projected_target.norm().item():.6f}")
            
            # Manual cosine similarity
            cos_sim = torch.nn.functional.cosine_similarity(final_output, projected_target, dim=-1)
            dialogue_loss = 1.0 - torch.mean(cos_sim)
            
            print(f"      cosine_similarity: {torch.mean(cos_sim).item():.6f}")
            print(f"      dialogue_loss (1-cos): {dialogue_loss.item():.6f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å
            if torch.mean(cos_sim).item() > 0.999:
                print(f"      [ALERT] –ü–†–û–ë–õ–ï–ú–ê: –ü–æ—á—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã!")
                print(f"         final_output[:5]: {final_output[0][:5]}")
                print(f"         projected_target[:5]: {projected_target[0][:5]}")
                print(f"         difference: {(final_output[0][:5] - projected_target[0][:5]).abs()}")
                
                # –í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: projection –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ—Ç –∂–µ —Ç–µ–Ω–∑–æ—Ä
                print(f"      [MAGNIFY] –ü—Ä–æ–≤–µ—Ä–∫–∞ projection layer:")
                print(f"         Input to projection: {target_embedding[0][:5]}")
                print(f"         Output from projection: {projected_target[0][:5]}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Å–∞ projection layer
                if hasattr(trainer.loss_function.embedding_to_surface, 'weight'):
                    proj_weights = trainer.loss_function.embedding_to_surface.weight
                    print(f"         Projection weights: shape={proj_weights.shape}")
                    print(f"         Projection weights mean: {proj_weights.mean().item():.6f}")
                    print(f"         Projection weights std: {proj_weights.std().item():.6f}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            print(f"      [TEST] –¢–µ—Å—Ç —Å random –≤–µ–∫—Ç–æ—Ä–∞–º–∏:")
            random_output = torch.randn_like(final_output)
            random_target = torch.randn_like(projected_target)
            random_cos = torch.nn.functional.cosine_similarity(random_output, random_target, dim=-1)
            random_loss = 1.0 - torch.mean(random_cos)
            print(f"         random cosine_similarity: {torch.mean(random_cos).item():.6f}")
            print(f"         random dialogue_loss: {random_loss.item():.6f}")
            
            if random_loss.item() > 0.5:
                print(f"         [OK] Random vectors –¥–∞—é—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π loss > 0.5")
            else:
                print(f"         [ALERT] –î–∞–∂–µ random vectors –¥–∞—é—Ç –Ω–∏–∑–∫–∏–π loss!")
    
    def _test_surface_consistency_loss(self, trainer, outputs, targets):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ surface consistency loss"""
        print(f"\n   [HOME] SURFACE CONSISTENCY LOSS:")
        
        input_surface = outputs['input_surface']
        output_surface = outputs['final_output']  # –≠—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å output_surface
        
        print(f"      input_surface: shape={input_surface.shape}, norm={input_surface.norm().item():.6f}")
        print(f"      output_surface: shape={output_surface.shape}, norm={output_surface.norm().item():.6f}")
        
        # Manual surface consistency loss
        surface_loss = torch.nn.functional.mse_loss(input_surface, output_surface)
        print(f"      surface_consistency_loss (MSE): {surface_loss.item():.6f}")
        
        if surface_loss.item() < 0.001:
            print(f"      [ALERT] –ü–†–û–ë–õ–ï–ú–ê: –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π surface loss!")
            print(f"         –†–∞–∑–Ω–æ—Å—Ç—å: {(input_surface - output_surface).abs().mean().item():.8f}")
            print(f"         –í–æ–∑–º–æ–∂–Ω–æ input_surface == output_surface")
        else:
            print(f"      [OK] Surface loss –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
    
    def _test_internal_dynamics_loss(self, trainer, outputs, targets):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ internal dynamics loss"""
        print(f"\n   [GEAR] INTERNAL DYNAMICS LOSS:")
        
        if 'internal_state' in outputs:
            internal_state = outputs['internal_state']
            print(f"      internal_state: shape={internal_state.shape}, norm={internal_state.norm().item():.6f}")
            
            # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–∞–∫–æ–π-—Ç–æ regularization loss
            # –û–±—ã—á–Ω–æ —ç—Ç–æ energy —Ñ—É–Ω–∫—Ü–∏—è –∏–ª–∏ stability loss
            internal_loss = torch.mean(internal_state**2)  # L2 regularization
            print(f"      internal_dynamics_loss (L2): {internal_loss.item():.6f}")
            
            if internal_loss.item() < 0.001:
                print(f"      [ALERT] –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π internal loss")
            else:
                print(f"      [OK] Internal loss –≤—ã–≥–ª—è–¥–∏—Ç —Ä–∞–∑—É–º–Ω–æ")
        else:
            print(f"      [WARNING] –ù–µ—Ç internal_state –≤ outputs")
    
    def _test_full_loss_computation(self, trainer, dataset):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ loss computation"""
        print(f"\n[TARGET] –ü–û–õ–ù–´–ô LOSS COMPUTATION:")
        
        # –ü–æ–ª—É—á–∞–µ–º sample
        sample = dataset[0]
        if isinstance(sample, tuple):
            question_emb, answer_emb = sample
            question_emb = question_emb.unsqueeze(0).to(self.device)
            answer_emb = answer_emb.unsqueeze(0).to(self.device)
        else:
            question_emb = sample['question_embedding'].unsqueeze(0).to(self.device)
            answer_emb = sample['answer_embedding'].unsqueeze(0).to(self.device)
        
        # Forward pass
        with torch.no_grad():
            outputs = trainer.forward(question_emb)
            
        targets = {
            'target_embedding': answer_emb,
            'target_surface': outputs['input_surface']
        }
        
        # Compute loss —á–µ—Ä–µ–∑ trainer
        losses = trainer.compute_loss(outputs, targets)
        
        print(f"   –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã loss –æ—Ç trainer.compute_loss:")
        total_loss = 0
        for key, loss_tensor in losses.items():
            if torch.is_tensor(loss_tensor):
                loss_val = loss_tensor.item()
                total_loss += loss_val
                print(f"      {key}: {loss_val:.6f}")
                
                if loss_val == 0.0:
                    print(f"         [ALERT] {key} = 0.0 - –ü–†–û–ë–õ–ï–ú–ê!")
                elif loss_val < 0.01:
                    print(f"         [WARNING] {key} –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π")
                else:
                    print(f"         [OK] {key} –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
        
        print(f"   Total loss: {total_loss:.6f}")
        
        if total_loss == 0.0:
            print(f"   [ALERT] –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: Total loss = 0.0!")
            print(f"      –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã loss —Ä–∞–≤–Ω—ã –Ω—É–ª—é")
            print(f"      –û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤ —Ç–∞–∫–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏")
        elif total_loss < 0.1:
            print(f"   [WARNING] Total loss –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π - –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–æ–π")
        else:
            print(f"   [OK] Total loss –≤—ã–≥–ª—è–¥–∏—Ç —Ä–∞–∑—É–º–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è")
    
    def _test_different_scenarios(self, trainer):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö scenarios"""
        print(f"\n[MASK] –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–ó–ù–´–• SCENARIOS:")
        
        # Scenario 1: –û—á–µ–Ω—å —Ä–∞–∑–Ω—ã–µ embeddings
        print(f"   Scenario 1: –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã")
        question_emb = torch.ones(1, 768, device=self.device)
        answer_emb = -torch.ones(1, 768, device=self.device)
        
        with torch.no_grad():
            outputs = trainer.forward(question_emb)
            
        targets = {'target_embedding': answer_emb, 'target_surface': outputs['input_surface']}
        losses = trainer.compute_loss(outputs, targets)
        
        scenario1_loss = sum(loss.item() for loss in losses.values() if torch.is_tensor(loss))
        print(f"      Total loss —Å –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏: {scenario1_loss:.6f}")
        
        # Scenario 2: Random embeddings
        print(f"   Scenario 2: Random –≤–µ–∫—Ç–æ—Ä—ã")
        question_emb = torch.randn(1, 768, device=self.device)
        answer_emb = torch.randn(1, 768, device=self.device)
        
        with torch.no_grad():
            outputs = trainer.forward(question_emb)
            
        targets = {'target_embedding': answer_emb, 'target_surface': outputs['input_surface']}
        losses = trainer.compute_loss(outputs, targets)
        
        scenario2_loss = sum(loss.item() for loss in losses.values() if torch.is_tensor(loss))
        print(f"      Total loss —Å random –≤–µ–∫—Ç–æ—Ä–∞–º–∏: {scenario2_loss:.6f}")
        
        # Scenario 3: –ù—É–ª–µ–≤—ã–µ embeddings
        print(f"   Scenario 3: –ù—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã")
        question_emb = torch.zeros(1, 768, device=self.device)
        answer_emb = torch.zeros(1, 768, device=self.device)
        
        with torch.no_grad():
            outputs = trainer.forward(question_emb)
            
        targets = {'target_embedding': answer_emb, 'target_surface': outputs['input_surface']}
        losses = trainer.compute_loss(outputs, targets)
        
        scenario3_loss = sum(loss.item() for loss in losses.values() if torch.is_tensor(loss))
        print(f"      Total loss —Å –Ω—É–ª–µ–≤—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏: {scenario3_loss:.6f}")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n   [DATA] –ê–Ω–∞–ª–∏–∑ scenarios:")
        if scenario1_loss > 1.0 and scenario2_loss > 0.5:
            print(f"      [OK] Loss function —Ä–∞–±–æ—Ç–∞–µ—Ç - —Ä–∞–∑–Ω—ã–µ inputs –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ losses")
        elif all(loss < 0.01 for loss in [scenario1_loss, scenario2_loss, scenario3_loss]):
            print(f"      [ALERT] –ü–†–û–ë–õ–ï–ú–ê: –í—Å–µ scenarios –¥–∞—é—Ç –Ω—É–ª–µ–≤–æ–π loss!")
            print(f"         Loss function –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
        else:
            print(f"      [WARNING] –ß–∞—Å—Ç–∏—á–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ - –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ scenarios –¥–∞—é—Ç –Ω—É–ª–µ–≤–æ–π loss")

def main():
    """–ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ loss function"""
    diagnostics = LossFunctionDiagnostics()
    diagnostics.run_diagnostics()
    
    print("\n[TARGET] –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print("1. –ï—Å–ª–∏ loss function —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ - –ø—Ä–æ–±–ª–µ–º–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
    print("2. –ï—Å–ª–∏ loss = 0 –≤–æ –≤—Å–µ—Ö scenarios - –ø—Ä–æ–±–ª–µ–º–∞ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ loss")
    print("3. –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ real data –¥–∞–µ—Ç 0 - –ø—Ä–æ–±–ª–µ–º–∞ –≤ embeddings –∏–ª–∏ projection")

if __name__ == "__main__":
    main() 