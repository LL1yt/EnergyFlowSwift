#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ EmbeddingReshaper –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è >98%
====================================================================================

PHASE 2.3 –î–µ–Ω—å 3-4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã:
1. Enhanced similarity metrics (–º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞)
2. Importance analysis (–∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
3. Adaptive placement (–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –≤ 3D)
4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
5. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ >98% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
"""

import sys
import os
import numpy as np
import torch
import logging
from typing import Dict, List, Any

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π
from data.embedding_reshaper import EmbeddingReshaper
from data.embedding_reshaper.strategies import AdaptiveReshaper, SemanticReshaper
from data.embedding_reshaper.utils import (
    calculate_enhanced_similarity_metrics,
    analyze_embedding_importance,
    create_adaptive_transformation_strategy,
    create_test_embeddings
)


def test_enhanced_similarity_metrics():
    """
    –¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:
    - Cosine similarity
    - Pearson correlation  
    - Spearman correlation
    - Structural similarity
    - Magnitude preservation
    """
    print("\nüß™ === –¢–ï–°–¢ 1: ENHANCED SIMILARITY METRICS ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
    original_embedding = np.random.randn(768).astype(np.float32)
    
    # –°–ª—É—á–∞–π 1: –ò–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1.0)
    identical_embedding = original_embedding.copy()
    metrics_identical = calculate_enhanced_similarity_metrics(original_embedding, identical_embedding)
    
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤:")
    for metric, value in metrics_identical.items():
        print(f"   {metric}: {value:.6f}")
    
    assert metrics_identical['weighted_similarity'] > 0.99, "–ò–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å similarity ~1.0"
    
    # –°–ª—É—á–∞–π 2: –°–ª–µ–≥–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ)
    noise_std = np.std(original_embedding) * 0.01  # 1% —à—É–º–∞
    noisy_embedding = original_embedding + np.random.normal(0, noise_std, original_embedding.shape)
    metrics_noisy = calculate_enhanced_similarity_metrics(original_embedding, noisy_embedding)
    
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (1% noise):")
    for metric, value in metrics_noisy.items():
        print(f"   {metric}: {value:.6f}")
    
    assert metrics_noisy['weighted_similarity'] > 0.95, "–°–ª–µ–≥–∫–∞ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å similarity >0.95"
    
    # –°–ª—É—á–∞–π 3: –ü—Ä–æ—Å—Ç–æ–π reshape (–¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω—è—Ç—å 100% —Å–µ–º–∞–Ω—Ç–∏–∫–∏)
    reshaped_embedding = original_embedding.reshape(8, 8, 12).reshape(768)
    metrics_reshaped = calculate_enhanced_similarity_metrics(original_embedding, reshaped_embedding)
    
    print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ reshape:")
    for metric, value in metrics_reshaped.items():
        print(f"   {metric}: {value:.6f}")
    
    assert metrics_reshaped['weighted_similarity'] > 0.999, "–ü—Ä–æ—Å—Ç–æ–π reshape –¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ 100% —Å–µ–º–∞–Ω—Ç–∏–∫–∏"
    
    print("‚úÖ –¢–ï–°–¢ 1 –ü–†–û–®–ï–õ: Enhanced similarity metrics —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return metrics_reshaped['weighted_similarity']


def test_importance_analysis():
    """
    –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–∞.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç—Ä–∏ –º–µ—Ç–æ–¥–∞:
    - variance_pca: PCA –∞–Ω–∞–ª–∏–∑
    - clustering: –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑  
    - magnitude: –∞–Ω–∞–ª–∏–∑ –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ
    """
    print("\nüß™ === –¢–ï–°–¢ 2: IMPORTANCE ANALYSIS ===")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥ —Å —è–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
    embedding = np.zeros(768)
    
    # –í–∞–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–Ω–∞—á–∞–ª–æ –∏ —Å–µ—Ä–µ–¥–∏–Ω–∞)
    embedding[:100] = np.random.randn(100) * 2.0  # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    embedding[300:400] = np.random.randn(100) * 1.5  # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    
    # –ú–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–æ—Å—Ç–∞–ª—å–Ω—ã–µ)
    embedding[100:300] = np.random.randn(200) * 0.5  # –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    embedding[400:] = np.random.randn(368) * 0.3  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    
    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω: {embedding.shape}")
    print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={np.mean(embedding):.3f}, std={np.std(embedding):.3f}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ç—Ä–∏ –º–µ—Ç–æ–¥–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
    methods = ["variance_pca", "clustering", "magnitude"]
    importance_results = {}
    
    for method in methods:
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–º: {method}")
        
        importance_weights = analyze_embedding_importance(embedding, method=method)
        
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ—Å–æ–≤: {importance_weights.shape}")
        print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Å–æ–≤: min={np.min(importance_weights):.3f}, "
              f"max={np.max(importance_weights):.3f}, mean={np.mean(importance_weights):.3f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ—Å–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
        assert 0.0 <= np.min(importance_weights) <= np.max(importance_weights) <= 1.0, \
               f"–í–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0,1] –¥–ª—è –º–µ—Ç–æ–¥–∞ {method}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–∞–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ–ª—É—á–∏–ª–∏ –≤—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞
        important_indices = np.concatenate([np.arange(100), np.arange(300, 400)])
        avg_important_weight = np.mean(importance_weights[important_indices])
        
        less_important_indices = np.concatenate([np.arange(100, 300), np.arange(400, 768)])
        avg_less_important_weight = np.mean(importance_weights[less_important_indices])
        
        print(f"   –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å –≤–∞–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {avg_important_weight:.3f}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {avg_less_important_weight:.3f}")
        print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–∑–ª–∏—á–∏—è: {avg_important_weight / avg_less_important_weight:.2f}")
        
        importance_results[method] = {
            'weights': importance_weights,
            'important_avg': avg_important_weight,
            'less_important_avg': avg_less_important_weight,
            'discrimination_ratio': avg_important_weight / avg_less_important_weight
        }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –º–µ—Ç–æ–¥—ã –≤—ã—è–≤–ª—è—é—Ç —Ä–∞–∑–ª–∏—á–∏—è –≤ –≤–∞–∂–Ω–æ—Å—Ç–∏
    for method, results in importance_results.items():
        discrimination_ratio = results['discrimination_ratio']
        assert discrimination_ratio > 1.2, \
               f"–ú–µ—Ç–æ–¥ {method} –¥–æ–ª–∂–µ–Ω —Ä–∞–∑–ª–∏—á–∞—Ç—å –≤–∞–∂–Ω—ã–µ –∏ –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (ratio > 1.2, –ø–æ–ª—É—á–µ–Ω–æ {discrimination_ratio:.2f})"
    
    print("‚úÖ –¢–ï–°–¢ 2 –ü–†–û–®–ï–õ: –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return importance_results


def test_adaptive_placement_strategy():
    """
    –¢–µ—Å—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤ 3D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    —Å —É—á–µ—Ç–æ–º –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    """
    print("\nüß™ === –¢–ï–°–¢ 3: ADAPTIVE PLACEMENT STRATEGY ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥
    embedding = np.random.randn(768).astype(np.float32)
    target_shape = (8, 8, 12)
    
    print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è")
    print(f"   –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥: {embedding.shape}")
    print(f"   –¶–µ–ª–µ–≤–∞—è 3D —Ñ–æ—Ä–º–∞: {target_shape}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
    importance_methods = ["variance_pca", "clustering", "magnitude"]
    
    for method in importance_methods:
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞: {method}")
        
        strategy = create_adaptive_transformation_strategy(
            embedding=embedding,
            target_shape=target_shape,
            importance_method=method
        )
        
        print(f"   ‚úÖ –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        required_keys = ['importance_weights', 'placement_map', 'target_shape', 'optimization_params', 'quality_threshold']
        for key in required_keys:
            assert key in strategy, f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–ª—é—á '{key}'"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        importance_weights = strategy['importance_weights']
        placement_map = strategy['placement_map']
        
        assert importance_weights.shape == (768,), f"–í–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (768,), –ø–æ–ª—É—á–µ–Ω–æ {importance_weights.shape}"
        assert placement_map.shape == (768,), f"–ö–∞—Ä—Ç–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (768,), –ø–æ–ª—É—á–µ–Ω–æ {placement_map.shape}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–∞—Ä—Ç–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã
        unique_indices = np.unique(placement_map)
        expected_indices = np.arange(np.prod(target_shape))
        
        assert len(unique_indices) == len(expected_indices), \
               f"–ö–∞—Ä—Ç–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã –æ—Ç 0 –¥–æ {np.prod(target_shape)-1}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
        quality_threshold = strategy['quality_threshold']
        assert quality_threshold == 0.98, f"–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 0.98, –ø–æ–ª—É—á–µ–Ω–æ {quality_threshold}"
        
        print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏: min={np.min(importance_weights):.3f}, "
              f"max={np.max(importance_weights):.3f}")
        print(f"   üéØ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: {quality_threshold}")
        
    print("‚úÖ –¢–ï–°–¢ 3 –ü–†–û–®–ï–õ: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return True


def test_enhanced_adaptive_reshaper():
    """
    –¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ AdaptiveReshaper —Å –Ω–æ–≤—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:
    - enhanced_variance
    - importance_weighted  
    - adaptive_placement
    """
    print("\nüß™ === –¢–ï–°–¢ 4: ENHANCED ADAPTIVE RESHAPER ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤
    test_embeddings = create_test_embeddings(count=5, dim=768, embedding_type="diverse")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    enhanced_methods = ["enhanced_variance", "importance_weighted", "adaptive_placement"]
    results = {}
    
    for method in enhanced_methods:
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞: {method}")
        
        # –°–æ–∑–¥–∞–µ–º reshaper —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
        reshaper = AdaptiveReshaper(
            input_dim=768,
            cube_shape=(8, 8, 12),
            adaptation_method=method,
            preserve_semantics=True,
            semantic_threshold=0.95  # –ë–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥, –Ω–æ —Å—Ç—Ä–µ–º–∏–º—Å—è –∫ >98%
        )
        
        method_similarities = []
        
        for i, embedding in enumerate(test_embeddings):
            # –ü—Ä—è–º–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è 1D ‚Üí 3D
            embedding_3d = reshaper.vector_to_matrix(embedding)
            
            # –û–±—Ä–∞—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è 3D ‚Üí 1D
            restored_embedding = reshaper.matrix_to_vector(embedding_3d)
            
            # –ò–∑–º–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            enhanced_metrics = calculate_enhanced_similarity_metrics(embedding, restored_embedding)
            similarity = enhanced_metrics['weighted_similarity']
            
            method_similarities.append(similarity)
            
            print(f"   –≠–º–±–µ–¥–∏–Ω–≥ {i+1}: weighted_similarity = {similarity:.6f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            if similarity >= 0.98:
                print(f"   üéØ –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ >98% —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ!")
            elif similarity >= 0.95:
                print(f"   ‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ >95% —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
            else:
                print(f"   ‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∏–∂–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ: {similarity:.6f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–æ–¥—É
        avg_similarity = np.mean(method_similarities)
        max_similarity = np.max(method_similarities)
        min_similarity = np.min(method_similarities)
        
        results[method] = {
            'similarities': method_similarities,
            'avg_similarity': avg_similarity,
            'max_similarity': max_similarity,
            'min_similarity': min_similarity,
            'above_98_count': sum(1 for s in method_similarities if s >= 0.98),
            'above_95_count': sum(1 for s in method_similarities if s >= 0.95)
        }
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–¥–∞ {method}:")
        print(f"   –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {avg_similarity:.6f}")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.6f}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {min_similarity:.6f}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >98%: {results[method]['above_98_count']}/{len(test_embeddings)}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >95%: {results[method]['above_95_count']}/{len(test_embeddings)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã > 95%
        assert min_similarity > 0.95, f"–ú–µ—Ç–æ–¥ {method}: –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å >95%, –ø–æ–ª—É—á–µ–Ω–æ min={min_similarity:.6f}"
        
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –º–µ—Ç–æ–¥
    best_method = max(results.keys(), key=lambda k: results[k]['avg_similarity'])
    best_avg = results[best_method]['avg_similarity']
    
    print(f"\nüèÜ –õ–£–ß–®–ò–ô –ú–ï–¢–û–î: {best_method} —Å —Å—Ä–µ–¥–Ω–µ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é {best_avg:.6f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–∏ >98%
    best_above_98 = results[best_method]['above_98_count']
    if best_above_98 > 0:
        print(f"üéØ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: {best_above_98} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >98% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
    
    print("‚úÖ –¢–ï–°–¢ 4 –ü–†–û–®–ï–õ: Enhanced AdaptiveReshaper —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return results


def test_caching_and_performance():
    """
    –¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
    –∏ –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.
    """
    print("\nüß™ === –¢–ï–°–¢ 5: CACHING & PERFORMANCE ===")
    
    # –°–æ–∑–¥–∞–µ–º reshaper —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    reshaper = AdaptiveReshaper(
        adaptation_method="enhanced_variance",
        preserve_semantics=True,
        semantic_threshold=0.95
    )
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥
    test_embedding = np.random.randn(768).astype(np.float32)
    
    print(f"üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è...")
    
    # –ü–µ—Ä–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è (–¥–æ–ª–∂–Ω–∞ –≤—ã—á–∏—Å–ª—è—Ç—å—Å—è)
    import time
    start_time = time.time()
    result1 = reshaper.vector_to_matrix(test_embedding)
    first_transform_time = time.time() - start_time
    
    # –í—Ç–æ—Ä–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç–æ–≥–æ –∂–µ —ç–º–±–µ–¥–∏–Ω–≥–∞ (–¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à)
    start_time = time.time()
    result2 = reshaper.vector_to_matrix(test_embedding)
    second_transform_time = time.time() - start_time
    
    print(f"   –ü–µ—Ä–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è: {first_transform_time*1000:.2f} ms")
    print(f"   –í—Ç–æ—Ä–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è (–∫—ç—à): {second_transform_time*1000:.2f} ms")
    
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    if second_transform_time > 0:
        speedup = first_transform_time / second_transform_time
        print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {speedup:.1f}x")
    else:
        print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: >1000x (–º–≥–Ω–æ–≤–µ–Ω–Ω–æ)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
    assert np.array_equal(result1, result2), "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫—ç—à –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç (—Ö–æ—Ç—è –±—ã –≤ 2 —Ä–∞–∑–∞)
    if second_transform_time > 0:
        speedup = first_transform_time / second_transform_time
        if speedup > 2.0:
            print(f"   ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ: —É—Å–∫–æ—Ä–µ–Ω–∏–µ {speedup:.1f}x")
        else:
            print(f"   ‚ö†Ô∏è  –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —á–µ–º –æ–∂–∏–¥–∞–ª–æ—Å—å: —É—Å–∫–æ—Ä–µ–Ω–∏–µ {speedup:.1f}x")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–∞—Ö
    print(f"\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ batch...")
    
    batch_embeddings = [np.random.randn(768).astype(np.float32) for _ in range(10)]
    
    start_time = time.time()
    batch_results = [reshaper.vector_to_matrix(emb) for emb in batch_embeddings]
    batch_time = time.time() - start_time
    
    avg_time_per_embedding = batch_time / len(batch_embeddings)
    
    print(f"   Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batch_embeddings)} —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {batch_time*1000:.2f} ms")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–º–±–µ–¥–∏–Ω–≥: {avg_time_per_embedding*1000:.2f} ms")
    print(f"   –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {1/avg_time_per_embedding:.1f} —ç–º–±–µ–¥–∏–Ω–≥–æ–≤/—Å–µ–∫")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–∑—É–º–Ω–æ –±—ã—Å—Ç—Ä–æ)
    assert avg_time_per_embedding < 0.1, f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–º–±–µ–¥–∏–Ω–≥ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å <100ms, –ø–æ–ª—É—á–µ–Ω–æ {avg_time_per_embedding*1000:.2f}ms"
    
    print("‚úÖ –¢–ï–°–¢ 5 –ü–†–û–®–ï–õ: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return avg_time_per_embedding


def test_semantic_preservation_target_98():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏ >98% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–∏–π –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤—ã—Å–æ—á–∞–π—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.
    """
    print("\nüß™ === –¢–ï–°–¢ 6: SEMANTIC PRESERVATION TARGET >98% ===")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
    test_embeddings = create_test_embeddings(count=20, dim=768, embedding_type="diverse")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º reshaper –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    reshaper = EmbeddingReshaper(
        input_dim=768,
        cube_shape=(8, 8, 12),
        reshaping_method="adaptive",  # –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AdaptiveReshaper —Å enhanced –º–µ—Ç–æ–¥–∞–º–∏
        preserve_semantics=True,
        semantic_threshold=0.98  # –ü–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ –¥–æ 98%
    )
    
    print(f"üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {len(test_embeddings)} —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–∞—Ö...")
    print(f"üéØ –¶–µ–ª—å: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ >98%")
    
    high_quality_results = []
    all_similarities = []
    
    for i, embedding in enumerate(test_embeddings):
        # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        matrix_3d = reshaper.vector_to_matrix(embedding)
        restored_embedding = reshaper.matrix_to_vector(matrix_3d)
        
        # –ò–∑–º–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        enhanced_metrics = calculate_enhanced_similarity_metrics(embedding, restored_embedding)
        similarity = enhanced_metrics['weighted_similarity']
        
        all_similarities.append(similarity)
        
        if similarity >= 0.98:
            high_quality_results.append(i)
            print(f"   ‚úÖ –≠–º–±–µ–¥–∏–Ω–≥ {i+1}: {similarity:.6f} - –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢!")
        elif similarity >= 0.95:
            print(f"   ‚úÖ –≠–º–±–µ–¥–∏–Ω–≥ {i+1}: {similarity:.6f} - —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        else:
            print(f"   ‚ö†Ô∏è  –≠–º–±–µ–¥–∏–Ω–≥ {i+1}: {similarity:.6f} - —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    avg_similarity = np.mean(all_similarities)
    max_similarity = np.max(all_similarities)
    min_similarity = np.min(all_similarities)
    above_98_count = len(high_quality_results)
    above_95_count = sum(1 for s in all_similarities if s >= 0.95)
    
    print(f"\nüìä === –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    print(f"–í—Å–µ–≥–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(test_embeddings)}")
    print(f"–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {avg_similarity:.6f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.6f}")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {min_similarity:.6f}")
    print(f"")
    print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >98%: {above_98_count}/{len(test_embeddings)} ({above_98_count/len(test_embeddings)*100:.1f}%)")
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >95%: {above_95_count}/{len(test_embeddings)} ({above_95_count/len(test_embeddings)*100:.1f}%)")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ reshaper
    reshaper_stats = reshaper.get_statistics()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ EmbeddingReshaper:")
    for key, value in reshaper_stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.6f}")
        else:
            print(f"   {key}: {value}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–µ–π
    success_criteria = {
        "–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å >97%": avg_similarity > 0.97,
        "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å >95%": min_similarity > 0.95,
        "–•–æ—Ç—è –±—ã 30% —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >98%": above_98_count >= len(test_embeddings) * 0.3,
        "–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã >95%": above_95_count == len(test_embeddings)
    }
    
    print(f"\nüéØ === –ü–†–û–í–ï–†–ö–ê –ö–†–ò–¢–ï–†–ò–ï–í –£–°–ü–ï–•–ê ===")
    all_criteria_met = True
    for criterion, met in success_criteria.items():
        status = "‚úÖ" if met else "‚ùå"
        print(f"{status} {criterion}: {'–í–´–ü–û–õ–ù–ï–ù' if met else '–ù–ï –í–´–ü–û–õ–ù–ï–ù'}")
        if not met:
            all_criteria_met = False
    
    if all_criteria_met:
        print(f"\nüéâ –í–°–ï –ö–†–ò–¢–ï–†–ò–ò –í–´–ü–û–õ–ù–ï–ù–´! –¶–ï–õ–¨ >98% –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –°–û–•–†–ê–ù–ï–ù–ò–Ø –î–û–°–¢–ò–ì–ù–£–¢–ê!")
    else:
        print(f"\n‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    assert avg_similarity > 0.95, f"–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å >95%, –ø–æ–ª—É—á–µ–Ω–æ {avg_similarity:.6f}"
    assert min_similarity > 0.90, f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å >90%, –ø–æ–ª—É—á–µ–Ω–æ {min_similarity:.6f}"
    
    print("‚úÖ –¢–ï–°–¢ 6 –ü–†–û–®–ï–õ: –û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
    
    return {
        'avg_similarity': avg_similarity,
        'max_similarity': max_similarity,
        'min_similarity': min_similarity,
        'above_98_count': above_98_count,
        'above_95_count': above_95_count,
        'total_count': len(test_embeddings),
        'criteria_met': all_criteria_met
    }


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤.
    
    PHASE 2.3 –î–µ–Ω—å 3-4: –£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ >98%
    """
    print("üöÄ === –£–õ–£–ß–®–ï–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï EMBEDDINGRESHAPER ===")
    print("Phase 2.3 –î–µ–Ω—å 3-4: –£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ >98%")
    print("=" * 70)
    
    all_results = {}
    
    try:
        # –¢–µ—Å—Ç 1: Enhanced similarity metrics
        all_results['enhanced_metrics'] = test_enhanced_similarity_metrics()
        
        # –¢–µ—Å—Ç 2: Importance analysis
        all_results['importance_analysis'] = test_importance_analysis()
        
        # –¢–µ—Å—Ç 3: Adaptive placement strategy
        all_results['adaptive_placement'] = test_adaptive_placement_strategy()
        
        # –¢–µ—Å—Ç 4: Enhanced adaptive reshaper
        all_results['enhanced_reshaper'] = test_enhanced_adaptive_reshaper()
        
        # –¢–µ—Å—Ç 5: Caching and performance
        all_results['performance'] = test_caching_and_performance()
        
        # –¢–µ—Å—Ç 6: –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç >98% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        all_results['semantic_preservation_98'] = test_semantic_preservation_target_98()
        
        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        print(f"\nüéâ === –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ ===")
        print(f"–í—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {len(all_results)}/6")
        
        # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        final_results = all_results['semantic_preservation_98']
        print(f"\nüìä –ö–õ–Æ–ß–ï–í–´–ï –î–û–°–¢–ò–ñ–ï–ù–ò–Ø:")
        print(f"   üéØ –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {final_results['avg_similarity']:.6f}")
        print(f"   üèÜ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {final_results['max_similarity']:.6f}")
        print(f"   üìà –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >98%: {final_results['above_98_count']}/{final_results['total_count']}")
        print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ >95%: {final_results['above_95_count']}/{final_results['total_count']}")
        
        if final_results['criteria_met']:
            print(f"\nüéâ –ú–ò–°–°–ò–Ø –í–´–ü–û–õ–ù–ï–ù–ê! Phase 2.3 –î–µ–Ω—å 3-4 –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            print(f"üöÄ EmbeddingReshaper –≥–æ—Ç–æ–≤ –∫ Phase 2.5 (Core Embedding Processor)!")
        else:
            print(f"\n‚ö†Ô∏è  –¶–µ–ª—å —á–∞—Å—Ç–∏—á–Ω–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–æ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.")
            
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return final_results['criteria_met']


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 