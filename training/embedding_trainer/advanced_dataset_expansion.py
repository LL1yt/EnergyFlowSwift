"""
Advanced Dataset Expansion –¥–ª—è Stage 2.3
–°–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 100+ high-quality dialogue pairs —Å multi-domain knowledge
"""

import json
import random
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import numpy as np
import torch
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã
from .dialogue_dataset import DialogueDataset, DialogueConfig
from data.embedding_loader import EmbeddingLoader


@dataclass
class DatasetExpansionConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è dataset"""
    target_pairs: int = 100                    # –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä
    domains: List[str] = None                  # –î–æ–º–µ–Ω—ã –∑–Ω–∞–Ω–∏–π
    complexity_levels: List[str] = None        # –£—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    min_semantic_similarity: float = 0.4       # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å
    max_semantic_similarity: float = 0.8       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è (–∏–∑–±–µ–≥–∞–µ–º —Å–ª–∏—à–∫–æ–º —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö)
    diversity_threshold: float = 0.15          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏
    teacher_models: List[str] = None           # –ú–æ–¥–µ–ª–∏ –¥–ª—è multi-teacher distillation
    quality_score_threshold: float = 0.3       # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –±–∞–ª–ª (–ø–æ–Ω–∏–∂–µ–Ω –¥–ª—è –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ —á–∏—Å–ª–∞ –ø–∞—Ä)
    
    def __post_init__(self):
        if self.domains is None:
            self.domains = [
                "artificial_intelligence", "machine_learning", "computer_science",
                "programming", "data_science", "nlp", "psychology", "mathematics",
                "statistics", "algorithms", "software_engineering", "deep_learning"
            ]
        
        if self.complexity_levels is None:
            self.complexity_levels = ["beginner", "intermediate", "advanced", "expert"]
        
        if self.teacher_models is None:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
            try:
                from utils.config_loader import get_multi_teacher_config
                config = get_multi_teacher_config()
                self.teacher_models = config.get('models', ['distilbert'])
                print(f"[INFO] Loaded teacher models from config: {self.teacher_models}")
            except Exception:
                self.teacher_models = ["llama3-8b-local", "distilbert", "roberta"]  # Fallback
                print(f"[WARNING] Using fallback teacher models: {self.teacher_models}")


class AdvancedDatasetExpander:
    """
    –°–∏—Å—Ç–µ–º–∞ –¥–ª—è intelligent —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è dialogue dataset
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - Multi-domain knowledge expansion
    - Quality scoring –∏ filtering
    - Diversity metrics
    - Curriculum learning preparation
    """
    
    def __init__(self, config: Optional[DatasetExpansionConfig] = None):
        self.config = config or DatasetExpansionConfig()
        self.embedding_loader = EmbeddingLoader()
        
        # –ì–æ—Ç–æ–≤—ã–µ –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
        self.domain_templates = self._create_domain_templates()
        
        # –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_scorer = QualityScorer(self.embedding_loader)
        
        print(f"üöÄ AdvancedDatasetExpander initialized")
        print(f"   Target pairs: {self.config.target_pairs}")
        print(f"   Domains: {len(self.config.domains)}")
        print(f"   Teacher models: {len(self.config.teacher_models)}")
    
    def _create_domain_templates(self) -> Dict[str, List[Dict]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞ –∑–Ω–∞–Ω–∏–π"""
        templates = {}
        
        # AI/ML Domain
        templates["artificial_intelligence"] = [
            {"question": "What is artificial intelligence and how does it differ from traditional programming?", 
             "answer": "Artificial intelligence is the simulation of human intelligence in machines, differing from traditional programming by enabling systems to learn and adapt rather than following pre-defined rules."},
            {"question": "Explain the difference between supervised and unsupervised learning", 
             "answer": "Supervised learning uses labeled data to train models for prediction, while unsupervised learning finds patterns in unlabeled data without predetermined outcomes."},
            {"question": "What are the main types of machine learning algorithms?", 
             "answer": "The main types are supervised learning (classification and regression), unsupervised learning (clustering and dimensionality reduction), and reinforcement learning (learning through rewards)."},
            {"question": "How do neural networks work at a fundamental level?", 
             "answer": "Neural networks process information through interconnected nodes that apply weights and activation functions to inputs, learning patterns through backpropagation and gradient descent."},
            {"question": "What is the curse of dimensionality in machine learning?", 
             "answer": "The curse of dimensionality refers to problems that arise when analyzing data in high-dimensional spaces, where distances become meaningless and data becomes sparse."},
        ]
        
        # Computer Science
        templates["computer_science"] = [
            {"question": "What is the difference between stack and queue data structures?", 
             "answer": "A stack follows Last-In-First-Out (LIFO) principle, while a queue follows First-In-First-Out (FIFO) principle for data access."},
            {"question": "Explain the concept of Big O notation", 
             "answer": "Big O notation describes the upper bound of algorithm complexity, expressing how execution time or space grows relative to input size."},
            {"question": "What are the fundamental principles of object-oriented programming?", 
             "answer": "The four main principles are encapsulation (data hiding), inheritance (code reuse), polymorphism (multiple forms), and abstraction (essential features)."},
            {"question": "How does garbage collection work in programming languages?", 
             "answer": "Garbage collection automatically manages memory by identifying and freeing unused objects, preventing memory leaks and reducing manual memory management."},
            {"question": "What is the difference between synchronous and asynchronous programming?", 
             "answer": "Synchronous programming executes operations sequentially, blocking until completion, while asynchronous programming allows operations to run concurrently without blocking."},
        ]
        
        # Programming
        templates["programming"] = [
            {"question": "What is the difference between compiled and interpreted languages?", 
             "answer": "Compiled languages translate source code to machine code before execution, while interpreted languages execute code line-by-line at runtime."},
            {"question": "Explain the concept of recursion with an example", 
             "answer": "Recursion is when a function calls itself to solve smaller instances of the same problem, like calculating factorial where n! = n √ó (n-1)!."},
            {"question": "What are design patterns and why are they important?", 
             "answer": "Design patterns are reusable solutions to common programming problems, promoting code reusability, maintainability, and communication among developers."},
            {"question": "How does version control help in software development?", 
             "answer": "Version control tracks changes to code over time, enabling collaboration, rollback capabilities, branching for features, and maintaining project history."},
            {"question": "What is the difference between unit testing and integration testing?", 
             "answer": "Unit testing verifies individual components in isolation, while integration testing ensures different components work together correctly."},
        ]
        
        # Data Science
        templates["data_science"] = [
            {"question": "What steps are involved in the data science process?", 
             "answer": "The process includes data collection, cleaning, exploration, modeling, validation, and deployment, often iterating through these steps."},
            {"question": "How do you handle missing data in datasets?", 
             "answer": "Missing data can be handled by deletion, imputation (mean, median, mode), or advanced techniques like multiple imputation or using algorithms that handle missing values."},
            {"question": "What is the difference between correlation and causation?", 
             "answer": "Correlation indicates a statistical relationship between variables, while causation implies one variable directly influences another."},
            {"question": "Explain the bias-variance tradeoff in machine learning", 
             "answer": "Bias is error from overly simplistic assumptions, variance is error from sensitivity to small data fluctuations. Both contribute to total error."},
            {"question": "What are the key metrics for evaluating classification models?", 
             "answer": "Key metrics include accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix, each providing different insights into model performance."},
        ]
        
        # NLP
        templates["nlp"] = [
            {"question": "What is tokenization in natural language processing?", 
             "answer": "Tokenization is the process of breaking down text into individual units (tokens) like words, phrases, or characters for computational processing."},
            {"question": "Explain the difference between stemming and lemmatization", 
             "answer": "Stemming removes word endings to find root forms, while lemmatization finds the actual dictionary form of words using linguistic analysis."},
            {"question": "What are word embeddings and how do they work?", 
             "answer": "Word embeddings are dense vector representations of words that capture semantic relationships, learned from large text corpora using techniques like Word2Vec or GloVe."},
            {"question": "How do transformer models like BERT improve upon earlier NLP approaches?", 
             "answer": "Transformers use attention mechanisms to capture long-range dependencies and bidirectional context, significantly improving understanding of language nuances."},
            {"question": "What is the attention mechanism in neural networks?", 
             "answer": "Attention mechanisms allow models to focus on relevant parts of input sequences, weighing the importance of different elements for current processing."},
        ]
        
        return templates
    
    def generate_domain_pairs(self, domain: str, num_pairs: int = 5) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞—Ä –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞"""
        if domain not in self.domain_templates:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á –∏–∑ domain_templates
            available_domains = list(self.domain_templates.keys())
            if not available_domains:
                return []
            domain = available_domains[0]  # Fallback –∫ –ø–µ—Ä–≤–æ–º—É –¥–æ—Å—Ç—É–ø–Ω–æ–º—É
        
        domain_pairs = self.domain_templates[domain]
        return domain_pairs[:min(num_pairs, len(domain_pairs))]
    
    def compute_quality_score(self, question: str, answer: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –±–∞–ª–ª–∞ –¥–ª—è Q&A –ø–∞—Ä—ã"""
        return self.quality_scorer.score_pair({
            "question": question,
            "answer": answer
        })
    
    def create_expanded_dataset(self) -> DialogueDataset:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ dataset —Å multi-domain knowledge
        
        Returns:
            DialogueDataset —Å 100+ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–∏
        """
        print("üéØ Creating expanded dataset...")
        
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –ø–∞—Ä –∏–∑ —à–∞–±–ª–æ–Ω–æ–≤
        base_pairs = self._generate_base_pairs()
        print(f"   Generated {len(base_pairs)} base pairs from templates")
        
        # 2. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä
        synthetic_pairs = self._generate_synthetic_pairs(base_pairs)
        print(f"   Generated {len(synthetic_pairs)} synthetic pairs")
        
        # 3. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        all_pairs = base_pairs + synthetic_pairs
        filtered_pairs = self._filter_by_quality(all_pairs)
        print(f"   Quality filtered to {len(filtered_pairs)} pairs")
        
        # 4. Diversity filtering
        diverse_pairs = self._ensure_diversity(filtered_pairs)
        print(f"   Diversity filtered to {len(diverse_pairs)} pairs")
        
        # 5. Curriculum learning —Ä–∞–∑–º–µ—Ç–∫–∞
        curriculum_pairs = self._add_curriculum_metadata(diverse_pairs)
        print(f"   Added curriculum metadata to {len(curriculum_pairs)} pairs")
        
        # 6. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ dataset
        final_dataset = self._create_final_dataset(curriculum_pairs)
        
        print(f"[SUCCESS] Expanded dataset created successfully!")
        print(f"   Final pairs: {len(curriculum_pairs)}")
        print(f"   Quality score range: {self._get_quality_range(curriculum_pairs)}")
        
        return final_dataset
    
    def _generate_base_pairs(self) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –ø–∞—Ä –∏–∑ —à–∞–±–ª–æ–Ω–æ–≤ –¥–æ–º–µ–Ω–æ–≤"""
        all_pairs = []
        
        for domain, templates in self.domain_templates.items():
            for template in templates:
                pair = {
                    "question": template["question"],
                    "answer": template["answer"],
                    "domain": domain,
                    "complexity": "intermediate",  # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
                    "source": "template"
                }
                all_pairs.append(pair)
        
        return all_pairs
    
    def _generate_synthetic_pairs(self, base_pairs: List[Dict]) -> List[Dict]:
        """–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä"""
        synthetic_pairs = []
        needed_pairs = max(0, self.config.target_pairs - len(base_pairs))
        
        if needed_pairs <= 0:
            return synthetic_pairs
        
        print(f"   Generating {needed_pairs} synthetic pairs...")
        
        # –í–∞—Ä–∏–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ä
        for i in range(needed_pairs):
            base_pair = random.choice(base_pairs)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
            synthetic_pair = self._create_variation(base_pair)
            if synthetic_pair:
                synthetic_pairs.append(synthetic_pair)
        
        return synthetic_pairs
    
    def _create_variation(self, base_pair: Dict) -> Optional[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –±–∞–∑–æ–≤–æ–π –ø–∞—Ä—ã"""
        variations = {
            "question_rephrasing": [
                f"Could you explain {base_pair['question'].lower()}",
                f"What does it mean when we say {base_pair['question'].lower()}",
                f"How would you describe {base_pair['question'].lower()}",
                f"Can you clarify {base_pair['question'].lower()}"
            ],
            "complexity_increase": {
                "beginner": "intermediate",
                "intermediate": "advanced", 
                "advanced": "expert"
            }
        }
        
        # –ü—Ä–æ—Å—Ç–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è —á–µ—Ä–µ–∑ rephrase (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã LLM)
        if random.random() < 0.5:
            new_question = random.choice(variations["question_rephrasing"])
            return {
                "question": new_question,
                "answer": base_pair["answer"],
                "domain": base_pair["domain"],
                "complexity": base_pair["complexity"],
                "source": "synthetic_variation"
            }
        
        return None
    
    def _filter_by_quality(self, pairs: List[Dict]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–∞—Ä –ø–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º"""
        print("   Applying quality filtering...")
        
        filtered_pairs = []
        quality_scores = []
        
        for i, pair in enumerate(pairs):
            quality_score = self.quality_scorer.score_pair(pair)
            quality_scores.append(quality_score)
            
            if quality_score >= self.config.quality_score_threshold:
                pair["quality_score"] = quality_score
                filtered_pairs.append(pair)
            
            # –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 –ø–∞—Ä
            if i < 5:
                print(f"      Pair {i+1}: Q='{pair['question'][:50]}...' Score={quality_score:.3f}")
        
        if not filtered_pairs and quality_scores:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º top –ø–æ–ª–æ–≤–∏–Ω—É
            avg_score = sum(quality_scores) / len(quality_scores)
            print(f"   [WARNING] No pairs passed threshold {self.config.quality_score_threshold:.2f}")
            print(f"   üìä Average quality score: {avg_score:.3f}")
            print(f"   [REFRESH] Using pairs with score > {avg_score:.3f}")
            
            for pair, score in zip(pairs, quality_scores):
                if score >= avg_score:
                    pair["quality_score"] = score
                    filtered_pairs.append(pair)
        
        print(f"   ‚úÖ Quality filtering: {len(filtered_pairs)}/{len(pairs)} pairs kept")
        return filtered_pairs
    
    def _ensure_diversity(self, pairs: List[Dict]) -> List[Dict]:
        """–û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø–∞—Ä —á–µ—Ä–µ–∑ embedding similarity"""
        print("   Ensuring diversity...")
        
        if len(pairs) <= self.config.target_pairs:
            return pairs
        
        # –ü—Ä–æ—Å—Ç–∞—è diversity —á–µ—Ä–µ–∑ domain balance
        domain_counts = {}
        diverse_pairs = []
        
        for pair in pairs:
            domain = pair["domain"]
            domain_counts[domain] = domain_counts.get(domain, 0)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –Ω–∞ –¥–æ–º–µ–Ω
            max_per_domain = self.config.target_pairs // len(self.config.domains) + 2
            
            if domain_counts[domain] < max_per_domain:
                diverse_pairs.append(pair)
                domain_counts[domain] += 1
                
                if len(diverse_pairs) >= self.config.target_pairs:
                    break
        
        return diverse_pairs
    
    def _add_curriculum_metadata(self, pairs: List[Dict]) -> List[Dict]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è curriculum learning"""
        print("   Adding curriculum metadata...")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è curriculum learning
        complexity_order = {"beginner": 1, "intermediate": 2, "advanced": 3, "expert": 4}
        
        for pair in pairs:
            # Curriculum level (0-1, –≥–¥–µ 0 = –ª–µ–≥–∫–∏–π, 1 = —Å–ª–æ–∂–Ω—ã–π)
            complexity = pair.get("complexity", "intermediate")
            pair["curriculum_level"] = (complexity_order.get(complexity, 2) - 1) / 3
            
            # Estimated difficulty score
            question_length = len(pair["question"].split())
            answer_length = len(pair["answer"].split())
            
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            difficulty = min(1.0, (question_length + answer_length) / 50)
            pair["difficulty_score"] = difficulty
        
        return pairs
    
    def _create_final_dataset(self, pairs: List[Dict]) -> DialogueDataset:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ DialogueDataset"""
        if not pairs:
            raise ValueError("Cannot create dataset: no dialogue pairs available")
        
        config = DialogueConfig(
            teacher_model=self.config.teacher_models[0],  # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
            validation_split=0.2,
            enable_quality_filter=False,  # –£–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ
            semantic_similarity_threshold=self.config.min_semantic_similarity
        )
        
        print(f"   Creating DialogueDataset with {len(pairs)} pairs")
        return DialogueDataset(
            config=config,
            dialogue_pairs=pairs
        )
    
    def _get_quality_range(self, pairs: List[Dict]) -> Tuple[float, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫"""
        scores = [pair.get("quality_score", 0.5) for pair in pairs]
        return (min(scores), max(scores))
    
    def get_expansion_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è dataset"""
        return {
            "target_pairs": self.config.target_pairs,
            "domains": len(self.config.domains),
            "complexity_levels": len(self.config.complexity_levels),
            "teacher_models": len(self.config.teacher_models),
            "quality_threshold": self.config.quality_score_threshold,
            "diversity_threshold": self.config.diversity_threshold
        }


class QualityScorer:
    """–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ dialogue pairs"""
    
    def __init__(self, embedding_loader: EmbeddingLoader):
        self.embedding_loader = embedding_loader
    
    def score_pair(self, pair: Dict) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Q&A –ø–∞—Ä—ã (0-1)
        
        –ö—Ä–∏—Ç–µ—Ä–∏–∏:
        - Semantic relevance –º–µ–∂–¥—É Q&A
        - –î–ª–∏–Ω–∞ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
        - –Ø—Å–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞
        """
        try:
            # 1. Semantic relevance (–æ—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π)
            q_embedding = self.embedding_loader.get_embeddings([pair["question"]])[0]
            a_embedding = self.embedding_loader.get_embeddings([pair["answer"]])[0]
            
            semantic_score = torch.cosine_similarity(
                torch.tensor(q_embedding).unsqueeze(0),
                torch.tensor(a_embedding).unsqueeze(0)
            ).item()
            
            # 2. Length –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            question_words = len(pair["question"].split())
            answer_words = len(pair["answer"].split())
            
            length_score = min(1.0, (question_words * 0.1 + answer_words * 0.05))
            
            # 3. –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
            quality_score = (semantic_score * 0.7 + length_score * 0.3)
            
            return max(0, min(1, quality_score))
        
        except Exception as e:
            # Fallback –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return 0.5


# ================================
# HELPER FUNCTIONS
# ================================

def create_expanded_dataset(target_pairs: int = 100,
                          domains: Optional[List[str]] = None,
                          quality_threshold: float = 0.3) -> DialogueDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ dataset
    
    Args:
        target_pairs: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä
        domains: –î–æ–º–µ–Ω—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è
        quality_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞
        
    Returns:
        DialogueDataset —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    config = DatasetExpansionConfig(
        target_pairs=target_pairs,
        domains=domains,
        quality_score_threshold=quality_threshold
    )
    
    expander = AdvancedDatasetExpander(config)
    return expander.create_expanded_dataset()


def analyze_dataset_diversity(dataset: DialogueDataset) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è dataset"""
    # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    return {
        "total_pairs": len(dataset),
        "embedding_dimension": dataset.config.embedding_dim,
        "teacher_model": dataset.config.teacher_model
    }


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    print("üöÄ Testing Advanced Dataset Expansion...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ dataset
    expanded_dataset = create_expanded_dataset(target_pairs=100)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = expanded_dataset.get_statistics()
    print(f"üìä Dataset Statistics:")
    print(f"   Total pairs: {stats['total_dialogue_pairs']}")
    print(f"   Embedding dimension: {stats['embedding_dimension']}")
    print(f"   Teacher model: {stats['teacher_model']}")
    
    print("\n‚úÖ Advanced Dataset Expansion system ready!") 