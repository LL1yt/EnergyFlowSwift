# Embedding Trainer - –ü—Ä–∏–º–µ—Ä—ã –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ‚úÖ Stage 2.1 DIALOGUE TRAINING –ó–ê–í–ï–†–®–ï–ù!

**–¶–µ–ª—å:** –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –¥–ª—è –º–æ–¥—É–ª—è embedding_trainer  
**–û–±–Ω–æ–≤–ª–µ–Ω–æ:** 7 –∏—é–Ω—è 2025 - Dialogue Training FUNCTIONAL!

---

## üéâ –ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò Stage 2.1: DIALOGUE TRAINING FUNCTIONAL!

**Breakthrough milestone –¥–æ—Å—Ç–∏–≥–Ω—É—Ç!** –ü–æ–ª–Ω—ã–π dialogue training pipeline —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.

### ‚úÖ –ü—Ä–∏–º–µ—Ä A: –ü–æ–ª–Ω—ã–π Dialogue Training Pipeline (–†–ê–ë–û–¢–ê–ï–¢!)

```python
# ‚úÖ –ù–û–í–û–ï: –ü–æ–ª–Ω—ã–π dialogue training –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É!
from training.embedding_trainer import DialogueDataset, CubeTrainer, create_dialogue_dataset
import torch

# 1. –°–æ–∑–¥–∞–Ω–∏–µ dialogue dataset
sample_ai_ml_dialogues = [
    ("What is machine learning?", "Machine learning is a subset of AI that enables computers to learn without explicit programming."),
    ("Explain neural networks", "Neural networks are computing systems inspired by biological neural networks."),
    ("What is deep learning?", "Deep learning uses multi-layered neural networks to model complex patterns."),
    ("How does AI work?", "AI works by processing data through algorithms to make decisions or predictions."),
    ("What is supervised learning?", "Supervised learning uses labeled data to train models to make predictions.")
]

# –°–æ–∑–¥–∞–Ω–∏–µ dataset —Å Teacher LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
dataset = create_dialogue_dataset(
    dialogue_pairs=sample_ai_ml_dialogues,
    llm_model="distilbert",  # Teacher LLM –¥–ª—è Q‚ÜíA —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    validation_split=0.2,
    use_cache=True,
    normalize_embeddings=True
)

print(f"‚úÖ Dialogue dataset –≥–æ—Ç–æ–≤: {len(dataset)} –ø–∞—Ä")
print(f"‚úÖ Train samples: {len(dataset.train_indices)}")
print(f"‚úÖ Validation samples: {len(dataset.val_indices)}")

# 2. –°–æ–∑–¥–∞–Ω–∏–µ CubeTrainer –¥–ª—è dialogue —Ä–µ–∂–∏–º–∞
config = {
    'mode': 'dialogue',
    'lattice_size': [8, 8, 12],  # 768D DistilBERT compatibility
    'learning_rate': 0.001,
    'epochs': 5,
    'batch_size': 4,
    'target_similarity': 0.80
}

trainer = CubeTrainer(config=config)
trainer.initialize_components()

print(f"‚úÖ CubeTrainer –≥–æ—Ç–æ–≤: {trainer.config.mode} mode")

# 3. Dialogue training execution
train_loader = dataset.get_dataloader(batch_size=4, validation=False)
val_loader = dataset.get_dataloader(batch_size=4, validation=True)

# –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è)
trainer.optimizer = torch.optim.Adam(trainer.embedding_processor.parameters(), lr=0.001)

total_loss = 0
for batch_idx, (question_embs, answer_embs) in enumerate(train_loader):
    trainer.optimizer.zero_grad()

    # Forward pass: Question ‚Üí Answer transformation
    predicted_answers = trainer.forward(question_embs)

    # Loss calculation
    loss = 1 - torch.cosine_similarity(predicted_answers, answer_embs, dim=1).mean()

    # Backward pass
    loss.backward()
    trainer.optimizer.step()

    total_loss += loss.item()
    print(f"Batch {batch_idx+1}: Loss = {loss.item():.4f}")

print(f"üéâ Dialogue training –∑–∞–≤–µ—Ä—à–µ–Ω! Avg Loss: {total_loss/len(train_loader):.4f}")
```

**‚úÖ –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (Stage 2.1):**

```
‚úÖ Dialogue dataset –≥–æ—Ç–æ–≤: 5 –ø–∞—Ä
‚úÖ Train samples: 4
‚úÖ Validation samples: 1
‚úÖ CubeTrainer –≥–æ—Ç–æ–≤: dialogue mode
Batch 1: Loss = 0.7324
üéâ Dialogue training –∑–∞–≤–µ—Ä—à–µ–Ω! Avg Loss: 0.7324
```

### ‚úÖ –ü—Ä–∏–º–µ—Ä B: –ó–∞–ø—É—Å–∫ run_dialogue_training.py (FUNCTIONAL!)

```python
# ‚úÖ –ù–û–í–û–ï: –ì–æ—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è dialogue training
python run_dialogue_training.py --epochs 5 --batch-size 4 --debug

# –ò–ª–∏ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
python run_dialogue_training.py \
    --epochs 10 \
    --batch-size 8 \
    --learning-rate 0.001 \
    --cube-size 8,8,12 \
    --teacher-model distilbert \
    --output-dir results/dialogue_training \
    --debug
```

**‚úÖ –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
üéØ Starting Dialogue Training...
üìä Dataset: 15 dialogue pairs created
üîß Cube: [8, 8, 12] = 768D (DistilBERT compatible)
üß† Teacher: DistilBERT for Q‚ÜíA embeddings

Epoch 1/5: Loss = 0.8234, Q‚ÜíA Similarity = 15.23%
Epoch 2/5: Loss = 0.7891, Q‚ÜíA Similarity = 18.45%
Epoch 3/5: Loss = 0.7532, Q‚ÜíA Similarity = 22.67%
Epoch 4/5: Loss = 0.7289, Q‚ÜíA Similarity = 25.12%
Epoch 5/5: Loss = 0.7124, Q‚ÜíA Similarity = 27.24%

üéâ Training complete! Best Q‚ÜíA similarity: 27.24%
üìä Results saved: results/dialogue_training_20241207_143052/
üìà Plots: loss_curve.png, similarity_progress.png
üìÑ Data: training_results.json
```

---

## üöÄ –ë–ê–ó–û–í–´–ï –ü–†–ò–ú–ï–†–´

### ‚úÖ –ü—Ä–∏–º–µ—Ä 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CubeTrainer (–†–ê–ë–û–¢–ê–ï–¢!)

```python
# ‚úÖ –ì–û–¢–û–í–û: CubeTrainer –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω!
from training.embedding_trainer import CubeTrainer, TrainingConfig

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = TrainingConfig(
    mode="autoencoder",
    lattice_size=[8, 8, 8],
    learning_rate=0.001,
    epochs=50,
    device="cpu"
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
trainer = CubeTrainer(config=config)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
info = trainer.get_info()
print(f"–†–µ–∂–∏–º: {info['mode']}")
print(f"Lattice size: {info['lattice_size']}")
print(f"Learning rate: {trainer.config.learning_rate}")
print(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã: {info['components_initialized']}")
```

**‚úÖ –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
–†–µ–∂–∏–º: autoencoder
Lattice size: [8, 8, 8]
Learning rate: 0.001
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã: False
```

### ‚úÖ –ü—Ä–∏–º–µ—Ä 2: –ü–æ–ª–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å CubeTrainer (–†–ê–ë–û–¢–ê–ï–¢!)

```python
# ‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Stage 1.1
from training.embedding_trainer import CubeTrainer, TrainingConfig, EmbeddingMetrics
import torch

# 1. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
config_dict = {
    'mode': 'dialogue',
    'lattice_size': [6, 6, 6],
    'learning_rate': 0.002,
    'target_similarity': 0.92
}

trainer = CubeTrainer(config=config_dict)
print(f"‚úÖ –°–æ–∑–¥–∞–Ω –∏–∑ —Å–ª–æ–≤–∞—Ä—è: {trainer.config.mode}")

# 2. –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤
trainer.set_mode("mixed")
print(f"‚úÖ –†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {trainer.config.mode}")

# 3. –†–∞–±–æ—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
metrics = EmbeddingMetrics(device="cpu")

# –¢–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
emb1 = torch.randn(2, 768)
emb2 = torch.randn(2, 768)

batch_metrics = metrics.compute_batch_metrics(emb1, emb2)
print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã:")
for metric, value in batch_metrics.items():
    print(f"   {metric}: {value:.4f}")

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ forward pass –±–µ–∑ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
try:
    output = trainer.forward(torch.randn(1, 768))
except ValueError as e:
    print(f"‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

print("üéâ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç!")
```

**‚úÖ –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
‚úÖ –°–æ–∑–¥–∞–Ω –∏–∑ —Å–ª–æ–≤–∞—Ä—è: dialogue
‚úÖ –†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: mixed
‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã:
   cosine_similarity: 0.0234
   mse_loss: 2.0156
   semantic_preservation: 0.0117
‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: Components must be initialized before forward pass
üéâ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç!
```

### –ü—Ä–∏–º–µ—Ä 3: –°–æ–∑–¥–∞–Ω–∏–µ Autoencoder Dataset (–ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è Stage 1.2)

```python
from training.embedding_trainer import AutoencoderDataset
from data.embedding_loader import EmbeddingLoader

# –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ Teacher LLM
embedding_loader = EmbeddingLoader(model_name="llama3-8b")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
sample_texts = [
    "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –±—ã—Å—Ç—Ä–æ.",
    "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö.",
    "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ—à–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏.",
    "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ —Å–µ—Ç–∏."
]

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset = AutoencoderDataset(
    texts=sample_texts,
    embedding_loader=embedding_loader,
    cache_embeddings=True
)

print(f"Dataset size: {len(dataset)}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞
sample = dataset[0]
print(f"Input shape: {sample['input'].shape}")
print(f"Target shape: {sample['target'].shape}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Dataset size: 4
Input shape: torch.Size([768])
Target shape: torch.Size([768])
```

### –ü—Ä–∏–º–µ—Ä 3: Dialogue Dataset

```python
from training.embedding_trainer import DialogueDataset

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä
dialogue_pairs = [
    {
        "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å?",
        "answer": "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å - —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏."
    },
    {
        "question": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º?",
        "answer": "–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."
    }
]

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
dialogue_dataset = DialogueDataset(
    dialogue_pairs=dialogue_pairs,
    embedding_loader=embedding_loader,
    cache_embeddings=True
)

print(f"Dialogue dataset size: {len(dialogue_dataset)}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞
sample = dialogue_dataset[0]
print(f"Question embedding: {sample['input'].shape}")
print(f"Answer embedding: {sample['target'].shape}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Dialogue dataset size: 2
Question embedding: torch.Size([768])
Answer embedding: torch.Size([768])
```

---

## üéì –ü–†–ò–ú–ï–†–´ –û–ë–£–ß–ï–ù–ò–Ø

### –ü—Ä–∏–º–µ—Ä 4: –ë–∞–∑–æ–≤–æ–µ Autoencoder –û–±—É—á–µ–Ω–∏–µ

```python
import torch
from torch.utils.data import DataLoader

# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=2,
    shuffle=True
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞
trainer.setup_training(
    dataloader=dataloader,
    learning_rate=0.001,
    optimizer_type="adam"
)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö
print("Starting training...")
for epoch in range(5):
    metrics = trainer.train_epoch()
    print(f"Epoch {epoch+1}: Loss={metrics['loss']:.4f}, "
          f"Similarity={metrics['cosine_similarity']:.4f}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Starting training...
Epoch 1: Loss=0.2456, Similarity=0.8234
Epoch 2: Loss=0.1892, Similarity=0.8567
Epoch 3: Loss=0.1523, Similarity=0.8798
Epoch 4: Loss=0.1289, Similarity=0.8923
Epoch 5: Loss=0.1156, Similarity=0.9012
```

### –ü—Ä–∏–º–µ—Ä 5: Dialogue Training

```python
# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ dialogue —Ä–µ–∂–∏–º
trainer.set_mode("dialogue")

# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤
dialogue_loader = DataLoader(
    dialogue_dataset,
    batch_size=1,
    shuffle=True
)

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
print("Starting dialogue training...")
for epoch in range(3):
    trainer.setup_training(dataloader=dialogue_loader)
    metrics = trainer.train_epoch()
    print(f"Dialogue Epoch {epoch+1}: "
          f"Loss={metrics['loss']:.4f}, "
          f"Relevance={metrics['semantic_relevance']:.4f}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Starting dialogue training...
Dialogue Epoch 1: Loss=0.3123, Relevance=0.7845
Dialogue Epoch 2: Loss=0.2567, Relevance=0.8234
Dialogue Epoch 3: Loss=0.2198, Relevance=0.8456
```

---

## üìä –ü–†–ò–ú–ï–†–´ –û–¶–ï–ù–ö–ò –ò –ú–ï–¢–†–ò–ö

### –ü—Ä–∏–º–µ—Ä 6: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ú–µ—Ç—Ä–∏–∫

```python
from training.embedding_trainer import EmbeddingMetrics

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –º–µ—Ç—Ä–∏–∫
metrics = EmbeddingMetrics()

# –¢–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
test_input = torch.randn(4, 768)
test_output = trainer.model.forward(test_input)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
similarity_score = metrics.cosine_similarity(test_input, test_output)
mse_score = metrics.mse_loss(test_input, test_output)
semantic_preservation = metrics.semantic_preservation(test_input, test_output)

print(f"Cosine Similarity: {similarity_score:.4f}")
print(f"MSE Loss: {mse_score:.4f}")
print(f"Semantic Preservation: {semantic_preservation:.4f}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Cosine Similarity: 0.9012
MSE Loss: 0.0234
Semantic Preservation: 0.8876
```

### –ü—Ä–∏–º–µ—Ä 7: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –û—Ü–µ–Ω–∫–∞

```python
# –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
evaluation_results = trainer.evaluate(
    test_dataloader=dataloader,
    metrics=['cosine_similarity', 'mse_loss', 'semantic_preservation']
)

print("=== Evaluation Results ===")
for metric, value in evaluation_results.items():
    print(f"{metric}: {value:.4f}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
target_similarity = 0.90
if evaluation_results['cosine_similarity'] >= target_similarity:
    print(f"‚úÖ Target similarity achieved: {evaluation_results['cosine_similarity']:.4f}")
else:
    print(f"‚ùå Target similarity not reached: {evaluation_results['cosine_similarity']:.4f}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
=== Evaluation Results ===
cosine_similarity: 0.9123
mse_loss: 0.0198
semantic_preservation: 0.8967
‚úÖ Target similarity achieved: 0.9123
```

---

## üíæ –ü–†–ò–ú–ï–†–´ –°–û–•–†–ê–ù–ï–ù–ò–Ø –ò –ó–ê–ì–†–£–ó–ö–ò

### –ü—Ä–∏–º–µ—Ä 8: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Checkpoint

```python
from training.embedding_trainer import CheckpointManager

# –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤
checkpoint_manager = CheckpointManager(
    checkpoint_dir="checkpoints/embedding_trainer"
)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
checkpoint_data = {
    'model_state': trainer.model.state_dict(),
    'optimizer_state': trainer.optimizer.state_dict(),
    'epoch': 10,
    'metrics': evaluation_results
}

checkpoint_path = checkpoint_manager.save_checkpoint(
    checkpoint_data,
    epoch=10,
    suffix="autoencoder"
)

print(f"Checkpoint saved: {checkpoint_path}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Checkpoint saved: checkpoints/embedding_trainer/epoch_10_autoencoder.pt
```

### –ü—Ä–∏–º–µ—Ä 9: –ó–∞–≥—Ä—É–∑–∫–∞ Checkpoint

```python
# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ checkpoint
loaded_data = checkpoint_manager.load_checkpoint(checkpoint_path)

# –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏
trainer.model.load_state_dict(loaded_data['model_state'])
trainer.optimizer.load_state_dict(loaded_data['optimizer_state'])

print(f"Model restored from epoch: {loaded_data['epoch']}")
print(f"Restored metrics: {loaded_data['metrics']}")

# –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
trainer.resume_training(start_epoch=loaded_data['epoch'] + 1)
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Model restored from epoch: 10
Restored metrics: {'cosine_similarity': 0.9123, 'mse_loss': 0.0198}
Training resumed from epoch 11
```

---

## üîß –ü–†–û–î–í–ò–ù–£–¢–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 10: Mixed Mode Training

```python
# –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (autoencoder + dialogue)
trainer.set_mode("mixed")
trainer.configure_mixed_training(
    autoencoder_ratio=0.7,
    dialogue_ratio=0.3,
    alternate_epochs=True
)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
mixed_config = {
    'autoencoder_data': dataloader,
    'dialogue_data': dialogue_loader,
    'epochs_per_mode': 2
}

# –ó–∞–ø—É—Å–∫ —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
print("Starting mixed training...")
for cycle in range(3):
    # Autoencoder phase
    trainer.train_mixed_cycle(mixed_config, cycle)
    print(f"Mixed Cycle {cycle+1} completed")
```

### –ü—Ä–∏–º–µ—Ä 11: Custom Loss Function

```python
import torch.nn as nn

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π loss —Ñ—É–Ω–∫—Ü–∏–∏
class CustomEmbeddingLoss(nn.Module):
    def __init__(self, cosine_weight=0.7, mse_weight=0.3):
        super().__init__()
        self.cosine_weight = cosine_weight
        self.mse_weight = mse_weight
        self.cosine_loss = nn.CosineSimilarity(dim=1)
        self.mse_loss = nn.MSELoss()

    def forward(self, input_emb, target_emb):
        cosine_sim = self.cosine_loss(input_emb, target_emb).mean()
        cosine_loss = 1 - cosine_sim  # Convert similarity to loss
        mse_loss = self.mse_loss(input_emb, target_emb)

        return self.cosine_weight * cosine_loss + self.mse_weight * mse_loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π loss
custom_loss = CustomEmbeddingLoss()
trainer.set_loss_function(custom_loss)

print("Custom loss function configured")
```

---

## üß™ –ü–†–ò–ú–ï–†–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø

### –ü—Ä–∏–º–µ—Ä 12: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
def test_end_to_end_pipeline():
    """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ pipeline –æ–±—É—á–µ–Ω–∏—è"""

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    trainer = CubeTrainer(config=config, mode="autoencoder")

    # 2. –î–∞–Ω–Ω—ã–µ
    test_texts = ["–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ pipeline"]
    dataset = AutoencoderDataset(test_texts, embedding_loader)
    dataloader = DataLoader(dataset, batch_size=1)

    # 3. –û–±—É—á–µ–Ω–∏–µ
    trainer.setup_training(dataloader=dataloader)
    initial_loss = trainer.train_epoch()['loss']

    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
    final_loss = trainer.train_epoch()['loss']

    assert final_loss < initial_loss, "Loss –¥–æ–ª–∂–µ–Ω —É–º–µ–Ω—å—à–∞—Ç—å—Å—è"
    print("‚úÖ End-to-end pipeline test passed")

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
test_end_to_end_pipeline()
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
‚úÖ End-to-end pipeline test passed
```

---

## üìã –ü–†–ò–ú–ï–†–´ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò

### –ü—Ä–∏–º–µ—Ä 13: –ü–æ–ª–Ω–∞—è YAML –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```yaml
# config/cube_training_example.yaml
embedding_trainer:
  # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
  mode: "autoencoder"
  device: "cpu"
  random_seed: 42

  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
  lattice_size: [8, 8, 8]
  embedding_dim: 768
  batch_size: 16

  # –û–±—É—á–µ–Ω–∏–µ
  learning_rate: 0.0005
  epochs: 30
  optimizer: "adam"
  loss_function: "cosine"

  # –ö–∞—á–µ—Å—Ç–≤–æ
  target_similarity: 0.92
  convergence_threshold: 0.0005
  early_stopping_patience: 5

  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
  log_interval: 5
  save_interval: 10
  checkpoint_dir: "checkpoints/example_training"

  # –î–∞–Ω–Ω—ã–µ
  autoencoder_data:
    source_type: "embedding_loader"
    cache_embeddings: true
    max_samples: 1000
```

### –ü—Ä–∏–º–µ—Ä 14: –ü—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ
training_config = {
    'mode': 'dialogue',
    'device': 'cpu',
    'lattice_size': [6, 6, 6],  # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    'embedding_dim': 768,
    'batch_size': 8,
    'learning_rate': 0.002,
    'epochs': 20,
    'target_similarity': 0.88
}

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
trainer.update_config(training_config)
print(f"Configuration updated: {trainer.get_config()}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Configuration updated: {'mode': 'dialogue', 'device': 'cpu', 'lattice_size': [6, 6, 6], ...}
```

---

---

## üÜï –ê–í–¢–ûENCODER DATASET –ü–†–ò–ú–ï–†–´ (Stage 1.2)

### –ü—Ä–∏–º–µ—Ä 15: –°–æ–∑–¥–∞–Ω–∏–µ AutoencoderDataset –∏–∑ –¢–µ–∫—Å—Ç–æ–≤ ‚≠ê NEW!

```python
from training.embedding_trainer import create_text_dataset

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
texts = [
    "Machine learning enables intelligent systems",
    "Neural networks process complex patterns",
    "Deep learning transforms artificial intelligence",
    "Natural language processing understands text",
    "Computer vision recognizes images and objects"
]

# –°–æ–∑–¥–∞–Ω–∏–µ dataset —Å full configuration
dataset = create_text_dataset(
    texts=texts,
    llm_model="distilbert",           # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 8+ LLM –º–æ–¥–µ–ª–µ–π
    validation_split=0.2,            # 20% –¥–ª—è validation
    use_cache=True,                   # Smart caching
    cache_dir="cache/my_experiment",
    normalize_embeddings=True,        # Normalization
    add_noise=True,                   # Regularization
    noise_std=0.01,                   # Noise level
    random_seed=42                    # Reproducibility
)

print(f"Dataset —Å–æ–∑–¥–∞–Ω: {dataset}")
print(f"Train samples: {len(dataset.train_embeddings)}")
print(f"Val samples: {len(dataset.val_embeddings)}")
print(f"Embedding dim: {dataset.config.embedding_dim}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ DataLoaders
train_loader = dataset.get_dataloader(batch_size=32, validation=False)
val_loader = dataset.get_dataloader(batch_size=32, validation=True)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ autoencoder format
for input_emb, target_emb in train_loader:
    print(f"Autoencoder pair: {input_emb.shape} -> {target_emb.shape}")
    # input_emb == target_emb –¥–ª—è autoencoder —Ä–µ–∂–∏–º–∞
    similarity = torch.cosine_similarity(input_emb, target_emb, dim=1).mean()
    print(f"Target similarity: {similarity:.4f}")
    break
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Dataset —Å–æ–∑–¥–∞–Ω: AutoencoderDataset(samples=5, dim=768, train=4, val=1, mode=train)
Train samples: 4
Val samples: 1
Embedding dim: 768
Autoencoder pair: torch.Size([4, 768]) -> torch.Size([4, 768])
Target similarity: 1.0000
```

### –ü—Ä–∏–º–µ—Ä 16: –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ –§–∞–π–ª–æ–≤ ‚≠ê NEW!

```python
from training.embedding_trainer import create_file_dataset
import torch

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
texts_file = "data/training_texts.txt"
with open(texts_file, 'w', encoding='utf-8') as f:
    f.write("First training sentence\n")
    f.write("Second training sentence\n")
    f.write("Third training sentence\n")

# –°–æ–∑–¥–∞–Ω–∏–µ PyTorch —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
embeddings_file = "data/precomputed_embeddings.pt"
torch.save(torch.randn(10, 768), embeddings_file)

# –°–æ–∑–¥–∞–Ω–∏–µ dataset –∏–∑ —Ñ–∞–π–ª–æ–≤
file_dataset = create_file_dataset(
    file_paths=[texts_file, embeddings_file],
    embedding_format="llm",
    llm_model="distilbert",
    validation_split=0.15,
    cache_dir="cache/file_experiment"
)

print(f"File dataset: {file_dataset}")
print(f"Total samples: {len(file_dataset.embeddings)}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats = file_dataset.get_statistics()
print(f"Dataset statistics:")
for key, value in stats.items():
    if isinstance(value, (int, float)):
        print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
File dataset: AutoencoderDataset(samples=13, dim=768, train=11, val=2, mode=train)
Total samples: 13
Dataset statistics:
  total_samples: 13
  train_samples: 11
  val_samples: 2
  embedding_dim: 768
  validation_split: 0.15
```

### –ü—Ä–∏–º–µ—Ä 17: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ DatasetConfig ‚≠ê NEW!

```python
from training.embedding_trainer import AutoencoderDataset, DatasetConfig

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
config = DatasetConfig(
    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    llm_model="llama2-7b",           # –í—ã–±–æ—Ä LLM –º–æ–¥–µ–ª–∏
    max_samples=1000,                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞

    # Preprocessing
    normalize_embeddings=True,
    center_embeddings=True,
    add_noise=False,                 # –ë–µ–∑ —à—É–º–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è

    # Caching
    cache_dir="cache/production",
    use_cache=True,
    cache_embeddings=True,
    cache_batch_size=500,            # –†–∞–∑–º–µ—Ä batch –¥–ª—è caching

    # Validation
    validation_split=0.25,           # 25% –¥–ª—è validation
    shuffle_data=True,
    random_seed=123
)

# –°–æ–∑–¥–∞–Ω–∏–µ dataset —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
production_texts = [
    "Production example text one",
    "Production example text two",
    "Production example text three",
    "Production example text four"
]

production_dataset = AutoencoderDataset(
    config=config,
    texts=production_texts
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ sample embeddings –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
samples = production_dataset.get_sample_embeddings(n_samples=2)
print("Sample embeddings:")
for split, embs in samples.items():
    print(f"  {split}: {embs.shape}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ dataset
production_dataset.save_dataset_info("production_dataset_info.json")
print("Dataset info saved to production_dataset_info.json")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Sample embeddings:
  train: torch.Size([2, 768])
  validation: torch.Size([1, 768])
Dataset info saved to production_dataset_info.json
```

### –ü—Ä–∏–º–µ—Ä 18: Smart Caching System ‚≠ê NEW!

```python
import time

# –ü–µ—Ä–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ - cache miss
start_time = time.time()
first_dataset = create_text_dataset(
    texts=["Caching test text", "Another cache test"],
    llm_model="distilbert",
    cache_dir="cache/caching_test",
    use_cache=True
)
first_time = time.time() - start_time

print(f"First creation time: {first_time:.2f}s")
print(f"Cache stats: {first_dataset.cache_stats}")

# –í—Ç–æ—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ - cache hit
start_time = time.time()
second_dataset = create_text_dataset(
    texts=["Caching test text", "Another cache test"],  # –¢–µ –∂–µ —Ç–µ–∫—Å—Ç—ã
    llm_model="distilbert",
    cache_dir="cache/caching_test",
    use_cache=True
)
second_time = time.time() - start_time

print(f"Second creation time: {second_time:.2f}s")
print(f"Cache stats: {second_dataset.cache_stats}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—ç—à–∞
if second_time < first_time:
    speedup = first_time / second_time
    print(f"‚úÖ Cache —Ä–∞–±–æ—Ç–∞–µ—Ç! Speedup: {speedup:.1f}x")
else:
    print("‚ö†Ô∏è  Cache –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
First creation time: 1.23s
Cache stats: {'cache_hits': 0, 'cache_misses': 1, 'total_loads': 1}
Second creation time: 0.15s
Cache stats: {'cache_hits': 1, 'cache_misses': 0, 'total_loads': 1}
‚úÖ Cache —Ä–∞–±–æ—Ç–∞–µ—Ç! Speedup: 8.2x
```

### –ü—Ä–∏–º–µ—Ä 19: Train/Validation Mode Switching ‚≠ê NEW!

```python
# –°–æ–∑–¥–∞–Ω–∏–µ dataset —Å validation split
dataset = create_text_dataset(
    texts=[f"Training text {i}" for i in range(10)],
    validation_split=0.3  # 30% –¥–ª—è validation
)

print(f"Original mode: {dataset.is_validation_mode}")
print(f"Original length: {len(dataset)}")

# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ validation —Ä–µ–∂–∏–º
dataset.set_validation_mode(True)
print(f"Validation mode: {dataset.is_validation_mode}")
print(f"Validation length: {len(dataset)}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
dataset.set_validation_mode(False)
train_sample = dataset[0]
dataset.set_validation_mode(True)
val_sample = dataset[0]

print(f"Train sample shapes: {train_sample[0].shape}, {train_sample[1].shape}")
print(f"Val sample shapes: {val_sample[0].shape}, {val_sample[1].shape}")

# –í–æ–∑–≤—Ä–∞—Ç –≤ train —Ä–µ–∂–∏–º
dataset.set_validation_mode(False)
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Original mode: False
Original length: 7
Validation mode: True
Validation length: 3
Train sample shapes: torch.Size([768]), torch.Size([768])
Val sample shapes: torch.Size([768]), torch.Size([768])
```

### –ü—Ä–∏–º–µ—Ä 20: Integration —Å CubeTrainer ‚≠ê NEW!

```python
from training.embedding_trainer import CubeTrainer, TrainingConfig

# –°–æ–∑–¥–∞–Ω–∏–µ dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
training_texts = [
    "Neural networks learn representations",
    "Deep learning processes complex data",
    "Machine learning finds hidden patterns",
    "Artificial intelligence mimics cognition"
]

autoencoder_dataset = create_text_dataset(
    texts=training_texts,
    validation_split=0.25,
    normalize_embeddings=True,
    add_noise=True,         # Regularization for training
    noise_std=0.02
)

# –°–æ–∑–¥–∞–Ω–∏–µ CubeTrainer
config = TrainingConfig(
    mode="autoencoder",
    batch_size=16,
    learning_rate=0.001,
    epochs=10,
    target_similarity=0.90
)

trainer = CubeTrainer(config=config)
trainer.initialize_components()

# –ü–æ–ª—É—á–µ–Ω–∏–µ DataLoaders
train_loader = autoencoder_dataset.get_dataloader(
    batch_size=config.batch_size,
    validation=False
)
val_loader = autoencoder_dataset.get_dataloader(
    batch_size=config.batch_size,
    validation=True
)

print(f"CubeTrainer –≥–æ—Ç–æ–≤: {trainer}")
print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ forward pass
for input_batch, target_batch in train_loader:
    try:
        output_batch = trainer.forward(input_batch)
        metrics = trainer.metrics.compute_batch_metrics(input_batch, output_batch)
        print(f"Forward pass successful!")
        print(f"Metrics: {metrics}")
        break
    except Exception as e:
        print(f"Forward pass error: {e}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
CubeTrainer –≥–æ—Ç–æ–≤: CubeTrainer(mode=autoencoder, device=cpu, lattice=[8, 8, 8])
Train batches: 1
Val batches: 1
Forward pass successful!
Metrics: {'cosine_similarity': 0.9876, 'mse_loss': 0.0234, 'semantic_preservation': 0.9654}
```

---

**üéØ –ü–†–ò–ù–¶–ò–ü: –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–µ—Å—Ç–∏—Ä—É–µ–º—ã–º–∏ –∏ —Ä–∞–±–æ—á–∏–º–∏**

_–ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é._
