"""
Advanced Training Stage 2.3 - Integrated System
–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è 50%+ Q‚ÜíA similarity
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import numpy as np
import json
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö —Å–∏—Å—Ç–µ–º
from .cube_trainer import CubeTrainer, TrainingConfig
from .advanced_dataset_expansion import AdvancedDatasetExpander, create_expanded_dataset
from .advanced_loss_functions import AdvancedLossFunction, create_advanced_loss_function, NegativeSampler
from .multi_teacher_distillation import MultiTeacherDistillation, create_multi_teacher_system
from .dialogue_dataset import DialogueDataset


@dataclass
class Stage23Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Stage 2.3 Advanced Training Enhancement"""
    # Dataset expansion
    target_pairs: int = 100                        # –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä
    quality_threshold: float = 0.6                 # –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    
    # Advanced loss functions
    use_curriculum_learning: bool = True           # Curriculum learning
    use_triplet_loss: bool = True                  # Triplet loss
    use_contrastive_loss: bool = True              # Contrastive learning
    curriculum_warmup_epochs: int = 5              # –≠–ø–æ—Ö–∏ –¥–ª—è curriculum warmup
    
    # Multi-teacher distillation
    use_multi_teacher: bool = True                 # Multi-teacher knowledge distillation
    teacher_models: List[str] = None               # Teacher –º–æ–¥–µ–ª–∏
    distillation_temperature: float = 3.0          # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ distillation
    
    # Training optimization
    learning_rate: float = 0.0003                  # Optimized learning rate
    batch_size: int = 6                            # Optimized batch size
    epochs: int = 15                               # Target epochs
    
    # Target metrics
    target_qa_similarity: float = 0.50            # 50%+ Q‚ÜíA similarity goal
    convergence_threshold: float = 0.01           # Training convergence
    validation_patience: int = 5                   # Early stopping patience
    
    def __post_init__(self):
        if self.teacher_models is None:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
            try:
                from utils.config_loader import get_multi_teacher_config
                config = get_multi_teacher_config()
                self.teacher_models = config.get('models', ['distilbert'])
                print(f"üìã Loaded teacher models from config: {self.teacher_models}")
            except Exception:
                self.teacher_models = ["llama3-8b-local", "distilbert", "roberta"]  # Fallback
                print(f"‚ö†Ô∏è Using fallback teacher models: {self.teacher_models}")


class AdvancedTrainingStage23:
    """
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Stage 2.3 Advanced Training Enhancement
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç:
    - Advanced Dataset Expansion (100+ pairs)
    - Advanced Loss Functions (curriculum + triplet + contrastive)
    - Multi-Teacher Knowledge Distillation
    - Optimized Training Pipeline
    """
    
    def __init__(self, config: Optional[Stage23Config] = None):
        self.config = config or Stage23Config()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.dataset_expander = None
        self.multi_teacher = None
        self.advanced_loss_fn = None
        self.negative_sampler = None
        self.cube_trainer = None
        
        # Training state
        self.current_epoch = 0
        self.training_history = []
        self.best_qa_similarity = 0.0
        self.patience_counter = 0
        
        print(f"üöÄ AdvancedTrainingStage23 initialized")
        print(f"   Target Q‚ÜíA similarity: {self.config.target_qa_similarity:.1%}")
        print(f"   Target dataset size: {self.config.target_pairs} pairs")
        print(f"   Multi-teacher models: {len(self.config.teacher_models)}")
    
    def setup_training_components(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        print("üîß Setting up advanced training components...")
        
        # 1. Advanced Loss Functions
        self.advanced_loss_fn = create_advanced_loss_function(
            use_curriculum=self.config.use_curriculum_learning,
            use_triplet=self.config.use_triplet_loss,
            use_contrastive=self.config.use_contrastive_loss,
            curriculum_warmup_epochs=self.config.curriculum_warmup_epochs
        )
        
        # 2. Negative Sampler –¥–ª—è contrastive learning
        self.negative_sampler = NegativeSampler(embedding_dim=768)
        
        # 3. Multi-Teacher System
        if self.config.use_multi_teacher:
            self.multi_teacher = create_multi_teacher_system(
                teacher_models=self.config.teacher_models,
                adaptive_weighting=True,
                distillation_temperature=self.config.distillation_temperature
            )
        
        # 4. CubeTrainer —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        training_config = TrainingConfig(
            mode="dialogue",
            lattice_size=[8, 8, 12],  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è 768D —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
            learning_rate=self.config.learning_rate,
            batch_size=self.config.batch_size,
            epochs=self.config.epochs,
            optimizer="adamw",
            loss_function="advanced"  # –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É advanced loss
        )
        
        self.cube_trainer = CubeTrainer(config=training_config)
        self.cube_trainer.initialize_components()
        
        print("‚úÖ All training components setup complete!")
    
    def _normalize_embedding_dimensions(self, embeddings: torch.Tensor, target_dim: int = 768) -> torch.Tensor:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–æ target_dim
        
        Args:
            embeddings: –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
            target_dim: –¶–µ–ª–µ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 768)
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ —Ä–∞–∑–º–µ—Ä–∞ target_dim
        """
        if embeddings.size(-1) == target_dim:
            return embeddings
        
        # –ï—Å–ª–∏ —ç–º–±–µ–¥–∏–Ω–≥–∏ –±–æ–ª—å—à–µ —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ - –æ–±—Ä–µ–∑–∞–µ–º
        if embeddings.size(-1) > target_dim:
            return embeddings[..., :target_dim]
        
        # –ï—Å–ª–∏ —ç–º–±–µ–¥–∏–Ω–≥–∏ –º–µ–Ω—å—à–µ - –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
        else:
            padding_size = target_dim - embeddings.size(-1)
            padding = torch.zeros(*embeddings.shape[:-1], padding_size, dtype=embeddings.dtype, device=embeddings.device)
            return torch.cat([embeddings, padding], dim=-1)
    
    def create_enhanced_dataset(self) -> DialogueDataset:
        """–°–æ–∑–¥–∞–Ω–∏–µ enhanced dataset —Å expanded data –∏ multi-teacher embeddings"""
        print("üéØ Creating enhanced dataset for Stage 2.3...")
        
        # 1. Dataset Expansion –¥–æ 100+ pairs
        expanded_dataset = create_expanded_dataset(
            target_pairs=self.config.target_pairs,
            quality_threshold=0.6  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        )
        
        print(f"   ‚úÖ Dataset expanded to {len(expanded_dataset)} pairs")
        
        # 2. Multi-Teacher Enhancement (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if self.config.use_multi_teacher and self.multi_teacher:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ dialogue pairs –∏–∑ dataset
            dialogue_pairs = []
            for i in range(len(expanded_dataset)):
                question_emb, answer_emb = expanded_dataset[i]
                # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º placeholder
                dialogue_pairs.append({
                    "question": f"Enhanced question {i}",
                    "answer": f"Enhanced answer {i}"
                })
            
            # –°–æ–∑–¥–∞–Ω–∏–µ ensemble dataset
            ensemble_data = self.multi_teacher.create_ensemble_dataset(
                dialogue_pairs, validation_split=0.2
            )
            
            print(f"   ‚úÖ Multi-teacher ensemble created")
            print(f"      Train samples: {len(ensemble_data['train']['question_embeddings'])}")
            print(f"      Validation samples: {len(ensemble_data['validation']['question_embeddings'])}")
        
        return expanded_dataset
    
    def run_advanced_training(self, dataset: DialogueDataset) -> Dict[str, float]:
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Stage 2.3"""
        print("üöÄ Starting Stage 2.3 Advanced Training...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ dataloader
        train_dataloader = dataset.get_dataloader(
            batch_size=self.config.batch_size,
            shuffle=True,
            validation=False
        )
        
        val_dataloader = dataset.get_dataloader(
            batch_size=self.config.batch_size,
            shuffle=False,
            validation=True
        )
        
        # Training loop
        for epoch in range(self.config.epochs):
            self.current_epoch = epoch
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ curriculum learning progress
            self.advanced_loss_fn.update_epoch(epoch, self.config.epochs)
            
            # Training phase
            train_metrics = self._train_epoch(train_dataloader)
            
            # Validation phase
            val_metrics = self._validate_epoch(val_dataloader)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self._log_epoch_progress(epoch, train_metrics, val_metrics)
            
            # Early stopping check
            if self._check_early_stopping(val_metrics):
                print(f"üõë Early stopping at epoch {epoch}")
                break
            
            # Save checkpoint –µ—Å–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ
            if val_metrics["qa_similarity"] > self.best_qa_similarity:
                self.best_qa_similarity = val_metrics["qa_similarity"]
                self._save_checkpoint(epoch, val_metrics)
                self.patience_counter = 0
            else:
                self.patience_counter += 1
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_results = self._compute_final_results()
        
        print(f"üéâ Stage 2.3 Training Complete!")
        print(f"   Best Q‚ÜíA similarity: {self.best_qa_similarity:.1%}")
        print(f"   Target achieved: {'‚úÖ' if self.best_qa_similarity >= self.config.target_qa_similarity else '‚ùå'}")
        
        return final_results
    
    def _train_epoch(self, dataloader) -> Dict[str, float]:
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.cube_trainer.embedding_processor.train()
        
        epoch_losses = []
        epoch_qa_similarities = []
        
        for batch_idx, (question_embeddings, answer_embeddings) in enumerate(dataloader):
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–æ –æ–∂–∏–¥–∞–µ–º–æ–π —Å–∏—Å—Ç–µ–º–æ–π (768D)
            question_embeddings = self._normalize_embedding_dimensions(question_embeddings)
            answer_embeddings = self._normalize_embedding_dimensions(answer_embeddings)
            
            # Forward pass —á–µ—Ä–µ–∑ cube
            output_embeddings = self.cube_trainer.forward(question_embeddings)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            negative_embeddings = self.negative_sampler.sample_random_negatives(
                answer_embeddings, num_negatives=5
            )
            
            # Difficulty scores –¥–ª—è curriculum learning (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            difficulty_scores = torch.rand(question_embeddings.size(0))
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ advanced loss
            losses = self.advanced_loss_fn(
                input_embeddings=question_embeddings,
                target_embeddings=answer_embeddings,
                output_embeddings=output_embeddings,
                difficulty_scores=difficulty_scores,
                negative_embeddings=negative_embeddings
            )
            
            # Backward pass
            self.cube_trainer.optimizer.zero_grad()
            losses["total_loss"].backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.cube_trainer.embedding_processor.parameters(), 
                max_norm=1.0
            )
            
            self.cube_trainer.optimizer.step()
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            qa_similarity = torch.cosine_similarity(
                output_embeddings, answer_embeddings, dim=1
            ).mean().item()
            
            epoch_losses.append(losses["total_loss"].item())
            epoch_qa_similarities.append(qa_similarity)
        
        return {
            "train_loss": np.mean(epoch_losses),
            "qa_similarity": np.mean(epoch_qa_similarities)
        }
    
    def _validate_epoch(self, dataloader) -> Dict[str, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.cube_trainer.embedding_processor.eval()
        
        val_losses = []
        val_qa_similarities = []
        
        with torch.no_grad():
            for question_embeddings, answer_embeddings in dataloader:
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
                question_embeddings = self._normalize_embedding_dimensions(question_embeddings)
                answer_embeddings = self._normalize_embedding_dimensions(answer_embeddings)
                
                # Forward pass
                output_embeddings = self.cube_trainer.forward(question_embeddings)
                
                # Simple validation loss (cosine similarity)
                loss = 1 - torch.cosine_similarity(
                    output_embeddings, answer_embeddings, dim=1
                ).mean()
                
                qa_similarity = torch.cosine_similarity(
                    output_embeddings, answer_embeddings, dim=1
                ).mean().item()
                
                val_losses.append(loss.item())
                val_qa_similarities.append(qa_similarity)
        
        return {
            "val_loss": np.mean(val_losses),
            "qa_similarity": np.mean(val_qa_similarities)
        }
    
    def _log_epoch_progress(self, epoch: int, train_metrics: Dict, val_metrics: Dict):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —ç–ø–æ—Ö–∏"""
        curriculum_progress = self.advanced_loss_fn.get_curriculum_progress()
        
        print(f"Epoch {epoch+1}/{self.config.epochs}:")
        print(f"  Train Loss: {train_metrics['train_loss']:.4f} | "
              f"Q‚ÜíA Sim: {train_metrics['qa_similarity']:.1%}")
        print(f"  Val Loss: {val_metrics['val_loss']:.4f} | "
              f"Q‚ÜíA Sim: {val_metrics['qa_similarity']:.1%}")
        print(f"  Curriculum Progress: {curriculum_progress:.1%} | "
              f"Best Q‚ÜíA: {self.best_qa_similarity:.1%}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.training_history.append({
            "epoch": epoch + 1,
            "train_loss": train_metrics["train_loss"],
            "train_qa_similarity": train_metrics["qa_similarity"],
            "val_loss": val_metrics["val_loss"],
            "val_qa_similarity": val_metrics["qa_similarity"],
            "curriculum_progress": curriculum_progress
        })
    
    def _check_early_stopping(self, val_metrics: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ early stopping"""
        return self.patience_counter >= self.config.validation_patience
    
    def _save_checkpoint(self, epoch: int, metrics: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint"""
        checkpoint_dir = Path("checkpoints/stage_2_3")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            "epoch": epoch,
            "model_state": self.cube_trainer.embedding_processor.state_dict(),
            "optimizer_state": self.cube_trainer.optimizer.state_dict(),
            "metrics": metrics,
            "config": self.config,
            "training_history": self.training_history
        }
        
        torch.save(checkpoint, checkpoint_dir / f"best_model_epoch_{epoch}.pt")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ JSON
        with open(checkpoint_dir / "training_results.json", "w") as f:
            json.dump(self.training_history, f, indent=2)
    
    def _compute_final_results(self) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        return {
            "best_qa_similarity": self.best_qa_similarity,
            "target_achieved": self.best_qa_similarity >= self.config.target_qa_similarity,
            "improvement_from_stage_2_2": self.best_qa_similarity - 0.3189,  # Stage 2.2 result
            "total_epochs": self.current_epoch + 1,
            "final_curriculum_progress": self.advanced_loss_fn.get_curriculum_progress()
        }
    
    def get_training_summary(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ summary –æ–±—É—á–µ–Ω–∏—è"""
        return {
            "config": {
                "target_pairs": self.config.target_pairs,
                "target_qa_similarity": self.config.target_qa_similarity,
                "use_multi_teacher": self.config.use_multi_teacher,
                "use_curriculum_learning": self.config.use_curriculum_learning
            },
            "results": self._compute_final_results(),
            "training_history": self.training_history,
            "loss_weights": self.advanced_loss_fn.get_loss_weights() if self.advanced_loss_fn else {}
        }


# ================================
# HELPER FUNCTIONS
# ================================

def run_stage_2_3_training(
    target_qa_similarity: float = 0.50,
    target_pairs: int = 100,
    epochs: int = 15,
    use_multi_teacher: bool = True
) -> Dict[str, float]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Stage 2.3 training
    
    Args:
        target_qa_similarity: –¶–µ–ª–µ–≤–∞—è Q‚ÜíA similarity (0.50 = 50%)
        target_pairs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ dialogue pairs
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        use_multi_teacher: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å multi-teacher distillation
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    """
    config = Stage23Config(
        target_qa_similarity=target_qa_similarity,
        target_pairs=target_pairs,
        epochs=epochs,
        use_multi_teacher=use_multi_teacher
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ training system
    trainer = AdvancedTrainingStage23(config)
    trainer.setup_training_components()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ enhanced dataset
    dataset = trainer.create_enhanced_dataset()
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    results = trainer.run_advanced_training(dataset)
    
    return results


def analyze_stage_2_3_progress(training_history: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ Stage 2.3"""
    if not training_history:
        return {"error": "No training history available"}
    
    final_epoch = training_history[-1]
    first_epoch = training_history[0]
    
    return {
        "total_epochs": len(training_history),
        "final_qa_similarity": final_epoch["val_qa_similarity"],
        "initial_qa_similarity": first_epoch["val_qa_similarity"],
        "improvement": final_epoch["val_qa_similarity"] - first_epoch["val_qa_similarity"],
        "best_qa_similarity": max(epoch["val_qa_similarity"] for epoch in training_history),
        "convergence_achieved": final_epoch["val_loss"] < 0.1,
        "curriculum_completed": final_epoch["curriculum_progress"] > 0.9
    }


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Stage 2.3
    print("üöÄ Testing Stage 2.3 Advanced Training Enhancement...")
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    results = run_stage_2_3_training(
        target_qa_similarity=0.50,  # 50% target
        target_pairs=100,
        epochs=15,
        use_multi_teacher=True
    )
    
    print(f"üìä Stage 2.3 Results:")
    for key, value in results.items():
        if isinstance(value, float):
            if "similarity" in key:
                print(f"   {key}: {value:.1%}")
            else:
                print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    print("\n‚úÖ Stage 2.3 Advanced Training Enhancement system ready!") 