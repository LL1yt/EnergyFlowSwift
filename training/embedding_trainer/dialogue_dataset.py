"""
DialogueDataset - –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫ –æ–±—É—á–µ–Ω–∏—é –∫—É–±–∞ –≤ dialogue —Ä–µ–∂–∏–º–µ

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core
–Ω–∞ –∑–∞–¥–∞—á–∞—Ö –¥–∏–∞–ª–æ–≥–∞ (question_embedding ‚Üí answer_embedding).

–ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Teacher LLM (LLaMA 3, Mistral, etc.) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ Q&A
- Conversation pairs: (question_embedding, answer_embedding)
- Smart caching –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
- Multi-turn dialogue support
- Quality filtering –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä
- Context-aware training –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: v1.0.0 (Phase 3.1 - Stage 1.3)
–î–∞—Ç–∞: 6 –∏—é–Ω—è 2025
"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
import logging
from typing import Dict, Any, Optional, Union, List, Tuple
import json
import pickle
import hashlib
from dataclasses import dataclass
import random
import re

# –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from data.embedding_loader import EmbeddingLoader
    from data.embedding_loader.format_handlers import SUPPORTED_LLM_MODELS
    EMBEDDING_LOADER_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  Warning: EmbeddingLoader not available: {e}")
    EMBEDDING_LOADER_AVAILABLE = False


@dataclass
class DialogueConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è DialogueDataset"""
    # Teacher LLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    teacher_model: str = "llama3-8b"    # –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    fallback_model: str = "distilbert"   # –ó–∞–ø–∞—Å–Ω–∞—è –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
    embedding_dim: int = 768             # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    
    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    data_sources: List[str] = None       # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –¥–∏–∞–ª–æ–≥–∞–º–∏
    dialogue_format: str = "qa_pairs"    # qa_pairs | conversation | json
    max_conversations: int = 5000        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤
    min_conversations: int = 50          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤
    
    # Multi-turn –¥–∏–∞–ª–æ–≥–∏
    support_multiturn: bool = True       # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    max_turns_per_conversation: int = 10 # –ú–∞–∫—Å–∏–º—É–º —Ä–µ–ø–ª–∏–∫ –≤ –¥–∏–∞–ª–æ–≥–µ
    context_window: int = 2              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–ø–ª–∏–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    # Quality filtering
    enable_quality_filter: bool = True   # –í–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–∞—á–µ—Å—Ç–≤–∞
    min_question_length: int = 5         # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞ (—Å–∏–º–≤–æ–ª—ã)
    min_answer_length: int = 10          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)
    max_question_length: int = 512       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞
    max_answer_length: int = 1024        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
    semantic_similarity_threshold: float = 0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å Q&A
    
    # Preprocessing
    normalize_embeddings: bool = True    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    center_embeddings: bool = True       # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    add_context_noise: bool = False      # –î–æ–±–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —à—É–º
    context_noise_std: float = 0.02     # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —à—É–º–∞
    
    # Caching
    cache_dir: str = "cache/dialogue_dataset"
    use_cache: bool = True
    cache_embeddings: bool = True
    cache_batch_size: int = 500          # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    
    # Validation split
    validation_split: float = 0.2        # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    shuffle_conversations: bool = True    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∏–∞–ª–æ–≥–∏
    random_seed: int = 42                # Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    def __post_init__(self):
        if self.data_sources is None:
            self.data_sources = []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫—ç—à–∞
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._load_from_central_config()
    
    def _load_from_central_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            from utils.config_loader import config_manager
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º teacher models –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            teacher_config = config_manager.get_teacher_models_config()
            if teacher_config:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é
                if 'models' in teacher_config and teacher_config['models']:
                    available_models = teacher_config['models']
                    self.teacher_model = available_models[0]
                    if len(available_models) > 1:
                        self.fallback_model = available_models[1]
                    
                    print(f"üìã Loaded teacher models from central config:")
                    print(f"   Primary: {self.teacher_model}")
                    print(f"   Fallback: {self.fallback_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            general_config = config_manager.get_config()
            if general_config:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
                if 'dialogue_dataset' in general_config:
                    dialogue_settings = general_config['dialogue_dataset']
                    
                    if 'quality_filter' in dialogue_settings:
                        quality_settings = dialogue_settings['quality_filter']
                        self.min_question_length = quality_settings.get('min_question_length', self.min_question_length)
                        self.min_answer_length = quality_settings.get('min_answer_length', self.min_answer_length)
                        self.semantic_similarity_threshold = quality_settings.get('semantic_similarity_threshold', self.semantic_similarity_threshold)
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
                    if 'caching' in dialogue_settings:
                        cache_settings = dialogue_settings['caching']
                        self.use_cache = cache_settings.get('enabled', self.use_cache)
                        self.cache_batch_size = cache_settings.get('batch_size', self.cache_batch_size)
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    if 'validation' in dialogue_settings:
                        val_settings = dialogue_settings['validation']
                        self.validation_split = val_settings.get('split', self.validation_split)
                        self.random_seed = val_settings.get('seed', self.random_seed)
                
                print(f"‚úÖ DialogueConfig integrated with central configuration")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load from central config ({e}), using defaults")


def map_model_name_to_key(model_name: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –≤ –∫–ª—é—á –¥–ª—è SUPPORTED_LLM_MODELS
    
    Args:
        model_name: –ü–æ–ª–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "distilbert-base-uncased")
        
    Returns:
        –ö–ª—é—á –º–æ–¥–µ–ª–∏ –¥–ª—è SUPPORTED_LLM_MODELS (–Ω–∞–ø—Ä–∏–º–µ—Ä, "distilbert")
    """
    # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ value -> key
    name_to_key = {v: k for k, v in SUPPORTED_LLM_MODELS.items()}
    
    # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if model_name in name_to_key:
        return name_to_key[model_name]
    
    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ –∫–ª—é—á
    if model_name in SUPPORTED_LLM_MODELS:
        return model_name
    
    # –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
    for model_value, model_key in name_to_key.items():
        if model_name in model_value or model_value in model_name:
            return model_key
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ mappings –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–º–µ–Ω
    common_mappings = {
        "distilbert-base-uncased": "distilbert",
        "distilbert": "distilbert",
        "roberta-base": "roberta", 
        "roberta": "roberta",
        "gpt2": "gpt2",
        "sentence-transformers/all-MiniLM-L6-v2": "distilbert",  # fallback
        "sentence-transformers/all-mpnet-base-v2": "distilbert", # fallback
    }
    
    if model_name in common_mappings:
        return common_mappings[model_name]
    
    # Fallback - –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º distilbert
    print(f"‚ö†Ô∏è Model '{model_name}' not found in SUPPORTED_LLM_MODELS, using 'distilbert' as fallback")
    return "distilbert"


class DialogueDataset(Dataset):
    """
    Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core –≤ dialogue —Ä–µ–∂–∏–º–µ
    
    –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä—ã (question_embedding, answer_embedding) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å ‚Üí –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Teacher LLM —ç–º–±–µ–¥–∏–Ω–≥–∏.
    """
    
    def __init__(self, 
                 config: Optional[Union[DialogueConfig, Dict, str]] = None,
                 dialogue_pairs: Optional[List[Dict]] = None,
                 conversations: Optional[List[List[Dict]]] = None,
                 question_embeddings: Optional[torch.Tensor] = None,
                 answer_embeddings: Optional[torch.Tensor] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DialogueDataset
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è dataset (DialogueConfig, dict –∏–ª–∏ –ø—É—Ç—å –∫ JSON)
            dialogue_pairs: –°–ø–∏—Å–æ–∫ Q&A –ø–∞—Ä [{"question": str, "answer": str}, ...]
            conversations: –°–ø–∏—Å–æ–∫ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ [[{role, text}, ...], ...]
            question_embeddings: –ì–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            answer_embeddings: –ì–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç–≤–µ—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info("üöÄ Initializing DialogueDataset for Stage 1.3...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        if not EMBEDDING_LOADER_AVAILABLE:
            raise ImportError("EmbeddingLoader is required for DialogueDataset. "
                            "Make sure data.embedding_loader is implemented.")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(config)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed
        random.seed(self.config.random_seed)
        np.random.seed(self.config.random_seed)
        torch.manual_seed(self.config.random_seed)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Teacher LLM Encoder
        self.embedding_loader = EmbeddingLoader(
            cache_dir=str(Path(self.config.cache_dir) / "embedding_loader_cache")
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ teacher –º–æ–¥–µ–ª–∏
        self._validate_teacher_model()
        
        # –î–∞–Ω–Ω—ã–µ
        self.question_embeddings: torch.Tensor = None
        self.answer_embeddings: torch.Tensor = None
        self.train_questions: torch.Tensor = None
        self.train_answers: torch.Tensor = None
        self.val_questions: torch.Tensor = None
        self.val_answers: torch.Tensor = None
        self.is_validation_mode: bool = False
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.dialogue_metadata = []
        self.dataset_info = {}
        self.cache_stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'total_loads': 0,
            'quality_filtered': 0
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if question_embeddings is not None and answer_embeddings is not None:
            self.logger.info("Using provided Q&A embeddings")
            self._load_from_embeddings(question_embeddings, answer_embeddings)
        elif dialogue_pairs is not None:
            self.logger.info("Generating embeddings from dialogue pairs")
            self._load_from_dialogue_pairs(dialogue_pairs)
        elif conversations is not None:
            self.logger.info("Processing multi-turn conversations")
            self._load_from_conversations(conversations)
        elif self.config.data_sources:
            self.logger.info("Loading dialogues from configured sources")
            self._load_from_sources()
        else:
            raise ValueError("No dialogue data provided. Specify dialogue_pairs, conversations, "
                           "embeddings, or data_sources in config.")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ train/val split
        self._create_train_val_split()
        
        self.logger.info(f"‚úÖ DialogueDataset initialized successfully")
        self.logger.info(f"   Total conversation pairs: {len(self.question_embeddings)}")
        self.logger.info(f"   Train pairs: {len(self.train_questions)}")
        self.logger.info(f"   Val pairs: {len(self.val_questions)}")
        self.logger.info(f"   Embedding dim: {self.question_embeddings.shape[1]}")
        self.logger.info(f"   Teacher model: {self.config.teacher_model}")
        self.logger.info(f"   Quality filtered: {self.cache_stats['quality_filtered']} pairs")
    
    def _load_config(self, config: Optional[Union[DialogueConfig, Dict, str]]) -> DialogueConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if config is None:
            return DialogueConfig()
        
        elif isinstance(config, DialogueConfig):
            return config
        
        elif isinstance(config, dict):
            return DialogueConfig(**config)
        
        elif isinstance(config, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ JSON —Ñ–∞–π–ª–∞
            try:
                with open(config, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                return DialogueConfig(**config_data)
            except Exception as e:
                self.logger.error(f"Failed to load config from {config}: {e}")
                return DialogueConfig()
        
        else:
            self.logger.warning(f"Unknown config type: {type(config)}. Using default config.")
            return DialogueConfig()
    
    def _validate_teacher_model(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ teacher –º–æ–¥–µ–ª–∏"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –≤ –∫–ª—é—á
            teacher_model_key = map_model_name_to_key(self.config.teacher_model)
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
            test_embedding = self.embedding_loader.load_from_llm(
                texts=["Test message"],
                model_key=teacher_model_key
            )
            self.logger.info(f"‚úÖ Teacher model {self.config.teacher_model} (key: {teacher_model_key}) is available")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º config —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–ª—é—á–æ–º
            self.config.teacher_model = teacher_model_key
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  Teacher model {self.config.teacher_model} not available: {e}")
            
            # –ü—Ä–æ–±—É–µ–º fallback
            fallback_key = map_model_name_to_key(self.config.fallback_model)
            self.logger.info(f"Switching to fallback model: {self.config.fallback_model} (key: {fallback_key})")
            self.config.teacher_model = fallback_key
    
    def _load_from_embeddings(self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö Q&A —ç–º–±–µ–¥–∏–Ω–≥–æ–≤"""
        if not isinstance(question_embeddings, torch.Tensor):
            question_embeddings = torch.from_numpy(np.array(question_embeddings)).float()
        if not isinstance(answer_embeddings, torch.Tensor):
            answer_embeddings = torch.from_numpy(np.array(answer_embeddings)).float()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        if len(question_embeddings.shape) != 2 or len(answer_embeddings.shape) != 2:
            raise ValueError(f"Embeddings should be 2D tensors, got shapes: "
                           f"{question_embeddings.shape}, {answer_embeddings.shape}")
        
        if question_embeddings.shape[0] != answer_embeddings.shape[0]:
            raise ValueError(f"Number of questions and answers must match: "
                           f"{question_embeddings.shape[0]} vs {answer_embeddings.shape[0]}")
        
        if question_embeddings.shape[1] != self.config.embedding_dim:
            self.logger.warning(f"Question embedding dimension mismatch: got {question_embeddings.shape[1]}, "
                              f"expected {self.config.embedding_dim}")
            self.config.embedding_dim = question_embeddings.shape[1]
        
        self.question_embeddings = question_embeddings
        self.answer_embeddings = answer_embeddings
        self._update_dataset_info("provided_embeddings", question_embeddings.shape[0])
    
    def _load_from_dialogue_pairs(self, dialogue_pairs: List[Dict]):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä —á–µ—Ä–µ–∑ Teacher LLM"""
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        if self.config.enable_quality_filter:
            dialogue_pairs = self._filter_dialogue_quality(dialogue_pairs)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∏–∞–ª–æ–≥–æ–≤
        if len(dialogue_pairs) > self.config.max_conversations:
            dialogue_pairs = dialogue_pairs[:self.config.max_conversations]
            self.logger.info(f"Limited to {self.config.max_conversations} dialogue pairs")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = self._create_cache_key_for_dialogues(dialogue_pairs)
        cached_data = self._load_from_cache(cache_key)
        
        if cached_data is not None and self.config.use_cache:
            self.logger.info("Loading dialogue embeddings from cache")
            self.question_embeddings = cached_data['questions']
            self.answer_embeddings = cached_data['answers']
            self.dialogue_metadata = cached_data.get('metadata', [])
            self.cache_stats['cache_hits'] += 1
        else:
            self.logger.info(f"Generating embeddings from {len(dialogue_pairs)} dialogue pairs "
                           f"using Teacher LLM: {self.config.teacher_model}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
            questions = [pair['question'] for pair in dialogue_pairs]
            answers = [pair['answer'] for pair in dialogue_pairs]
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Teacher LLM
            question_embeddings = self.embedding_loader.batch_load_from_llm(
                texts=questions,
                model_key=self.config.teacher_model,
                batch_size=self.config.cache_batch_size
            )
            
            answer_embeddings = self.embedding_loader.batch_load_from_llm(
                texts=answers,
                model_key=self.config.teacher_model,
                batch_size=self.config.cache_batch_size
            )
            
            # Preprocessing
            if self.config.normalize_embeddings or self.config.center_embeddings:
                question_embeddings = self.embedding_loader.preprocess_embeddings(
                    question_embeddings,
                    normalize=self.config.normalize_embeddings,
                    center=self.config.center_embeddings
                )
                answer_embeddings = self.embedding_loader.preprocess_embeddings(
                    answer_embeddings,
                    normalize=self.config.normalize_embeddings,
                    center=self.config.center_embeddings
                )
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —à—É–º –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            if self.config.add_context_noise:
                noise_std = self.config.context_noise_std
                question_noise = torch.randn_like(question_embeddings) * noise_std
                answer_noise = torch.randn_like(answer_embeddings) * noise_std
                question_embeddings += question_noise
                answer_embeddings += answer_noise
            
            self.question_embeddings = question_embeddings
            self.answer_embeddings = answer_embeddings
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            self.dialogue_metadata = [
                {
                    'question': pair['question'],
                    'answer': pair['answer'],
                    'question_length': len(pair['question']),
                    'answer_length': len(pair['answer'])
                }
                for pair in dialogue_pairs
            ]
            
            self.cache_stats['cache_misses'] += 1
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            if self.config.cache_embeddings:
                cache_data = {
                    'questions': question_embeddings,
                    'answers': answer_embeddings,
                    'metadata': self.dialogue_metadata
                }
                self._save_to_cache(cache_key, cache_data)
        
        self._update_dataset_info("generated_from_dialogue_pairs", len(dialogue_pairs))
    
    def _load_from_conversations(self, conversations: List[List[Dict]]):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
        self.logger.info(f"Processing {len(conversations)} multi-turn conversations")
        
        dialogue_pairs = []
        
        for conversation in conversations:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Q&A –ø–∞—Ä –∏–∑ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞
            pairs = self._extract_qa_pairs_from_conversation(conversation)
            dialogue_pairs.extend(pairs)
        
        self.logger.info(f"Extracted {len(dialogue_pairs)} Q&A pairs from conversations")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ –ø–∞—Ä—ã
        self._load_from_dialogue_pairs(dialogue_pairs)
        self._update_dataset_info("generated_from_conversations", len(conversations))
    
    def _extract_qa_pairs_from_conversation(self, conversation: List[Dict]) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Q&A –ø–∞—Ä –∏–∑ –æ–¥–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞"""
        pairs = []
        
        # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –∫–∞–∂–¥–∞—è —Ä–µ–ø–ª–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ‚Üí —Å–ª–µ–¥—É—é—â–∞—è —Ä–µ–ø–ª–∏–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        for i in range(len(conversation) - 1):
            current_turn = conversation[i]
            next_turn = conversation[i + 1]
            
            # –ò—â–µ–º –ø–∞—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ‚Üí –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
            if (current_turn.get('role', '').lower() in ['user', 'human', 'question'] and
                next_turn.get('role', '').lower() in ['assistant', 'bot', 'answer']):
                
                pairs.append({
                    'question': current_turn.get('text', current_turn.get('content', '')),
                    'answer': next_turn.get('text', next_turn.get('content', ''))
                })
        
        return pairs
    
    def _load_from_sources(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_dialogue_pairs = []
        
        for source_path in self.config.data_sources:
            source_path = Path(source_path)
            
            if not source_path.exists():
                self.logger.warning(f"Source file not found: {source_path}")
                continue
            
            try:
                if source_path.suffix in ['.json', '.jsonl']:
                    # JSON —Ñ–∞–π–ª —Å –¥–∏–∞–ª–æ–≥–∞–º–∏
                    pairs = self._load_dialogues_from_json(source_path)
                    
                elif source_path.suffix in ['.txt']:
                    # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (–ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç Q: A:)
                    pairs = self._load_dialogues_from_text(source_path)
                    
                elif source_path.suffix in ['.csv']:
                    # CSV —Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ question, answer
                    pairs = self._load_dialogues_from_csv(source_path)
                    
                else:
                    self.logger.warning(f"Unsupported file format: {source_path}")
                    continue
                
                all_dialogue_pairs.extend(pairs)
                self.logger.info(f"Loaded {len(pairs)} dialogue pairs from {source_path}")
                
            except Exception as e:
                self.logger.error(f"Failed to load from {source_path}: {e}")
                continue
        
        if not all_dialogue_pairs:
            raise ValueError("No valid dialogue sources found")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
        self._load_from_dialogue_pairs(all_dialogue_pairs)
        self._update_dataset_info("loaded_from_sources", len(self.config.data_sources))
    
    def _load_dialogues_from_json(self, file_path: Path) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        with open(file_path, 'r', encoding='utf-8') as f:
            if file_path.suffix == '.jsonl':
                # JSONL —Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–∫–∞ = JSON –æ–±—ä–µ–∫—Ç)
                dialogues = [json.loads(line) for line in f if line.strip()]
            else:
                # –û–±—ã—á–Ω—ã–π JSON
                data = json.load(f)
                if isinstance(data, list):
                    dialogues = data
                else:
                    dialogues = data.get('dialogues', data.get('conversations', []))
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        pairs = []
        for dialogue in dialogues:
            if 'question' in dialogue and 'answer' in dialogue:
                # –ü—Ä–æ—Å—Ç–æ–π Q&A —Ñ–æ—Ä–º–∞—Ç
                pairs.append({
                    'question': dialogue['question'],
                    'answer': dialogue['answer']
                })
            elif 'conversations' in dialogue:
                # –ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
                conversation_pairs = self._extract_qa_pairs_from_conversation(dialogue['conversations'])
                pairs.extend(conversation_pairs)
        
        return pairs
    
    def _load_dialogues_from_text(self, file_path: Path) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (—Ñ–æ—Ä–º–∞—Ç Q: A:)"""
        pairs = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –†–∞–∑–±–æ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ Q: ... A: ...
        qa_pattern = r'Q:\s*(.+?)\s*A:\s*(.+?)(?=Q:|$)'
        matches = re.findall(qa_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for question, answer in matches:
            pairs.append({
                'question': question.strip(),
                'answer': answer.strip()
            })
        
        return pairs
    
    def _load_dialogues_from_csv(self, file_path: Path) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        import csv
        pairs = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
                question = (row.get('question') or row.get('Q') or 
                          row.get('input') or row.get('user_message', ''))
                answer = (row.get('answer') or row.get('A') or 
                        row.get('response') or row.get('assistant_message', ''))
                
                if question and answer:
                    pairs.append({
                        'question': question.strip(),
                        'answer': answer.strip()
                    })
        
        return pairs
    
    def _filter_dialogue_quality(self, dialogue_pairs: List[Dict]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
        filtered_pairs = []
        filtered_count = 0
        
        for pair in dialogue_pairs:
            question = pair.get('question', '')
            answer = pair.get('answer', '')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
            if (len(question) < self.config.min_question_length or 
                len(answer) < self.config.min_answer_length or
                len(question) > self.config.max_question_length or
                len(answer) > self.config.max_answer_length):
                filtered_count += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –ø–æ–≤—Ç–æ—Ä—ã
            if (not question.strip() or not answer.strip() or
                question.strip() == answer.strip()):
                filtered_count += 1
                continue
            
            # TODO: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ)
            # if self._check_semantic_relevance(question, answer) < self.config.semantic_similarity_threshold:
            #     filtered_count += 1
            #     continue
            
            filtered_pairs.append(pair)
        
        self.cache_stats['quality_filtered'] = filtered_count
        self.logger.info(f"Quality filter: kept {len(filtered_pairs)}, filtered {filtered_count}")
        
        return filtered_pairs
    
    def _create_train_val_split(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train/validation"""
        total_pairs = len(self.question_embeddings)
        
        if self.config.validation_split > 0:
            val_size = int(total_pairs * self.config.validation_split)
            train_size = total_pairs - val_size
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            indices = torch.arange(total_pairs)
            if self.config.shuffle_conversations:
                indices = indices[torch.randperm(total_pairs)]
            
            train_indices = indices[:train_size]
            val_indices = indices[train_size:]
            
            self.train_questions = self.question_embeddings[train_indices]
            self.train_answers = self.answer_embeddings[train_indices]
            self.val_questions = self.question_embeddings[val_indices]
            self.val_answers = self.answer_embeddings[val_indices]
        else:
            # –ù–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            self.train_questions = self.question_embeddings
            self.train_answers = self.answer_embeddings
            self.val_questions = torch.empty(0, self.config.embedding_dim)
            self.val_answers = torch.empty(0, self.config.embedding_dim)
    
    def _create_cache_key_for_dialogues(self, dialogue_pairs: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä"""
        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –æ—Ç —á–∞—Å—Ç–∏ –¥–∏–∞–ª–æ–≥–æ–≤ + –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        sample_dialogues = dialogue_pairs[:50]  # –ü–µ—Ä–≤—ã–µ 50 –¥–ª—è —Ö—ç—à–∞
        dialogue_content = "\n".join([f"Q:{d['question']} A:{d['answer']}" for d in sample_dialogues])
        config_content = f"{self.config.teacher_model}_{self.config.embedding_dim}_{len(dialogue_pairs)}"
        content = f"{dialogue_content}_{config_content}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞"""
        if not self.config.use_cache:
            return None
        
        cache_path = Path(self.config.cache_dir) / f"dialogue_{cache_key}.pt"
        
        if cache_path.exists():
            try:
                return torch.load(cache_path)
            except Exception as e:
                self.logger.warning(f"Failed to load from cache {cache_path}: {e}")
                return None
        
        return None
    
    def _save_to_cache(self, cache_key: str, cache_data: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ –∫—ç—à"""
        if not self.config.cache_embeddings:
            return
        
        cache_path = Path(self.config.cache_dir) / f"dialogue_{cache_key}.pt"
        
        try:
            torch.save(cache_data, cache_path)
            self.logger.info(f"Cached dialogue embeddings to {cache_path}")
        except Exception as e:
            self.logger.warning(f"Failed to cache dialogue embeddings: {e}")
    
    def _update_dataset_info(self, source_type: str, sample_count: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö dataset"""
        self.dataset_info.update({
            'source_type': source_type,
            'dialogue_pairs_count': sample_count,
            'embedding_dim': self.config.embedding_dim,
            'teacher_model': self.config.teacher_model,
            'config': self.config.__dict__
        })
    
    def set_validation_mode(self, is_validation: bool = True):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É train/validation —Ä–µ–∂–∏–º–∞–º–∏"""
        self.is_validation_mode = is_validation
    
    def __len__(self) -> int:
        """–†–∞–∑–º–µ—Ä dataset"""
        if self.is_validation_mode:
            return len(self.val_questions)
        else:
            return len(self.train_questions)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ dataset
        
        Returns:
            Tuple[question_embedding, answer_embedding] –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core
        """
        if self.is_validation_mode:
            question_emb = self.val_questions[idx]
            answer_emb = self.val_answers[idx]
        else:
            question_emb = self.train_questions[idx]
            answer_emb = self.train_answers[idx]
        
        return question_emb, answer_emb
    
    def get_dataloader(self, 
                      batch_size: int = 32, 
                      shuffle: bool = True,
                      num_workers: int = 0,
                      validation: bool = False) -> DataLoader:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ DataLoader –¥–ª—è dialogue –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            shuffle: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
            num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            validation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å validation –Ω–∞–±–æ—Ä
            
        Returns:
            DataLoader –¥–ª—è dialogue training
        """
        # –í—Ä–µ–º–µ–Ω–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –≤ –Ω—É–∂–Ω—ã–π —Ä–µ–∂–∏–º
        original_mode = self.is_validation_mode
        self.set_validation_mode(validation)
        
        dataloader = DataLoader(
            self,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º
        self.set_validation_mode(original_mode)
        
        return dataloader
    
    def get_sample_dialogues(self, n_samples: int = 5) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        if not self.dialogue_metadata:
            return {"error": "No dialogue metadata available"}
        
        n_samples = min(n_samples, len(self.dialogue_metadata))
        sample_indices = random.sample(range(len(self.dialogue_metadata)), n_samples)
        
        samples = []
        for idx in sample_indices:
            metadata = self.dialogue_metadata[idx]
            question_emb = self.question_embeddings[idx]
            answer_emb = self.answer_embeddings[idx]
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ Q&A
            cosine_similarity = torch.cosine_similarity(
                question_emb.unsqueeze(0), 
                answer_emb.unsqueeze(0)
            ).item()
            
            samples.append({
                'question': metadata['question'],
                'answer': metadata['answer'],
                'question_length': metadata['question_length'],
                'answer_length': metadata['answer_length'],
                'qa_similarity': cosine_similarity,
                'question_embedding_shape': question_emb.shape,
                'answer_embedding_shape': answer_emb.shape
            })
        
        return {
            'samples': samples,
            'dataset_size': len(self.dialogue_metadata),
            'teacher_model': self.config.teacher_model
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ dataset"""
        if len(self.question_embeddings) == 0:
            return {"error": "Dataset is empty"}
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'total_dialogue_pairs': len(self.question_embeddings),
            'train_pairs': len(self.train_questions),
            'validation_pairs': len(self.val_questions),
            'embedding_dimension': self.question_embeddings.shape[1],
            'teacher_model': self.config.teacher_model,
            'cache_stats': self.cache_stats,
            'config_summary': {
                'quality_filtering': self.config.enable_quality_filter,
                'multiturn_support': self.config.support_multiturn,
                'normalization': self.config.normalize_embeddings,
                'context_noise': self.config.add_context_noise
            }
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        question_norms = torch.norm(self.question_embeddings, dim=1)
        answer_norms = torch.norm(self.answer_embeddings, dim=1)
        
        qa_similarities = torch.cosine_similarity(
            self.question_embeddings, 
            self.answer_embeddings, 
            dim=1
        )
        
        stats.update({
            'embedding_quality': {
                'question_norm_mean': question_norms.mean().item(),
                'question_norm_std': question_norms.std().item(),
                'answer_norm_mean': answer_norms.mean().item(),
                'answer_norm_std': answer_norms.std().item(),
                'qa_similarity_mean': qa_similarities.mean().item(),
                'qa_similarity_std': qa_similarities.std().item(),
                'qa_similarity_range': [qa_similarities.min().item(), qa_similarities.max().item()]
            }
        })
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
        if self.dialogue_metadata:
            question_lengths = [m['question_length'] for m in self.dialogue_metadata]
            answer_lengths = [m['answer_length'] for m in self.dialogue_metadata]
            
            stats.update({
                'text_statistics': {
                    'question_length_mean': np.mean(question_lengths),
                    'question_length_std': np.std(question_lengths),
                    'answer_length_mean': np.mean(answer_lengths),
                    'answer_length_std': np.std(answer_lengths)
                }
            })
        
        return stats
    
    def save_dataset_info(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ dataset"""
        info = {
            'dataset_info': self.dataset_info,
            'statistics': self.get_statistics(),
            'sample_dialogues': self.get_sample_dialogues(3)
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"Dataset info saved to: {path}")
    
    def __repr__(self):
        return (f"DialogueDataset(pairs={len(self.question_embeddings)}, "
                f"train={len(self.train_questions)}, val={len(self.val_questions)}, "
                f"dim={self.config.embedding_dim}, teacher={self.config.teacher_model})")


# ================================
# HELPER FUNCTIONS
# ================================

def create_dialogue_dataset(dialogue_pairs: List[Dict], 
                          teacher_model: str = "llama3-8b",
                          validation_split: float = 0.2,
                          **kwargs) -> DialogueDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DialogueDataset –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä
    
    Args:
        dialogue_pairs: –°–ø–∏—Å–æ–∫ Q&A –ø–∞—Ä [{"question": str, "answer": str}, ...]
        teacher_model: Teacher LLM –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
    Returns:
        –ì–æ—Ç–æ–≤—ã–π DialogueDataset
    """
    config = DialogueConfig(
        teacher_model=teacher_model,
        validation_split=validation_split,
        **kwargs
    )
    
    return DialogueDataset(
        config=config,
        dialogue_pairs=dialogue_pairs
    )


def create_conversation_dataset(conversations: List[List[Dict]],
                              teacher_model: str = "llama3-8b", 
                              validation_split: float = 0.2,
                              **kwargs) -> DialogueDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DialogueDataset –∏–∑ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    
    Args:
        conversations: –°–ø–∏—Å–æ–∫ –¥–∏–∞–ª–æ–≥–æ–≤ [[{role, text}, ...], ...]
        teacher_model: Teacher LLM –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
    Returns:
        –ì–æ—Ç–æ–≤—ã–π DialogueDataset
    """
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º support_multiturn=True, –Ω–æ –ø–æ–∑–≤–æ–ª—è–µ–º override —á–µ—Ä–µ–∑ kwargs
    kwargs.setdefault('support_multiturn', True)
    
    config = DialogueConfig(
        teacher_model=teacher_model,
        validation_split=validation_split,
        **kwargs
    )
    
    return DialogueDataset(
        config=config,
        conversations=conversations
    )


def load_dialogue_dataset_from_files(file_paths: List[str],
                                   dialogue_format: str = "qa_pairs",
                                   teacher_model: str = "llama3-8b",
                                   validation_split: float = 0.2,
                                   **kwargs) -> DialogueDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ DialogueDataset –∏–∑ —Ñ–∞–π–ª–æ–≤
    
    Args:
        file_paths: –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –¥–∏–∞–ª–æ–≥–∞–º–∏
        dialogue_format: –§–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–æ–≤ (qa_pairs, conversation, json)
        teacher_model: Teacher LLM –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
    Returns:
        –ì–æ—Ç–æ–≤—ã–π DialogueDataset
    """
    config = DialogueConfig(
        data_sources=file_paths,
        dialogue_format=dialogue_format,
        teacher_model=teacher_model,
        validation_split=validation_split,
        **kwargs
    )
    
    return DialogueDataset(config=config)