"""
üîó Universal Adapter Integration
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è CubeTrainer
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö teacher –º–æ–¥–µ–ª–µ–π –∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫—É–±–∞
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass
import logging
from pathlib import Path
import yaml

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã
from .cube_trainer import CubeTrainer, TrainingConfig, EmbeddingMetrics
from data.embedding_adapter.universal_adapter import (
    UniversalEmbeddingAdapter, 
    AdapterManager, 
    create_adapter_for_cube,
    KNOWN_MODELS
)

logger = logging.getLogger(__name__)


class SimpleWrapper:
    """
    –ü—Ä–æ—Å—Ç–æ–π wrapper –¥–ª—è EmbeddingProcessor —á—Ç–æ–±—ã –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å CubeTrainer API
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é —Å surface embeddings
    """
    
    def __init__(self, embedding_processor, device: str, learning_rate: float):
        self.embedding_processor = embedding_processor
        self.device = device
        
        # –°–æ–∑–¥–∞–µ–º optimizer –¥–ª—è embedding_processor
        self.optimizer = optim.AdamW(
            self.embedding_processor.parameters(), 
            lr=learning_rate
        )
    
    def forward(self, surface_embeddings: torch.Tensor) -> torch.Tensor:
        """Forward pass —á–µ—Ä–µ–∑ embedding processor"""
        return self.embedding_processor.forward(surface_embeddings)
    
    def get_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ processor"""
        return {
            "type": "SimpleWrapper",
            "device": self.device,
            "total_parameters": sum(p.numel() for p in self.embedding_processor.parameters())
        }


@dataclass
class AdapterIntegrationConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è"""
    
    # Teacher model settings
    teacher_model: str = "Meta-Llama-3-8B"
    teacher_embedding_dim: Optional[int] = None  # Auto-detect from KNOWN_MODELS
    
    # Cube settings
    cube_dimensions: Tuple[int, int, int] = (15, 15, 11)
    surface_strategy: str = "single"  # single | triple | full
    
    # Adapter settings
    adapter_strategy: str = "learned_linear"  # learned_linear | hierarchical | attention_based | autoencoder
    use_reconstruction_loss: bool = True
    reconstruction_weight: float = 0.1
    
    # Training settings
    adapter_learning_rate: float = 0.001
    cube_learning_rate: float = 0.0005
    joint_training: bool = True  # –û–±—É—á–∞–µ–º adapter + cube —Å–æ–≤–º–µ—Å—Ç–Ω–æ
    
    # Integration settings
    gradient_balancing: bool = True
    adapter_warmup_epochs: int = 3
    
    def __post_init__(self):
        if self.teacher_embedding_dim is None:
            if self.teacher_model in KNOWN_MODELS:
                self.teacher_embedding_dim = KNOWN_MODELS[self.teacher_model]["embedding_dim"]
            else:
                raise ValueError(f"Unknown teacher model: {self.teacher_model}")


class AdapterCubeTrainer:
    """
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∞–¥–∞–ø—Ç–µ—Ä–æ–º
    
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
    - UniversalEmbeddingAdapter (–ª—é–±–∞—è –º–æ–¥–µ–ª—å ‚Üí surface —Ä–∞–∑–º–µ—Ä)
    - CubeTrainer (3D cubic processing)
    - Joint training pipeline
    """
    
    def __init__(self, 
                 config: Optional[Union[AdapterIntegrationConfig, str, Dict]] = None,
                 device: str = "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info("üîó Initializing AdapterCubeTrainer...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(config)
        self.device = torch.device(device)
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.adapter = None
        self.cube_trainer = None
        self.adapter_optimizer = None
        self.joint_optimizer = None
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.training_history = []
        self.adapter_warmup_complete = False
        
        self.logger.info(f"‚úÖ AdapterCubeTrainer configured:")
        self.logger.info(f"   Teacher: {self.config.teacher_model} ({self.config.teacher_embedding_dim}D)")
        self.logger.info(f"   Cube: {self.config.cube_dimensions}")
        self.logger.info(f"   Surface strategy: {self.config.surface_strategy}")
        self.logger.info(f"   Adapter strategy: {self.config.adapter_strategy}")
    
    def _load_config(self, config: Optional[Union[AdapterIntegrationConfig, str, Dict]]) -> AdapterIntegrationConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if config is None:
            return AdapterIntegrationConfig()
        
        elif isinstance(config, AdapterIntegrationConfig):
            return config
        
        elif isinstance(config, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ YAML
            try:
                with open(config, 'r', encoding='utf-8') as f:
                    config_data = yaml.safe_load(f)
                return AdapterIntegrationConfig(**config_data)
            except Exception as e:
                self.logger.warning(f"Failed to load config from {config}: {e}")
                return AdapterIntegrationConfig()
        
        elif isinstance(config, dict):
            try:
                return AdapterIntegrationConfig(**config)
            except Exception as e:
                self.logger.warning(f"Failed to create config from dict: {e}")
                return AdapterIntegrationConfig()
        
        else:
            return AdapterIntegrationConfig()
    
    def initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        self.logger.info("üîß Initializing integrated training components...")
        
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞
        self._setup_adapter()
        
        # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CubeTrainer
        self._setup_cube_trainer()
        
        # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
        self._setup_optimizers()
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self._validate_integration()
        
        self.logger.info("üéØ All integrated components initialized successfully!")
    
    def _setup_adapter(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞"""
        self.logger.info(f"üîß Setting up adapter: {self.config.teacher_model} ‚Üí surface...")
        
        # –í—ã—á–∏—Å–ª—è–µ–º surface —Ä–∞–∑–º–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        surface_size = self._calculate_surface_size()
        
        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä
        self.adapter = UniversalEmbeddingAdapter(
            input_dim=self.config.teacher_embedding_dim,
            output_dim=surface_size,
            strategy=self.config.adapter_strategy
        )
        
        self.adapter.to(self.device)
        
        compression_ratio = self.adapter.get_compression_ratio()
        param_count = self.adapter.get_parameter_count()
        
        self.logger.info(f"‚úÖ Adapter created:")
        self.logger.info(f"   {self.config.teacher_embedding_dim}D ‚Üí {surface_size}D")
        self.logger.info(f"   Compression: {compression_ratio:.3f} ({compression_ratio*100:.1f}%)")
        self.logger.info(f"   Parameters: {param_count:,}")
    
    def _setup_cube_trainer(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CubeTrainer —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏"""
        self.logger.info("üîß Setting up CubeTrainer...")
        
        # –î–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ –Ω–∞–º –Ω—É–∂–µ–Ω direct EmbeddingProcessor
        # –±–µ–∑ EmbeddingReshaper, —Ç–∞–∫ –∫–∞–∫ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º —Å surface embeddings
        surface_size = self._calculate_surface_size()
        
        # –°–æ–∑–¥–∞–µ–º EmbeddingProcessor –Ω–∞–ø—Ä—è–º—É—é
        from core.embedding_processor import EmbeddingProcessor, EmbeddingConfig
        
        processor_config = EmbeddingConfig(
            lattice_size=tuple(self.config.cube_dimensions),
            device=str(self.device),
            input_dim=surface_size,  # Surface —Ä–∞–∑–º–µ—Ä
            output_dim=surface_size,  # Surface —Ä–∞–∑–º–µ—Ä
            processing_mode="surface_direct"  # –ù–æ–≤—ã–π —Ä–µ–∂–∏–º –¥–ª—è surface-only
        )
        
        self.embedding_processor = EmbeddingProcessor(config=processor_config)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π wrapper –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å CubeTrainer API
        self.cube_trainer = SimpleWrapper(
            embedding_processor=self.embedding_processor,
            device=self.device,
            learning_rate=self.config.cube_learning_rate
        )
        
        self.logger.info(f"‚úÖ Direct EmbeddingProcessor initialized for {surface_size}D surface embeddings")
    
    def _setup_optimizers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è joint training"""
        self.logger.info("üîß Setting up optimizers...")
        
        if self.config.joint_training:
            # Joint optimizer –¥–ª—è –æ–±–æ–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            all_params = list(self.adapter.parameters()) + list(self.embedding_processor.parameters())
            self.joint_optimizer = optim.AdamW(all_params, lr=self.config.cube_learning_rate)
            self.logger.info("‚úÖ Joint optimizer configured (adapter + embedding_processor)")
        else:
            # Separate optimizers
            self.adapter_optimizer = optim.Adam(self.adapter.parameters(), lr=self.config.adapter_learning_rate)
            # cube_trainer —É–∂–µ –∏–º–µ–µ—Ç —Å–≤–æ–π optimizer
            self.logger.info("‚úÖ Separate optimizers configured")
    
    def _calculate_surface_size(self) -> int:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ surface –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        x, y, z = self.config.cube_dimensions
        face_size = x * y
        
        if self.config.surface_strategy == "single":
            return face_size
        elif self.config.surface_strategy == "triple":
            return 3 * face_size  # front, back, top
        elif self.config.surface_strategy == "full":
            return 6 * face_size  # all faces
        else:
            raise ValueError(f"Unknown surface strategy: {self.config.surface_strategy}")
    
    def _validate_integration(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        self.logger.info("üîç Validating component integration...")
        
        # –¢–µ—Å—Ç–æ–≤—ã–π forward pass
        test_input = torch.randn(2, self.config.teacher_embedding_dim).to(self.device)
        
        # Adapter forward
        adapter_output = self.adapter(test_input)
        expected_surface_size = self._calculate_surface_size()
        
        assert adapter_output.shape == (2, expected_surface_size), \
            f"Adapter output shape mismatch: {adapter_output.shape} vs expected (2, {expected_surface_size})"
        
        # EmbeddingProcessor forward  
        processor_output = self.embedding_processor.forward(adapter_output)
        
        assert processor_output.shape == adapter_output.shape, \
            f"Processor output shape mismatch: {processor_output.shape} vs expected {adapter_output.shape}"
        
        self.logger.info("‚úÖ Integration validation passed!")
        self.logger.info(f"   Pipeline: {test_input.shape} ‚Üí {adapter_output.shape} ‚Üí {processor_output.shape}")
    
    def forward(self, teacher_embeddings: torch.Tensor, return_intermediate: bool = False) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Full forward pass —á–µ—Ä–µ–∑ adapter + cube
        
        Args:
            teacher_embeddings: –≠–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏ [batch, teacher_dim]
            return_intermediate: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            
        Returns:
            output: –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç [batch, surface_size]
            intermediate: –°–ª–æ–≤–∞—Ä—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
        """
        # 1. Adapter: teacher_dim ‚Üí surface_size
        if self.config.use_reconstruction_loss:
            surface_embeddings, reconstructed = self.adapter(teacher_embeddings, return_reconstruction=True)
        else:
            surface_embeddings = self.adapter(teacher_embeddings)
            reconstructed = None
        
        # 2. Cube: surface_size ‚Üí surface_size
        processed_embeddings = self.embedding_processor.forward(surface_embeddings)
        
        if return_intermediate:
            return {
                "output": processed_embeddings,
                "surface_embeddings": surface_embeddings,
                "reconstructed": reconstructed,
                "teacher_embeddings": teacher_embeddings
            }
        else:
            return processed_embeddings
    
    def compute_loss(self, 
                    teacher_embeddings: torch.Tensor, 
                    target_embeddings: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π loss function
        
        Args:
            teacher_embeddings: Input —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏
            target_embeddings: Target —ç–º–±–µ–¥–∏–Ω–≥–∏ (—Ç–∞–∫–∂–µ –æ—Ç teacher –º–æ–¥–µ–ª–∏)
            
        Returns:
            Dict —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ loss
        """
        # Forward pass —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results = self.forward(teacher_embeddings, return_intermediate=True)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º target —Ç–∞–∫–∂–µ —á–µ—Ä–µ–∑ adapter
        target_surface = self.adapter(target_embeddings)
        
        # Main task loss: processed surface ‚Üí target surface
        main_loss = nn.functional.cosine_embedding_loss(
            results["output"], 
            target_surface,
            torch.ones(target_surface.size(0)).to(self.device)
        )
        
        losses = {"main_loss": main_loss}
        
        # Reconstruction loss (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if self.config.use_reconstruction_loss and results["reconstructed"] is not None:
            reconstruction_loss = self.adapter.compute_reconstruction_loss(
                teacher_embeddings, 
                results["reconstructed"]
            )
            losses["reconstruction_loss"] = reconstruction_loss
        
        # Total loss
        total_loss = losses["main_loss"]
        if "reconstruction_loss" in losses:
            total_loss = total_loss + self.config.reconstruction_weight * losses["reconstruction_loss"]
        
        losses["total_loss"] = total_loss
        
        return losses
    
    def train_step(self, 
                  question_embeddings: torch.Tensor, 
                  answer_embeddings: torch.Tensor) -> Dict[str, float]:
        """
        –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            question_embeddings: Question —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏
            answer_embeddings: Answer —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        if self.config.joint_training:
            return self._joint_train_step(question_embeddings, answer_embeddings)
        else:
            return self._separate_train_step(question_embeddings, answer_embeddings)
    
    def _joint_train_step(self, 
                         question_embeddings: torch.Tensor, 
                         answer_embeddings: torch.Tensor) -> Dict[str, float]:
        """Joint training step (adapter + cube –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)"""
        self.adapter.train()
        self.embedding_processor.train()
        
        # Forward pass
        losses = self.compute_loss(question_embeddings, answer_embeddings)
        
        # Backward pass
        self.joint_optimizer.zero_grad()
        losses["total_loss"].backward()
        
        # Gradient clipping
        if self.config.gradient_balancing:
            nn.utils.clip_grad_norm_(self.adapter.parameters(), max_norm=1.0)
            nn.utils.clip_grad_norm_(self.embedding_processor.parameters(), max_norm=1.0)
        
        self.joint_optimizer.step()
        
        # Metrics
        with torch.no_grad():
            results = self.forward(question_embeddings, return_intermediate=True)
            target_surface = self.adapter(answer_embeddings)
            
            qa_similarity = torch.cosine_similarity(
                results["output"], 
                target_surface, 
                dim=1
            ).mean().item()
        
        return {
            "total_loss": losses["total_loss"].item(),
            "main_loss": losses["main_loss"].item(),
            "reconstruction_loss": losses.get("reconstruction_loss", torch.tensor(0.0)).item(),
            "qa_similarity": qa_similarity
        }
    
    def _separate_train_step(self, 
                           question_embeddings: torch.Tensor, 
                           answer_embeddings: torch.Tensor) -> Dict[str, float]:
        """Separate training (adapter first, then cube)"""
        if not self.adapter_warmup_complete and self.current_epoch < self.config.adapter_warmup_epochs:
            # Adapter warmup phase
            return self._adapter_warmup_step(question_embeddings, answer_embeddings)
        else:
            # Main training phase
            self.adapter_warmup_complete = True
            return self._cube_training_step(question_embeddings, answer_embeddings)
    
    def _adapter_warmup_step(self, 
                           question_embeddings: torch.Tensor, 
                           answer_embeddings: torch.Tensor) -> Dict[str, float]:
        """Adapter warmup step (—Ç–æ–ª—å–∫–æ adapter)"""
        self.adapter.train()
        self.embedding_processor.eval()  # Freeze cube
        
        # Adapter training —Ç–æ–ª—å–∫–æ –Ω–∞ reconstruction
        _, reconstructed = self.adapter(question_embeddings, return_reconstruction=True)
        reconstruction_loss = self.adapter.compute_reconstruction_loss(question_embeddings, reconstructed)
        
        self.adapter_optimizer.zero_grad()
        reconstruction_loss.backward()
        self.adapter_optimizer.step()
        
        return {
            "total_loss": reconstruction_loss.item(),
            "reconstruction_loss": reconstruction_loss.item(),
            "qa_similarity": 0.0,  # Not applicable during warmup
            "phase": "adapter_warmup"
        }
    
    def _cube_training_step(self, 
                          question_embeddings: torch.Tensor, 
                          answer_embeddings: torch.Tensor) -> Dict[str, float]:
        """Cube training step (—Ç–æ–ª—å–∫–æ cube, adapter frozen)"""
        self.adapter.eval()  # Freeze adapter
        self.embedding_processor.train()
        
        # Forward —á–µ—Ä–µ–∑ frozen adapter
        with torch.no_grad():
            question_surface = self.adapter(question_embeddings)
            answer_surface = self.adapter(answer_embeddings)
        
        # Cube training
        processed_surface = self.embedding_processor.forward(question_surface)
        
        main_loss = nn.functional.cosine_embedding_loss(
            processed_surface,
            answer_surface,
            torch.ones(answer_surface.size(0)).to(self.device)
        )
        
        self.cube_trainer.optimizer.zero_grad()
        main_loss.backward()
        self.cube_trainer.optimizer.step()
        
        # Metrics
        with torch.no_grad():
            qa_similarity = torch.cosine_similarity(
                processed_surface, 
                answer_surface, 
                dim=1
            ).mean().item()
        
        return {
            "total_loss": main_loss.item(),
            "main_loss": main_loss.item(),
            "qa_similarity": qa_similarity,
            "phase": "cube_training"
        }
    
    def get_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ"""
        adapter_info = {
            "input_dim": self.adapter.input_dim if self.adapter else None,
            "output_dim": self.adapter.output_dim if self.adapter else None,
            "strategy": self.config.adapter_strategy,
            "compression_ratio": self.adapter.get_compression_ratio() if self.adapter else None,
            "parameters": self.adapter.get_parameter_count() if self.adapter else None
        }
        
        cube_info = self.cube_trainer.get_info() if self.cube_trainer else {}
        
        return {
            "teacher_model": self.config.teacher_model,
            "cube_dimensions": self.config.cube_dimensions,
            "surface_strategy": self.config.surface_strategy,
            "joint_training": self.config.joint_training,
            "current_epoch": self.current_epoch,
            "adapter_warmup_complete": self.adapter_warmup_complete,
            "adapter": adapter_info,
            "cube": cube_info,
            "total_parameters": (adapter_info.get("parameters", 0) + 
                               cube_info.get("total_parameters", 0) if cube_info else 0)
        }


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–µ—Ä–æ–≤

def create_llama3_cube_trainer(cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
                              adapter_strategy: str = "learned_linear",
                              device: str = "cpu") -> AdapterCubeTrainer:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è LLaMA-3-8B"""
    config = AdapterIntegrationConfig(
        teacher_model="Meta-Llama-3-8B",
        cube_dimensions=cube_dimensions,
        adapter_strategy=adapter_strategy,
        joint_training=True
    )
    
    trainer = AdapterCubeTrainer(config=config, device=device)
    trainer.initialize_components()
    
    return trainer


def create_distilbert_cube_trainer(cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
                                  adapter_strategy: str = "learned_linear", 
                                  device: str = "cpu") -> AdapterCubeTrainer:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è DistilBERT (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)"""
    config = AdapterIntegrationConfig(
        teacher_model="DistilBERT",
        cube_dimensions=cube_dimensions,
        adapter_strategy=adapter_strategy,
        joint_training=True
    )
    
    trainer = AdapterCubeTrainer(config=config, device=device)
    trainer.initialize_components()
    
    return trainer


# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    "AdapterIntegrationConfig",
    "AdapterCubeTrainer", 
    "create_llama3_cube_trainer",
    "create_distilbert_cube_trainer"
] 