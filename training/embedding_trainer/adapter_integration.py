"""
[LINK] Universal Adapter Integration
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è CubeTrainer
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö teacher –º–æ–¥–µ–ª–µ–π –∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫—É–±–∞
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass
import logging
from pathlib import Path
import yaml

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã
from .cube_trainer import CubeTrainer, TrainingConfig, EmbeddingMetrics
from data.embedding_adapter.universal_adapter import (
    UniversalEmbeddingAdapter,
    AdapterManager,
    create_adapter_for_cube,
    KNOWN_MODELS,
)

# –£—Å–ª–æ–≤–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è EmbeddingProcessor.SURFACE_ONLY
try:
    from core.embedding_processor import (
        EmbeddingProcessor,
        create_surface_only_config,
        ProcessingMode,
    )

    EMBEDDING_PROCESSOR_AVAILABLE = True
except ImportError:
    # Fallback —Ä–µ–∂–∏–º –±–µ–∑ EmbeddingProcessor
    EMBEDDING_PROCESSOR_AVAILABLE = False
    EmbeddingProcessor = None
    create_surface_only_config = None
    ProcessingMode = None

logger = logging.getLogger(__name__)


@dataclass
class AdapterIntegrationConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è"""

    # Teacher model settings
    teacher_model: str = "Meta-Llama-3-8B"
    teacher_embedding_dim: Optional[int] = None  # Auto-detect from KNOWN_MODELS

    # Cube settings
    cube_dimensions: Tuple[int, int, int] = (15, 15, 11)
    surface_strategy: str = "single"  # single | triple | full

    # Adapter settings
    adapter_strategy: str = (
        "learned_linear"  # learned_linear | hierarchical | attention_based | autoencoder
    )
    use_reconstruction_loss: bool = True
    reconstruction_weight: float = 0.1

    # Training settings
    adapter_learning_rate: float = 0.001
    cube_learning_rate: float = 0.0005
    joint_training: bool = True  # –û–±—É—á–∞–µ–º adapter + cube —Å–æ–≤–º–µ—Å—Ç–Ω–æ

    # Integration settings
    gradient_balancing: bool = True
    adapter_warmup_epochs: int = 3

    def __post_init__(self):
        if self.teacher_embedding_dim is None:
            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞—à–∏ –∫–ª—é—á–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
            model_mapping = {
                "distilbert": "DistilBERT",
                "distilbert-base-uncased": "DistilBERT",
                "roberta": "RoBERTa-base",
                "roberta-base": "RoBERTa-base",
                "gpt2": "GPT-3.5",  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                "dialogpt": "GPT-3.5",  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                "llama3-8b-local": "Meta-Llama-3-8B",
                "llama3-8b": "Meta-Llama-3-8B",
                "meta-llama/meta-llama-3-8b": "Meta-Llama-3-8B",
            }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–ª–∏ —á–µ—Ä–µ–∑ –º–∞–ø–ø–∏–Ω–≥
            if self.teacher_model in KNOWN_MODELS:
                self.teacher_embedding_dim = KNOWN_MODELS[self.teacher_model][
                    "embedding_dim"
                ]
            elif self.teacher_model.lower() in model_mapping:
                mapped_model = model_mapping[self.teacher_model.lower()]
                self.teacher_embedding_dim = KNOWN_MODELS[mapped_model]["embedding_dim"]
                # –û–±–Ω–æ–≤–ª—è–µ–º teacher_model –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                self.teacher_model = mapped_model
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
                if "distilbert" in self.teacher_model.lower():
                    self.teacher_embedding_dim = 768
                    self.teacher_model = "DistilBERT"
                elif "roberta" in self.teacher_model.lower():
                    self.teacher_embedding_dim = 768
                    self.teacher_model = "RoBERTa-base"
                elif "llama" in self.teacher_model.lower():
                    self.teacher_embedding_dim = 4096
                    self.teacher_model = "Meta-Llama-3-8B"
                elif "gpt" in self.teacher_model.lower():
                    self.teacher_embedding_dim = 1536
                    self.teacher_model = "GPT-3.5"
                else:
                    raise ValueError(
                        f"Unknown teacher model: {self.teacher_model}. "
                        f"Known models: {list(KNOWN_MODELS.keys())} or supported keys: {list(model_mapping.keys())}"
                    )


class AdapterCubeTrainer:
    """
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∞–¥–∞–ø—Ç–µ—Ä–æ–º

    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
    - UniversalEmbeddingAdapter (–ª—é–±–∞—è –º–æ–¥–µ–ª—å ‚Üí surface —Ä–∞–∑–º–µ—Ä)
    - EmbeddingProcessor.SURFACE_ONLY (surface processing)
    - Joint training pipeline
    """

    def __init__(
        self,
        config: Optional[Union[AdapterIntegrationConfig, str, Dict]] = None,
        device: str = "cpu",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info("[LINK] Initializing AdapterCubeTrainer...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(config)
        self.device = torch.device(device)

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.adapter = None
        self.embedding_processor = (
            None  # –ó–∞–º–µ–Ω—è–µ–º cube_trainer –Ω–∞ direct EmbeddingProcessor
        )
        self.adapter_optimizer = None
        self.joint_optimizer = None

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.training_history = []
        self.adapter_warmup_complete = False

        self.logger.info(f"‚úÖ AdapterCubeTrainer configured:")
        self.logger.info(
            f"   Teacher: {self.config.teacher_model} ({self.config.teacher_embedding_dim}D)"
        )
        self.logger.info(f"   Cube: {self.config.cube_dimensions}")
        self.logger.info(f"   Surface strategy: {self.config.surface_strategy}")
        self.logger.info(f"   Adapter strategy: {self.config.adapter_strategy}")

    def _load_config(
        self, config: Optional[Union[AdapterIntegrationConfig, str, Dict]]
    ) -> AdapterIntegrationConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if config is None:
            return AdapterIntegrationConfig()

        elif isinstance(config, AdapterIntegrationConfig):
            return config

        elif isinstance(config, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ YAML
            try:
                with open(config, "r", encoding="utf-8") as f:
                    config_data = yaml.safe_load(f)
                return AdapterIntegrationConfig(**config_data)
            except Exception as e:
                self.logger.warning(f"Failed to load config from {config}: {e}")
                return AdapterIntegrationConfig()

        elif isinstance(config, dict):
            try:
                return AdapterIntegrationConfig(**config)
            except Exception as e:
                self.logger.warning(f"Failed to create config from dict: {e}")
                return AdapterIntegrationConfig()

        else:
            return AdapterIntegrationConfig()

    def initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        self.logger.info("[CONFIG] Initializing integrated training components...")

        # 1. –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞
        self._setup_adapter()

        # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ EmbeddingProcessor (–∑–∞–º–µ–Ω—è–µ—Ç CubeTrainer)
        self._setup_embedding_processor()

        # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
        self._setup_optimizers()

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self._validate_integration()

        self.logger.info("üéØ All integrated components initialized successfully!")

    def _setup_adapter(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞"""
        self.logger.info(
            f"[CONFIG] Setting up adapter: {self.config.teacher_model} ‚Üí surface..."
        )

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.logger.info(f"[DIAGNOSTIC] Teacher model: {self.config.teacher_model}")
        self.logger.info(
            f"[DIAGNOSTIC] Teacher embedding dim: {self.config.teacher_embedding_dim}"
        )

        # –í—ã—á–∏—Å–ª—è–µ–º surface —Ä–∞–∑–º–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        surface_size = self._calculate_surface_size()
        self.logger.info(f"[DIAGNOSTIC] Surface size: {surface_size}")

        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä
        self.adapter = UniversalEmbeddingAdapter(
            input_dim=self.config.teacher_embedding_dim,
            output_dim=surface_size,
            strategy=self.config.adapter_strategy,
        )

        self.adapter.to(self.device)

        compression_ratio = self.adapter.get_compression_ratio()
        param_count = self.adapter.get_parameter_count()

        self.logger.info(f"‚úÖ Adapter created:")
        self.logger.info(f"   {self.config.teacher_embedding_dim}D ‚Üí {surface_size}D")
        self.logger.info(
            f"   Compression: {compression_ratio:.3f} ({compression_ratio*100:.1f}%)"
        )
        self.logger.info(f"   Parameters: {param_count:,}")

    def _setup_embedding_processor(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ EmbeddingProcessor.SURFACE_ONLY (–∑–∞–º–µ–Ω—è–µ—Ç SimpleWrapper)"""
        if not EMBEDDING_PROCESSOR_AVAILABLE:
            self.logger.warning(
                "[WARNING] EmbeddingProcessor not available, using simple identity processor"
            )
            self.embedding_processor = self._create_simple_processor()
            return

        self.logger.info("[CONFIG] Setting up EmbeddingProcessor.SURFACE_ONLY...")

        # –í—ã—á–∏—Å–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã surface processing
        surface_size = self._calculate_surface_size()
        surface_dims = self.config.cube_dimensions[:2]  # (15, 15)

        # –°–æ–∑–¥–∞–µ–º surface-only –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        processor_config = create_surface_only_config(
            surface_size=surface_size, surface_dims=surface_dims
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        processor_config.device = str(self.device)

        # –°–æ–∑–¥–∞–µ–º EmbeddingProcessor –Ω–∞–ø—Ä—è–º—É—é
        self.embedding_processor = EmbeddingProcessor(processor_config)
        self.embedding_processor.to(self.device)

        self.logger.info(f"‚úÖ EmbeddingProcessor.SURFACE_ONLY initialized:")
        self.logger.info(f"   Mode: {ProcessingMode.SURFACE_ONLY.value}")
        self.logger.info(f"   Surface size: {surface_size}D")
        self.logger.info(f"   Surface dims: {surface_dims}")
        self.logger.info(
            f"   Processing depth: {processor_config.surface_processing_depth}"
        )

    def _create_simple_processor(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π identity processor –∫–∞–∫ fallback"""
        import torch.nn as nn

        class SimpleIdentityProcessor(nn.Module):
            def __init__(self):
                super().__init__()
                # –î–æ–±–∞–≤–ª—è–µ–º dummy parameter —á—Ç–æ–±—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–µ –ª–æ–º–∞–ª—Å—è
                self.dummy_param = nn.Parameter(torch.zeros(1), requires_grad=False)

            def forward(self, x):
                if isinstance(x, dict) and "processed_embeddings" in x:
                    return x  # –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                return {"processed_embeddings": x, "surface_output": x}

        return SimpleIdentityProcessor()

    def _setup_optimizers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è joint training"""
        self.logger.info("[CONFIG] Setting up optimizers...")

        # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤
        self.adapter_optimizer = optim.Adam(
            self.adapter.parameters(), lr=self.config.adapter_learning_rate
        )

        # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π optimizer –¥–ª—è EmbeddingProcessor
        processor_params = list(self.embedding_processor.parameters())

        if len(processor_params) > 0:
            self.processor_optimizer = optim.AdamW(
                processor_params, lr=self.config.cube_learning_rate
            )
            self.logger.info(
                f"‚úÖ Processor optimizer created with {len(processor_params)} parameters"
            )
        else:
            self.processor_optimizer = None
            self.logger.info(
                "[WARNING] Processor optimizer skipped (no trainable parameters)"
            )

        if self.config.joint_training:
            # Joint optimizer –¥–ª—è –æ–±–æ–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            all_params = list(self.adapter.parameters())
            if processor_params:
                all_params.extend(processor_params)

            if len(all_params) > 0:
                self.joint_optimizer = optim.AdamW(
                    all_params, lr=self.config.cube_learning_rate
                )
                self.logger.info(
                    f"‚úÖ Joint optimizer configured with {len(all_params)} total parameters"
                )
            else:
                self.joint_optimizer = None
                self.logger.warning(
                    "[WARNING] Joint optimizer creation failed: no parameters"
                )
        else:
            self.joint_optimizer = None

        self.logger.info("‚úÖ All optimizers configured (adapter, processor, joint)")

    def _calculate_surface_size(self) -> int:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ surface –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        x, y, z = self.config.cube_dimensions
        face_size = x * y

        if self.config.surface_strategy == "single":
            return face_size
        elif self.config.surface_strategy == "triple":
            return 3 * face_size  # front, back, top
        elif self.config.surface_strategy == "full":
            return 6 * face_size  # all faces
        else:
            raise ValueError(
                f"Unknown surface strategy: {self.config.surface_strategy}"
            )

    def _validate_integration(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        self.logger.info("[MAGNIFY] Validating component integration...")

        # –¢–µ—Å—Ç–æ–≤—ã–π forward pass
        test_input = torch.randn(2, self.config.teacher_embedding_dim).to(self.device)

        # Adapter forward
        adapter_output = self.adapter(test_input)
        expected_surface_size = self._calculate_surface_size()

        assert adapter_output.shape == (
            2,
            expected_surface_size,
        ), f"Adapter output shape mismatch: {adapter_output.shape} vs expected (2, {expected_surface_size})"

        # EmbeddingProcessor forward
        processor_result = self.embedding_processor.forward(adapter_output)

        # Handle case when processor returns dict
        if isinstance(processor_result, dict):
            processor_output = processor_result.get(
                "processed_embeddings",
                processor_result.get("surface_output", adapter_output),
            )
        else:
            processor_output = processor_result

        assert (
            processor_output.shape == adapter_output.shape
        ), f"Processor output shape mismatch: {processor_output.shape} vs expected {adapter_output.shape}"

        self.logger.info("‚úÖ Integration validation passed!")
        self.logger.info(
            f"   Pipeline: {test_input.shape} ‚Üí {adapter_output.shape} ‚Üí {processor_output.shape}"
        )

    def forward(
        self, teacher_embeddings: torch.Tensor, return_intermediate: bool = False
    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Full forward pass —á–µ—Ä–µ–∑ adapter + EmbeddingProcessor.SURFACE_ONLY

        Args:
            teacher_embeddings: –≠–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏ [batch, teacher_dim]
            return_intermediate: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

        Returns:
            output: –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç [batch, surface_size]
            intermediate: –°–ª–æ–≤–∞—Ä—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
        """
        # 1. Adapter: teacher_dim ‚Üí surface_size
        if self.config.use_reconstruction_loss:
            surface_embeddings, reconstructed = self.adapter(
                teacher_embeddings, return_reconstruction=True
            )
        else:
            surface_embeddings = self.adapter(teacher_embeddings)
            reconstructed = None

        # 2. EmbeddingProcessor.SURFACE_ONLY: surface_size ‚Üí surface_size
        processed_result = self.embedding_processor.forward(surface_embeddings)

        # Handle case when processor returns dict
        if isinstance(processed_result, dict):
            processed_embeddings = processed_result.get(
                "processed_embeddings",
                processed_result.get("surface_output", surface_embeddings),
            )
        else:
            processed_embeddings = processed_result

        if return_intermediate:
            return {
                "output": processed_embeddings,
                "surface_embeddings": surface_embeddings,
                "reconstructed": reconstructed,
                "teacher_embeddings": teacher_embeddings,
            }
        else:
            return processed_embeddings

    def compute_loss(
        self, teacher_embeddings: torch.Tensor, target_embeddings: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π loss function

        Args:
            teacher_embeddings: Input —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏
            target_embeddings: Target —ç–º–±–µ–¥–∏–Ω–≥–∏ (—Ç–∞–∫–∂–µ –æ—Ç teacher –º–æ–¥–µ–ª–∏)

        Returns:
            Dict —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ loss
        """
        # Forward pass —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results = self.forward(teacher_embeddings, return_intermediate=True)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º target —Ç–∞–∫–∂–µ —á–µ—Ä–µ–∑ adapter
        target_surface = self.adapter(target_embeddings)

        # Main task loss: processed surface ‚Üí target surface
        main_loss = nn.functional.cosine_embedding_loss(
            results["output"],
            target_surface,
            torch.ones(target_surface.size(0)).to(self.device),
        )

        losses = {"main_loss": main_loss}

        # Reconstruction loss (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if self.config.use_reconstruction_loss and results["reconstructed"] is not None:
            reconstruction_loss = self.adapter.compute_reconstruction_loss(
                teacher_embeddings, results["reconstructed"]
            )
            losses["reconstruction_loss"] = reconstruction_loss

        # Total loss
        total_loss = losses["main_loss"]
        if "reconstruction_loss" in losses:
            total_loss = (
                total_loss
                + self.config.reconstruction_weight * losses["reconstruction_loss"]
            )

        losses["total_loss"] = total_loss

        return losses

    def train_step(
        self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor
    ) -> Dict[str, float]:
        """
        –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è

        Args:
            question_embeddings: Question —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏
            answer_embeddings: Answer —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç teacher –º–æ–¥–µ–ª–∏

        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        if self.config.joint_training:
            return self._joint_train_step(question_embeddings, answer_embeddings)
        else:
            return self._separate_train_step(question_embeddings, answer_embeddings)

    def _joint_train_step(
        self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor
    ) -> Dict[str, float]:
        """Joint training step (adapter + EmbeddingProcessor –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)"""
        self.adapter.train()
        self.embedding_processor.train()

        # Forward pass
        losses = self.compute_loss(question_embeddings, answer_embeddings)

        # Backward pass
        self.joint_optimizer.zero_grad()
        losses["total_loss"].backward()

        # Gradient clipping
        if self.config.gradient_balancing:
            nn.utils.clip_grad_norm_(self.adapter.parameters(), max_norm=1.0)
            nn.utils.clip_grad_norm_(
                self.embedding_processor.parameters(), max_norm=1.0
            )

        self.joint_optimizer.step()

        # Metrics
        with torch.no_grad():
            results = self.forward(question_embeddings, return_intermediate=True)
            target_surface = self.adapter(answer_embeddings)

            qa_similarity = (
                torch.cosine_similarity(results["output"], target_surface, dim=1)
                .mean()
                .item()
            )

        return {
            "total_loss": losses["total_loss"].item(),
            "main_loss": losses["main_loss"].item(),
            "reconstruction_loss": losses.get(
                "reconstruction_loss", torch.tensor(0.0)
            ).item(),
            "qa_similarity": qa_similarity,
        }

    def _separate_train_step(
        self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor
    ) -> Dict[str, float]:
        """Separate training (adapter first, then EmbeddingProcessor)"""
        if (
            not self.adapter_warmup_complete
            and self.current_epoch < self.config.adapter_warmup_epochs
        ):
            # Adapter warmup phase
            return self._adapter_warmup_step(question_embeddings, answer_embeddings)
        else:
            # Main training phase
            self.adapter_warmup_complete = True
            return self._processor_training_step(question_embeddings, answer_embeddings)

    def _adapter_warmup_step(
        self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor
    ) -> Dict[str, float]:
        """Adapter warmup step (—Ç–æ–ª—å–∫–æ adapter)"""
        self.adapter.train()
        self.embedding_processor.eval()  # Freeze EmbeddingProcessor

        # Adapter training —Ç–æ–ª—å–∫–æ –Ω–∞ reconstruction
        _, reconstructed = self.adapter(question_embeddings, return_reconstruction=True)
        reconstruction_loss = self.adapter.compute_reconstruction_loss(
            question_embeddings, reconstructed
        )

        self.adapter_optimizer.zero_grad()
        reconstruction_loss.backward()
        self.adapter_optimizer.step()

        return {
            "total_loss": reconstruction_loss.item(),
            "reconstruction_loss": reconstruction_loss.item(),
            "qa_similarity": 0.0,  # Not applicable during warmup
            "phase": "adapter_warmup",
        }

    def _processor_training_step(
        self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor
    ) -> Dict[str, float]:
        """EmbeddingProcessor training step (—Ç–æ–ª—å–∫–æ EmbeddingProcessor, adapter frozen)"""
        self.adapter.eval()  # Freeze adapter
        self.embedding_processor.train()

        # Forward —á–µ—Ä–µ–∑ frozen adapter
        with torch.no_grad():
            question_surface = self.adapter(question_embeddings)
            answer_surface = self.adapter(answer_embeddings)

        # EmbeddingProcessor training
        processed_surface = self.embedding_processor.forward(question_surface)

        # Handle case when processor returns dict
        if isinstance(processed_surface, dict):
            processed_surface = processed_surface.get(
                "processed_embeddings",
                processed_surface.get("surface_output", processed_surface),
            )

        main_loss = nn.functional.cosine_embedding_loss(
            processed_surface,
            answer_surface,
            torch.ones(answer_surface.size(0)).to(self.device),
        )

        # Backward pass (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å trainable parameters)
        if self.processor_optimizer is not None:
            self.processor_optimizer.zero_grad()
            main_loss.backward()
            self.processor_optimizer.step()
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç trainable parameters, loss –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω—É–∂–µ–Ω –¥–ª—è –º–µ—Ç—Ä–∏–∫
            pass

        # Metrics
        with torch.no_grad():
            qa_similarity = (
                torch.cosine_similarity(processed_surface, answer_surface, dim=1)
                .mean()
                .item()
            )

        return {
            "total_loss": main_loss.item(),
            "main_loss": main_loss.item(),
            "qa_similarity": qa_similarity,
            "phase": "processor_training",
        }

    def get_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ"""
        adapter_info = {
            "input_dim": self.adapter.input_dim if self.adapter else None,
            "output_dim": self.adapter.output_dim if self.adapter else None,
            "strategy": self.config.adapter_strategy,
            "compression_ratio": (
                self.adapter.get_compression_ratio() if self.adapter else None
            ),
            "parameters": self.adapter.get_parameter_count() if self.adapter else None,
        }

        processor_info = {
            "type": "EmbeddingProcessor.SURFACE_ONLY",
            "mode": ProcessingMode.SURFACE_ONLY.value,
            "surface_size": self._calculate_surface_size(),
            "surface_dims": self.config.cube_dimensions[:2],
            "total_parameters": (
                sum(p.numel() for p in self.embedding_processor.parameters())
                if self.embedding_processor
                else 0
            ),
        }

        return {
            "teacher_model": self.config.teacher_model,
            "cube_dimensions": self.config.cube_dimensions,
            "surface_strategy": self.config.surface_strategy,
            "joint_training": self.config.joint_training,
            "current_epoch": self.current_epoch,
            "adapter_warmup_complete": self.adapter_warmup_complete,
            "adapter": adapter_info,
            "processor": processor_info,
            "total_parameters": (
                adapter_info.get("parameters", 0)
                + processor_info.get("total_parameters", 0)
            ),
        }


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–µ—Ä–æ–≤


def create_llama3_cube_trainer(
    cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
    adapter_strategy: str = "learned_linear",
    device: str = "cpu",
) -> AdapterCubeTrainer:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è LLaMA-3-8B"""
    config = AdapterIntegrationConfig(
        teacher_model="Meta-Llama-3-8B",
        cube_dimensions=cube_dimensions,
        adapter_strategy=adapter_strategy,
        joint_training=True,
    )

    trainer = AdapterCubeTrainer(config=config, device=device)
    trainer.initialize_components()

    return trainer


def create_distilbert_cube_trainer(
    cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
    adapter_strategy: str = "learned_linear",
    device: str = "cpu",
) -> AdapterCubeTrainer:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è DistilBERT (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)"""
    config = AdapterIntegrationConfig(
        teacher_model="DistilBERT",
        cube_dimensions=cube_dimensions,
        adapter_strategy=adapter_strategy,
        joint_training=True,
    )

    trainer = AdapterCubeTrainer(config=config, device=device)
    trainer.initialize_components()

    return trainer


# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    "AdapterIntegrationConfig",
    "AdapterCubeTrainer",
    "create_llama3_cube_trainer",
    "create_distilbert_cube_trainer",
]
