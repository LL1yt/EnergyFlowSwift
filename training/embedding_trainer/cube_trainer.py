"""
CubeTrainer - –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D –∫–ª–µ—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
–Ω–∞ —ç–º–±–µ–¥–∏–Ω–≥‚Üí—ç–º–±–µ–¥–∏–Ω–≥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è—Ö.

–ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- Autoencoder —Ä–µ–∂–∏–º (embedding ‚Üí embedding)
- Dialogue —Ä–µ–∂–∏–º (question_embedding ‚Üí answer_embedding)
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≥–æ—Ç–æ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ (EmbeddingProcessor, EmbeddingReshaper)
- –°–∏—Å—Ç–µ–º–∞ –º–µ—Ç—Ä–∏–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- Checkpoint —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: v1.0.0 (Phase 3.1 - Stage 1.1)
–î–∞—Ç–∞: 6 –∏—é–Ω—è 2025
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from pathlib import Path
import logging
from typing import Dict, Any, Optional, Union, List
from dataclasses import dataclass
import yaml

# –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from core.embedding_processor import EmbeddingProcessor
    from data.embedding_reshaper import EmbeddingReshaper
    from data.embedding_loader import EmbeddingLoader
    from utils.config_manager import ConfigManager, ConfigManagerSettings
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  Warning: Some dependencies not available: {e}")
    DEPENDENCIES_AVAILABLE = False


@dataclass
class TrainingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    mode: str = "autoencoder"  # autoencoder | dialogue | mixed
    device: str = "cpu"        # cpu | cuda
    random_seed: int = 42
    
    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    lattice_size: List[int] = None
    embedding_dim: int = 768
    batch_size: int = 32
    
    # –û–±—É—á–µ–Ω–∏–µ
    learning_rate: float = 0.001
    epochs: int = 50
    optimizer: str = "adam"    # adam | sgd | adamw
    loss_function: str = "cosine"  # cosine | mse | combined
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
    target_similarity: float = 0.90
    convergence_threshold: float = 0.001
    early_stopping_patience: int = 10
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    log_interval: int = 10
    save_interval: int = 25
    checkpoint_dir: str = "checkpoints/embedding_trainer"
    
    def __post_init__(self):
        if self.lattice_size is None:
            self.lattice_size = [8, 8, 8]


class EmbeddingMetrics:
    """–°–∏—Å—Ç–µ–º–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.cosine_similarity = nn.CosineSimilarity(dim=1)
        self.mse_loss = nn.MSELoss()
        
    def calculate_cosine_similarity(self, input_emb: torch.Tensor, output_emb: torch.Tensor) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ cosine similarity –º–µ–∂–¥—É —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏"""
        with torch.no_grad():
            similarity = self.cosine_similarity(input_emb, output_emb)
            return similarity.mean().item()
    
    def calculate_mse_loss(self, input_emb: torch.Tensor, output_emb: torch.Tensor) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ MSE loss –º–µ–∂–¥—É —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏"""
        with torch.no_grad():
            loss = self.mse_loss(input_emb, output_emb)
            return loss.item()
    
    def calculate_semantic_preservation(self, input_emb: torch.Tensor, output_emb: torch.Tensor) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)"""
        cosine_sim = self.calculate_cosine_similarity(input_emb, output_emb)
        mse_loss = self.calculate_mse_loss(input_emb, output_emb)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞: –≤—ã—Å–æ–∫–∞—è cosine similarity + –Ω–∏–∑–∫–∏–π MSE
        semantic_score = cosine_sim * (1.0 / (1.0 + mse_loss))
        return semantic_score
    
    def compute_batch_metrics(self, input_batch: torch.Tensor, output_batch: torch.Tensor) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è batch"""
        return {
            'cosine_similarity': self.calculate_cosine_similarity(input_batch, output_batch),
            'mse_loss': self.calculate_mse_loss(input_batch, output_batch),
            'semantic_preservation': self.calculate_semantic_preservation(input_batch, output_batch)
        }


class CubeTrainer:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core
    
    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å–∏—Å—Ç–µ–º—ã,
    –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è—Å—å —Å –≥–æ—Ç–æ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
    """
    
    def __init__(self, 
                 config: Optional[Union[TrainingConfig, str, Dict]] = None,
                 mode: str = "autoencoder",
                 device: str = "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CubeTrainer
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è (TrainingConfig, –ø—É—Ç—å –∫ YAML –∏–ª–∏ dict)
            mode: –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è (autoencoder, dialogue, mixed)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (cpu, cuda)
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = logging.getLogger(__name__)
        self._setup_logging()
        
        self.logger.info("üöÄ Initializing CubeTrainer...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError("Required dependencies are not available. "
                            "Make sure core.embedding_processor, data.embedding_reshaper, "
                            "and data.embedding_loader are implemented.")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(config, mode, device)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        self.device = torch.device(self.config.device)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        self._set_random_seed(self.config.random_seed)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.embedding_processor = None
        self.embedding_reshaper = None
        self.embedding_loader = None
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        self.optimizer = None
        self.loss_function = None
        self.metrics = EmbeddingMetrics(device=self.config.device)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.training_history = []
        self.best_metrics = {}
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è checkpoint'–æ–≤
        self.checkpoint_dir = Path(self.config.checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"‚úÖ CubeTrainer initialized successfully")
        self.logger.info(f"   Mode: {self.config.mode}")
        self.logger.info(f"   Device: {self.config.device}")
        self.logger.info(f"   Lattice size: {self.config.lattice_size}")
        
    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ - –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def _load_config(self, config: Optional[Union[TrainingConfig, str, Dict]], 
                    mode: str, device: str) -> TrainingConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if config is None:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return TrainingConfig(mode=mode, device=device)
        
        elif isinstance(config, TrainingConfig):
            return config
        
        elif isinstance(config, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ YAML —Ñ–∞–π–ª–∞
            try:
                with open(config, 'r', encoding='utf-8') as f:
                    config_data = yaml.safe_load(f)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ embedding_trainer –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'embedding_trainer' in config_data:
                    config_data = config_data['embedding_trainer']
                
                return TrainingConfig(**config_data)
            
            except Exception as e:
                self.logger.warning(f"Failed to load config from {config}: {e}")
                return TrainingConfig(mode=mode, device=device)
        
        elif isinstance(config, dict):
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ —Å–ª–æ–≤–∞—Ä—è
            try:
                return TrainingConfig(**config)
            except Exception as e:
                self.logger.warning(f"Failed to create config from dict: {e}")
                return TrainingConfig(mode=mode, device=device)
        
        else:
            self.logger.warning(f"Unknown config type: {type(config)}")
            return TrainingConfig(mode=mode, device=device)
    
    def _set_random_seed(self, seed: int):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"""
        torch.manual_seed(seed)
        np.random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
    
    def initialize_components(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        –°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç:
        - EmbeddingProcessor (3D Cubic Core)
        - EmbeddingReshaper (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤)
        - EmbeddingLoader (–∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö)
        """
        self.logger.info("üîß Initializing training components...")
        
        try:
            # 1. EmbeddingReshaper –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤
            self.embedding_reshaper = EmbeddingReshaper(
                input_dim=self.config.embedding_dim,
                cube_shape=self.config.lattice_size
            )
            self.logger.info(f"‚úÖ EmbeddingReshaper initialized: {self.config.embedding_dim}D ‚Üî {self.config.lattice_size}")
            
            # 2. EmbeddingProcessor (3D Cubic Core)
            self.embedding_processor = EmbeddingProcessor(
                lattice_size=self.config.lattice_size,
                device=self.config.device
            )
            self.logger.info(f"‚úÖ EmbeddingProcessor initialized: {self.config.lattice_size}")
            
            # 3. EmbeddingLoader –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
            self.embedding_loader = EmbeddingLoader()
            self.logger.info(f"‚úÖ EmbeddingLoader initialized")
            
            # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ loss function
            self._setup_loss_function()
            
            # 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ optimizer
            self._setup_optimizer()
            
            self.logger.info("üéØ All training components initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to initialize components: {e}")
            raise
    
    def _setup_loss_function(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å"""
        if self.config.loss_function == "cosine":
            # Cosine similarity loss (1 - cosine_similarity)
            self.loss_function = lambda x, y: 1 - nn.functional.cosine_similarity(x, y, dim=1).mean()
        
        elif self.config.loss_function == "mse":
            self.loss_function = nn.MSELoss()
        
        elif self.config.loss_function == "combined":
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss function
            mse_loss = nn.MSELoss()
            cosine_weight = 0.7
            mse_weight = 0.3
            
            def combined_loss(x, y):
                cosine_loss = 1 - nn.functional.cosine_similarity(x, y, dim=1).mean()
                mse_loss_val = mse_loss(x, y)
                return cosine_weight * cosine_loss + mse_weight * mse_loss_val
            
            self.loss_function = combined_loss
        
        else:
            self.logger.warning(f"Unknown loss function: {self.config.loss_function}, using cosine")
            self.loss_function = lambda x, y: 1 - nn.functional.cosine_similarity(x, y, dim=1).mean()
        
        self.logger.info(f"‚úÖ Loss function configured: {self.config.loss_function}")
    
    def _setup_optimizer(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫—É–±–∞!)
        if self.embedding_processor is None:
            raise ValueError("EmbeddingProcessor must be initialized before setting up optimizer")
        
        # –í–∞–∂–Ω–æ: –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã 3D Cubic Core
        trainable_params = list(self.embedding_processor.parameters())
        
        if self.config.optimizer == "adam":
            self.optimizer = optim.Adam(trainable_params, lr=self.config.learning_rate)
        elif self.config.optimizer == "sgd":
            self.optimizer = optim.SGD(trainable_params, lr=self.config.learning_rate)
        elif self.config.optimizer == "adamw":
            self.optimizer = optim.AdamW(trainable_params, lr=self.config.learning_rate)
        else:
            self.logger.warning(f"Unknown optimizer: {self.config.optimizer}, using Adam")
            self.optimizer = optim.Adam(trainable_params, lr=self.config.learning_rate)
        
        self.logger.info(f"‚úÖ Optimizer configured: {self.config.optimizer}")
        self.logger.info(f"   Trainable parameters: {sum(p.numel() for p in trainable_params)}")
    
    def forward(self, input_embedding: torch.Tensor) -> torch.Tensor:
        """
        Forward pass —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π pipeline
        
        Args:
            input_embedding: –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ (batch_size, embedding_dim)
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥ (batch_size, embedding_dim)
        """
        if self.embedding_reshaper is None or self.embedding_processor is None:
            raise ValueError("Components must be initialized before forward pass")
        
        # 1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è 1D ‚Üí 3D
        matrix_3d = self.embedding_reshaper.vector_to_matrix(input_embedding)
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ 3D Cubic Core
        processed_matrix = self.embedding_processor.process(matrix_3d)
        
        # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è 3D ‚Üí 1D
        output_embedding = self.embedding_reshaper.matrix_to_vector(processed_matrix)
        
        return output_embedding
    
    def get_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–µ–Ω–µ—Ä–µ"""
        return {
            'mode': self.config.mode,
            'device': self.config.device,
            'lattice_size': self.config.lattice_size,
            'embedding_dim': self.config.embedding_dim,
            'current_epoch': self.current_epoch,
            'optimizer': self.config.optimizer,
            'loss_function': self.config.loss_function,
            'target_similarity': self.config.target_similarity,
            'components_initialized': all([
                self.embedding_processor is not None,
                self.embedding_reshaper is not None,
                self.embedding_loader is not None
            ])
        }
    
    def set_mode(self, mode: str):
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if mode in ["autoencoder", "dialogue", "mixed"]:
            self.config.mode = mode
            self.logger.info(f"‚úÖ Training mode changed to: {mode}")
        else:
            raise ValueError(f"Unknown mode: {mode}. Supported: autoencoder, dialogue, mixed")
    
    def __repr__(self):
        return (f"CubeTrainer(mode={self.config.mode}, "
                f"device={self.config.device}, "
                f"lattice_size={self.config.lattice_size})")


# –≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
__all__ = ['CubeTrainer', 'TrainingConfig', 'EmbeddingMetrics'] 