"""
Advanced Loss Functions –¥–ª—è Stage 2.3
–°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Q‚ÜíA similarity
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import numpy as np


@dataclass
class LossFunctionConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö loss functions"""
    # –ë–∞–∑–æ–≤—ã–µ loss functions
    use_cosine_loss: bool = True
    use_mse_loss: bool = True
    cosine_weight: float = 0.7
    mse_weight: float = 0.3
    
    # Curriculum learning
    use_curriculum_loss: bool = True
    curriculum_warmup_epochs: int = 5
    easy_weight_start: float = 0.8      # –ù–∞—á–∞–ª—å–Ω—ã–π –≤–µ—Å –ª–µ–≥–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    easy_weight_end: float = 0.3        # –ö–æ–Ω–µ—á–Ω—ã–π –≤–µ—Å –ª–µ–≥–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    
    # Triplet loss
    use_triplet_loss: bool = True
    triplet_margin: float = 0.2
    triplet_weight: float = 0.1
    
    # Contrastive learning
    use_contrastive_loss: bool = True
    contrastive_temperature: float = 0.5
    contrastive_weight: float = 0.15
    
    # Multi-objective optimization
    diversity_weight: float = 0.05      # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    semantic_alignment_weight: float = 0.85  # –ì–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å - semantic alignment


# Alias –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
AdvancedLossConfig = LossFunctionConfig


class AdvancedLossFunction(nn.Module):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ loss functions –¥–ª—è Stage 2.3
    
    –í–∫–ª—é—á–∞–µ—Ç:
    - Curriculum learning loss (easy‚Üíhard progression)
    - Triplet loss –¥–ª—è enhanced semantic alignment
    - Contrastive learning approaches
    - Multi-objective optimization (similarity + diversity)
    """
    
    def __init__(self, config: Optional[LossFunctionConfig] = None):
        super().__init__()
        self.config = config or LossFunctionConfig()
        
        # –ë–∞–∑–æ–≤—ã–µ loss functions
        self.mse_loss = nn.MSELoss()
        self.cosine_loss = CosineSimilarityLoss()
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ loss functions
        self.triplet_loss = TripletMarginLoss(margin=self.config.triplet_margin)
        self.contrastive_loss = ContrastiveLoss(temperature=self.config.contrastive_temperature)
        
        # Curriculum learning state
        self.current_epoch = 0
        self.total_epochs = 100  # Will be updated during training
        
        print(f"üöÄ AdvancedLossFunction initialized")
        print(f"   Curriculum learning: {self.config.use_curriculum_loss}")
        print(f"   Triplet loss: {self.config.use_triplet_loss}")
        print(f"   Contrastive loss: {self.config.use_contrastive_loss}")
    
    def forward(self, 
                input_embeddings: torch.Tensor,
                target_embeddings: torch.Tensor,
                output_embeddings: torch.Tensor,
                difficulty_scores: Optional[torch.Tensor] = None,
                negative_embeddings: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π loss function
        
        Args:
            input_embeddings: –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ (batch_size, embedding_dim)
            target_embeddings: –¶–µ–ª–µ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ (batch_size, embedding_dim)
            output_embeddings: –í—ã—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ –æ—Ç –º–æ–¥–µ–ª–∏ (batch_size, embedding_dim)
            difficulty_scores: Scores —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è curriculum learning (batch_size,)
            negative_embeddings: –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è contrastive learning (batch_size, embedding_dim)
            
        Returns:
            Dict —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ loss –∏ –æ–±—â–∏–º loss
        """
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –∫ –æ–¥–Ω–æ–º—É —Ç–∏–ø—É (float32) –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        input_embeddings = input_embeddings.float()
        target_embeddings = target_embeddings.float()
        output_embeddings = output_embeddings.float()
        
        if negative_embeddings is not None:
            negative_embeddings = negative_embeddings.float()
        if difficulty_scores is not None:
            difficulty_scores = difficulty_scores.float()
        
        losses = {}
        
        # 1. –ë–∞–∑–æ–≤—ã–µ loss functions
        if self.config.use_cosine_loss:
            cosine_loss = self.cosine_loss(output_embeddings, target_embeddings)
            losses['cosine_loss'] = cosine_loss
        
        if self.config.use_mse_loss:
            mse_loss = self.mse_loss(output_embeddings, target_embeddings)
            losses['mse_loss'] = mse_loss
        
        # 2. Curriculum learning loss
        if self.config.use_curriculum_loss and difficulty_scores is not None:
            curriculum_loss = self._compute_curriculum_loss(
                output_embeddings, target_embeddings, difficulty_scores
            )
            losses['curriculum_loss'] = curriculum_loss
        
        # 3. Triplet loss –¥–ª—è semantic alignment
        if self.config.use_triplet_loss and negative_embeddings is not None:
            triplet_loss = self._compute_triplet_loss(
                input_embeddings, output_embeddings, negative_embeddings
            )
            losses['triplet_loss'] = triplet_loss
        
        # 4. Contrastive learning loss
        if self.config.use_contrastive_loss and negative_embeddings is not None:
            contrastive_loss = self._compute_contrastive_loss(
                output_embeddings, target_embeddings, negative_embeddings
            )
            losses['contrastive_loss'] = contrastive_loss
        
        # 5. Diversity penalty
        diversity_loss = self._compute_diversity_loss(output_embeddings)
        losses['diversity_loss'] = diversity_loss
        
        # 6. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö loss components
        total_loss = self._combine_losses(losses)
        losses['total_loss'] = total_loss
        
        return losses
    
    def _compute_curriculum_loss(self, 
                               output_embeddings: torch.Tensor,
                               target_embeddings: torch.Tensor, 
                               difficulty_scores: torch.Tensor) -> torch.Tensor:
        """Curriculum learning loss —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ curriculum weight
        progress = self.current_epoch / max(self.total_epochs, 1)
        progress = min(1.0, progress)
        
        # Linear interpolation –æ—Ç easy_weight_start –∫ easy_weight_end
        easy_weight = (self.config.easy_weight_start * (1 - progress) + 
                      self.config.easy_weight_end * progress)
        
        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º difficulty scores: –ª–µ–≥–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã = –≤—ã—Å–æ–∫–∏–π –≤–µ—Å
        easy_weights = 1.0 - difficulty_scores  # 0-1, –≥–¥–µ 1 = –ª–µ–≥–∫–∏–π
        hard_weights = difficulty_scores        # 0-1, –≥–¥–µ 1 = —Å–ª–æ–∂–Ω—ã–π
        
        # Adaptive weighting based on curriculum phase
        sample_weights = easy_weights * easy_weight + hard_weights * (1 - easy_weight)
        
        # Weighted cosine similarity loss
        cosine_similarities = F.cosine_similarity(output_embeddings, target_embeddings, dim=1)
        cosine_losses = 1 - cosine_similarities
        
        # Apply curriculum weights
        weighted_losses = cosine_losses * sample_weights
        
        return weighted_losses.mean()
    
    def _compute_triplet_loss(self,
                            anchor_embeddings: torch.Tensor,
                            positive_embeddings: torch.Tensor,
                            negative_embeddings: torch.Tensor) -> torch.Tensor:
        """Triplet loss –¥–ª—è enhanced semantic alignment"""
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –∫ –æ–¥–Ω–æ–º—É —Ç–∏–ø—É (float32)
        anchor_embeddings = anchor_embeddings.float()
        positive_embeddings = positive_embeddings.float()
        negative_embeddings = negative_embeddings.float()
        
        batch_size = anchor_embeddings.size(0)
        embedding_dim = anchor_embeddings.size(1)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        if negative_embeddings.size(0) != batch_size:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ negative_embeddings —Å–æ–¥–µ—Ä–∂–∏—Ç num_negatives –Ω–∞ –∫–∞–∂–¥—ã–π anchor
            num_negatives = negative_embeddings.size(0) // batch_size
            
            # Reshape negatives: (batch_size * num_negatives, embedding_dim) -> (batch_size, num_negatives, embedding_dim)
            negative_embeddings = negative_embeddings.view(batch_size, num_negatives, embedding_dim)
            
            # Expand anchors and positives –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            anchor_expanded = anchor_embeddings.unsqueeze(1).expand(-1, num_negatives, -1)  # (batch_size, num_negatives, embedding_dim)
            positive_expanded = positive_embeddings.unsqueeze(1).expand(-1, num_negatives, -1)
            
            # Distances –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
            pos_distance = F.pairwise_distance(
                anchor_expanded.reshape(-1, embedding_dim), 
                positive_expanded.reshape(-1, embedding_dim)
            )  # (batch_size * num_negatives,)
            
            neg_distance = F.pairwise_distance(
                anchor_expanded.reshape(-1, embedding_dim), 
                negative_embeddings.reshape(-1, embedding_dim)
            )  # (batch_size * num_negatives,)
            
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π: –æ–¥–∏–Ω –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–∏–º–µ—Ä –Ω–∞ anchor
            pos_distance = F.pairwise_distance(anchor_embeddings, positive_embeddings)
            neg_distance = F.pairwise_distance(anchor_embeddings, negative_embeddings)
        
        # Triplet margin loss
        triplet_loss = F.relu(pos_distance - neg_distance + self.config.triplet_margin)
        
        return triplet_loss.mean()
    
    def _compute_contrastive_loss(self,
                                output_embeddings: torch.Tensor,
                                target_embeddings: torch.Tensor,
                                negative_embeddings: torch.Tensor) -> torch.Tensor:
        """Contrastive learning loss"""
        
        batch_size = output_embeddings.size(0)
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –∫ –æ–¥–Ω–æ–º—É —Ç–∏–ø—É (float32)
        output_embeddings = output_embeddings.float()
        target_embeddings = target_embeddings.float()
        negative_embeddings = negative_embeddings.float()
        
        # Positive pairs (output vs target)
        pos_similarities = F.cosine_similarity(output_embeddings, target_embeddings, dim=1)
        pos_similarities = pos_similarities / self.config.contrastive_temperature
        
        # Negative pairs (output vs negatives)
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        if negative_embeddings.size(0) == batch_size:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π: –æ–¥–∏–Ω –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–∏–º–µ—Ä –Ω–∞ anchor
            neg_similarities = F.cosine_similarity(
                output_embeddings, negative_embeddings, dim=1
            ).unsqueeze(1)
        else:
            # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            neg_similarities = torch.mm(output_embeddings, negative_embeddings.T)
        
        neg_similarities = neg_similarities / self.config.contrastive_temperature
        
        # Contrastive loss (InfoNCE)
        logits = torch.cat([pos_similarities.unsqueeze(1), neg_similarities], dim=1)
        labels = torch.zeros(batch_size, dtype=torch.long, device=output_embeddings.device)
        
        contrastive_loss = F.cross_entropy(logits, labels)
        
        return contrastive_loss
    
    def _compute_diversity_loss(self, embeddings: torch.Tensor) -> torch.Tensor:
        """–®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ batch"""
        
        # Pairwise cosine similarities within batch
        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)
        similarity_matrix = torch.mm(normalized_embeddings, normalized_embeddings.T)
        
        # –£–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å (self-similarities)
        mask = torch.eye(embeddings.size(0), device=embeddings.device).bool()
        similarity_matrix = similarity_matrix.masked_fill(mask, 0)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ã—Å–æ–∫—É—é similarity (–Ω–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)
        diversity_penalty = similarity_matrix.abs().mean()
        
        return diversity_penalty
    
    def _combine_losses(self, losses: Dict[str, torch.Tensor]) -> torch.Tensor:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ loss"""
        total_loss = torch.tensor(0.0, device=next(iter(losses.values())).device)
        
        # –ë–∞–∑–æ–≤—ã–µ losses
        if 'cosine_loss' in losses:
            total_loss += self.config.cosine_weight * losses['cosine_loss']
        
        if 'mse_loss' in losses:
            total_loss += self.config.mse_weight * losses['mse_loss']
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ losses
        if 'curriculum_loss' in losses:
            total_loss += self.config.semantic_alignment_weight * losses['curriculum_loss']
        
        if 'triplet_loss' in losses:
            total_loss += self.config.triplet_weight * losses['triplet_loss']
        
        if 'contrastive_loss' in losses:
            total_loss += self.config.contrastive_weight * losses['contrastive_loss']
        
        if 'diversity_loss' in losses:
            total_loss += self.config.diversity_weight * losses['diversity_loss']
        
        return total_loss
    
    def update_epoch(self, current_epoch: int, total_epochs: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç–ø–æ—Ö–∞—Ö –¥–ª—è curriculum learning"""
        self.current_epoch = current_epoch
        self.total_epochs = total_epochs
    
    def get_curriculum_progress(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ curriculum learning (0-1)"""
        return min(1.0, self.current_epoch / max(self.total_epochs, 1))
    
    def get_loss_weights(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –≤–µ—Å–æ–≤ loss components"""
        progress = self.get_curriculum_progress()
        easy_weight = (self.config.easy_weight_start * (1 - progress) + 
                      self.config.easy_weight_end * progress)
        
        return {
            'cosine_weight': self.config.cosine_weight,
            'mse_weight': self.config.mse_weight,
            'triplet_weight': self.config.triplet_weight,
            'contrastive_weight': self.config.contrastive_weight,
            'diversity_weight': self.config.diversity_weight,
            'current_easy_weight': easy_weight,
            'curriculum_progress': progress
        }


class CosineSimilarityLoss(nn.Module):
    """Cosine similarity loss (1 - cosine_similarity)"""
    
    def forward(self, input_embeddings: torch.Tensor, target_embeddings: torch.Tensor) -> torch.Tensor:
        cosine_sim = F.cosine_similarity(input_embeddings, target_embeddings, dim=1)
        return (1 - cosine_sim).mean()


class TripletMarginLoss(nn.Module):
    """Triplet margin loss –¥–ª—è semantic alignment"""
    
    def __init__(self, margin: float = 0.2):
        super().__init__()
        self.margin = margin
    
    def forward(self, 
                anchor: torch.Tensor, 
                positive: torch.Tensor, 
                negative: torch.Tensor) -> torch.Tensor:
        
        pos_distance = F.pairwise_distance(anchor, positive)
        neg_distance = F.pairwise_distance(anchor, negative)
        
        triplet_loss = F.relu(pos_distance - neg_distance + self.margin)
        return triplet_loss.mean()


class ContrastiveLoss(nn.Module):
    """Contrastive learning loss (InfoNCE)"""
    
    def __init__(self, temperature: float = 0.5):
        super().__init__()
        self.temperature = temperature
    
    def forward(self,
                query: torch.Tensor,
                positive: torch.Tensor,
                negatives: torch.Tensor) -> torch.Tensor:
        
        # Positive similarities
        pos_sim = F.cosine_similarity(query, positive, dim=1) / self.temperature
        
        # Negative similarities
        neg_sim = torch.mm(query, negatives.T) / self.temperature
        
        # InfoNCE loss
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)
        labels = torch.zeros(query.size(0), dtype=torch.long, device=query.device)
        
        return F.cross_entropy(logits, labels)


class NegativeSampler:
    """–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
    
    def __init__(self, embedding_dim: int = 768):
        self.embedding_dim = embedding_dim
    
    def sample_random_negatives(self, 
                              positive_embeddings: torch.Tensor,
                              num_negatives: int = 5) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        batch_size = positive_embeddings.size(0)
        device = positive_embeddings.device
        
        # Random negatives
        negatives = torch.randn(
            batch_size, num_negatives, self.embedding_dim, 
            device=device
        )
        
        # Normalize
        negatives = F.normalize(negatives, p=2, dim=-1)
        
        return negatives.reshape(-1, self.embedding_dim)
    
    def sample_hard_negatives(self,
                            anchor_embeddings: torch.Tensor,
                            all_embeddings: torch.Tensor,
                            num_negatives: int = 5) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–±–ª–∏–∑–∫–∏—Ö –∫ anchor)"""
        
        batch_size = anchor_embeddings.size(0)
        
        # Compute similarities with all embeddings
        similarities = torch.mm(anchor_embeddings, all_embeddings.T)
        
        # Select hard negatives (highest similarities excluding self)
        _, indices = similarities.topk(num_negatives + 1, dim=1)
        
        # Exclude self (first index is usually self if anchor is in all_embeddings)
        negative_indices = indices[:, 1:num_negatives+1]
        
        # Gather hard negatives
        hard_negatives = all_embeddings[negative_indices.flatten()]
        
        return hard_negatives


# ================================
# HELPER FUNCTIONS
# ================================

def create_advanced_loss_function(
    use_curriculum: bool = True,
    use_triplet: bool = True,
    use_contrastive: bool = True,
    curriculum_warmup_epochs: int = 5
) -> AdvancedLossFunction:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π loss function
    
    Args:
        use_curriculum: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å curriculum learning
        use_triplet: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å triplet loss
        use_contrastive: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å contrastive loss
        curriculum_warmup_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è curriculum warmup
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è AdvancedLossFunction
    """
    config = LossFunctionConfig(
        use_curriculum_loss=use_curriculum,
        use_triplet_loss=use_triplet,
        use_contrastive_loss=use_contrastive,
        curriculum_warmup_epochs=curriculum_warmup_epochs
    )
    
    return AdvancedLossFunction(config)


def compute_loss_with_negatives(
    loss_function: AdvancedLossFunction,
    input_embeddings: torch.Tensor,
    target_embeddings: torch.Tensor,
    output_embeddings: torch.Tensor,
    difficulty_scores: Optional[torch.Tensor] = None,
    negative_sampler: Optional[NegativeSampler] = None
) -> Dict[str, torch.Tensor]:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    
    Returns:
        Dict —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ loss
    """
    negative_embeddings = None
    
    if negative_sampler is not None:
        negative_embeddings = negative_sampler.sample_random_negatives(
            target_embeddings, num_negatives=5
        )
    
    return loss_function(
        input_embeddings=input_embeddings,
        target_embeddings=target_embeddings,
        output_embeddings=output_embeddings,
        difficulty_scores=difficulty_scores,
        negative_embeddings=negative_embeddings
    )


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    print("üöÄ Testing Advanced Loss Functions...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ loss function
    loss_fn = create_advanced_loss_function()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size, embedding_dim = 8, 768
    input_emb = torch.randn(batch_size, embedding_dim)
    target_emb = torch.randn(batch_size, embedding_dim)
    output_emb = torch.randn(batch_size, embedding_dim)
    difficulty = torch.rand(batch_size)  # 0-1 difficulty scores
    
    # Negative sampler
    neg_sampler = NegativeSampler(embedding_dim)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
    losses = compute_loss_with_negatives(
        loss_fn, input_emb, target_emb, output_emb, difficulty, neg_sampler
    )
    
    print(f"üìä Loss Components:")
    for name, loss in losses.items():
        print(f"   {name}: {loss.item():.4f}")
    
    print(f"\nüéØ Current loss weights:")
    weights = loss_fn.get_loss_weights()
    for name, weight in weights.items():
        print(f"   {name}: {weight:.3f}")
    
    print("\n‚úÖ Advanced Loss Functions system ready!") 