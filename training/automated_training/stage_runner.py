"""
Training Stage Runner - –í—ã–ø–æ–ª–Ω–∏—Ç–µ–ª—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Å—Ç–∞–¥–∏–π

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç–∞–¥–∏–π –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–∞–º–∫–∞—Ö
–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞. –£–ø—Ä–∞–≤–ª—è–µ—Ç subprocess-–∞–º–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç
–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
"""

import sys
import time
import logging
import subprocess
import json
import tempfile
from typing import Dict, Any, Optional, List
from datetime import datetime

from .types import StageConfig, StageResult
from .process_runner import run_process
from utils.config_manager.dynamic_config import DynamicConfigManager
from utils.config_manager import get_global_config_manager

logger = logging.getLogger(__name__)


class TrainingStageRunner:
    """–í—ã–ø–æ–ª–Ω–∏—Ç–µ–ª—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Å—Ç–∞–¥–∏–π"""

    def __init__(
        self,
        mode: str = "development",
        scale: Optional[float] = None,
        timeout_multiplier: float = 2.0,
        verbose: bool = False,
    ):
        """
        Args:
            mode: –†–µ–∂–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (development, research, etc.)
            scale: Custom scale factor
            timeout_multiplier: Multiplier for the timeout
            verbose: Enable verbose logging for subprocess operations
        """
        self.mode = mode
        self.scale = scale
        self.timeout_multiplier = timeout_multiplier
        self.verbose = verbose

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if timeout_multiplier > 2.0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            logger.warning(f"[RUNNER] High timeout multiplier: {timeout_multiplier}")
        if verbose:
            logger.info(f"[RUNNER] Verbose mode enabled for subprocess logging")

    def run_stage(
        self, stage_config: StageConfig, estimated_time: float
    ) -> Optional[StageResult]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —Å—Ç–∞–¥–∏—é –æ–±—É—á–µ–Ω–∏—è

        Args:
            stage_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç–∞–¥–∏–∏
            estimated_time: –û—Ü–µ–Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö

        Returns:
            StageResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        output_json_path = None
        temp_config_path = None
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with tempfile.NamedTemporaryFile(
                mode="w", delete=False, suffix=".json", encoding="utf-8"
            ) as tmp_json_file:
                output_json_path = tmp_json_file.name

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            temp_config_path = self._generate_temp_config(stage_config)
            if not temp_config_path:
                logger.error(
                    f"‚ùå Stage {stage_config.stage} failed: Could not generate temp config."
                )
                return None

            # –°—Ç—Ä–æ–∏–º –∫–æ–º–∞–Ω–¥—É —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
            cmd = self._build_command(stage_config, output_json_path, temp_config_path)

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º verbose –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            start_time = time.time()
            timeout_seconds = estimated_time * 60 * self.timeout_multiplier

            if self.verbose:
                logger.info(
                    f"üîÑ Starting Stage {stage_config.stage}: {stage_config.description}"
                )
                logger.info(
                    f"   Dataset: {stage_config.dataset_limit:,} samples, {stage_config.epochs} epochs"
                )
                logger.info(f"   Timeout: {timeout_seconds/60:.1f} minutes")
                logger.info(f"   Temp config: {temp_config_path}")

            result = run_process(
                cmd,
                timeout_seconds,
                process_id=f"Stage {stage_config.stage}",
                verbose=self.verbose,
            )

            end_time = time.time()
            actual_time = (end_time - start_time) / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö

            if result is None:
                logger.error(
                    f"‚ùå Stage {stage_config.stage} failed after {actual_time:.1f}min"
                )
                return None

            return self._process_result(
                result, output_json_path, stage_config, actual_time, estimated_time
            )

        except Exception as e:
            logger.error(f"‚ùå Stage {stage_config.stage} exception: {e}")
            return None
        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            import os

            if output_json_path and os.path.exists(output_json_path):
                os.remove(output_json_path)
            if temp_config_path and os.path.exists(temp_config_path):
                os.remove(temp_config_path)

    def _generate_temp_config(
        self, stage_config: Optional[StageConfig] = None
    ) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–π YAML —Ñ–∞–π–ª —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
        try:
            logger.info(
                f"Generating temporary config for subprocess (mode={self.mode}, scale={self.scale})..."
            )

            # 1. –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
            dynamic_manager = DynamicConfigManager()

            # 2. –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π scale, –ø—Ä–∏–º–µ–Ω—è–µ–º –µ–≥–æ
            if self.scale is not None:
                setattr(dynamic_manager.generator.scale_settings, self.mode, self.scale)
                logger.info(
                    f"Applied custom scale factor: {self.scale} for mode '{self.mode}'"
                )

            # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
            config_data = dynamic_manager.create_config_for_mode(self.mode)

            # === PHASE 4 INTEGRATION: Plasticity & Optimization ===
            if stage_config is not None:
                config_data = self._prepare_config_with_optimizations(
                    config_data, stage_config
                )

            with tempfile.NamedTemporaryFile(
                mode="w", delete=False, suffix=".yaml", encoding="utf-8"
            ) as tmp_file:
                import yaml

                yaml.dump(config_data, tmp_file, allow_unicode=True)
                logger.info(f"Temporary config generated at {tmp_file.name}")
                return tmp_file.name
        except Exception as e:
            logger.error(
                f"Failed to generate temporary config file: {e}", exc_info=True
            )
            return None

    def _prepare_config_with_optimizations(
        self, config: Dict[str, Any], stage_config: StageConfig
    ) -> Dict[str, Any]:
        """
        === PHASE 4 INTEGRATION ===
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –ø–∞–º—è—Ç–∏
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–∫—Ü–∏–π
            dynamic_manager = DynamicConfigManager()
            generator = dynamic_manager.generator

            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∞–¥–∏–∏
            stage_context = {
                "plasticity_profile": stage_config.plasticity_profile,
                "clustering_enabled": stage_config.clustering_enabled,
                "activity_threshold": stage_config.activity_threshold,
                "memory_optimizations": stage_config.memory_optimizations,
                "emergence_tracking": stage_config.emergence_tracking,
                "sparse_connection_ratio": stage_config.sparse_connection_ratio,
                "progressive_scaling": stage_config.progressive_scaling,
                "decoder_monitoring": stage_config.decoder_monitoring,
                "stage_number": stage_config.stage,
            }

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏–∏ –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            plasticity_section = generator.generate_plasticity_section(stage_context)
            optimization_section = generator.generate_optimization_section(
                stage_context
            )

            # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if plasticity_section:
                config["plasticity"] = plasticity_section
                logger.info(
                    f"üß† Applied plasticity profile: {stage_config.plasticity_profile}"
                )

            if optimization_section:
                config["optimization"] = optimization_section
                logger.info(
                    f"üîß Applied memory optimizations: {stage_config.memory_optimizations}"
                )

            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            if stage_config.progressive_scaling:
                adaptive_dims = self._get_adaptive_dimensions(stage_config.stage)
                if adaptive_dims:
                    config["lattice"]["lattice_width"] = adaptive_dims[0]
                    config["lattice"]["lattice_height"] = adaptive_dims[1]
                    config["lattice"]["lattice_depth"] = adaptive_dims[2]
                    logger.info(
                        f"üìê Progressive scaling: {adaptive_dims[0]}√ó{adaptive_dims[1]}√ó{adaptive_dims[2]}"
                    )

            # === PHASE 4 FIX: Explicit GPU device configuration ===
            import torch

            if torch.cuda.is_available():
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ GPU –≤–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
                if "lattice_3d" not in config:
                    config["lattice_3d"] = {}
                config["lattice_3d"]["gpu_enabled"] = True
                config["lattice_3d"]["parallel_processing"] = True

                # –î–æ–±–∞–≤–ª—è–µ–º device –≤ training —Å–µ–∫—Ü–∏—é
                if "training" not in config:
                    config["training"] = {}
                config["training"]["device"] = "cuda"
                config["training"]["pin_memory"] = True

                # GPU optimizations –¥–ª—è memory efficiency
                if stage_config.memory_optimizations:
                    config["training"]["mixed_precision"] = True
                    config["training"]["gradient_checkpointing"] = True

                logger.info(
                    f"üöÄ GPU configuration enabled: {torch.cuda.get_device_name(0)}"
                )
            else:
                logger.warning("‚ö†Ô∏è  CUDA not available - using CPU")
                if "training" not in config:
                    config["training"] = {}
                config["training"]["device"] = "cpu"

            return config

        except Exception as e:
            logger.error(f"Failed to prepare config with optimizations: {e}")
            return config  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ

    def _get_adaptive_dimensions(self, stage: int) -> Optional[tuple]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ —Å—Ç–∞–¥–∏—è–º (TIER 2 scaling)
        SCALING_PROGRESSION = {
            1: (16, 16, 16),  # Baseline testing
            2: (20, 20, 20),  # Small growth
            3: (24, 24, 24),  # Medium scale + clustering
            4: (32, 32, 24),  # Large scale + consolidation
            5: (40, 40, 30),  # Production scale
            6: (48, 48, 36),  # Advanced scale
            7: (64, 64, 48),  # Large production
            8: (80, 80, 60),  # Ultra scale
        }

        return SCALING_PROGRESSION.get(
            stage, SCALING_PROGRESSION[5]
        )  # Default to stage 5

    def _build_command(
        self, config: StageConfig, output_json_path: str, temp_config_path: str
    ) -> List[str]:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        cmd = [
            sys.executable,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π Python –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä
            "smart_resume_training.py",
            "--config-path",  # –ö–ª—é—á–µ–≤–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            temp_config_path,
            "--mode",
            self.mode,
            "--dataset-limit",
            str(config.dataset_limit),
            "--additional-epochs",
            str(config.epochs),
            "--batch-size",
            str(config.batch_size),
            "--output-json-path",  # –ù–æ–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            output_json_path,
        ]

        if self.scale:
            cmd.extend(["--scale", str(self.scale)])

        if self.verbose:
            cmd.append("--verbose")

        return cmd

    def _process_result(
        self,
        process_result: Dict[str, Any],
        output_json_path: str,
        config: StageConfig,
        actual_time: float,
        estimated_time: float,
    ) -> Optional[StageResult]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, —á–∏—Ç–∞—è JSON-—Ñ–∞–π–ª."""

        if process_result["return_code"] != 0:
            logger.error(
                f"‚ùå Stage {config.stage} failed (exit code: {process_result['return_code']})"
            )
            stderr_lines = process_result["stderr"].split("\n")
            if stderr_lines:
                logger.error(f"   Last error: {stderr_lines[-1][:100]}...")

            return StageResult(
                stage=config.stage,
                config=config,
                success=False,
                actual_time_minutes=actual_time,
                estimated_time_minutes=estimated_time,
                error=(
                    process_result["stderr"][-500:]
                    if process_result["stderr"]
                    else "Unknown error"
                ),
                stdout=None,
            )

        # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ - —á–∏—Ç–∞–µ–º JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            with open(output_json_path, "r") as f:
                training_results = json.load(f)
            final_similarity = training_results.get("final_similarity")
        except (FileNotFoundError, json.JSONDecodeError, TypeError) as e:
            logger.error(
                f"Failed to read or parse results from {output_json_path}: {e}"
            )
            final_similarity = None

        if final_similarity or actual_time > estimated_time * 1.5:
            logger.warning(f"‚úÖ Stage {config.stage}: {actual_time:.1f}min")
            if final_similarity:
                logger.warning(f"   Similarity: {final_similarity:.3f}")

        return StageResult(
            stage=config.stage,
            config=config,
            success=True,
            actual_time_minutes=actual_time,
            estimated_time_minutes=estimated_time,
            final_similarity=final_similarity,
            stdout=(
                process_result["stdout"][-1000:] if process_result["stdout"] else None
            ),
        )

    def _extract_similarity_from_output(self, output: str) -> Optional[float]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 'final_similarity' –∏–∑ –≤—ã–≤–æ–¥–∞ —Å–∫—Ä–∏–ø—Ç–∞.
        –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –≤—ã–≤–æ–¥ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞:
        'final_similarity=0.123'
        """
        similarity = None
        try:
            for line in reversed(output.splitlines()):
                if "final_similarity=" in line:
                    similarity_str = line.split("=")[1].strip()
                    similarity = float(similarity_str)
                    break
        except (ValueError, IndexError) as e:
            logger.warning(f"Could not parse similarity from output: {e}")

        return similarity
