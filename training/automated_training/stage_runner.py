"""
Training Stage Runner - –í—ã–ø–æ–ª–Ω–∏—Ç–µ–ª—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Å—Ç–∞–¥–∏–π

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç–∞–¥–∏–π –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–∞–º–∫–∞—Ö
–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞. –£–ø—Ä–∞–≤–ª—è–µ—Ç subprocess-–∞–º–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç
–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
"""

import sys
import time
import logging
import subprocess
import json
import tempfile
from typing import Dict, Any, Optional, List
from datetime import datetime

from .types import StageConfig, StageResult
from .process_runner import run_process
from utils.config_manager.dynamic_config import DynamicConfigManager
from utils.config_manager import get_global_config_manager

logger = logging.getLogger(__name__)


class TrainingStageRunner:
    """–í—ã–ø–æ–ª–Ω–∏—Ç–µ–ª—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Å—Ç–∞–¥–∏–π"""

    def __init__(
        self,
        mode: str = "development",
        scale: Optional[float] = None,
        timeout_multiplier: float = 2.0,
        verbose: bool = False,
    ):
        """
        Args:
            mode: –†–µ–∂–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (development, research, etc.)
            scale: Custom scale factor
            timeout_multiplier: Multiplier for the timeout
            verbose: Enable verbose logging for subprocess operations
        """
        self.mode = mode
        self.scale = scale
        self.timeout_multiplier = timeout_multiplier
        self.verbose = verbose

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if timeout_multiplier > 2.0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            logger.warning(f"[RUNNER] High timeout multiplier: {timeout_multiplier}")
        if verbose:
            logger.info(f"[RUNNER] Verbose mode enabled for subprocess logging")

    def run_stage(
        self, stage_config: StageConfig, estimated_time: float
    ) -> Optional[StageResult]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —Å—Ç–∞–¥–∏—é –æ–±—É—á–µ–Ω–∏—è

        Args:
            stage_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç–∞–¥–∏–∏
            estimated_time: –û—Ü–µ–Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö

        Returns:
            StageResult: –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        output_json_path = None
        temp_config_path = None
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with tempfile.NamedTemporaryFile(
                mode="w", delete=False, suffix=".json", encoding="utf-8"
            ) as tmp_json_file:
                output_json_path = tmp_json_file.name

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            temp_config_path = self._generate_temp_config(stage_config)
            if not temp_config_path:
                logger.error(
                    f"‚ùå Stage {stage_config.stage} failed: Could not generate temp config."
                )
                return None

            # –°—Ç—Ä–æ–∏–º –∫–æ–º–∞–Ω–¥—É —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
            cmd = self._build_command(stage_config, output_json_path, temp_config_path)

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º verbose –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            start_time = time.time()
            timeout_seconds = estimated_time * 60 * self.timeout_multiplier

            if self.verbose:
                logger.info(
                    f"üîÑ Starting Stage {stage_config.stage}: {stage_config.description}"
                )
                logger.info(
                    f"   Dataset: {stage_config.dataset_limit:,} samples, {stage_config.epochs} epochs"
                )
                logger.info(f"   Timeout: {timeout_seconds/60:.1f} minutes")
                logger.info(f"   Temp config: {temp_config_path}")

            result = run_process(
                cmd,
                timeout_seconds,
                process_id=f"Stage {stage_config.stage}",
                verbose=self.verbose,
            )

            end_time = time.time()
            actual_time = (end_time - start_time) / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö

            if result is None:
                logger.error(
                    f"‚ùå Stage {stage_config.stage} failed after {actual_time:.1f}min"
                )
                return None

            return self._process_result(
                result, output_json_path, stage_config, actual_time, estimated_time
            )

        except Exception as e:
            logger.error(f"‚ùå Stage {stage_config.stage} exception: {e}")
            return None
        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            import os

            if output_json_path and os.path.exists(output_json_path):
                os.remove(output_json_path)
            if temp_config_path and os.path.exists(temp_config_path):
                os.remove(temp_config_path)

    def _generate_temp_config(
        self, stage_config: Optional[StageConfig] = None
    ) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–π YAML —Ñ–∞–π–ª —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
        try:
            logger.info(
                f"Generating temporary config for subprocess (mode={self.mode}, scale={self.scale})..."
            )

            # 1. –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
            dynamic_manager = DynamicConfigManager()

            # 2. –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π scale, –ø—Ä–∏–º–µ–Ω—è–µ–º –µ–≥–æ
            if self.scale is not None:
                setattr(dynamic_manager.generator.scale_settings, self.mode, self.scale)
                logger.info(
                    f"Applied custom scale factor: {self.scale} for mode '{self.mode}'"
                )

            # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
            config_data = dynamic_manager.create_config_for_mode(self.mode)

            # === PHASE 4 INTEGRATION: Plasticity & Optimization ===
            if stage_config is not None:
                config_data = self._prepare_config_with_optimizations(
                    config_data, stage_config
                )

            with tempfile.NamedTemporaryFile(
                mode="w", delete=False, suffix=".yaml", encoding="utf-8"
            ) as tmp_file:
                import yaml

                yaml.dump(config_data, tmp_file, allow_unicode=True)
                logger.info(f"Temporary config generated at {tmp_file.name}")
                return tmp_file.name
        except Exception as e:
            logger.error(
                f"Failed to generate temporary config file: {e}", exc_info=True
            )
            return None

    def _prepare_config_with_optimizations(
        self, config_data: Dict[str, Any], stage_config: StageConfig
    ) -> Dict[str, Any]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Phase 4 –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å—Ç–∞–¥–∏–∏ –æ–±—É—á–µ–Ω–∏—è

        Args:
            config_data: –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            stage_config: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç–∞–¥–∏–∏ –æ–±—É—á–µ–Ω–∏—è

        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        # PHASE 4 CRITICAL FIX: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º hybrid –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        self._apply_hybrid_architecture(config_data)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ –ø—Ä–æ—Ñ–∏–ª—é
        self._apply_plasticity_profile(config_data, stage_config)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º progressive scaling
        if getattr(stage_config, "progressive_scaling", False):
            self._apply_progressive_scaling(config_data, stage_config)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º memory optimizations
        if getattr(stage_config, "memory_optimizations", False):
            self._apply_memory_optimizations(config_data, stage_config)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º emergence tracking
        if getattr(stage_config, "emergence_tracking", False):
            self._apply_emergence_tracking(config_data, stage_config)

        return config_data

    def _apply_hybrid_architecture(self, config_data: Dict[str, Any]):
        """
        PHASE 4 CRITICAL: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç hybrid NCA+gMLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        """
        logger.info("üîß PHASE 4: Applying hybrid NCA+gMLP architecture...")

        # 1. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º hybrid mode
        config_data["architecture"] = {
            "hybrid_mode": True,
            "neuron_architecture": "minimal_nca",
            "connection_architecture": "gated_mlp",
            "disable_nca_scaling": True,
        }

        # 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è lattice –¥–ª—è hybrid —Ä–µ–∂–∏–º–∞
        if "lattice_3d" not in config_data:
            config_data["lattice_3d"] = {}

        # –û–±–Ω–æ–≤–ª—è–µ–º lattice –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        config_data["lattice_3d"].update(
            {
                "dimensions": [16, 16, 16],  # PHASE 4: 16√ó16√ó16 –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ä—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                "total_cells": 4096,  # 16√ó16√ó16 = 4096
                "neighbors": 26,  # PHASE 4: 26 neighbors –≤–º–µ—Å—Ç–æ 6
                "neighbor_finding_strategy": "tiered",
            }
        )

        # 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è NCA –Ω–µ–π—Ä–æ–Ω–æ–≤
        config_data["minimal_nca_cell"] = {
            "state_size": 4,
            "neighbor_count": 26,  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å lattice
            "hidden_dim": 3,
            "external_input_size": 1,
            "activation": "tanh",
            "target_params": 362,
            "alpha": 0.1,
            "beta": 0.05,
            "enable_lattice_scaling": False,
        }

        # 4. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è gMLP —Å–≤—è–∑–µ–π
        config_data["gmlp_cell"] = {
            "state_size": 8,
            "neighbor_count": 26,  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å lattice
            "hidden_dim": 16,
            "external_input_size": 4,
            "use_memory": False,  # PHASE 4 FIX: –æ—Ç–∫–ª—é—á–∞–µ–º memory
            "target_params": 2000,
            "activation": "gelu",
        }

        # 5. Cell prototype configuration
        config_data["cell_prototype"] = {
            "prototype_name": "minimal_nca_cell",  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º NCA
            "minimal_nca_cell": config_data["minimal_nca_cell"],
            "gmlp_cell": config_data["gmlp_cell"],
        }

        # 6. –û–±–Ω–æ–≤–ª—è–µ–º fallback –∑–Ω–∞—á–µ–Ω–∏—è
        if "lattice" not in config_data:
            config_data["lattice"] = {}

        config_data["lattice"].update(
            {
                "xs": 16,
                "ys": 16,
                "zs": 16,
                "total_neurons": 4096,
                "connectivity": "26-neighbors",
            }
        )

        logger.info("‚úÖ PHASE 4: Hybrid architecture configuration applied!")
        logger.info(f"   - Architecture: hybrid NCA+gMLP")
        logger.info(f"   - Lattice: 16√ó16√ó16 = 4096 cells")
        logger.info(f"   - Neighbors: 26 (3D Moore)")
        logger.info(
            f"   - NCA params: {config_data['minimal_nca_cell']['target_params']}"
        )
        logger.info(f"   - gMLP params: {config_data['gmlp_cell']['target_params']}")

    def _apply_plasticity_profile(
        self, config_data: Dict[str, Any], stage_config: StageConfig
    ):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ –ø—Ä–æ—Ñ–∏–ª—é
        """
        # –ü–æ–ª—É—á–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–∫—Ü–∏–π
        dynamic_manager = DynamicConfigManager()
        generator = dynamic_manager.generator

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∞–¥–∏–∏
        stage_context = {
            "plasticity_profile": stage_config.plasticity_profile,
            "clustering_enabled": stage_config.clustering_enabled,
            "activity_threshold": stage_config.activity_threshold,
            "memory_optimizations": stage_config.memory_optimizations,
            "emergence_tracking": stage_config.emergence_tracking,
            "sparse_connection_ratio": stage_config.sparse_connection_ratio,
            "progressive_scaling": stage_config.progressive_scaling,
            "decoder_monitoring": stage_config.decoder_monitoring,
            "stage_number": stage_config.stage,
        }

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏–∏ –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        plasticity_section = generator.generate_plasticity_section(stage_context)
        optimization_section = generator.generate_optimization_section(stage_context)

        # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if plasticity_section:
            config_data["plasticity"] = plasticity_section
            logger.info(
                f"üß† Applied plasticity profile: {stage_config.plasticity_profile}"
            )

        if optimization_section:
            config_data["optimization"] = optimization_section
            logger.info(
                f"üîß Applied memory optimizations: {stage_config.memory_optimizations}"
            )

    def _apply_progressive_scaling(
        self, config_data: Dict[str, Any], stage_config: StageConfig
    ):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç progressive scaling
        """
        adaptive_dims = self._get_adaptive_dimensions(stage_config.stage)
        if adaptive_dims:
            config_data["lattice"]["lattice_width"] = adaptive_dims[0]
            config_data["lattice"]["lattice_height"] = adaptive_dims[1]
            config_data["lattice"]["lattice_depth"] = adaptive_dims[2]
            logger.info(
                f"üìê Progressive scaling: {adaptive_dims[0]}√ó{adaptive_dims[1]}√ó{adaptive_dims[2]}"
            )

    def _apply_memory_optimizations(
        self, config_data: Dict[str, Any], stage_config: StageConfig
    ):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç memory optimizations
        """
        # === PHASE 4 FIX: Explicit GPU device configuration ===
        import torch

        if torch.cuda.is_available():
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ GPU –≤–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
            if "lattice_3d" not in config_data:
                config_data["lattice_3d"] = {}
            config_data["lattice_3d"]["gpu_enabled"] = True
            config_data["lattice_3d"]["parallel_processing"] = True

            # –î–æ–±–∞–≤–ª—è–µ–º device –≤ training —Å–µ–∫—Ü–∏—é
            if "training" not in config_data:
                config_data["training"] = {}
            config_data["training"]["device"] = "cuda"
            config_data["training"]["pin_memory"] = True

            # GPU optimizations –¥–ª—è memory efficiency
            if stage_config.memory_optimizations:
                config_data["training"]["mixed_precision"] = True
                config_data["training"]["gradient_checkpointing"] = True

            logger.info(
                f"üöÄ GPU configuration enabled: {torch.cuda.get_device_name(0)}"
            )
        else:
            logger.warning("‚ö†Ô∏è  CUDA not available - using CPU")
            if "training" not in config_data:
                config_data["training"] = {}
            config_data["training"]["device"] = "cpu"

    def _apply_emergence_tracking(
        self, config_data: Dict[str, Any], stage_config: StageConfig
    ):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç emergence tracking
        """
        # === PHASE 4 FIX: Explicit GPU device configuration ===
        import torch

        if torch.cuda.is_available():
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ GPU –≤–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
            if "lattice_3d" not in config_data:
                config_data["lattice_3d"] = {}
            config_data["lattice_3d"]["gpu_enabled"] = True
            config_data["lattice_3d"]["parallel_processing"] = True

            # –î–æ–±–∞–≤–ª—è–µ–º device –≤ training —Å–µ–∫—Ü–∏—é
            if "training" not in config_data:
                config_data["training"] = {}
            config_data["training"]["device"] = "cuda"
            config_data["training"]["pin_memory"] = True

            # GPU optimizations –¥–ª—è memory efficiency
            if stage_config.memory_optimizations:
                config_data["training"]["mixed_precision"] = True
                config_data["training"]["gradient_checkpointing"] = True

            logger.info(
                f"üöÄ GPU configuration enabled: {torch.cuda.get_device_name(0)}"
            )
        else:
            logger.warning("‚ö†Ô∏è  CUDA not available - using CPU")
            if "training" not in config_data:
                config_data["training"] = {}
            config_data["training"]["device"] = "cpu"

    def _get_adaptive_dimensions(self, stage: int) -> Optional[tuple]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ —Å—Ç–∞–¥–∏—è–º (TIER 2 scaling)
        SCALING_PROGRESSION = {
            1: (16, 16, 16),  # Baseline testing
            2: (20, 20, 20),  # Small growth
            3: (24, 24, 24),  # Medium scale + clustering
            4: (32, 32, 24),  # Large scale + consolidation
            5: (40, 40, 30),  # Production scale
            6: (48, 48, 36),  # Advanced scale
            7: (64, 64, 48),  # Large production
            8: (80, 80, 60),  # Ultra scale
        }

        return SCALING_PROGRESSION.get(
            stage, SCALING_PROGRESSION[5]
        )  # Default to stage 5

    def _build_command(
        self, config: StageConfig, output_json_path: str, temp_config_path: str
    ) -> List[str]:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        cmd = [
            sys.executable,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π Python –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä
            "smart_resume_training.py",
            "--config-path",  # –ö–ª—é—á–µ–≤–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            temp_config_path,
            "--mode",
            self.mode,
            "--dataset-limit",
            str(config.dataset_limit),
            "--additional-epochs",
            str(config.epochs),
            "--batch-size",
            str(config.batch_size),
            "--output-json-path",  # –ù–æ–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            output_json_path,
        ]

        # PHASE 4: –£–±–∏—Ä–∞–µ–º scale –ø–∞—Ä–∞–º–µ—Ç—Ä - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        # if self.scale:
        #     cmd.extend(["--scale", str(self.scale)])

        if self.verbose:
            cmd.append("--verbose")

        return cmd

    def _process_result(
        self,
        process_result: Dict[str, Any],
        output_json_path: str,
        config: StageConfig,
        actual_time: float,
        estimated_time: float,
    ) -> Optional[StageResult]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, —á–∏—Ç–∞—è JSON-—Ñ–∞–π–ª."""

        if process_result["return_code"] != 0:
            logger.error(
                f"‚ùå Stage {config.stage} failed (exit code: {process_result['return_code']})"
            )
            stderr_lines = process_result["stderr"].split("\n")
            if stderr_lines:
                logger.error(f"   Last error: {stderr_lines[-1][:100]}...")

            return StageResult(
                stage=config.stage,
                config=config,
                success=False,
                actual_time_minutes=actual_time,
                estimated_time_minutes=estimated_time,
                error=(
                    process_result["stderr"][-500:]
                    if process_result["stderr"]
                    else "Unknown error"
                ),
                stdout=None,
            )

        # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ - —á–∏—Ç–∞–µ–º JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            with open(output_json_path, "r") as f:
                training_results = json.load(f)
            final_similarity = training_results.get("final_similarity")
        except (FileNotFoundError, json.JSONDecodeError, TypeError) as e:
            logger.error(
                f"Failed to read or parse results from {output_json_path}: {e}"
            )
            final_similarity = None

        if final_similarity or actual_time > estimated_time * 1.5:
            logger.warning(f"‚úÖ Stage {config.stage}: {actual_time:.1f}min")
            if final_similarity:
                logger.warning(f"   Similarity: {final_similarity:.3f}")

        return StageResult(
            stage=config.stage,
            config=config,
            success=True,
            actual_time_minutes=actual_time,
            estimated_time_minutes=estimated_time,
            final_similarity=final_similarity,
            stdout=(
                process_result["stdout"][-1000:] if process_result["stdout"] else None
            ),
        )

    def _extract_similarity_from_output(self, output: str) -> Optional[float]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 'final_similarity' –∏–∑ –≤—ã–≤–æ–¥–∞ —Å–∫—Ä–∏–ø—Ç–∞.
        –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –≤—ã–≤–æ–¥ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞:
        'final_similarity=0.123'
        """
        similarity = None
        try:
            for line in reversed(output.splitlines()):
                if "final_similarity=" in line:
                    similarity_str = line.split("=")[1].strip()
                    similarity = float(similarity_str)
                    break
        except (ValueError, IndexError) as e:
            logger.warning(f"Could not parse similarity from output: {e}")

        return similarity
