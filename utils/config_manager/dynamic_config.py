"""
Dynamic Configuration System –¥–ª—è 3D Cellular Neural Network
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤–µ–Ω—Ç—Ä–æ–ª–∞—Ç–µ—Ä–∞–ª—å–Ω–æ–π –ø—Ä–µ—Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω–æ–π –∫–æ—Ä—ã (vlPFC)
"""

import math
import yaml
import torch
from dataclasses import dataclass
from typing import Dict, Any, Optional, Union
from pathlib import Path
import logging
import re

logger = logging.getLogger(__name__)


@dataclass
class BiologicalConstants:
    """–ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã vlPFC"""

    # –ù–µ–π—Ä–æ–∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ vlPFC
    neurons_one_hemisphere: int = 93_750_000
    neurons_both_hemispheres: int = 180_000_000
    target_neurons_average: int = 136_875_000

    # –°–∏–Ω–∞–ø—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    synapses_per_neuron_min: int = 5_000
    synapses_per_neuron_max: int = 15_000
    synapses_per_neuron_avg: int = 10_000

    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
    depth_to_width_ratio: float = 0.5  # depth = 0.5 √ó width

    # –ë–∞–∑–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏ –¥–ª—è 100% –º–∞—Å—à—Ç–∞–±–∞
    base_width: int = 666
    base_height: int = 666

    @property
    def base_depth(self) -> int:
        """–ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –≥–ª—É–±–∏–Ω–∞"""
        return int(self.base_width * self.depth_to_width_ratio)


@dataclass
class ScaleSettings:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤"""

    development: float = 0.01  # 1% - –±—ã—Å—Ç—Ä–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞
    research: float = 0.1  # 10% - –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    validation: float = 0.3  # 30% - –≤–∞–ª–∏–¥–∞—Ü–∏—è
    production: float = 1.0  # 100% - –ø—Ä–æ–¥–∞–∫—à–µ–Ω

    def get_scale(self, mode: str) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–µ–∂–∏–º–∞"""
        return getattr(self, mode, self.development)


class ExpressionEvaluator:
    """–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""

    def __init__(self):
        self.bio_constants = BiologicalConstants()

    def smart_round(self, value: float) -> int:
        """–£–º–Ω–æ–µ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª"""
        return int(round(value))

    def evaluate_expression(self, expr: str, context: Dict[str, Any]) -> Any:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        if (
            not isinstance(expr, str)
            or not expr.startswith("{")
            or not expr.endswith("}")
        ):
            return expr

        # –£–±–∏—Ä–∞–µ–º —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏
        expression = expr[1:-1]

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        eval_context = {
            **context,
            "smart_round": self.smart_round,
            "round": round,
            "int": int,
            "float": float,
            "min": min,
            "max": max,
            "math": math,
        }

        try:
            result = eval(expression, {"__builtins__": {}}, eval_context)
            logger.debug(f"üìê Evaluated '{expr}' = {result}")
            return result
        except Exception as e:
            logger.error(f"[ERROR] Error evaluating expression '{expr}': {e}")
            return expr

    def process_config_dict(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, –≤—ã—á–∏—Å–ª–∏–≤ –≤—Å–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è"""

        def flatten_dict(d, parent_key="", sep="_"):
            """–ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –≤ –ø–ª–æ—Å–∫–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
            items = []
            for k, v in d.items():
                new_key = f"{parent_key}{sep}{k}" if parent_key else k
                if isinstance(v, dict):
                    items.extend(flatten_dict(v, new_key, sep=sep).items())
                else:
                    items.append((new_key, v))
            return dict(items)

        # –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ–º–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        global_context = {}

        def collect_simple_values(data, prefix=""):
            """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –ø—Ä–æ—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
            for key, value in data.items():
                full_key = f"{prefix}_{key}" if prefix else key

                if isinstance(value, dict):
                    collect_simple_values(value, full_key)
                elif isinstance(value, (int, float)) or (
                    isinstance(value, str) and not value.startswith("{")
                ):
                    global_context[key] = value  # –õ–æ–∫–∞–ª—å–Ω–æ–µ –∏–º—è
                    global_context[full_key] = value  # –ü–æ–ª–Ω–æ–µ –∏–º—è

        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        collect_simple_values(config)

        def process_section(data):
            """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–µ–∫—Ü–∏—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
            result = {}

            # –ö–æ–ø–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
            for key, value in data.items():
                if isinstance(value, dict):
                    result[key] = process_section(value)
                elif isinstance(value, (int, float)) or (
                    isinstance(value, str) and not value.startswith("{")
                ):
                    result[key] = value
                    global_context[key] = value  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                else:
                    result[key] = value

            # –í—ã—á–∏—Å–ª—è–µ–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            max_iterations = 15
            for iteration in range(max_iterations):
                changed = False

                for key, value in data.items():
                    if isinstance(value, str) and value.startswith("{"):
                        new_value = self.evaluate_expression(value, global_context)
                        if new_value != result.get(key) and not isinstance(
                            new_value, str
                        ):
                            result[key] = new_value
                            global_context[key] = new_value  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                            changed = True

                if not changed:
                    break

            return result

        return process_section(config)


class DynamicConfigGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""

    def __init__(self):
        self.bio_constants = BiologicalConstants()
        self.scale_settings = ScaleSettings()
        self.evaluator = ExpressionEvaluator()

    def detect_hardware_mode(self) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ GPU –ø–∞–º—è—Ç–∏"""
        try:
            if torch.cuda.is_available():
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (
                    1024**3
                )
                logger.info(f"[PC] Detected GPU memory: {gpu_memory_gb:.1f}GB")

                if gpu_memory_gb >= 20:
                    return "validation"  # RTX 5090+
                elif gpu_memory_gb >= 12:
                    return "research"  # RTX 4070 Ti+
                else:
                    return "development"  # –ú–µ–Ω—å—à–µ 12GB
            else:
                logger.warning("[WARNING] CUDA not available, using development mode")
                return "development"
        except Exception as e:
            logger.warning(
                f"[WARNING] Hardware detection failed: {e}, using development mode"
            )
            return "development"

    def create_base_config_template(self) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏"""
        return {
            "lattice": {
                # –ë–∞–∑–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                "x": self.bio_constants.base_width,
                "y": self.bio_constants.base_height,
                "z": "{smart_round(x*0.5)}",  # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ)
                "scale_factor": 0.1,  # Placeholder
                "xs": "{smart_round(x*scale_factor)}",
                "ys": "{smart_round(y*scale_factor)}",
                "zs": "{smart_round(z*scale_factor)}",
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
                "total_neurons": "{xs * ys * zs}",
                "surface_size": "{xs * ys}",
                "volume": "{xs * ys * zs}",
            },
            "embeddings": {
                "embedding_dim": "{smart_round(xs*ys)}",  # = surface_size
                "teacher_embedding_dim": 768,
            },
            "training": {
                "batch_size": 1024,  # –ë—É–¥–µ—Ç —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ —Ä–µ–∂–∏–º—É
                "learning_rate": 0.001,
                "epochs": 100,
            },
            "gmlp": {
                # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                # –ü—Ä–∏ scale=1.0 ‚Üí ~10,000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–∏ scale=0.06 ‚Üí ~600 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                "target_params": "{smart_round(10000 * scale_factor)}",  # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω–∞–ø—Å–æ–≤
                "neighbor_count": 6,  # –í—Å–µ–≥–¥–∞ 6 –¥–ª—è 3D —Ä–µ—à–µ—Ç–∫–∏
                # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ scale_factor:
                "state_size": "{smart_round(max(8, min(32, target_params ** 0.5 / 3)))}",  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π state
                "hidden_dim": "{smart_round(max(8, min(128, target_params ** 0.5 / 4)))}",  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π hidden
                "external_input_size": "{smart_round(max(4, min(12, target_params ** 0.5 / 8)))}",  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π input
                "memory_dim": "{smart_round(max(4, min(32, target_params ** 0.5 / 6)))}",  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–∞–º—è—Ç—å
                # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                "use_memory": True,
                "activation": "gelu",
                "dropout": 0.1,
            },
            # –ù–û–í–ê–Ø –°–ï–ö–¶–ò–Ø: Minimal NCA Configuration
            "nca": {
                # NCA –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: target 68-10000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                "enabled": True,  # –§–ª–∞–≥ –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è gMLP ‚Üí NCA
                "target_params": "{smart_round(max(68, min(10000, 10000 * scale_factor)))}",  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ gMLP
                "neighbor_count": 6,  # –°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å 3D —Ä–µ—à–µ—Ç–∫–æ–π
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è NCA (—Ñ–æ—Ä–º—É–ª—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è NCA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã):
                "state_size": "{smart_round(max(4, min(20, (target_params / 8) ** 0.6)))}",  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                "hidden_dim": "{smart_round(max(2, min(12, (target_params / 12) ** 0.5)))}",  # –ú–µ–Ω—å—à–µ —á–µ–º gMLP
                "external_input_size": "{smart_round(max(1, min(6, target_params / 50)))}",  # –õ–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                # NCA —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                "activation": "tanh",  # Bounded activation –¥–ª—è stability
                "dropout": 0.0,  # NCA –æ–±—ã—á–Ω–æ –±–µ–∑ dropout
                "use_memory": False,  # NCA –∏–º–µ–µ—Ç implicit memory
                "alpha_init": 0.1,  # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ update strength
                "beta_init": 0.05,  # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ neighbor influence
            },
            # –ù–û–í–ê–Ø –°–ï–ö–¶–ò–Ø: Emergent Training Configuration
            "emergent_training": {
                # Base configuration (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ)
                "teacher_model": "Meta-Llama-3-8B",
                "cube_dimensions": "{[xs, ys, zs]}",  # –°–ø–∏—Å–æ–∫ –∏–∑ lattice –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                # Emergent processing settings (–ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï)
                "enable_full_cube_gradient": True,
                "spatial_propagation_depth": "{zs}",  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏–≤—è–∑–∞–Ω–æ –∫ –≥–ª—É–±–∏–Ω–µ —Ä–µ—à–µ—Ç–∫–∏
                "emergent_specialization": True,
                # gMLP config –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –∏–∑ —Å–µ–∫—Ü–∏–∏ gmlp (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
                "gmlp_config": None,  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –≤ post-processing
                # Multi-objective loss configuration
                "loss_weights": {
                    "surface_reconstruction": 0.3,
                    "internal_consistency": 0.3,
                    "dialogue_similarity": 0.4,
                },
                # Training settings (–ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ–∫—Ü–∏–∏ training)
                "learning_rate": "{learning_rate}",
                "batch_size": "{batch_size}",
                "epochs": "{epochs}",
                "warmup_epochs": 3,
                # Optimization settings
                "gradient_balancing": True,
                "adaptive_loss_weighting": True,
                "gradient_clip_norm": 1.0,
                "weight_decay": 0.01,
            },
        }

    def adjust_config_for_mode(
        self, config: Dict[str, Any], mode: str
    ) -> Dict[str, Any]:
        """–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        scale_factor = self.scale_settings.get_scale(mode)

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º scale_factor
        config["lattice"]["scale_factor"] = scale_factor

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º batch_size –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        if mode == "development":
            config["training"]["batch_size"] = 16
        elif mode == "research":
            config["training"]["batch_size"] = 32
        elif mode == "validation":
            config["training"]["batch_size"] = 64
        elif mode == "production":
            config["training"]["batch_size"] = 128

        logger.info(f"[TARGET] Configured for {mode} mode (scale={scale_factor})")
        return config

    def generate_config(self, mode: str = "auto") -> Dict[str, Any]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ä–µ–∂–∏–º–∞"""

        # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if mode == "auto":
            mode = self.detect_hardware_mode()

        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω
        config = self.create_base_config_template()

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–æ–¥ —Ä–µ–∂–∏–º
        config = self.adjust_config_for_mode(config, mode)

        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        processed_config = self.evaluator.process_config_dict(config)

        # POST-PROCESSING: –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è emergent_training
        if "emergent_training" in processed_config:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            use_nca = processed_config.get("nca", {}).get("enabled", False)

            if use_nca and "nca" in processed_config:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º NCA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
                processed_config["emergent_training"]["cell_architecture"] = "nca"
                processed_config["emergent_training"]["gmlp_config"] = processed_config[
                    "nca"
                ].copy()
                processed_config["emergent_training"]["nca_config"] = processed_config[
                    "nca"
                ].copy()
                logger.info(
                    "[POST-PROCESS] Using NCA architecture for emergent training"
                )
            elif "gmlp" in processed_config:
                # Fallback –Ω–∞ gMLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
                processed_config["emergent_training"]["cell_architecture"] = "gmlp"
                processed_config["emergent_training"]["gmlp_config"] = processed_config[
                    "gmlp"
                ].copy()
                logger.debug(
                    "[POST-PROCESS] Using gMLP architecture for emergent training"
                )
            else:
                logger.warning("[POST-PROCESS] No valid cell architecture found!")

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        processed_config["_metadata"] = {
            "mode": mode,
            "scale_factor": processed_config["lattice"]["scale_factor"],
            "generated_by": "DynamicConfigGenerator",
            "bio_constants_version": "1.0",
        }

        logger.info(f"[OK] Generated config for {mode} mode:")
        logger.info(
            f"   Lattice: {processed_config['lattice']['xs']}x{processed_config['lattice']['ys']}x{processed_config['lattice']['zs']}"
        )

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª
        total_neurons = processed_config["lattice"]["total_neurons"]
        embedding_dim = processed_config["embeddings"]["embedding_dim"]

        if isinstance(total_neurons, (int, float)):
            logger.info(f"   Total neurons: {total_neurons:,}")
        else:
            logger.info(f"   Total neurons: {total_neurons}")

        if isinstance(embedding_dim, (int, float)):
            logger.info(f"   Embedding dim: {embedding_dim:,}")
        else:
            logger.info(f"   Embedding dim: {embedding_dim}")

        return processed_config


class DynamicConfigManager:
    """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""

    def __init__(self, config_dir: Optional[Path] = None):
        self.config_dir = config_dir or Path("config")
        self.generator = DynamicConfigGenerator()

    def create_config_for_mode(self, mode: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        return self.generator.generate_config(mode)

    def save_config(self, config: Dict[str, Any], filename: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Ñ–∞–π–ª"""
        self.config_dir.mkdir(exist_ok=True)
        filepath = self.config_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

        logger.info(f"[SAVE] Saved config to {filepath}")
        return filepath

    def create_and_save_all_modes(self) -> Dict[str, Path]:
        """–°–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤"""
        modes = ["development", "research", "validation", "production"]
        saved_files = {}

        for mode in modes:
            config = self.create_config_for_mode(mode)
            filename = f"dynamic_config_{mode}.yaml"
            filepath = self.save_config(config, filename)
            saved_files[mode] = filepath

        return saved_files


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def generate_config_for_current_hardware() -> Dict[str, Any]:
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∂–µ–ª–µ–∑–∞ (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)"""
    manager = DynamicConfigManager()
    return manager.create_config_for_mode("auto")


def get_recommended_config() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (–∞–ª–∏–∞—Å)"""
    return generate_config_for_current_hardware()


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
    print("[BRAIN] Testing Dynamic Configuration System...")

    manager = DynamicConfigManager()

    # –¢–µ—Å—Ç –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    auto_config = manager.create_config_for_mode("auto")
    print(f"\n[TARGET] Auto-detected mode: {auto_config['_metadata']['mode']}")

    # –¢–µ—Å—Ç –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤
    for mode in ["development", "research", "validation"]:
        config = manager.create_config_for_mode(mode)
        lattice = config["lattice"]
        gmlp = config["gmlp"]
        print(f"\n[DATA] {mode.upper()} mode:")
        print(f"   Lattice: {lattice['xs']}x{lattice['ys']}x{lattice['zs']}")
        print(f"   Neurons: {lattice['total_neurons']:,}")
        print(f"   Batch: {config['training']['batch_size']}")
        print(f"   [BRAIN] gMLP target: {gmlp['target_params']} parameters")
        print(f"   state_size={gmlp['state_size']}, hidden_dim={gmlp['hidden_dim']}")
        print(
            f"   external_input={gmlp['external_input_size']}, memory={gmlp['memory_dim']}"
        )

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å scale=0.06 (–∫–∞–∫ –≤ –∫–æ–º–∞–Ω–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
    print(f"\n[TARGET] SPECIAL TEST: Development mode with scale=0.06:")
    setattr(manager.generator.scale_settings, "development", 0.06)
    config_006 = manager.create_config_for_mode("development")
    lattice_006 = config_006["lattice"]
    gmlp_006 = config_006["gmlp"]
    print(f"   Lattice: {lattice_006['xs']}x{lattice_006['ys']}x{lattice_006['zs']}")
    print(f"   Neurons: {lattice_006['total_neurons']:,}")
    print(f"   [BRAIN] gMLP target: {gmlp_006['target_params']} parameters")
    print(
        f"   state_size={gmlp_006['state_size']}, hidden_dim={gmlp_006['hidden_dim']}"
    )
    print(
        f"   external_input={gmlp_006['external_input_size']}, memory={gmlp_006['memory_dim']}"
    )

    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    state_size = gmlp_006["state_size"]
    hidden_dim = gmlp_006["hidden_dim"]
    ext_input = gmlp_006["external_input_size"]
    memory_dim = gmlp_006["memory_dim"]
    neighbor_count = 6

    # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –¥–ª—è gMLP)
    input_size = neighbor_count * state_size + state_size + ext_input
    approx_params = (
        input_size * hidden_dim + hidden_dim * state_size + memory_dim * hidden_dim
    )
    print(
        f"   [DATA] Estimated gMLP params: ~{approx_params} (target: {gmlp_006['target_params']})"
    )
