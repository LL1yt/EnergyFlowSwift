#!/usr/bin/env python3
"""
Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ DistilBERT Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°.

Ğ­Ñ‚Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ DistilBERT Ñ HuggingFace Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ 
ĞµĞ³Ğ¾ Ğ² Ğ¿Ğ°Ğ¿ĞºĞµ models/local_cache Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°.
"""

import os
import sys
from pathlib import Path

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ñ€ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ² sys.path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def download_distilbert():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ DistilBERT Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾."""
    
    try:
        from transformers import AutoModel, AutoTokenizer
        
        model_name = "distilbert-base-uncased"
        local_path = os.path.join("models", "local_cache", "distilbert-base-uncased")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
        os.makedirs(local_path, exist_ok=True)
        
        print(f"ğŸ“¥ Downloading DistilBERT: {model_name}")
        print(f"ğŸ’¾ Saving to: {local_path}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ¶Ğµ
        if os.path.exists(os.path.join(local_path, "config.json")):
            print("âœ… DistilBERT already downloaded!")
            return local_path
        
        print("ğŸ”„ Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(local_path)
        print("âœ… Tokenizer saved!")
        
        print("ğŸ”„ Downloading model...")
        model = AutoModel.from_pretrained(model_name)
        model.save_pretrained(local_path)
        print("âœ… Model saved!")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğµ
        required_files = ["config.json", "tokenizer.json", "pytorch_model.bin"]
        missing_files = []
        
        for file in required_files:
            if not os.path.exists(os.path.join(local_path, file)):
                missing_files.append(file)
        
        if missing_files:
            print(f"âš ï¸  Warning: Missing files: {missing_files}")
        else:
            print("ğŸ‰ DistilBERT successfully downloaded and cached!")
            print(f"ğŸ“ Location: {os.path.abspath(local_path)}")
        
        return local_path
        
    except ImportError:
        print("âŒ Error: transformers library not installed")
        print("Install with: pip install transformers")
        return None
        
    except Exception as e:
        print(f"âŒ Error downloading DistilBERT: {e}")
        return None

def test_cached_model():
    """Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ."""
    
    try:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ½Ğ°Ñˆ LLMHandler Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        from data.embedding_loader.format_handlers import create_llm_handler
        
        print("\nğŸ§ª Testing cached DistilBERT...")
        
        handler = create_llm_handler("distilbert")
        test_text = "This is a test sentence for DistilBERT."
        
        embedding = handler.generate_embeddings([test_text])
        
        print(f"âœ… Test successful!")
        print(f"ğŸ“Š Embedding shape: {embedding.shape}")
        print(f"ğŸ§  Model info: {handler.get_model_info()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

def get_model_size():
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
    
    local_path = os.path.join("models", "local_cache", "distilbert-base-uncased")
    
    if not os.path.exists(local_path):
        print("ğŸ“ Model not found locally")
        return
    
    total_size = 0
    file_count = 0
    
    for root, dirs, files in os.walk(local_path):
        for file in files:
            file_path = os.path.join(root, file)
            size = os.path.getsize(file_path)
            total_size += size
            file_count += 1
            
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
            if size > 1024 * 1024:  # > 1MB
                print(f"  ğŸ“„ {file}: {size / (1024*1024):.1f} MB")
    
    print(f"ğŸ“Š Total size: {total_size / (1024*1024):.1f} MB ({file_count} files)")

def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°."""
    
    print("ğŸ¯ DistilBERT Local Cache Setup")
    print("=" * 40)
    
    # 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    local_path = download_distilbert()
    
    if local_path:
        # 2. ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€
        print("\nğŸ“Š Model Information:")
        get_model_size()
        
        # 3. Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼
        if test_cached_model():
            print("\nğŸ‰ Setup completed successfully!")
            print("ğŸ’¡ Now you can use DistilBERT without internet connection")
            print("ğŸš€ Run: python real_llama_training_production.py --distilbert")
        else:
            print("\nâš ï¸  Setup completed but test failed")
            print("ğŸ”§ You may need to check your installation")
    else:
        print("\nâŒ Setup failed")
        print("ğŸ”§ Please check your internet connection and try again")

if __name__ == "__main__":
    main() 