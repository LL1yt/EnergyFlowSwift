#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ DistilBERT –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –æ–¥–∏–Ω —Ä–∞–∑ –∑–∞–≥—Ä—É–∂–∞–µ—Ç DistilBERT —Å HuggingFace –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 
–µ–≥–æ –≤ –ø–∞–ø–∫–µ models/local_cache –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def download_distilbert():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DistilBERT –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ."""
    
    try:
        from transformers import AutoModel, AutoTokenizer
        
        model_name = "distilbert-base-uncased"
        local_path = os.path.join("models", "local_cache", "distilbert-base-uncased")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(local_path, exist_ok=True)
        
        print(f"üì• Downloading DistilBERT: {model_name}")
        print(f"[SAVE] Saving to: {local_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ
        if os.path.exists(os.path.join(local_path, "config.json")):
            print("[OK] DistilBERT already downloaded!")
            return local_path
        
        print("[REFRESH] Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(local_path)
        print("[OK] Tokenizer saved!")
        
        print("[REFRESH] Downloading model...")
        model = AutoModel.from_pretrained(model_name)
        model.save_pretrained(local_path)
        print("[OK] Model saved!")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –º–µ—Å—Ç–µ
        required_files = ["config.json", "tokenizer.json", "pytorch_model.bin"]
        missing_files = []
        
        for file in required_files:
            if not os.path.exists(os.path.join(local_path, file)):
                missing_files.append(file)
        
        if missing_files:
            print(f"[WARNING]  Warning: Missing files: {missing_files}")
        else:
            print("[SUCCESS] DistilBERT successfully downloaded and cached!")
            print(f"[FOLDER] Location: {os.path.abspath(local_path)}")
        
        return local_path
        
    except ImportError:
        print("[ERROR] Error: transformers library not installed")
        print("Install with: pip install transformers")
        return None
        
    except Exception as e:
        print(f"[ERROR] Error downloading DistilBERT: {e}")
        return None

def test_cached_model():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å."""
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à LLMHandler –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        from data.embedding_loader.format_handlers import create_llm_handler
        
        print("\n[TEST] Testing cached DistilBERT...")
        
        handler = create_llm_handler("distilbert")
        test_text = "This is a test sentence for DistilBERT."
        
        embedding = handler.generate_embeddings([test_text])
        
        print(f"[OK] Test successful!")
        print(f"[DATA] Embedding shape: {embedding.shape}")
        print(f"[BRAIN] Model info: {handler.get_model_info()}")
        
        return True
        
    except Exception as e:
        print(f"[ERROR] Test failed: {e}")
        return False

def get_model_size():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    
    local_path = os.path.join("models", "local_cache", "distilbert-base-uncased")
    
    if not os.path.exists(local_path):
        print("[FOLDER] Model not found locally")
        return
    
    total_size = 0
    file_count = 0
    
    for root, dirs, files in os.walk(local_path):
        for file in files:
            file_path = os.path.join(root, file)
            size = os.path.getsize(file_path)
            total_size += size
            file_count += 1
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã
            if size > 1024 * 1024:  # > 1MB
                print(f"  [FILE] {file}: {size / (1024*1024):.1f} MB")
    
    print(f"[DATA] Total size: {total_size / (1024*1024):.1f} MB ({file_count} files)")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    
    print("[TARGET] DistilBERT Local Cache Setup")
    print("=" * 40)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    local_path = download_distilbert()
    
    if local_path:
        # 2. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
        print("\n[DATA] Model Information:")
        get_model_size()
        
        # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º
        if test_cached_model():
            print("\n[SUCCESS] Setup completed successfully!")
            print("[IDEA] Now you can use DistilBERT without internet connection")
            print("[START] Run: python real_llama_training_production.py --distilbert")
        else:
            print("\n[WARNING]  Setup completed but test failed")
            print("[CONFIG] You may need to check your installation")
    else:
        print("\n[ERROR] Setup failed")
        print("[CONFIG] Please check your internet connection and try again")

if __name__ == "__main__":
    main() 