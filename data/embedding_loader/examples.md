# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: Embedding Loader Module

**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 5 –∏—é–Ω—è 2025  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é  
**–í–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è:** 1.0.0

---

## üöÄ –ë–ê–ó–û–í–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Word2Vec

```python
from data.embedding_loader import EmbeddingLoader

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
loader = EmbeddingLoader()

# –ó–∞–≥—Ä—É–∑–∫–∞ Word2Vec —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
embeddings = loader.load_embeddings(
    path="./data/embeddings/word2vec.txt",
    format_type="word2vec",
    preprocess=True
)

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
print(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {embeddings.dtype}")
print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {embeddings.device}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç–º–±–µ–¥–∏–Ω–≥–∞—Ö
info = loader.get_embedding_info(embeddings)
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {info}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**

```
–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: torch.Size([400000, 300])
–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: torch.float32
–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu
–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {
    'shape': torch.Size([400000, 300]),
    'memory_mb': 457.76,
    'min_value': -1.2345,
    'max_value': 1.8765,
    'mean_value': 0.0123,
    'std_value': 0.9876
}
```

### –ü—Ä–∏–º–µ—Ä 2: –ó–∞–≥—Ä—É–∑–∫–∞ GloVe —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π

```python
from data.embedding_loader import EmbeddingLoader, EmbeddingPreprocessor

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
loader = EmbeddingLoader(cache_dir="./my_cache/")
preprocessor = EmbeddingPreprocessor()

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
raw_embeddings = loader.load_embeddings(
    path="./data/embeddings/glove.6B.300d.txt",
    format_type="glove",
    preprocess=False  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
)

print(f"–°—ã—Ä—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏: {raw_embeddings.shape}")

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
processed_embeddings = preprocessor.preprocess(
    raw_embeddings,
    normalize=True,      # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    center=True,         # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
    clip_outliers=True,  # –û–±—Ä–µ–∑–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
    outlier_std=2.5      # –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤
)

print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏: {processed_embeddings.shape}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
stats = preprocessor.get_statistics()
print(f"–î–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {stats['original_mean']:.4f}")
print(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {stats['processed_mean']:.4f}")
```

### –ü—Ä–∏–º–µ—Ä 3: –†–∞–±–æ—Ç–∞ —Å BERT —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏

```python
from data.embedding_loader import EmbeddingLoader
import torch

loader = EmbeddingLoader()

# –ó–∞–≥—Ä—É–∑–∫–∞ BERT —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ PyTorch —Ñ–∞–π–ª–∞
bert_embeddings = loader.load_embeddings(
    path="./data/embeddings/bert_embeddings.pt",
    format_type="bert",
    preprocess=True
)

print(f"BERT —ç–º–±–µ–¥–∏–Ω–≥–∏: {bert_embeddings.shape}")
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {bert_embeddings.shape[1]} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 768 –¥–ª—è BERT-base)")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–∏—è
has_nan = torch.isnan(bert_embeddings).any()
print(f"–°–æ–¥–µ—Ä–∂–∏—Ç NaN: {has_nan}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
loader.cache_embeddings(bert_embeddings, "bert_base_processed")

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑
cached = loader.load_from_cache("bert_base_processed")
if cached is not None:
    print("–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞!")
```

---

## üîß –ü–†–û–î–í–ò–ù–£–¢–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Lattice3D

```python
from data.embedding_loader import EmbeddingLoader
from core.lattice_3d import Lattice3D
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
loader = EmbeddingLoader()
embeddings = loader.load_embeddings(
    path="./data/embeddings/word2vec.bin",  # –ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª
    format_type="word2vec",
    preprocess=True
)

# –°–æ–∑–¥–∞–Ω–∏–µ 3D —Ä–µ—à–µ—Ç–∫–∏
lattice = Lattice3D(width=10, height=10, depth=10)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ—Ç–∫–∏
# –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100 –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–π –≥—Ä–∞–Ω–∏
input_data = embeddings[:100]

# –ü–æ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—Ö–æ–¥–Ω—É—é –≥—Ä–∞–Ω—å —Ä–µ—à–µ—Ç–∫–∏
lattice.set_input_face(input_data)
print(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–∞–Ω—ã –Ω–∞ —Ä–µ—à–µ—Ç–∫—É: {input_data.shape}")

# –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
output = lattice.propagate()
print(f"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—à–µ—Ç–∫–∏: {output.shape}")

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
output_info = loader.get_embedding_info(output)
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—ã—Ö–æ–¥–∞: {output_info}")
```

### –ü—Ä–∏–º–µ—Ä 5: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤

```python
from data.embedding_loader import EmbeddingLoader
import torch
from pathlib import Path

def process_large_embeddings(file_path: str, batch_size: int = 10000):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –ø–æ —á–∞—Å—Ç—è–º.
    """
    loader = EmbeddingLoader()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
    embeddings = loader.load_embeddings(file_path, "glove", preprocess=True)

    total_size = embeddings.shape[0]
    print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size} –≤–µ–∫—Ç–æ—Ä–æ–≤")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –±–∞—Ç—á–∞–º
    processed_batches = []

    for i in range(0, total_size, batch_size):
        end_idx = min(i + batch_size, total_size)
        batch = embeddings[i:end_idx]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        batch_processed = loader.preprocessor.standardize_embeddings(batch)
        processed_batches.append(batch_processed)

        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –±–∞—Ç—á {i//batch_size + 1}: {batch.shape}")

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_result = torch.cat(processed_batches, dim=0)
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {final_result.shape}")

    return final_result

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = process_large_embeddings("./data/embeddings/glove.840B.300d.txt")
```

### –ü—Ä–∏–º–µ—Ä 6: –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π

```python
from data.embedding_loader import EmbeddingLoader
import yaml

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
custom_config = {
    'cache': {
        'cache_dir': './custom_cache/',
        'max_cache_size': '4GB'
    },
    'preprocessing': {
        'default': {
            'normalize': False,
            'center': True,
            'clip_outliers': True,
            'outlier_std': 2.0
        }
    }
}

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('./temp_config.yaml', 'w') as f:
    yaml.dump(custom_config, f)

# –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
loader = EmbeddingLoader(
    cache_dir=custom_config['cache']['cache_dir'],
    max_cache_size=custom_config['cache']['max_cache_size']
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
embeddings = loader.load_embeddings(
    path="./data/embeddings/test.txt",
    format_type="glove",
    preprocess=False  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é
)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
processed = loader.preprocess_embeddings(
    embeddings,
    normalize=custom_config['preprocessing']['default']['normalize'],
    center=custom_config['preprocessing']['default']['center']
)

print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {processed.shape}")
```

---

## üß™ –ü–†–ò–ú–ï–†–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø

### –ü—Ä–∏–º–µ—Ä 7: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
import time
from data.embedding_loader import EmbeddingLoader
import torch

def benchmark_loading(file_path: str, format_type: str, num_runs: int = 3):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤.
    """
    loader = EmbeddingLoader()

    times = []

    for run in range(num_runs):
        # –û—á–∏—â–∞–µ–º –∫—ç—à –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
        loader.clear_cache()

        start_time = time.time()
        embeddings = loader.load_embeddings(file_path, format_type, preprocess=True)
        end_time = time.time()

        load_time = end_time - start_time
        times.append(load_time)

        print(f"–ó–∞–ø—É—Å–∫ {run + 1}: {load_time:.2f} —Å–µ–∫, "
              f"–†–∞–∑–º–µ—Ä: {embeddings.shape}, "
              f"–°–∫–æ—Ä–æ—Å—Ç—å: {embeddings.shape[0] / load_time:.0f} vectors/sec")

    avg_time = sum(times) / len(times)
    print(f"\n–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏: {avg_time:.2f} —Å–µ–∫")

    return avg_time

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
benchmark_loading("./data/embeddings/glove.6B.100d.txt", "glove")
```

### –ü—Ä–∏–º–µ—Ä 8: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

```python
from data.embedding_loader import EmbeddingLoader
import time

loader = EmbeddingLoader()

# –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (—Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç)
print("=== –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–±–µ–∑ –∫—ç—à–∞) ===")
start = time.time()
embeddings1 = loader.load_embeddings("./data/embeddings/test.txt", "glove")
first_load_time = time.time() - start
print(f"–í—Ä–µ–º—è –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {first_load_time:.2f} —Å–µ–∫")

# –í—Ç–æ—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–∏–∑ –∫—ç—à–∞)
print("\n=== –í—Ç–æ—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–∏–∑ –∫—ç—à–∞) ===")
start = time.time()
embeddings2 = loader.load_embeddings("./data/embeddings/test.txt", "glove")
second_load_time = time.time() - start
print(f"–í—Ä–µ–º—è –≤—Ç–æ—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {second_load_time:.2f} —Å–µ–∫")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
are_identical = torch.equal(embeddings1, embeddings2)
print(f"–î–∞–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {are_identical}")

# –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –∫—ç—à–∞
speedup = first_load_time / second_load_time
print(f"–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –∫—ç—à–∞: {speedup:.1f}x")
```

---

## üîç –ü–†–ò–ú–ï–†–´ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò

### –ü—Ä–∏–º–µ—Ä 9: –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤

```python
from data.embedding_loader import EmbeddingLoader, EmbeddingPreprocessor
import torch
import numpy as np

def analyze_embedding_quality(embeddings: torch.Tensor):
    """
    –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤.
    """
    print("=== –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –≠–ú–ë–ï–î–ò–ù–ì–û–í ===")

    # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"–†–∞–∑–º–µ—Ä: {embeddings.shape}")
    print(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {embeddings.dtype}")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {embeddings.device}")
    print(f"–ü–∞–º—è—Ç—å: {embeddings.element_size() * embeddings.nelement() / 1024**2:.1f} MB")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–π
    print(f"\n–ú–∏–Ω –∑–Ω–∞—á–µ–Ω–∏–µ: {embeddings.min():.4f}")
    print(f"–ú–∞–∫—Å –∑–Ω–∞—á–µ–Ω–∏–µ: {embeddings.max():.4f}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ: {embeddings.mean():.4f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {embeddings.std():.4f}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã
    nan_count = torch.isnan(embeddings).sum().item()
    inf_count = torch.isinf(embeddings).sum().item()
    zero_vectors = (embeddings.norm(dim=1) == 0).sum().item()

    print(f"\n–ü—Ä–æ–±–ª–µ–º—ã:")
    print(f"NaN –∑–Ω–∞—á–µ–Ω–∏–π: {nan_count}")
    print(f"Inf –∑–Ω–∞—á–µ–Ω–∏–π: {inf_count}")
    print(f"–ù—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {zero_vectors}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–æ—Ä–º
    norms = torch.norm(embeddings, dim=1)
    print(f"\n–ù–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–æ–≤:")
    print(f"–°—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞: {norms.mean():.4f}")
    print(f"–ú–∏–Ω –Ω–æ—Ä–º–∞: {norms.min():.4f}")
    print(f"–ú–∞–∫—Å –Ω–æ—Ä–º–∞: {norms.max():.4f}")

    return {
        'shape': embeddings.shape,
        'has_issues': nan_count > 0 or inf_count > 0 or zero_vectors > 0,
        'mean_norm': float(norms.mean()),
        'std_norm': float(norms.std())
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
loader = EmbeddingLoader()
embeddings = loader.load_embeddings("./data/embeddings/test.txt", "glove")
quality_report = analyze_embedding_quality(embeddings)

if quality_report['has_issues']:
    print("\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏!")
else:
    print("\n‚úÖ –≠–º–±–µ–¥–∏–Ω–≥–∏ –≤—ã–≥–ª—è–¥—è—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏")
```

### –ü—Ä–∏–º–µ—Ä 10: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤

```python
from data.embedding_loader import EmbeddingLoader
import time

def compare_formats():
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤.
    """
    loader = EmbeddingLoader()

    files_to_test = [
        ("./data/embeddings/word2vec.txt", "word2vec"),
        ("./data/embeddings/word2vec.bin", "word2vec"),
        ("./data/embeddings/glove.txt", "glove"),
        ("./data/embeddings/bert.pt", "bert")
    ]

    results = []

    for file_path, format_type in files_to_test:
        try:
            print(f"\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {format_type}: {file_path} ===")

            start_time = time.time()
            embeddings = loader.load_embeddings(file_path, format_type)
            load_time = time.time() - start_time

            info = loader.get_embedding_info(embeddings)

            result = {
                'format': format_type,
                'file': file_path,
                'load_time': load_time,
                'shape': embeddings.shape,
                'memory_mb': info['memory_mb'],
                'speed_vectors_per_sec': embeddings.shape[0] / load_time
            }

            results.append(result)

            print(f"–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.2f} —Å–µ–∫")
            print(f"–†–∞–∑–º–µ—Ä: {embeddings.shape}")
            print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {result['speed_vectors_per_sec']:.0f} vectors/sec")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print("\n" + "="*80)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê")
    print("="*80)
    print(f"{'–§–æ—Ä–º–∞—Ç':<12} {'–í—Ä–µ–º—è (—Å–µ–∫)':<12} {'–†–∞–∑–º–µ—Ä':<20} {'–°–∫–æ—Ä–æ—Å—Ç—å (v/s)':<15}")
    print("-" * 80)

    for result in results:
        print(f"{result['format']:<12} {result['load_time']:<12.2f} "
              f"{str(result['shape']):<20} {result['speed_vectors_per_sec']:<15.0f}")

# –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
compare_formats()
```

---

## üìù –ó–ê–ú–ï–¢–ö–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ

### –í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:

1. **–ü–∞–º—è—Ç—å**: –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏
2. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
3. **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞**: –í—Å–µ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è–π—Ç–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
4. **–§–æ—Ä–º–∞—Ç—ã**: –ë–∏–Ω–∞—Ä–Ω—ã–µ Word2Vec —Ñ–∞–π–ª—ã —Ç—Ä–µ–±—É—é—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π gensim
5. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ batch –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π workflow:

```python
# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –∫—ç—à–µ–º
loader = EmbeddingLoader(cache_dir="./cache/")

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
embeddings = loader.load_embeddings(path, format_type, preprocess=True)

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
info = loader.get_embedding_info(embeddings)
print(f"–ö–∞—á–µ—Å—Ç–≤–æ: {info}")

# 4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ—à–µ—Ç–∫–æ–π
lattice.set_input_face(embeddings[:batch_size])

# 5. –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
loader.clear_cache()
```
