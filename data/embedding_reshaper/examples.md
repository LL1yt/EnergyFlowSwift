# EmbeddingReshaper Usage Examples

**–ú–æ–¥—É–ª—å:** data/embedding_reshaper  
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 6 –∏—é–Ω—è 2025  
**–í–µ—Ä—Å–∏—è:** 1.0.0

---

## üöÄ –ë–´–°–¢–†–´–ô –°–¢–ê–†–¢

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from data.embedding_reshaper import EmbeddingReshaper
import numpy as np

# –°–æ–∑–¥–∞–Ω–∏–µ reshaper —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
reshaper = EmbeddingReshaper()

# –ü—Ä–∏–º–µ—Ä 1D —ç–º–±–µ–¥–∏–Ω–≥–∞ (768 –∏–∑–º–µ—Ä–µ–Ω–∏–π)
embedding_1d = np.random.randn(768).astype(np.float32)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ 1D ‚Üí 3D
matrix_3d = reshaper.vector_to_matrix(embedding_1d)
print(f"–§–æ—Ä–º–∞ 3D –º–∞—Ç—Ä–∏—Ü—ã: {matrix_3d.shape}")  # (8, 8, 12)

# –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ 3D ‚Üí 1D
restored_1d = reshaper.matrix_to_vector(matrix_3d)
print(f"–§–æ—Ä–º–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞: {restored_1d.shape}")  # (768,)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
similarity = np.dot(embedding_1d, restored_1d) / (
    np.linalg.norm(embedding_1d) * np.linalg.norm(restored_1d)
)
print(f"Cosine similarity: {similarity:.6f}")  # –û–∂–∏–¥–∞–µ—Ç—Å—è ~1.0
```

---

## üéØ –ü–†–û–î–í–ò–ù–£–¢–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π

```python
from data.embedding_reshaper import EmbeddingReshaper
import torch

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —ç–º–±–µ–¥–∏–Ω–≥–∞
text_embedding = torch.randn(768, dtype=torch.float32)

# Linear —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (—Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è)
linear_reshaper = EmbeddingReshaper(
    reshaping_method="linear",
    preserve_semantics=True
)

# Adaptive —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (–±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞)
adaptive_reshaper = EmbeddingReshaper(
    reshaping_method="adaptive",
    preserve_semantics=True,
    semantic_threshold=0.98
)

# Semantic —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
semantic_reshaper = EmbeddingReshaper(
    reshaping_method="semantic",
    preserve_semantics=True,
    semantic_threshold=0.99
)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
for name, reshaper in [("Linear", linear_reshaper),
                      ("Adaptive", adaptive_reshaper),
                      ("Semantic", semantic_reshaper)]:

    matrix = reshaper.vector_to_matrix(text_embedding)
    restored = reshaper.matrix_to_vector(matrix)

    similarity = torch.cosine_similarity(
        text_embedding.unsqueeze(0),
        restored.unsqueeze(0)
    ).item()

    print(f"{name:10}: Similarity = {similarity:.6f}")
```

### –ü—Ä–∏–º–µ—Ä 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Teacher LLM Encoder

```python
from data.embedding_loader import EmbeddingLoader
from data.embedding_reshaper import EmbeddingReshaper

# –ü–æ–ª–Ω—ã–π pipeline: —Ç–µ–∫—Å—Ç ‚Üí —ç–º–±–µ–¥–∏–Ω–≥ ‚Üí 3D –∫—É–±
class TextToCubeProcessor:
    def __init__(self):
        self.encoder = EmbeddingLoader()
        self.reshaper = EmbeddingReshaper(
            reshaping_method="adaptive",
            preserve_semantics=True
        )

    def process_text(self, text_input):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ 3D —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∫—É–±–∞"""
        # –®–∞–≥ 1: –¢–µ–∫—Å—Ç ‚Üí –≠–º–±–µ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ Teacher LLM
        embeddings = self.encoder.load_from_llm(
            [text_input],
            model_key="distilbert"
        )
        text_embedding = embeddings[0]

        # –®–∞–≥ 2: 1D –≠–º–±–µ–¥–∏–Ω–≥ ‚Üí 3D –ö—É–±
        cube_input = self.reshaper.vector_to_matrix(text_embedding)

        return cube_input, text_embedding

    def restore_text_format(self, cube_output):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞ –∫—É–±–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ —ç–º–±–µ–¥–∏–Ω–≥ —Ñ–æ—Ä–º–∞—Ç"""
        return self.reshaper.matrix_to_vector(cube_output)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
processor = TextToCubeProcessor()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
input_text = "–°–æ–∑–¥–∞–Ω–∏–µ 3D –∫–ª–µ—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"
cube_input, original_embedding = processor.process_text(input_text)

print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥: {original_embedding.shape}")
print(f"3D –∫—É–± –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {cube_input.shape}")

# –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫—É–±–æ–º (–∑–¥–µ—Å—å –∫—É–± –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç –≤—Ö–æ–¥)
cube_output = cube_input.copy()

# –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç —ç–º–±–µ–¥–∏–Ω–≥–∞
final_embedding = processor.restore_text_format(cube_output)
print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥: {final_embedding.shape}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–ª–Ω–æ–≥–æ pipeline
similarity = np.dot(original_embedding, final_embedding) / (
    np.linalg.norm(original_embedding) * np.linalg.norm(final_embedding)
)
print(f"End-to-end similarity: {similarity:.6f}")
```

### –ü—Ä–∏–º–µ—Ä 3: Batch processing

```python
import time
from data.embedding_reshaper import EmbeddingReshaper, create_test_embeddings

# –°–æ–∑–¥–∞–Ω–∏–µ batch —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
batch_size = 50
test_embeddings = create_test_embeddings(
    count=batch_size,
    dim=768,
    embedding_type="diverse"
)

reshaper = EmbeddingReshaper(reshaping_method="adaptive")

# –ú–µ—Ç–æ–¥ 1: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
start_time = time.time()
sequential_results = []
for embedding in test_embeddings:
    matrix = reshaper.vector_to_matrix(embedding)
    restored = reshaper.matrix_to_vector(matrix)
    sequential_results.append(restored)
sequential_time = time.time() - start_time

# –ú–µ—Ç–æ–¥ 2: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏)
start_time = time.time()
# –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: batch processing –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±—É–¥—É—â–∏—Ö –≤–µ—Ä—Å–∏—è—Ö
batch_results = [
    reshaper.matrix_to_vector(reshaper.vector_to_matrix(emb))
    for emb in test_embeddings
]
batch_time = time.time() - start_time

print(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {batch_size} —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {sequential_time:.3f}s")
print(f"Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ {batch_size} —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {batch_time:.3f}s")
print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–º–±–µ–¥–∏–Ω–≥: {sequential_time/batch_size*1000:.1f}ms")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
similarities = []
for orig, restored in zip(test_embeddings, sequential_results):
    sim = np.dot(orig, restored) / (np.linalg.norm(orig) * np.linalg.norm(restored))
    similarities.append(sim)

print(f"–°—Ä–µ–¥–Ω—è—è similarity: {np.mean(similarities):.6f}")
print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è similarity: {np.min(similarities):.6f}")
```

---

## üîß –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 4: –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
from data.embedding_reshaper import EmbeddingReshaper

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (1536D)
large_reshaper = EmbeddingReshaper(
    input_dim=1536,
    cube_shape=(8, 12, 16),  # 8*12*16 = 1536
    reshaping_method="semantic",
    preserve_semantics=True,
    semantic_threshold=0.98
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (384D)
small_reshaper = EmbeddingReshaper(
    input_dim=384,
    cube_shape=(8, 8, 6),    # 8*8*6 = 384
    reshaping_method="linear",
    preserve_semantics=False  # –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –≤—ã—Å–æ—á–∞–π—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
quality_reshaper = EmbeddingReshaper(
    input_dim=768,
    cube_shape=(8, 8, 12),
    reshaping_method="semantic",
    preserve_semantics=True,
    semantic_threshold=0.995  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥
)

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
test_embedding = np.random.randn(768).astype(np.float32)

for name, reshaper in [("Quality", quality_reshaper),
                      ("Standard", EmbeddingReshaper())]:
    if reshaper.input_dim == test_embedding.shape[0]:
        matrix = reshaper.vector_to_matrix(test_embedding)
        restored = reshaper.matrix_to_vector(matrix)

        similarity = np.dot(test_embedding, restored) / (
            np.linalg.norm(test_embedding) * np.linalg.norm(restored)
        )

        print(f"{name:10}: Similarity = {similarity:.6f}")
```

### –ü—Ä–∏–º–µ—Ä 5: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```python
from data.embedding_reshaper import EmbeddingReshaper

reshaper = EmbeddingReshaper(
    reshaping_method="adaptive",
    preserve_semantics=True
)

# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
test_embeddings = create_test_embeddings(count=10, dim=768)

for i, embedding in enumerate(test_embeddings):
    matrix = reshaper.vector_to_matrix(embedding)
    restored = reshaper.matrix_to_vector(matrix)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω —ç–º–±–µ–¥–∏–Ω–≥ #{i+1}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
stats = reshaper.get_statistics()

print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
print(f"–í—Å–µ–≥–æ –æ–ø–µ—Ä–∞—Ü–∏–π vector_to_matrix: {stats['vector_to_matrix_calls']}")
print(f"–í—Å–µ–≥–æ –æ–ø–µ—Ä–∞—Ü–∏–π matrix_to_vector: {stats['matrix_to_vector_calls']}")
print(f"–°—Ä–µ–¥–Ω—è—è semantic similarity: {stats['avg_semantic_similarity']:.6f}")
print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è vector_to_matrix: {stats['avg_vector_to_matrix_time']:.3f}ms")
print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è matrix_to_vector: {stats['avg_matrix_to_vector_time']:.3f}ms")

# –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
reshaper.reset_statistics()
print("\n‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–±—Ä–æ—à–µ–Ω–∞")
```

---

## ‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 6: –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
from data.embedding_reshaper import benchmark_transformation_speed
from data.embedding_reshaper.utils import create_test_embeddings

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
embeddings_100 = create_test_embeddings(count=100, dim=768, embedding_type="diverse")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
strategies = ["linear", "adaptive", "semantic"]
results = {}

for strategy in strategies:
    reshaper = EmbeddingReshaper(reshaping_method=strategy)

    # –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    benchmark_results = benchmark_transformation_speed(
        reshaper=reshaper,
        test_embeddings=embeddings_100,
        num_iterations=5
    )

    results[strategy] = benchmark_results

    print(f"\nüìä {strategy.capitalize()} Strategy:")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è vector_to_matrix: {benchmark_results['avg_vector_to_matrix_time']:.3f}ms")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è matrix_to_vector: {benchmark_results['avg_matrix_to_vector_time']:.3f}ms")
    print(f"  –û–±—â–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ü–∏–∫–ª: {benchmark_results['avg_full_cycle_time']:.3f}ms")
    print(f"  –û–ø–µ—Ä–∞—Ü–∏–π –≤ —Å–µ–∫—É–Ω–¥—É: {benchmark_results['operations_per_second']:.1f}")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
print("\nüèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
for strategy, result in results.items():
    ops_per_sec = result['operations_per_second']
    print(f"{strategy:10}: {ops_per_sec:6.1f} ops/sec")
```

### –ü—Ä–∏–º–µ—Ä 7: –†–µ–∞–ª—å–Ω—ã–π use case - –¥–∏–∞–ª–æ–≥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞

```python
class DialogueEmbeddingProcessor:
    """–ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –¥–∏–∞–ª–æ–≥–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É"""

    def __init__(self):
        self.encoder = EmbeddingLoader()
        self.reshaper = EmbeddingReshaper(
            reshaping_method="adaptive",
            preserve_semantics=True,
            semantic_threshold=0.98
        )
        self.conversation_history = []

    def process_user_input(self, user_message):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —ç–º–±–µ–¥–∏–Ω–≥
        embeddings = self.encoder.load_from_llm(
            [user_message],
            model_key="distilbert"
        )
        user_embedding = embeddings[0]

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è 3D –∫—É–±–∞
        cube_input = self.reshaper.vector_to_matrix(user_embedding)

        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫—É–±–æ–º (—Å–∏–º—É–ª—è—Ü–∏—è)
        # cube_output = self.neural_cube.process(cube_input)
        cube_output = cube_input  # –ó–∞–≥–ª—É—à–∫–∞

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ —ç–º–±–µ–¥–∏–Ω–≥
        response_embedding = self.reshaper.matrix_to_vector(cube_output)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({
            'user_message': user_message,
            'user_embedding': user_embedding,
            'cube_input': cube_input,
            'cube_output': cube_output,
            'response_embedding': response_embedding
        })

        return response_embedding

    def get_conversation_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ –¥–∏–∞–ª–æ–≥–µ"""
        if not self.conversation_history:
            return {}

        similarities = []
        for turn in self.conversation_history:
            sim = np.dot(turn['user_embedding'], turn['response_embedding']) / (
                np.linalg.norm(turn['user_embedding']) *
                np.linalg.norm(turn['response_embedding'])
            )
            similarities.append(sim)

        return {
            'turns_count': len(self.conversation_history),
            'avg_similarity': np.mean(similarities),
            'min_similarity': np.min(similarities),
            'max_similarity': np.max(similarities)
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
dialogue_processor = DialogueEmbeddingProcessor()

# –°–∏–º—É–ª—è—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞
user_inputs = [
    "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
    "–†–∞—Å—Å–∫–∞–∂–∏ –æ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö",
    "–ß—Ç–æ —Ç–∞–∫–æ–µ 3D –∫–ª–µ—Ç–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞?",
    "–°–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!"
]

for user_input in user_inputs:
    response_emb = dialogue_processor.process_user_input(user_input)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: '{user_input}' ‚Üí —ç–º–±–µ–¥–∏–Ω–≥ {response_emb.shape}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞
stats = dialogue_processor.get_conversation_stats()
print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞:")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫: {stats['turns_count']}")
print(f"–°—Ä–µ–¥–Ω—è—è similarity: {stats['avg_similarity']:.6f}")
print(f"–î–∏–∞–ø–∞–∑–æ–Ω similarity: {stats['min_similarity']:.6f} - {stats['max_similarity']:.6f}")
```

---

## üß™ –¢–ï–°–¢–û–í–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 8: –Æ–Ω–∏—Ç-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import unittest
from data.embedding_reshaper import EmbeddingReshaper, create_test_embeddings

class TestEmbeddingReshaper(unittest.TestCase):

    def setUp(self):
        self.reshaper = EmbeddingReshaper()
        self.test_embedding = create_test_embeddings(count=1, dim=768)[0]

    def test_shape_consistency(self):
        """–¢–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π"""
        matrix = self.reshaper.vector_to_matrix(self.test_embedding)
        self.assertEqual(matrix.shape, (8, 8, 12))

        restored = self.reshaper.matrix_to_vector(matrix)
        self.assertEqual(restored.shape, (768,))

    def test_semantic_preservation(self):
        """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏"""
        matrix = self.reshaper.vector_to_matrix(self.test_embedding)
        restored = self.reshaper.matrix_to_vector(matrix)

        similarity = np.dot(self.test_embedding, restored) / (
            np.linalg.norm(self.test_embedding) * np.linalg.norm(restored)
        )

        self.assertGreater(similarity, 0.95, "Semantic similarity too low")

    def test_multiple_strategies(self):
        """–¢–µ—Å—Ç –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        strategies = ["linear", "adaptive", "semantic"]

        for strategy in strategies:
            with self.subTest(strategy=strategy):
                reshaper = EmbeddingReshaper(reshaping_method=strategy)
                matrix = reshaper.vector_to_matrix(self.test_embedding)
                restored = reshaper.matrix_to_vector(matrix)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                self.assertEqual(restored.shape, (768,))

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                similarity = np.dot(self.test_embedding, restored) / (
                    np.linalg.norm(self.test_embedding) * np.linalg.norm(restored)
                )
                self.assertGreater(similarity, 0.90)

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
if __name__ == '__main__':
    unittest.main()
```

---

## üìö –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –†–ï–°–£–†–°–´

### –ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

```python
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
from data.embedding_reshaper.utils import (
    validate_dimensions,
    calculate_similarity_metrics,
    create_test_embeddings
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
sparse_embeddings = create_test_embeddings(
    count=5, dim=768, embedding_type="sparse"
)

dense_embeddings = create_test_embeddings(
    count=5, dim=768, embedding_type="dense"
)

# –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
is_valid = validate_dimensions(
    input_dim=768,
    cube_shape=(8, 8, 12)
)
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã: {is_valid}")

# –†–∞—Å—á–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–∂–µ—Å—Ç–∏
embedding1 = sparse_embeddings[0]
embedding2 = dense_embeddings[0]

similarity_score = calculate_similarity_metrics(embedding1, embedding2)
print(f"Similarity score: {similarity_score:.6f}")
```

---

**üìñ –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –ø–æ–∫—Ä—ã–≤–∞—é—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è EmbeddingReshaper –º–æ–¥—É–ª—è –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.**

**‚úÖ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ production.**
