"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è EmbeddingReshaper
============================================

–°–æ–¥–µ—Ä–∂–∏—Ç —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞, —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π.
PHASE 2.3 –î–µ–Ω—å 3-4: –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è >98%
"""

import torch
import numpy as np
from typing import Union, Tuple, Dict, Any, List
import logging
from sklearn.metrics.pairwise import cosine_similarity 
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from scipy.spatial.distance import euclidean, cityblock
from scipy.stats import pearsonr, spearmanr
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è sklearn –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞
warnings.filterwarnings("ignore", category=UserWarning)


def validate_semantic_preservation(
    original: Union[torch.Tensor, np.ndarray],
    transformed: Union[torch.Tensor, np.ndarray],
    threshold: float = 0.95
) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    
    Args:
        original: –ò—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥
        transformed: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ cosine similarity (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.95)
        
    Returns:
        True –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
        
    Raises:
        ValueError: –ü—Ä–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö
    """
    try:
        similarity = calculate_similarity_metrics(original, transformed)
        return similarity >= threshold
    except Exception as e:
        logging.getLogger(__name__).error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        return False


def calculate_similarity_metrics(
    vec1: Union[torch.Tensor, np.ndarray],
    vec2: Union[torch.Tensor, np.ndarray]
) -> float:
    """
    –†–∞—Å—á–µ—Ç cosine similarity –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏.
    
    Args:
        vec1: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
        vec2: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä
        
    Returns:
        Cosine similarity –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
        
    Raises:
        ValueError: –ü—Ä–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö –∏–ª–∏ —Ç–∏–ø–∞—Ö
    """
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º—É —Ç–∏–ø—É –∏ —Ñ–æ—Ä–º–µ
    if isinstance(vec1, torch.Tensor):
        vec1_np = vec1.detach().cpu().numpy()
    else:
        vec1_np = vec1
    
    if isinstance(vec2, torch.Tensor):
        vec2_np = vec2.detach().cpu().numpy()
    else:
        vec2_np = vec2
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ 1D –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    vec1_flat = vec1_np.flatten()
    vec2_flat = vec2_np.flatten()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    if vec1_flat.shape != vec2_flat.shape:
        raise ValueError(
            f"–ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {vec1_flat.shape} vs {vec2_flat.shape}"
        )
    
    # –í—ã—á–∏—Å–ª—è–µ–º cosine similarity
    vec1_reshaped = vec1_flat.reshape(1, -1)
    vec2_reshaped = vec2_flat.reshape(1, -1)
    
    similarity = cosine_similarity(vec1_reshaped, vec2_reshaped)[0][0]
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1] (cosine –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç -1 –¥–æ 1)
    similarity_normalized = (similarity + 1) / 2
    
    return float(similarity_normalized)


# ==========================================
# üöÄ –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –°–û–•–†–ê–ù–ï–ù–ò–Ø >98%
# ==========================================

def calculate_enhanced_similarity_metrics(
    vec1: Union[torch.Tensor, np.ndarray],
    vec2: Union[torch.Tensor, np.ndarray],
    metrics_config: Dict[str, float] = None
) -> Dict[str, float]:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è >98% —Ç–æ—á–Ω–æ—Å—Ç–∏.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ–º:
    - Cosine similarity (–æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
    - Pearson correlation (–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
    - Spearman correlation (–º–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
    - Structural similarity (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
    - Magnitude preservation (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º)
    
    Args:
        vec1: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
        vec2: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä  
        metrics_config: –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –∏—Ç–æ–≥–æ–≤—ã–º weighted score
    """
    if metrics_config is None:
        metrics_config = {
            'cosine_weight': 0.4,        # –ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
            'pearson_weight': 0.2,       # –õ–∏–Ω–µ–π–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
            'spearman_weight': 0.15,     # –†–∞–Ω–≥–æ–≤–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è  
            'structural_weight': 0.15,   # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            'magnitude_weight': 0.1      # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º
        }
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º—É —Ç–∏–ø—É –∏ —Ñ–æ—Ä–º–µ
    if isinstance(vec1, torch.Tensor):
        vec1_np = vec1.detach().cpu().numpy()
    else:
        vec1_np = vec1
    
    if isinstance(vec2, torch.Tensor):
        vec2_np = vec2.detach().cpu().numpy()
    else:
        vec2_np = vec2
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ 1D –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    vec1_flat = vec1_np.flatten()
    vec2_flat = vec2_np.flatten()
    
    if vec1_flat.shape != vec2_flat.shape:
        raise ValueError(f"–ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {vec1_flat.shape} vs {vec2_flat.shape}")
    
    metrics = {}
    
    # 1. Cosine Similarity (–æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
    vec1_reshaped = vec1_flat.reshape(1, -1)
    vec2_reshaped = vec2_flat.reshape(1, -1)
    cosine_sim = cosine_similarity(vec1_reshaped, vec2_reshaped)[0][0]
    metrics['cosine_similarity'] = float((cosine_sim + 1) / 2)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0,1]
    
    # 2. Pearson Correlation (–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å) 
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (std = 0)
        if np.std(vec1_flat) == 0 or np.std(vec2_flat) == 0:
            # –ï—Å–ª–∏ –æ–±–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∏ —Ä–∞–≤–Ω—ã–µ
            if np.allclose(vec1_flat, vec2_flat):
                metrics['pearson_correlation'] = 1.0
            else:
                metrics['pearson_correlation'] = 0.0
        else:
            with np.errstate(divide='ignore', invalid='ignore'):
                pearson_corr, _ = pearsonr(vec1_flat, vec2_flat)
            metrics['pearson_correlation'] = float((pearson_corr + 1) / 2) if not np.isnan(pearson_corr) else 0.5
    except:
        metrics['pearson_correlation'] = 0.5
    
    # 3. Spearman Correlation (–º–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
    try:
        spearman_corr, _ = spearmanr(vec1_flat, vec2_flat)
        metrics['spearman_correlation'] = float((spearman_corr + 1) / 2) if not np.isnan(spearman_corr) else 0.5
    except:
        metrics['spearman_correlation'] = 0.5
    
    # 4. Structural Similarity (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π)
    structural_sim = _calculate_structural_similarity(vec1_flat, vec2_flat)
    metrics['structural_similarity'] = structural_sim
    
    # 5. Magnitude Preservation (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º)
    norm1 = np.linalg.norm(vec1_flat)
    norm2 = np.linalg.norm(vec2_flat)
    magnitude_sim = 1.0 - abs(norm1 - norm2) / max(norm1, norm2, 1e-8)
    metrics['magnitude_preservation'] = float(max(0.0, magnitude_sim))
    
    # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    weighted_score = (
        metrics['cosine_similarity'] * metrics_config['cosine_weight'] +
        metrics['pearson_correlation'] * metrics_config['pearson_weight'] +
        metrics['spearman_correlation'] * metrics_config['spearman_weight'] +
        metrics['structural_similarity'] * metrics_config['structural_weight'] +
        metrics['magnitude_preservation'] * metrics_config['magnitude_weight']
    )
    
    metrics['weighted_similarity'] = float(weighted_score)
    
    return metrics


def _calculate_structural_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    –†–∞—Å—á–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ - –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–Ω–≥–æ–≤—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø–æ–∑–∏—Ü–∏–π —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–Ω–≥–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏)
        ranks1 = vec1.argsort().argsort()
        ranks2 = vec2.argsort().argsort()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Ä–∞–Ω–≥–æ–≤
        rank_corr, _ = spearmanr(ranks1, ranks2)
        
        if np.isnan(rank_corr):
            return 0.5
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0,1]
        return float((rank_corr + 1) / 2)
    except:
        return 0.5


def analyze_embedding_importance(
    embedding: Union[torch.Tensor, np.ndarray],
    method: str = "variance_pca"
) -> np.ndarray:
    """
    –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    
    Args:
        embedding: –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥
        method: –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ ('variance_pca', 'clustering', 'magnitude')
        
    Returns:
        –ú–∞—Å—Å–∏–≤ –≤–µ—Å–æ–≤ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ [0,1]
    """
    if isinstance(embedding, torch.Tensor):
        embedding_np = embedding.detach().cpu().numpy()
    else:
        embedding_np = embedding
    
    embedding_flat = embedding_np.flatten()
    
    if method == "variance_pca":
        return _analyze_importance_pca(embedding_flat)
    elif method == "clustering": 
        return _analyze_importance_clustering(embedding_flat)
    elif method == "magnitude":
        return _analyze_importance_magnitude(embedding_flat)
    else:
        # Fallback - —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Å–∞
        return np.ones_like(embedding_flat) / len(embedding_flat)


def _analyze_importance_pca(embedding: np.ndarray) -> np.ndarray:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    """
    try:
        # –ü–æ–¥—Ö–æ–¥ 1: –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤
        embedding_std = np.std(embedding)
        magnitude_importance = np.abs(embedding)
        
        # –ü–æ–¥—Ö–æ–¥ 2: –õ–æ–∫–∞–ª—å–Ω–∞—è –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å (–∫–∞–∫ PCA –∑–∞–º–µ–Ω–∏—Ç–µ–ª—å)
        window_size = max(1, len(embedding) // 50)  # 2% –æ–∫–Ω–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        local_variance = np.zeros_like(embedding)
        
        for i in range(len(embedding)):
            start_idx = max(0, i - window_size)
            end_idx = min(len(embedding), i + window_size + 1)
            local_window = embedding[start_idx:end_idx]
            local_variance[i] = np.var(local_window) if len(local_window) > 1 else np.abs(embedding[i])
        
        # –ü–æ–¥—Ö–æ–¥ 3: –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏)
        gradient_importance = np.zeros_like(embedding)
        for i in range(len(embedding)):
            if i > 0 and i < len(embedding) - 1:
                # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å
                gradient_importance[i] = abs(embedding[i+1] - embedding[i-1]) / 2
            elif i == 0:
                # –ü—Ä—è–º–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å
                gradient_importance[i] = abs(embedding[i+1] - embedding[i]) if len(embedding) > 1 else abs(embedding[i])
            else:
                # –û–±—Ä–∞—Ç–Ω–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å
                gradient_importance[i] = abs(embedding[i] - embedding[i-1])
        
        # –ü–æ–¥—Ö–æ–¥ 4: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ
        mean_val = np.mean(embedding)
        deviation_importance = np.abs(embedding - mean_val)
        
        # –ü–æ–¥—Ö–æ–¥ 5: –ö–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        percentiles = np.percentile(np.abs(embedding), [25, 50, 75, 90, 95])
        quantile_importance = np.zeros_like(embedding)
        for i, val in enumerate(embedding):
            abs_val = abs(val)
            if abs_val >= percentiles[4]:  # Top 5%
                quantile_importance[i] = 1.0
            elif abs_val >= percentiles[3]:  # Top 10%
                quantile_importance[i] = 0.9
            elif abs_val >= percentiles[2]:  # Top 25%
                quantile_importance[i] = 0.7
            elif abs_val >= percentiles[1]:  # Above median
                quantile_importance[i] = 0.5
            elif abs_val >= percentiles[0]:  # Above Q1
                quantile_importance[i] = 0.3
            else:
                quantile_importance[i] = 0.1
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –ø–æ–¥—Ö–æ–¥—ã —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        combined_importance = (
            0.25 * magnitude_importance / (np.max(magnitude_importance) + 1e-8) +
            0.20 * local_variance / (np.max(local_variance) + 1e-8) +
            0.20 * gradient_importance / (np.max(gradient_importance) + 1e-8) +
            0.15 * deviation_importance / (np.max(deviation_importance) + 1e-8) +
            0.20 * quantile_importance
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–µ–ø–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏–π
        enhanced_importance = np.power(combined_importance, 1.5)  # –£—Å–∏–ª–∏–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–∏—è
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0,1]
        final_importance = enhanced_importance / (np.max(enhanced_importance) + 1e-8)
        
        return final_importance
        
    except Exception as e:
        # Fallback - —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã –∏ –ø–æ–∑–∏—Ü–∏–∏
        magnitude_importance = np.abs(embedding)
        n = len(embedding)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –≤–µ—Å–∞ (—ç–ª–µ–º–µ–Ω—Ç—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ)
        position_weights = np.ones(n)
        # –£—Å–∏–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª–æ (–ø–µ—Ä–≤—ã–µ 20%)
        position_weights[:n//5] *= 1.5
        # –£—Å–∏–ª–∏–≤–∞–µ–º –∫–æ–Ω–µ—Ü (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20%)
        position_weights[4*n//5:] *= 1.3
        # –£—Å–∏–ª–∏–≤–∞–µ–º —Å–µ—Ä–µ–¥–∏–Ω—É (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ 20%)
        center_start, center_end = 2*n//5, 3*n//5
        position_weights[center_start:center_end] *= 1.2
        
        combined = magnitude_importance * position_weights
        return combined / (np.max(combined) + 1e-8)


def _analyze_importance_clustering(embedding: np.ndarray) -> np.ndarray:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∏ –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–∫–Ω–æ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏.
    """
    try:
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤
        embedding_std = np.std(embedding)
        base_window = max(3, len(embedding) // 30)  # –ë–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ ~3% 
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö
        importance_scales = []
        
        # –ú–∞—Å—à—Ç–∞–± 1: –ú–µ–ª–∫–∏–µ –æ–∫–Ω–∞ (–¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
        small_window = max(2, base_window // 2)
        importance_small = np.zeros_like(embedding)
        
        for i in range(len(embedding)):
            start_idx = max(0, i - small_window)
            end_idx = min(len(embedding), i + small_window + 1)
            window = embedding[start_idx:end_idx]
            
            if len(window) > 1:
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                local_std = np.std(window)
                local_range = np.max(window) - np.min(window)
                local_energy = np.sum(np.square(window))
                
                importance_small[i] = local_std + 0.5 * local_range + 0.3 * np.sqrt(local_energy)
            else:
                importance_small[i] = np.abs(window[0])
        
        importance_scales.append(importance_small)
        
        # –ú–∞—Å—à—Ç–∞–± 2: –°—Ä–µ–¥–Ω–∏–µ –æ–∫–Ω–∞ (—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
        medium_window = base_window
        importance_medium = np.zeros_like(embedding)
        
        for i in range(len(embedding)):
            start_idx = max(0, i - medium_window)
            end_idx = min(len(embedding), i + medium_window + 1)
            window = embedding[start_idx:end_idx]
            
            if len(window) > 2:
                # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –≤ –æ–∫–Ω–µ
                x = np.arange(len(window))
                trend_coefficient = np.abs(np.corrcoef(x, window)[0, 1]) if len(window) > 1 else 0
                local_complexity = np.std(np.diff(window)) if len(window) > 1 else 0
                
                importance_medium[i] = trend_coefficient + local_complexity + np.abs(np.mean(window))
            else:
                importance_medium[i] = np.abs(np.mean(window))
        
        importance_scales.append(importance_medium)
        
        # –ú–∞—Å—à—Ç–∞–± 3: –ë–æ–ª—å—à–∏–µ –æ–∫–Ω–∞ (–≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        large_window = base_window * 2
        importance_large = np.zeros_like(embedding)
        
        for i in range(len(embedding)):
            start_idx = max(0, i - large_window)
            end_idx = min(len(embedding), i + large_window + 1)
            window = embedding[start_idx:end_idx]
            
            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤ –±–æ–ª—å—à–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            element_rank = np.sum(np.abs(window) < np.abs(embedding[i])) / len(window)
            local_contrast = np.abs(embedding[i] - np.median(window))
            
            importance_large[i] = element_rank + local_contrast
        
        importance_scales.append(importance_large)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–∞—Å—à—Ç–∞–±—ã —Å —É–±—ã–≤–∞—é—â–∏–º–∏ –≤–µ—Å–∞–º–∏ (–¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–µ–µ)
        weights = [0.5, 0.3, 0.2]  # –ú–µ–ª–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π, –∫—Ä—É–ø–Ω—ã–π –º–∞—Å—à—Ç–∞–±
        combined_importance = np.zeros_like(embedding)
        
        for scale_importance, weight in zip(importance_scales, weights):
            normalized_scale = scale_importance / (np.max(scale_importance) + 1e-8)
            combined_importance += weight * normalized_scale
        
        # –£—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è
        enhanced_importance = np.power(combined_importance, 1.3)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        final_importance = enhanced_importance / (np.max(enhanced_importance) + 1e-8)
        
        return final_importance
        
    except Exception as e:
        # Fallback - –ø—Ä–æ—Å—Ç–æ–π –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        magnitude = np.abs(embedding)
        
        # –õ–æ–∫–∞–ª—å–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è
        local_var = np.zeros_like(embedding)
        for i in range(len(embedding)):
            start = max(0, i - 2)
            end = min(len(embedding), i + 3)
            local_var[i] = np.var(embedding[start:end])
        
        combined = 0.7 * magnitude + 0.3 * local_var
        return combined / (np.max(combined) + 1e-8)


def _analyze_importance_magnitude(embedding: np.ndarray) -> np.ndarray:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    """
    n = len(embedding)
    
    # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ
    magnitude_importance = np.abs(embedding)
    
    # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ö–µ–º–∞)
    position_weights = np.ones(n)
    
    # –£—Å–∏–ª–∏–≤–∞–µ–º –∫—Ä–∞—è (–ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é)
    edge_size = max(1, n // 20)  # 5% —Å –∫–∞–∂–¥–æ–≥–æ –∫—Ä–∞—è
    position_weights[:edge_size] *= 1.4  # –ù–∞—á–∞–ª–æ
    position_weights[-edge_size:] *= 1.4  # –ö–æ–Ω–µ—Ü
    
    # –£—Å–∏–ª–∏–≤–∞–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –æ–±–ª–∞—Å—Ç—å (–≥–¥–µ –æ–±—ã—á–Ω–æ –æ—Å–Ω–æ–≤–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞)
    center_start, center_end = n//3, 2*n//3
    position_weights[center_start:center_end] *= 1.2
    
    # –ö–≤–∞—Ä—Ç–∏–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å (—ç–ª–µ–º–µ–Ω—Ç—ã –≤ –≤–µ—Ä—Ö–Ω–∏—Ö –∫–≤–∞—Ä—Ç–∏–ª—è—Ö –≤–∞–∂–Ω–µ–µ)
    percentiles = np.percentile(magnitude_importance, [50, 75, 90, 95, 99])
    quartile_weights = np.ones_like(magnitude_importance)
    
    for i, mag in enumerate(magnitude_importance):
        if mag >= percentiles[4]:  # Top 1%
            quartile_weights[i] = 2.0
        elif mag >= percentiles[3]:  # Top 5%
            quartile_weights[i] = 1.8
        elif mag >= percentiles[2]:  # Top 10%
            quartile_weights[i] = 1.5
        elif mag >= percentiles[1]:  # Top 25%
            quartile_weights[i] = 1.3
        elif mag >= percentiles[0]:  # Above median
            quartile_weights[i] = 1.0
        else:  # Below median
            quartile_weights[i] = 0.7
    
    # –ê–Ω–∞–ª–∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤ (–ø–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏)
    local_maxima_weights = np.ones_like(magnitude_importance)
    window_size = max(2, n // 50)  # 2% –æ–∫–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤
    
    for i in range(window_size, n - window_size):
        window = magnitude_importance[i-window_size:i+window_size+1]
        if magnitude_importance[i] == np.max(window):
            local_maxima_weights[i] = 1.5  # –£—Å–∏–ª–∏–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã
    
    # –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å (—ç–ª–µ–º–µ–Ω—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏)
    gradient_importance = np.ones_like(magnitude_importance)
    for i in range(1, n-1):
        left_diff = abs(embedding[i] - embedding[i-1])
        right_diff = abs(embedding[i+1] - embedding[i])
        max_diff = max(left_diff, right_diff)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç—É
        max_gradient = np.max([abs(embedding[i+1] - embedding[i]) for i in range(n-1)])
        if max_gradient > 0:
            gradient_importance[i] = 1.0 + (max_diff / max_gradient) * 0.5
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç—å (—ç–ª–µ–º–µ–Ω—Ç—ã –¥–∞–ª–µ–∫–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ)
    mean_val = np.mean(magnitude_importance)
    std_val = np.std(magnitude_importance)
    anomaly_weights = np.ones_like(magnitude_importance)
    
    for i, val in enumerate(magnitude_importance):
        z_score = abs(val - mean_val) / (std_val + 1e-8)
        if z_score > 2.0:  # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
            anomaly_weights[i] = 1.0 + min(z_score * 0.2, 1.0)  # –ú–∞–∫—Å–∏–º—É–º +100%
    
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
    combined_importance = (
        magnitude_importance * (
            0.40 * position_weights +
            0.25 * quartile_weights +
            0.15 * local_maxima_weights +
            0.10 * gradient_importance +
            0.10 * anomaly_weights
        )
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
    enhanced_importance = np.power(combined_importance / np.max(combined_importance), 1.2)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    return enhanced_importance / (np.max(enhanced_importance) + 1e-8)


def create_adaptive_transformation_strategy(
    embedding: Union[torch.Tensor, np.ndarray],
    target_shape: Tuple[int, int, int],
    importance_method: str = "variance_pca"
) -> Dict[str, Any]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
    
    Args:
        embedding: –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥
        target_shape: –¶–µ–ª–µ–≤–∞—è 3D —Ñ–æ—Ä–º–∞
        importance_method: –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    """
    if isinstance(embedding, torch.Tensor):
        embedding_np = embedding.detach().cpu().numpy()
    else:
        embedding_np = embedding
    
    embedding_flat = embedding_np.flatten()
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    importance_weights = analyze_embedding_importance(embedding_flat, importance_method)
    
    # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤ 3D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
    placement_map = _create_3d_placement_map(importance_weights, target_shape)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    strategy = {
        'importance_weights': importance_weights,
        'placement_map': placement_map,
        'target_shape': target_shape,
        'optimization_params': {
            'preserve_high_importance': True,
            'spatial_locality': True,
            'minimize_distortion': True
        },
        'quality_threshold': 0.98  # –¶–µ–ª–µ–≤–æ–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞
    }
    
    return strategy


def _create_3d_placement_map(
    importance_weights: np.ndarray, 
    target_shape: Tuple[int, int, int]
) -> np.ndarray:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ 3D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å —É—á–µ—Ç–æ–º –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏.
    
    –í–∞–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ä–∞–∑–º–µ—â–∞—é—Ç—Å—è –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É –∫—É–±–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏.
    """
    d, h, w = target_shape
    total_elements = d * h * w
    
    if len(importance_weights) != total_elements:
        raise ValueError(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: {len(importance_weights)} vs {total_elements}")
    
    # –°–æ–∑–¥–∞–µ–º 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
    coords = []
    for z in range(d):
        for y in range(h):
            for x in range(w):
                coords.append((z, y, x))
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç —Ü–µ–Ω—Ç—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
    center_z, center_y, center_x = d/2, h/2, w/2
    distances = []
    for z, y, x in coords:
        dist = np.sqrt((z - center_z)**2 + (y - center_y)**2 + (x - center_x)**2)
        distances.append(dist)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–∏ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –æ—Ç —Ü–µ–Ω—Ç—Ä–∞ (–±–ª–∏–∂–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
    sorted_indices = np.argsort(distances)
    importance_sorted_indices = np.argsort(importance_weights)[::-1]  # –û—Ç –≤–∞–∂–Ω—ã—Ö –∫ –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–º
    
    # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É —Ä–∞–∑–º–µ—â–µ–Ω–∏—è
    placement_map = np.zeros(total_elements, dtype=int)
    for i, (spatial_idx, importance_idx) in enumerate(zip(sorted_indices, importance_sorted_indices)):
        placement_map[importance_idx] = spatial_idx
    
    return placement_map


def optimize_shape_transformation(
    input_shape: Union[int, Tuple[int, ...]],
    target_shape: Tuple[int, int, int]
) -> Dict[str, Any]:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ñ–æ—Ä–º –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    
    Args:
        input_shape: –ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 768 –∏–ª–∏ (768,))
        target_shape: –¶–µ–ª–µ–≤–∞—è 3D —Ñ–æ—Ä–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, (8, 8, 12))
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    # –ü—Ä–∏–≤–æ–¥–∏–º input_shape –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
    if isinstance(input_shape, int):
        input_size = input_shape
    else:
        input_size = np.prod(input_shape)
    
    target_size = np.prod(target_shape)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    compatible = (input_size == target_size)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
    if len(target_shape) == 3:
        d, h, w = target_shape
        aspect_ratios = {
            'depth_height': d / h if h != 0 else float('inf'),
            'height_width': h / w if w != 0 else float('inf'),
            'depth_width': d / w if w != 0 else float('inf')
        }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ
        balanced = max(aspect_ratios.values()) / min(aspect_ratios.values()) < 2.0
    else:
        aspect_ratios = {}
        balanced = True
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—ã
    alternatives = []
    if not compatible:
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ–æ—Ä–º—ã
        factors = _find_factors(input_size)
        for i, f1 in enumerate(factors):
            for j, f2 in enumerate(factors[i:], i):
                for k, f3 in enumerate(factors[j:], j):
                    if f1 * f2 * f3 == input_size:
                        alternatives.append((f1, f2, f3))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ —Ü–µ–ª–µ–≤–æ–π —Ñ–æ—Ä–º–µ
        alternatives.sort(key=lambda x: sum(abs(a - b) for a, b in zip(x, target_shape)))
    
    return {
        'compatible': compatible,
        'input_size': input_size,
        'target_size': target_size,
        'aspect_ratios': aspect_ratios,
        'balanced_proportions': balanced,
        'alternative_shapes': alternatives[:5],  # –¢–æ–ø-5 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤
        'optimization_score': _calculate_optimization_score(input_size, target_shape)
    }


def _find_factors(n: int) -> List[int]:
    """–ü–æ–∏—Å–∫ –≤—Å–µ—Ö –¥–µ–ª–∏—Ç–µ–ª–µ–π —á–∏—Å–ª–∞ n."""
    factors = []
    for i in range(1, int(n**0.5) + 1):
        if n % i == 0:
            factors.append(i)
            if i != n // i:
                factors.append(n // i)
    return sorted(factors)


def _calculate_optimization_score(input_size: int, target_shape: Tuple[int, int, int]) -> float:
    """
    –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    
    Returns:
        –û—Ü–µ–Ω–∫–∞ –æ—Ç 0 –¥–æ 1 (1 = –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
    """
    target_size = np.prod(target_shape)
    
    # –ë–∞–∑–æ–≤–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    if input_size != target_size:
        return 0.0
    
    # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
    d, h, w = target_shape
    ratios = [d/h, h/w, d/w] if h != 0 and w != 0 else [1.0, 1.0, 1.0]
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –∫–æ–≥–¥–∞ –≤—Å–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –±–ª–∏–∑–∫–∏ –∫ 1 (–∫—É–±–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞)
    balance_score = 1.0 - (max(ratios) - min(ratios)) / max(ratios)
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∏–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è
    size_penalty = 0.0
    for dim in target_shape:
        if dim < 2 or dim > 32:  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ä–∞–∑–º–µ—Ä–æ–≤
            size_penalty += 0.1
    
    final_score = max(0.0, balance_score - size_penalty)
    return min(1.0, final_score)


def create_test_embeddings(
    count: int = 10,
    dim: int = 768,
    embedding_type: str = "random"
) -> List[np.ndarray]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
    
    Args:
        count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —ç–º–±–µ–¥–∏–Ω–≥–∞
        embedding_type: –¢–∏–ø —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ ('random', 'gaussian', 'normalized', 'diverse')
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    """
    embeddings = []
    
    if embedding_type == "diverse":
        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        patterns = [
            ("sparse", 0.1),           # –†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π (10% –Ω–µ–Ω—É–ª–µ–≤—ã—Ö)
            ("dense_positive", 1.0),   # –ü–ª–æ—Ç–Ω—ã–π –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
            ("mixed_signs", 0.5),      # –°–º–µ—à–∞–Ω–Ω—ã–µ –∑–Ω–∞–∫–∏
            ("gaussian_high", 2.0),    # –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
            ("gaussian_low", 0.1),     # –ù–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        ]
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        extended_patterns = []
        for i in range(count):
            pattern_name, param = patterns[i % len(patterns)]
            extended_patterns.append((pattern_name, param, i))
        
        for pattern_name, param, seed in extended_patterns:
            np.random.seed(seed + 42)  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            
            if pattern_name == "sparse":
                # –†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥
                emb = np.zeros(dim, dtype=np.float32)
                num_nonzero = int(dim * param)
                indices = np.random.choice(dim, num_nonzero, replace=False)
                emb[indices] = np.random.normal(0, 1, num_nonzero)
                
            elif pattern_name == "dense_positive":
                # –ü–ª–æ—Ç–Ω—ã–π –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
                emb = np.random.exponential(param, dim).astype(np.float32)
                
            elif pattern_name == "mixed_signs":
                # –°–º–µ—à–∞–Ω–Ω—ã–µ –∑–Ω–∞–∫–∏
                emb = np.random.normal(0, 1, dim).astype(np.float32)
                # –ü–æ–ª–æ–≤–∏–Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö
                neg_indices = np.random.choice(dim, dim//2, replace=False)
                emb[neg_indices] *= -1
                
            elif pattern_name == "gaussian_high":
                # –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
                emb = np.random.normal(0, param, dim).astype(np.float32)
                
            elif pattern_name == "gaussian_low":
                # –ù–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
                emb = np.random.normal(0, param, dim).astype(np.float32)
            
            embeddings.append(emb)
    else:
        # –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        for i in range(count):
            if embedding_type == "random":
                emb = np.random.random(dim).astype(np.float32)
            elif embedding_type == "gaussian":
                emb = np.random.normal(0, 1, dim).astype(np.float32)
            elif embedding_type == "normalized":
                emb = np.random.random(dim).astype(np.float32)
                emb = emb / np.linalg.norm(emb)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —ç–º–±–µ–¥–∏–Ω–≥–∞: {embedding_type}")
            
            embeddings.append(emb)
    
    return embeddings


def benchmark_transformation_speed(
    reshaper,
    test_embeddings: List[np.ndarray],
    num_iterations: int = 100
) -> Dict[str, float]:
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    
    Args:
        reshaper: –≠–∫–∑–µ–º–ø–ª—è—Ä EmbeddingReshaper
        test_embeddings: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        num_iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    import time
    
    if not test_embeddings:
        raise ValueError("–ù—É–∂–µ–Ω —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥")
    
    # –í—Ä–µ–º—è 1D ‚Üí 3D —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    start_time = time.time()
    for _ in range(num_iterations):
        for emb in test_embeddings:
            _ = reshaper.vector_to_matrix(emb)
    time_1d_to_3d = (time.time() - start_time) / (num_iterations * len(test_embeddings))
    
    # –í—Ä–µ–º—è 3D ‚Üí 1D —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    test_3d = [reshaper.vector_to_matrix(emb) for emb in test_embeddings]
    start_time = time.time()
    for _ in range(num_iterations):
        for emb_3d in test_3d:
            _ = reshaper.matrix_to_vector(emb_3d)
    time_3d_to_1d = (time.time() - start_time) / (num_iterations * len(test_embeddings))
    
    return {
        'avg_time_1d_to_3d_ms': time_1d_to_3d * 1000,
        'avg_time_3d_to_1d_ms': time_3d_to_1d * 1000,
        'total_throughput_per_sec': 1.0 / (time_1d_to_3d + time_3d_to_1d),
        'test_embeddings_count': len(test_embeddings),
        'iterations': num_iterations
    } 