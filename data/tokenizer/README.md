# Tokenizer Module - 3D Cellular Neural Network

**–ú–æ–¥—É–ª—å:** `data/tokenizer/`  
**–í–µ—Ä—Å–∏—è:** 1.0.0  
**–°—Ç–∞—Ç—É—Å:** üöß –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ  
**–§–∞–∑–∞:** Phase 2 (Core Functionality)

---

## üéØ –û–ø–∏—Å–∞–Ω–∏–µ

–ú–æ–¥—É–ª—å –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ —Ç–æ–∫–µ–Ω–∞–º–∏ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è 3D –∫–ª–µ—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.

## üîß –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—ã

- **BERT** (`bert-base-uncased`) - Bidirectional —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- **GPT-2** (`gpt2`) - Autoregressive —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- **SentencePiece** (`sentencepiece`) - Subword tokenization
- **Basic** (`basic`) - –ü—Ä–æ—Å—Ç–∞—è –±–µ–ª–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

### –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç ‚Üî —Ç–æ–∫–µ–Ω—ã ‚Üî ID
- ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ([CLS], [SEP], [PAD])
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤
- ‚úÖ Batch processing –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å embedding_loader
- ‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (max_length, padding, etc.)

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```bash
pip install transformers>=4.21.0
pip install sentencepiece>=0.1.96
pip install torch>=1.9.0
```

### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from data.tokenizer import TokenizerManager

# –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
tokenizer = TokenizerManager(tokenizer_type='bert-base-uncased')

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
text = "Hello, world! This is a test."
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
decoded_text = tokenizer.decode(tokens)
print(f"Decoded: {decoded_text}")
```

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Å—ã

```python
class TokenizerManager:
    """–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤"""

class TextProcessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""

class TokenizerAdapter:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤"""
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª—è

```
data/tokenizer/
‚îú‚îÄ‚îÄ __init__.py                  # –≠–∫—Å–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª—è
‚îú‚îÄ‚îÄ README.md                    # –≠—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îú‚îÄ‚îÄ plan.md                      # –ü–ª–∞–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
‚îú‚îÄ‚îÄ meta.md                      # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ errors.md                    # –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
‚îú‚îÄ‚îÄ diagram.mmd                  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
‚îú‚îÄ‚îÄ examples.md                  # –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
‚îú‚îÄ‚îÄ tokenizer.py                 # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å TokenizerManager
‚îú‚îÄ‚îÄ tokenizer_adapters.py        # –ê–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤
‚îú‚îÄ‚îÄ text_processor.py            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ tokenizer_config.yaml    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
```

## üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –° embedding_loader

```python
from data.embedding_loader import EmbeddingLoader
from data.tokenizer import TokenizerManager

# –°–æ–∑–¥–∞–Ω–∏–µ pipeline
tokenizer = TokenizerManager('bert-base-uncased')
embedding_loader = EmbeddingLoader()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
text = "Sample input text"
tokens = tokenizer.encode(text)
embeddings = embedding_loader.get_embeddings_for_tokens(tokens)
```

### –° lattice_3d

```python
from core.lattice_3d import Lattice3D
from data.tokenizer import TokenizerManager

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ—Ç–∫–∏
tokenizer = TokenizerManager()
lattice = Lattice3D(size=(5, 5, 5))

text = "Input for neural network"
tokens = tokenizer.encode(text, max_length=25)  # 5x5 –≤—Ö–æ–¥–Ω–∞—è –≥—Ä–∞–Ω—å
lattice.set_input_face(tokens)
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã tokenizer_config.yaml

```yaml
tokenizer:
  type: "bert-base-uncased" # –¢–∏–ø —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
  max_length: 512 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  padding: true # –î–æ–±–∞–≤–ª—è—Ç—å padding
  truncation: true # –û–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
  add_special_tokens: true # –î–æ–±–∞–≤–ª—è—Ç—å [CLS], [SEP]

text_processing:
  lowercase: true # –ü—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
  remove_punctuation: false # –£–¥–∞–ª—è—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
  remove_stopwords: false # –£–¥–∞–ª—è—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞

batch_processing:
  batch_size: 32 # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
  num_workers: 4 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
```

## üìä Performance

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

- **–°–∫–æ—Ä–æ—Å—Ç—å:** ~1000 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ –¥–ª—è BERT
- **–ü–∞–º—è—Ç—å:** ~200MB –¥–ª—è –º–æ–¥–µ–ª–∏ BERT base
- **Batch processing:** –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–æ 10K —Ç–µ–∫—Å—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ

### Benchmarks

| –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å (tokens/sec) | –ü–∞–º—è—Ç—å (MB) | –¢–æ—á–Ω–æ—Å—Ç—å |
| ---------- | --------------------- | ----------- | -------- |
| BERT       | 1000                  | 200         | 95%      |
| GPT-2      | 1500                  | 150         | 92%      |
| Basic      | 5000                  | 10          | 80%      |

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –º–æ–¥—É–ª—è
python -m pytest data/tokenizer/tests/

# –†—É—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python test_tokenizer_basic.py
```

## üêõ –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

- BERT —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –¥–ª—è –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
- SentencePiece –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–ß–∞—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ 3D Cellular Neural Network.  
–°–º. –æ—Å–Ω–æ–≤–Ω–æ–π README –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª–∏—Ü–µ–Ω–∑–∏–∏.

---

**üéØ –°—Ç–∞—Ç—É—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:** üöß –ú–æ–¥—É–ª—å –≤ –∞–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ  
**üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏:** `data/embedding_loader/`, `core/lattice_3d/`  
**üìÖ –ü–ª–∞–Ω–∏—Ä—É–µ–º–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ:** Phase 2 (—Ç–µ–∫—É—â–∞—è —Ñ–∞–∑–∞)
