"""
[CONFIG] Universal Embedding Adapter
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö Teacher –º–æ–¥–µ–ª–µ–π –∏ –ª—é–±—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∫—É–±–∞
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, Tuple, Union, List
import yaml
import json
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class UniversalEmbeddingAdapter(nn.Module):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –õ—é–±—ã–µ input —Ä–∞–∑–º–µ—Ä—ã (LLaMA, DistilBERT, GPT, etc.)
    - –õ—é–±—ã–µ output —Ä–∞–∑–º–µ—Ä—ã (surface —Ä–∞–∑–º–µ—Ä—ã –∫—É–±–∞)
    - –†–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    - Configuration-driven –ø–æ–¥—Ö–æ–¥
    """
    
    def __init__(self, 
                 input_dim: Optional[int] = None,
                 output_dim: Optional[int] = None,
                 strategy: str = "learned_linear",
                 config: Optional[Dict[str, Any]] = None):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.strategy = strategy
        self.config = config or {}
        
        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã –Ω–µ –∑–∞–¥–∞–Ω—ã, —Å–æ–∑–¥–∞–µ–º lazy –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        self.initialized = False
        
        if input_dim and output_dim:
            self._build_adapter()
            self.initialized = True
        
        logger.info(f"[CONFIG] UniversalEmbeddingAdapter —Å–æ–∑–¥–∞–Ω: {input_dim}D ‚Üí {output_dim}D, strategy={strategy}")
    
    def _build_adapter(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤"""
        if self.strategy == "learned_linear":
            self._build_learned_linear()
        elif self.strategy == "hierarchical":
            self._build_hierarchical()
        elif self.strategy == "attention_based":
            self._build_attention_based()
        elif self.strategy == "autoencoder":
            self._build_autoencoder()
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")
    
    def _build_learned_linear(self):
        """–ü—Ä–æ—Å—Ç–∞—è learned linear —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        # Compression
        self.compressor = nn.Sequential(
            nn.Linear(self.input_dim, self.output_dim),
            nn.LayerNorm(self.output_dim),
            nn.GELU()
        )
        
        # Decompression (–¥–ª—è reconstruction loss)
        self.decompressor = nn.Sequential(
            nn.Linear(self.output_dim, self.input_dim),
            nn.LayerNorm(self.input_dim),
            nn.GELU()
        )
        
        # Reconstruction loss
        self.reconstruction_loss = nn.MSELoss()
    
    def _build_hierarchical(self):
        """Hierarchical compression —á–µ—Ä–µ–∑ intermediate layers"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º intermediate —Ä–∞–∑–º–µ—Ä—ã
        intermediate_sizes = self._calculate_intermediate_sizes()
        
        # Encoder layers
        encoder_layers = []
        prev_size = self.input_dim
        
        for size in intermediate_sizes:
            encoder_layers.extend([
                nn.Linear(prev_size, size),
                nn.LayerNorm(size),
                nn.GELU(),
                nn.Dropout(0.1)
            ])
            prev_size = size
        
        # Final compression
        encoder_layers.append(nn.Linear(prev_size, self.output_dim))
        self.encoder = nn.Sequential(*encoder_layers)
        
        # Decoder layers (reverse)
        decoder_layers = []
        prev_size = self.output_dim
        
        for size in reversed(intermediate_sizes):
            decoder_layers.extend([
                nn.Linear(prev_size, size),
                nn.LayerNorm(size),
                nn.GELU(),
                nn.Dropout(0.1)
            ])
            prev_size = size
        
        # Final decompression
        decoder_layers.append(nn.Linear(prev_size, self.input_dim))
        self.decoder = nn.Sequential(*decoder_layers)
        
        self.reconstruction_loss = nn.MSELoss()
    
    def _build_attention_based(self):
        """Attention-based selective compression"""
        # –ï—Å–ª–∏ output –º–µ–Ω—å—à–µ input, –∏—Å–ø–æ–ª—å–∑—É–µ–º attention –¥–ª—è –≤—ã–±–æ—Ä–∞ –≤–∞–∂–Ω—ã—Ö dims
        if self.output_dim < self.input_dim:
            self.attention = nn.MultiheadAttention(
                embed_dim=self.input_dim,
                num_heads=min(8, self.input_dim // 64),
                batch_first=True
            )
            self.dimension_selector = nn.Linear(self.input_dim, self.output_dim)
        else:
            # –ï—Å–ª–∏ output –±–æ–ª—å—à–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º expansion
            self.dimension_expander = nn.Linear(self.input_dim, self.output_dim)
        
        # Reconstruction path
        self.reconstructor = nn.Linear(self.output_dim, self.input_dim)
        self.reconstruction_loss = nn.MSELoss()
    
    def _build_autoencoder(self):
        """Autoencoder —Å bottleneck –Ω–∞ output_dim"""
        bottleneck_dim = min(self.output_dim, self.input_dim // 2)
        
        self.encoder = nn.Sequential(
            nn.Linear(self.input_dim, self.input_dim // 2),
            nn.LayerNorm(self.input_dim // 2),
            nn.GELU(),
            nn.Linear(self.input_dim // 2, bottleneck_dim),
            nn.LayerNorm(bottleneck_dim),
            nn.GELU(),
            nn.Linear(bottleneck_dim, self.output_dim)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(self.output_dim, bottleneck_dim),
            nn.LayerNorm(bottleneck_dim),
            nn.GELU(),
            nn.Linear(bottleneck_dim, self.input_dim // 2),
            nn.LayerNorm(self.input_dim // 2),
            nn.GELU(),
            nn.Linear(self.input_dim // 2, self.input_dim)
        )
        
        self.reconstruction_loss = nn.MSELoss()
    
    def _calculate_intermediate_sizes(self) -> List[int]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–ª—è hierarchical approach"""
        ratio = self.output_dim / self.input_dim
        
        if ratio > 0.5:
            # –ù–µ–±–æ–ª—å—à–æ–µ —Å–∂–∞—Ç–∏–µ - –æ–¥–∏–Ω intermediate layer
            return [int((self.input_dim + self.output_dim) / 2)]
        else:
            # –°–∏–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ - –Ω–µ—Å–∫–æ–ª—å–∫–æ layers
            sizes = []
            current = self.input_dim
            target = self.output_dim
            
            while current > target * 2:
                current = int(current * 0.7)  # –£–º–µ–Ω—å—à–∞–µ–º –Ω–∞ 30% –∫–∞–∂–¥—ã–π —à–∞–≥
                sizes.append(current)
            
            return sizes
    
    def initialize_from_data(self, sample_input: torch.Tensor, target_output_dim: int):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if self.initialized:
            return
        
        if len(sample_input.shape) == 1:
            self.input_dim = sample_input.shape[0]
        else:
            self.input_dim = sample_input.shape[-1]
        
        self.output_dim = target_output_dim
        
        self._build_adapter()
        self.initialized = True
        
        logger.info(f"[CONFIG] Adapter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö: {self.input_dim}D ‚Üí {self.output_dim}D")
    
    def forward(self, x: torch.Tensor, return_reconstruction: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Forward pass —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä
        
        Args:
            x: Input tensor [batch_size, input_dim] –∏–ª–∏ [input_dim]
            return_reconstruction: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ reconstruction –¥–ª—è loss
            
        Returns:
            compressed: [batch_size, output_dim] –∏–ª–∏ [output_dim]
            reconstruction: [batch_size, input_dim] (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
        """
        if not self.initialized:
            raise RuntimeError("Adapter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ initialize_from_data() –∏–ª–∏ –∑–∞–¥–∞–π—Ç–µ —Ä–∞–∑–º–µ—Ä—ã –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        original_shape = x.shape
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ float32 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (LLaMA –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å float16)
        if x.dtype == torch.float16:
            x = x.float()
        
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
            single_sample = True
        else:
            single_sample = False
        
        # Compression
        if self.strategy == "learned_linear":
            compressed = self.compressor(x)
            if return_reconstruction:
                reconstructed = self.decompressor(compressed)
        
        elif self.strategy == "hierarchical":
            compressed = self.encoder(x)
            if return_reconstruction:
                reconstructed = self.decoder(compressed)
        
        elif self.strategy == "attention_based":
            if self.output_dim < self.input_dim:
                # Attention-based selection
                attended, _ = self.attention(x, x, x)
                compressed = self.dimension_selector(attended)
            else:
                # Direct expansion
                compressed = self.dimension_expander(x)
            
            if return_reconstruction:
                reconstructed = self.reconstructor(compressed)
        
        elif self.strategy == "autoencoder":
            compressed = self.encoder(x)
            if return_reconstruction:
                reconstructed = self.decoder(compressed)
        
        # –í–æ–∑–≤—Ä–∞—Ç –∫ original shape
        if single_sample:
            compressed = compressed.squeeze(0)
            if return_reconstruction:
                reconstructed = reconstructed.squeeze(0)
        
        if return_reconstruction:
            return compressed, reconstructed
        else:
            return compressed
    
    def compute_reconstruction_loss(self, original: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ reconstruction loss"""
        return self.reconstruction_loss(original, reconstructed)
    
    def get_compression_ratio(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∂–∞—Ç–∏—è"""
        if not self.initialized:
            return 0.0
        return self.output_dim / self.input_dim
    
    def get_parameter_count(self) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∞–¥–∞–ø—Ç–µ—Ä–∞"""
        return sum(p.numel() for p in self.parameters())
    
    def save_config(self, path: Union[str, Path]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞"""
        config = {
            "input_dim": self.input_dim,
            "output_dim": self.output_dim,
            "strategy": self.strategy,
            "compression_ratio": self.get_compression_ratio(),
            "parameter_count": self.get_parameter_count(),
            "config": self.config
        }
        
        with open(path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        logger.info(f"[SAVE] Adapter config —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
    
    @classmethod
    def from_config(cls, config_path: Union[str, Path]) -> 'UniversalEmbeddingAdapter':
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        return cls(
            input_dim=config["input_dim"],
            output_dim=config["output_dim"],
            strategy=config["strategy"],
            config=config.get("config", {})
        )


class AdapterManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –Ω—É–∂–Ω—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π/—Ä–∞–∑–º–µ—Ä–æ–≤
    """
    
    def __init__(self, config_dir: str = "config/adapters/"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        self.adapters: Dict[str, UniversalEmbeddingAdapter] = {}
        self.model_configs = {}
        
    def register_model(self, model_name: str, embedding_dim: int, **kwargs):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        self.model_configs[model_name] = {
            "embedding_dim": embedding_dim,
            "config": kwargs
        }
        logger.info(f"[WRITE] –ú–æ–¥–µ–ª—å –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞: {model_name} ({embedding_dim}D)")
    
    def get_adapter(self, 
                   source_model: str, 
                   target_surface_size: int,
                   strategy: str = "learned_linear") -> UniversalEmbeddingAdapter:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ/—Å–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã –º–æ–¥–µ–ª—å‚Üísurface"""
        
        adapter_key = f"{source_model}‚Üí{target_surface_size}_{strategy}"
        
        if adapter_key in self.adapters:
            return self.adapters[adapter_key]
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∞–¥–∞–ø—Ç–µ—Ä
        if source_model not in self.model_configs:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {source_model} –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞")
        
        source_dim = self.model_configs[source_model]["embedding_dim"]
        
        adapter = UniversalEmbeddingAdapter(
            input_dim=source_dim,
            output_dim=target_surface_size,
            strategy=strategy
        )
        
        self.adapters[adapter_key] = adapter
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_path = self.config_dir / f"{adapter_key}.yaml"
        adapter.save_config(config_path)
        
        logger.info(f"[CONFIG] –°–æ–∑–¥–∞–Ω –∞–¥–∞–ø—Ç–µ—Ä: {source_model} ({source_dim}D) ‚Üí surface ({target_surface_size}D)")
        
        return adapter
    
    def list_adapters(self) -> Dict[str, Dict[str, Any]]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–¥–∞–ø—Ç–µ—Ä–æ–≤"""
        result = {}
        for key, adapter in self.adapters.items():
            result[key] = {
                "input_dim": adapter.input_dim,
                "output_dim": adapter.output_dim,
                "strategy": adapter.strategy,
                "compression_ratio": adapter.get_compression_ratio(),
                "parameters": adapter.get_parameter_count()
            }
        return result


# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
KNOWN_MODELS = {
    "Meta-Llama-3-8B": {"embedding_dim": 4096},  # LLaMA 3 8B
    "Meta-Llama-3-70B": {"embedding_dim": 8192}, # LLaMA 3 70B  
    "DistilBERT": {"embedding_dim": 768},         # DistilBERT
    "BERT-base": {"embedding_dim": 768},          # BERT base
    "BERT-large": {"embedding_dim": 1024},        # BERT large
    "RoBERTa-base": {"embedding_dim": 768},       # RoBERTa base
    "RoBERTa-large": {"embedding_dim": 1024},     # RoBERTa large
    "GPT-3.5": {"embedding_dim": 1536},          # GPT-3.5
    "text-embedding-ada-002": {"embedding_dim": 1536}, # OpenAI embedding
}


def create_adapter_for_cube(cube_dimensions: Tuple[int, int, int],
                           teacher_model: str = "Meta-Llama-3-8B",
                           strategy: str = "learned_linear") -> UniversalEmbeddingAdapter:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫—É–±–∞
    
    Args:
        cube_dimensions: (x, y, z) —Ä–∞–∑–º–µ—Ä—ã –∫—É–±–∞
        teacher_model: –ù–∞–∑–≤–∞–Ω–∏–µ teacher –º–æ–¥–µ–ª–∏
        strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    
    Returns:
        UniversalEmbeddingAdapter: –ì–æ—Ç–æ–≤—ã–π –∞–¥–∞–ø—Ç–µ—Ä
    """
    # –í—ã—á–∏—Å–ª—è–µ–º surface size (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –≥—Ä–∞–Ω—å)
    surface_size = cube_dimensions[0] * cube_dimensions[1]
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª–∏
    if teacher_model not in KNOWN_MODELS:
        raise ValueError(f"Unknown model: {teacher_model}. Known models: {list(KNOWN_MODELS.keys())}")
    
    source_dim = KNOWN_MODELS[teacher_model]["embedding_dim"]
    
    adapter = UniversalEmbeddingAdapter(
        input_dim=source_dim,
        output_dim=surface_size,
        strategy=strategy
    )
    
    logger.info(f"üéØ –ê–¥–∞–ø—Ç–µ—Ä —Å–æ–∑–¥–∞–Ω –¥–ª—è –∫—É–±–∞ {cube_dimensions}: {teacher_model} ({source_dim}D) ‚Üí surface ({surface_size}D)")
    
    return adapter


# –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
__all__ = [
    "UniversalEmbeddingAdapter",
    "AdapterManager", 
    "KNOWN_MODELS",
    "create_adapter_for_cube"
] 