#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ dataset –º–æ–¥—É–ª—è —Å EnergyTrainer
===============================================

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ DatasetManager
- –°–æ–∑–¥–∞–Ω–∏–µ EnergyTrainer —Å –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
- –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
"""

import sys
from pathlib import Path
import torch

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from energy_flow.config import create_debug_config, set_energy_config
from energy_flow.dataset import (
    create_dataset_config_from_energy,
    create_dataset_manager
)
from energy_flow.training import EnergyTrainer
from energy_flow.utils.logging import get_logger

logger = get_logger(__name__)


def create_training_setup():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    print("üîß Setting up training environment...")
    
    # 1. Energy –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (debug —Ä–µ–∂–∏–º –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã)
    energy_config = create_debug_config()
    set_energy_config(energy_config)
    
    # 2. Dataset –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
    dataset_config = create_dataset_config_from_energy(
        energy_config,
        dataset_sources=["precomputed"],  # –¢–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        max_samples_per_source=100,  # –ù–µ–±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ–º–æ
        batch_size=4  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è debug
    )
    
    # 3. DatasetManager
    dataset_manager = create_dataset_manager(dataset_config, energy_config)
    
    # 4. EnergyTrainer
    trainer = EnergyTrainer(energy_config)
    
    return energy_config, dataset_manager, trainer


def run_training_demo(num_epochs: int = 2):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω–æ–≤—ã–º dataset –º–æ–¥—É–ª–µ–º"""
    print(f"üöÄ Starting training demo ({num_epochs} epochs)")
    print("=" * 60)
    
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
        energy_config, dataset_manager, trainer = create_training_setup()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        print("\n1Ô∏è‚É£ Validating setup...")
        validation = dataset_manager.validate_setup()
        
        if not validation['overall_status']:
            print("‚ùå Setup validation failed:")
            for error in validation['errors']:
                print(f"   - {error}")
            return False
        
        print("‚úÖ Setup validation passed")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataLoader
        print("\n2Ô∏è‚É£ Preparing data...")
        dataloader = dataset_manager.create_dataloader(
            batch_size=energy_config.batch_size,
            shuffle=True
        )
        
        if not dataloader:
            print("‚ùå Failed to create DataLoader")
            return False
        
        print(f"‚úÖ DataLoader ready: {len(dataloader)} batches")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        stats = dataset_manager.get_statistics()
        print(f"   Dataset: {stats.get('total_samples', 'N/A')} samples")
        print(f"   Sources: {', '.join(stats.get('providers_used', []))}")
        print(f"   Embedding dim: {stats.get('embedding_dimension', 'N/A')}")
        
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        print(f"\n3Ô∏è‚É£ Starting training ({num_epochs} epochs)...")
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            print("-" * 40)
            
            epoch_metrics = {
                'total_loss': 0.0,
                'energy_loss': 0.0,
                'text_loss': 0.0,
                'batches_processed': 0
            }
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –±–∞—Ç—á–∞–º
            for batch_idx, batch in enumerate(dataloader):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                input_texts = batch['input_text']
                target_texts = batch['target_text']
                input_embeddings = batch['input_embedding']
                target_embeddings = batch['target_embedding']
                
                # –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                step_metrics = trainer.train_step(
                    input_texts=input_texts,
                    target_texts=target_texts,
                    teacher_input_embeddings=input_embeddings,
                    teacher_target_embeddings=target_embeddings
                )
                
                # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                epoch_metrics['total_loss'] += step_metrics.get('total_loss', 0)
                epoch_metrics['energy_loss'] += step_metrics.get('energy_loss', 0)
                epoch_metrics['text_loss'] += step_metrics.get('text_loss', 0)
                epoch_metrics['batches_processed'] += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                if batch_idx % 5 == 0:  # –ö–∞–∂–¥—ã–π 5-–π –±–∞—Ç—á
                    print(f"  Batch {batch_idx + 1}/{len(dataloader)}: "
                          f"loss={step_metrics.get('total_loss', 0):.4f}")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –¥–µ–º–æ
                if batch_idx >= 10:  # –ú–∞–∫—Å–∏–º—É–º 10 –±–∞—Ç—á–µ–π
                    break
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–µ
            if epoch_metrics['batches_processed'] > 0:
                for key in ['total_loss', 'energy_loss', 'text_loss']:
                    epoch_metrics[key] /= epoch_metrics['batches_processed']
            
            print(f"  Epoch {epoch + 1} completed:")
            print(f"    Total loss: {epoch_metrics['total_loss']:.4f}")
            print(f"    Energy loss: {epoch_metrics['energy_loss']:.4f}")
            print(f"    Text loss: {epoch_metrics['text_loss']:.4f}")
            print(f"    Batches processed: {epoch_metrics['batches_processed']}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        print(f"\n4Ô∏è‚É£ Running post-training validation...")
        
        # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_batch = next(iter(dataloader))
        val_input_texts = val_batch['input_text'][:3]  # –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞
        val_target_texts = val_batch['target_text'][:3]
        val_input_embeddings = val_batch['input_embedding'][:3]
        val_target_embeddings = val_batch['target_embedding'][:3]
        
        val_results = trainer.validate(
            input_texts=val_input_texts,
            target_texts=val_target_texts,
            teacher_input_embeddings=val_input_embeddings,
            teacher_target_embeddings=val_target_embeddings
        )
        
        print(f"‚úÖ Validation completed:")
        print(f"   Validation loss: {val_results.get('total_loss', 'N/A'):.4f}")
        print(f"   Examples generated: {len(val_results.get('examples', []))}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if val_results.get('examples'):
            print(f"\nüìù Prediction examples:")
            for i, example in enumerate(val_results['examples'][:2]):
                print(f"   Example {i+1}:")
                print(f"     Input: '{example['input'][:80]}...'")
                print(f"     Target: '{example['target'][:80]}...'")
                print(f"     Predicted: '{example['predicted'][:80]}...'")
        
        print(f"\nüéâ Training demo completed successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Training demo failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_dataset_integration_only():
    """–¢–µ—Å—Ç —Ç–æ–ª—å–∫–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    print("\nüß™ Testing Dataset Integration Only")
    print("-" * 40)
    
    try:
        energy_config, dataset_manager, trainer = create_training_setup()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
        print("‚úÖ Components initialized")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –±–∞—Ç—á
        dataloader = dataset_manager.create_dataloader(batch_size=2)
        if dataloader:
            batch = next(iter(dataloader))
            
            print(f"‚úÖ Sample batch loaded:")
            print(f"   Texts: {len(batch['input_text'])} pairs")
            print(f"   Embeddings: {batch['input_embedding'].shape}")
            print(f"   Sample input: '{batch['input_text'][0][:50]}...'")
            
            # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ dataset manager
            test_texts = ["Hello world", "Machine learning is interesting"]
            embeddings = dataset_manager.get_teacher_embeddings(test_texts)
            print(f"‚úÖ Teacher embeddings generated: {embeddings.shape}")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        return False


if __name__ == "__main__":
    try:
        # –û—Å–Ω–æ–≤–Ω–æ–µ –¥–µ–º–æ –æ–±—É—á–µ–Ω–∏—è
        success = run_training_demo(num_epochs=1)
        
        if success:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            test_dataset_integration_only()
            
            print(f"\n‚ú® All demos completed successfully!")
            print(f"   The new dataset module is ready for production use.")
        else:
            print(f"\n‚ö†Ô∏è  Demo had issues, but components are available for debugging.")
        
    except KeyboardInterrupt:
        print("\nüõë Demo interrupted by user")
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()