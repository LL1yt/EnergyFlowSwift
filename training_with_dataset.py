#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ —Å –≥–æ—Ç–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
===========================

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
–¥–ª—è –æ–±—É—á–µ–Ω–∏—è EnergyTrainer –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏.
"""

import sys
from pathlib import Path
import torch

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from energy_flow.config import create_debug_config, set_energy_config
from energy_flow.training import EnergyTrainer
from energy_flow.utils.logging import get_logger

logger = get_logger(__name__)

# –ü—É—Ç—å –∫ –≥–æ—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
DATASET_PATH = "data/energy_flow/active/debug_precomputed_30pairs_20250729_110314.pt"


def load_dataset(dataset_path: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print(f"üìÅ Loading dataset from {dataset_path}")
    
    if not Path(dataset_path).exists():
        raise FileNotFoundError(f"Dataset file not found: {dataset_path}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = torch.load(dataset_path, map_location='cuda', weights_only=False)
    
    print(f"‚úÖ Dataset loaded:")
    print(f"   Total samples: {len(dataset['text_pairs'])}")
    print(f"   Embedding dimension: {dataset['input_embeddings'].shape[1]}")
    print(f"   Generated: {dataset['generation_info']['generation_timestamp']}")
    print(f"   Sources: {', '.join(dataset['generation_info']['sources'])}")
    
    return dataset


def create_training_setup():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    print("üîß Setting up training environment...")
    
    # Energy –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (debug —Ä–µ–∂–∏–º)
    energy_config = create_debug_config()
    set_energy_config(energy_config)
    
    # EnergyTrainer
    trainer = EnergyTrainer(energy_config)
    
    return energy_config, trainer


def create_dataloader_from_dataset(dataset, batch_size: int = 4, shuffle: bool = True):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    from torch.utils.data import DataLoader
    
    # –°–æ–∑–¥–∞–µ–º wrapper –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–∞—Ä–∞–º
    class DatasetWrapper:
        def __init__(self, dataset):
            self.dataset = dataset
            self.length = len(dataset['text_pairs'])
        
        def __len__(self):
            return self.length
        
        def __getitem__(self, idx):
            input_text, target_text = self.dataset['text_pairs'][idx]
            return {
                'input_embedding': self.dataset['input_embeddings'][idx],
                'target_embedding': self.dataset['target_embeddings'][idx],
                'input_text': input_text,
                'target_text': target_text
            }
    
    wrapped_dataset = DatasetWrapper(dataset)
    dataloader = DataLoader(
        wrapped_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        generator=torch.Generator(device=torch.get_default_device()) if shuffle else None,
        collate_fn=lambda batch: {
            'input_embedding': torch.stack([item['input_embedding'] for item in batch]),
            'target_embedding': torch.stack([item['target_embedding'] for item in batch]),
            'input_text': [item['input_text'] for item in batch],
            'target_text': [item['target_text'] for item in batch]
        }
    )
    
    return dataloader


def run_training_demo(dataset_path: str = DATASET_PATH, num_epochs: int = 2):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –≥–æ—Ç–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"""
    print(f"üöÄ Starting training demo ({num_epochs} epochs)")
    print("=" * 60)
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print("\n1Ô∏è‚É£ Loading dataset...")
        dataset = load_dataset(dataset_path)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞
        print("\n2Ô∏è‚É£ Setting up trainer...")
        energy_config, trainer = create_training_setup()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        print("\n3Ô∏è‚É£ Creating DataLoader...")
        dataloader = create_dataloader_from_dataset(
            dataset, 
            batch_size=energy_config.batch_size,
            shuffle=True
        )
        print(f"‚úÖ DataLoader ready: {len(dataloader)} batches")
        
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        print(f"\n4Ô∏è‚É£ Starting training ({num_epochs} epochs)...")
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            print("-" * 40)
            
            epoch_metrics = {
                'total_loss': 0.0,
                'energy_loss': 0.0,
                'text_loss': 0.0,
                'batches_processed': 0
            }
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –±–∞—Ç—á–∞–º
            for batch_idx, batch in enumerate(dataloader):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                input_texts = batch['input_text']
                target_texts = batch['target_text']
                input_embeddings = batch['input_embedding']
                target_embeddings = batch['target_embedding']
                
                # –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                step_metrics = trainer.train_step(
                    input_texts=input_texts,
                    target_texts=target_texts,
                    teacher_input_embeddings=input_embeddings,
                    teacher_target_embeddings=target_embeddings
                )
                
                # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                epoch_metrics['total_loss'] += step_metrics.get('total_loss', 0)
                epoch_metrics['energy_loss'] += step_metrics.get('energy_loss', 0)
                epoch_metrics['text_loss'] += step_metrics.get('text_loss', 0)
                epoch_metrics['batches_processed'] += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                if batch_idx % 5 == 0:  # –ö–∞–∂–¥—ã–π 5-–π –±–∞—Ç—á
                    print(f"  Batch {batch_idx + 1}/{len(dataloader)}: "
                          f"loss={step_metrics.get('total_loss', 0):.4f}")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –¥–µ–º–æ
                if batch_idx >= 10:  # –ú–∞–∫—Å–∏–º—É–º 10 –±–∞—Ç—á–µ–π
                    break
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–µ
            if epoch_metrics['batches_processed'] > 0:
                for key in ['total_loss', 'energy_loss', 'text_loss']:
                    epoch_metrics[key] /= epoch_metrics['batches_processed']
            
            print(f"  Epoch {epoch + 1} completed:")
            print(f"    Total loss: {epoch_metrics['total_loss']:.4f}")
            print(f"    Energy loss: {epoch_metrics['energy_loss']:.4f}")
            print(f"    Text loss: {epoch_metrics['text_loss']:.4f}")
            print(f"    Batches processed: {epoch_metrics['batches_processed']}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        print(f"\n5Ô∏è‚É£ Running post-training validation...")
        
        # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_batch = next(iter(dataloader))
        val_input_texts = val_batch['input_text'][:3]  # –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞
        val_target_texts = val_batch['target_text'][:3]
        val_input_embeddings = val_batch['input_embedding'][:3]
        val_target_embeddings = val_batch['target_embedding'][:3]
        
        val_results = trainer.validate(
            input_texts=val_input_texts,
            target_texts=val_target_texts,
            teacher_input_embeddings=val_input_embeddings,
            teacher_target_embeddings=val_target_embeddings
        )
        
        print(f"‚úÖ Validation completed:")
        print(f"   Validation loss: {val_results.get('total_loss', 'N/A'):.4f}")
        print(f"   Examples generated: {len(val_results.get('examples', []))}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if val_results.get('examples'):
            print(f"\nüìù Prediction examples:")
            for i, example in enumerate(val_results['examples'][:2]):
                print(f"   Example {i+1}:")
                print(f"     Input: '{example['input'][:80]}...'")
                print(f"     Target: '{example['target'][:80]}...'")
                print(f"     Predicted: '{example['predicted'][:80]}...'")
        
        print(f"\nüéâ Training demo completed successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Training demo failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_dataset_loading_only(dataset_path: str = DATASET_PATH):
    """–¢–µ—Å—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è"""
    print("\nüß™ Testing Dataset Loading Only")
    print("-" * 40)
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        dataset = load_dataset(dataset_path)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        dataloader = create_dataloader_from_dataset(dataset, batch_size=2)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –±–∞—Ç—á
        batch = next(iter(dataloader))
        
        print(f"‚úÖ Sample batch loaded:")
        print(f"   Batch size: {len(batch['input_text'])}")
        print(f"   Input embeddings shape: {batch['input_embedding'].shape}")
        print(f"   Target embeddings shape: {batch['target_embedding'].shape}")
        print(f"   Sample input text: '{batch['input_text'][0][:50]}...'")
        print(f"   Sample target text: '{batch['target_text'][0][:50]}...'")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Dataset loading test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Training with pre-generated dataset")
    parser.add_argument("--dataset", type=str, default=DATASET_PATH,
                       help="Path to dataset file")
    parser.add_argument("--epochs", type=int, default=1,
                       help="Number of training epochs")
    parser.add_argument("--test-only", action="store_true",
                       help="Only test dataset loading without training")
    
    args = parser.parse_args()
    
    try:
        if args.test_only:
            # –¢–æ–ª—å–∫–æ —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏
            success = test_dataset_loading_only(args.dataset)
        else:
            # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            success = run_training_demo(args.dataset, args.epochs)
            
            if success:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏
                test_dataset_loading_only(args.dataset)
                
                print(f"\n‚ú® Training completed successfully!")
                print(f"   Dataset: {args.dataset}")
                print(f"   Epochs: {args.epochs}")
        
        if success:
            print(f"\nüéØ Ready for production use!")
        else:
            print(f"\n‚ö†Ô∏è  Issues found, check logs for debugging.")
        
    except KeyboardInterrupt:
        print("\nüõë Interrupted by user")
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()