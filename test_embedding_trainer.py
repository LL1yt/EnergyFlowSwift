#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–≥–æ EmbeddingTrainer
==============================

–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è:
Text ‚Üí DistilBERT ‚Üí EmbeddingTransformer ‚Üí MoE Cube ‚Üí EmbeddingTransformer ‚Üí TextDecoder

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ RTX 5090.
"""

import torch
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(str(Path(__file__).parent))

from new_rebuild.config.simple_config import SimpleProjectConfig
from new_rebuild.core.training.embedding_trainer import create_embedding_trainer
from new_rebuild.utils.logging import get_logger

logger = get_logger(__name__)


def create_synthetic_embedding_dataset(
    config: SimpleProjectConfig, num_samples: int = 100
):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""

    embedding_dim = config.embedding.teacher_embedding_dim  # 768 –¥–ª—è DistilBERT

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    embeddings = []
    target_embeddings = []

    for i in range(num_samples):
        # –ë–∞–∑–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥
        base_embedding = torch.randn(embedding_dim) * 0.1

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        pattern = (
            torch.sin(
                torch.arange(embedding_dim, dtype=torch.float32)
                * (i / num_samples)
                * np.pi
                * 2
            )
            * 0.05
        )

        embedding = base_embedding + pattern

        # –¶–µ–ª–µ–≤–æ–π —ç–º–±–µ–¥–∏–Ω–≥ - —Å–ª–µ–≥–∫–∞ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
        target = embedding + torch.randn(embedding_dim) * 0.02

        embeddings.append(embedding)
        target_embeddings.append(target)

    embeddings = torch.stack(embeddings)
    target_embeddings = torch.stack(target_embeddings)

    logger.info(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {num_samples} –æ–±—Ä–∞–∑—Ü–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {embedding_dim}")
    logger.info(
        f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: mean={embeddings.mean():.4f}, std={embeddings.std():.4f}"
    )

    return embeddings, target_embeddings


def test_trainer_basic():
    """–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ forward pass"""
    print("\n=== –ë–ê–ó–û–í–´–ô –¢–ï–°–¢ EMBEDDING TRAINER ===")

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ DEBUG —É—Ä–æ–≤–µ–Ω—å
    import logging
    logging.basicConfig(level=logging.DEBUG, format='%(levelname)s - %(name)s - %(message)s')
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = SimpleProjectConfig()
    config.training_embedding.test_mode = True
    config.training_embedding.test_quick_iterations = 5
    config.logging.debug_mode = True  # –í–∫–ª—é—á–∞–µ–º debug –ª–æ–≥–∏

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ (–≤—Å–µ —Ä–∞–∑–º–µ—Ä—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞)
    trainer = create_embedding_trainer(config)

    print(f"‚úì –¢—Ä–µ–Ω–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {trainer.device}")
    print(
        f"‚úì –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ EmbeddingTransformer: {trainer.embedding_transformer.get_parameter_count()}"
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    embeddings, targets = create_synthetic_embedding_dataset(config, num_samples=32)

    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    dataset = TensorDataset(embeddings, targets)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    # –¢–µ—Å—Ç forward pass
    print("\n--- –¢–µ—Å—Ç Forward Pass ---")
    trainer.embedding_transformer.eval()
    trainer.lattice.eval()

    with torch.no_grad():
        batch = next(iter(dataloader))
        input_emb, target_emb = batch
        input_emb = input_emb.to(trainer.device)
        target_emb = target_emb.to(trainer.device)
        
        print(f"Input embedding shape: {input_emb.shape}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        surface_emb = trainer.embedding_transformer.transform_to_cube(input_emb)
        print(f"Surface embedding shape after transform: {surface_emb.shape}")
        print(f"Expected shape for lattice_mapper: [batch_size, {config.cube_embedding_dim}]")

        # Forward pass —á–µ—Ä–µ–∑ –≤–µ—Å—å pipeline
        losses = trainer._forward_pass(input_emb, target_emb)

        print(f"‚úì Forward pass —É—Å–ø–µ—à–µ–Ω")
        print(f"  Total Loss: {losses['total'].item():.6f}")
        print(f"  Reconstruction: {losses['reconstruction'].item():.6f}")
        print(f"  Similarity: {losses['similarity'].item():.6f}")
        print(f"  Diversity: {losses['diversity'].item():.6f}")
        print(f"  Emergence: {losses['emergence'].item():.6f}")

    return trainer, dataloader


def test_training_epoch(trainer, dataloader):
    """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
    print("\n--- –¢–µ—Å—Ç Training Epoch ---")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    initial_params = {}
    for name, param in trainer.embedding_transformer.named_parameters():
        if param.requires_grad:
            initial_params[name] = param.data.clone()

    # –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —ç–ø–æ—Ö–∏
    train_losses = trainer.train_epoch(dataloader)

    print(f"‚úì –≠–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"  Total Loss: {train_losses['total']:.6f}")
    print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {train_losses['count']}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–æ–≤–∏–ª–∏—Å—å
    params_updated = False
    for name, param in trainer.embedding_transformer.named_parameters():
        if param.requires_grad and name in initial_params:
            if not torch.equal(param.data, initial_params[name]):
                params_updated = True
                break

    if params_updated:
        print("‚úì –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–æ–≤–∏–ª–∏—Å—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è")
    else:
        print("‚ö† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã–π learning rate)")

    return train_losses


def test_validation_epoch(trainer, dataloader):
    """–¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    print("\n--- –¢–µ—Å—Ç Validation Epoch ---")

    val_losses = trainer.validate_epoch(dataloader)

    print(f"‚úì –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"  Total Loss: {val_losses['total']:.6f}")
    print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {val_losses['count']}")

    return val_losses


def test_checkpoint_save_load(trainer):
    """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ checkpoint'–æ–≤"""
    print("\n--- –¢–µ—Å—Ç Checkpoint Save/Load ---")

    checkpoint_path = "test_checkpoint.pth"

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    trainer.save_checkpoint(checkpoint_path, epoch=1, test_mode=True)
    print(f"‚úì Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {checkpoint_path}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    original_params = {}
    for name, param in trainer.embedding_transformer.named_parameters():
        original_params[name] = param.data.clone()

    # –ò–∑–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–µ–±–æ–ª—å—à–∏–º —à—É–º–æ–º)
    with torch.no_grad():
        for param in trainer.embedding_transformer.parameters():
            param.add_(torch.randn_like(param) * 0.01)

    # –ó–∞–≥—Ä—É–∑–∫–∞
    checkpoint_data = trainer.load_checkpoint(checkpoint_path)
    print(f"‚úì Checkpoint –∑–∞–≥—Ä—É–∂–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å
    params_restored = True
    for name, param in trainer.embedding_transformer.named_parameters():
        if name in original_params:
            if not torch.allclose(param.data, original_params[name], atol=1e-6):
                params_restored = False
                break

    if params_restored:
        print("‚úì –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    else:
        print("‚úó –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

    # –£–¥–∞–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    Path(checkpoint_path).unlink(missing_ok=True)

    return checkpoint_data


def test_performance_monitoring(trainer):
    """–¢–µ—Å—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print("\n--- –¢–µ—Å—Ç Performance Monitoring ---")

    summary = trainer.get_training_summary()

    print(f"‚úì –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {summary['device']}")
    print(f"‚úì –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {summary['total_parameters']:,}")

    if summary["performance_stats"]["avg_total_time"] > 0:
        print(
            f"‚úì –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –±–∞—Ç—á–∞: {summary['performance_stats']['avg_total_time']:.4f}s"
        )
        print(f"  Forward: {summary['performance_stats']['avg_forward_time']:.4f}s")
        print(f"  Backward: {summary['performance_stats']['avg_backward_time']:.4f}s")

    return summary


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï EMBEDDING TRAINER")
    print("=" * 50)

    try:
        # 1. –ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç
        trainer, dataloader = test_trainer_basic()

        # 2. –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è
        train_losses = test_training_epoch(trainer, dataloader)

        # 3. –¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_losses = test_validation_epoch(trainer, dataloader)

        # 4. –¢–µ—Å—Ç checkpoint'–æ–≤
        checkpoint_data = test_checkpoint_save_load(trainer)

        # 5. –¢–µ—Å—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        summary = test_performance_monitoring(trainer)

        print("\n" + "=" * 50)
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –£–°–ü–ï–®–ù–û –ü–†–û–ô–î–ï–ù–´!")
        print("=" * 50)

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(
            f"  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: DistilBERT ‚Üí EmbeddingTransformer ‚Üí MoE Cube ‚Üí TextDecoder"
        )
        print(
            f"  –†–µ—à–µ—Ç–∫–∞: {trainer.config.training_embedding.test_lattice_dim}√ó{trainer.config.training_embedding.test_lattice_dim}√ó{trainer.config.training_embedding.test_lattice_dim}"
        )
        print(f"  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {trainer.device}")
        print(f"  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {summary['total_parameters']:,}")
        print(f"  Train Loss: {train_losses['total']:.6f}")
        print(f"  Val Loss: {val_losses['total']:.6f}")

        if trainer.device.type == "cuda":
            print(f"  GPU Memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

        return True

    except Exception as e:
        print(f"\nüí• –û–®–ò–ë–ö–ê –í –¢–ï–°–¢–ê–•: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
