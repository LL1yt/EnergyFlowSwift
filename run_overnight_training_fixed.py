#!/usr/bin/env python3
"""
–ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï Overnight Training —Å Simple Fallback Embedding Loader
–û–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ transformers
"""

import torch
import torch.nn as nn
import time
import logging
import json
from datetime import datetime
from pathlib import Path
import sys
import os
import signal

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.append(str(Path(__file__).parent))

from training.embedding_trainer.emergent_training_stage_3_1_4_1 import EmergentCubeTrainer
from utils.config_manager.config_manager import ConfigManager
from simple_embedding_fallback import create_dialogue_dataset_simple_fallback
from model_weights_manager import ModelWeightsManager
from config_converter import convert_config_dict_to_object

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Unicode –ø—Ä–æ–±–ª–µ–º—ã –≤ Windows
import sys
import re

class EmojiFilter(logging.Filter):
    """–§–∏–ª—å—Ç—Ä –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —ç–º–æ–¥–∑–∏ –∏–∑ –ª–æ–≥–æ–≤ –Ω–∞ Windows"""
    
    def filter(self, record):
        if sys.platform == 'win32':
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —ç–º–æ–¥–∑–∏ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
            emoji_pattern = re.compile("["
                u"\U0001F600-\U0001F64F"  # emoticons
                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                u"\U00002702-\U000027B0"  # dingbats
                u"\U000024C2-\U0001F251"
                u"\U0001F900-\U0001F9FF"  # supplemental symbols
                u"\U00002600-\U000026FF"  # miscellaneous symbols
                u"\U00002700-\U000027BF"  # dingbats
                "]+", flags=re.UNICODE)
            
            # –ó–∞–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã
            emoji_replacements = {
                'üöÄ': '[START]',
                '‚öôÔ∏è': '[SETUP]',
                'üìö': '[DATA]',
                '‚úÖ': '[OK]',
                'üéØ': '[TARGET]',
                'üèÜ': '[BEST]',
                'üèÅ': '[DONE]',
                '‚ùå': '[ERROR]',
                'üìä': '[STATS]',
                'üíæ': '[SAVE]',
                'üîß': '[DEBUG]',
                'üìà': '[PROGRESS]',
                '‚è∞': '[TIME]',
                'üß™': '[TEST]'
            }
            
            message = record.getMessage()
            for emoji, replacement in emoji_replacements.items():
                message = message.replace(emoji, replacement)
            
            # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–º–æ–¥–∑–∏
            message = emoji_pattern.sub('', message)
            record.msg = message
            record.args = ()
        
        return True

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
console_handler = logging.StreamHandler()
file_handler = logging.FileHandler('logs/overnight_training_fixed.log', encoding='utf-8')

# –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –∫ –∫–æ–Ω—Å–æ–ª—å–Ω–æ–º—É –æ–±—Ä–∞–±–æ—Ç—á–∏–∫—É –Ω–∞ Windows
if sys.platform == 'win32':
    console_handler.addFilter(EmojiFilter())

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[file_handler, console_handler]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –∫–æ –≤—Å–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –ª–æ–≥–≥–µ—Ä–∞–º
if sys.platform == 'win32':
    for name in logging.Logger.manager.loggerDict:
        existing_logger = logging.getLogger(name)
        for handler in existing_logger.handlers:
            if isinstance(handler, logging.StreamHandler) and handler.stream == sys.stdout:
                handler.addFilter(EmojiFilter())

class FixedOvernightTrainer:
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π Overnight Trainer —Å –ø—Ä–æ—Å—Ç—ã–º embedding loader"""
    
    def __init__(self):
        self.trainer = None
        self.dataset = None
        self.config = None
        self.weights_manager = ModelWeightsManager()
        self.should_stop = False
        self.best_similarity = 0.0
        self.training_log = []
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info("üöÄ FixedOvernightTrainer initialized")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –∫ –≤—Å–µ–º –ª–æ–≥–≥–µ—Ä–∞–º –≤–∫–ª—é—á–∞—è trainer –ª–æ–≥–≥–µ—Ä—ã
        if sys.platform == 'win32':
            self._apply_emoji_filter_to_all_loggers()
    
    def _apply_emoji_filter_to_all_loggers(self):
        """–ü—Ä–∏–º–µ–Ω–∏—Ç—å —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –∫–æ –≤—Å–µ–º –ª–æ–≥–≥–µ—Ä–∞–º"""
        emoji_filter = EmojiFilter()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ª–æ–≥–≥–µ—Ä—ã
        all_loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
        all_loggers.append(logging.root)
        
        for logger_obj in all_loggers:
            for handler in logger_obj.handlers:
                if isinstance(handler, logging.StreamHandler):
                    handler.addFilter(emoji_filter)
    
    def _signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful –æ—Å—Ç–∞–Ω–æ–≤–∫–∏"""
        logger.info(f"üì° Received signal {signum}, stopping training gracefully...")
        self.should_stop = True
    
    def setup_training(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("‚öôÔ∏è Setting up training components...")
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_manager = ConfigManager()
        config_dict = config_manager.get_config()  # –ü–æ–ª—É—á–∞–µ–º –≤—Å—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.config = convert_config_dict_to_object(config_dict)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ–±—ä–µ–∫—Ç
        
        # 2. –°–æ–∑–¥–∞–µ–º trainer
        self.trainer = EmergentCubeTrainer(self.config)
        self.trainer.to('cuda' if torch.cuda.is_available() else 'cpu')
        
        # –ü–æ–≤—Ç–æ—Ä–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º —ç–º–æ–¥–∑–∏ —Ñ–∏–ª—å—Ç—Ä –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è trainer
        if sys.platform == 'win32':
            self._apply_emoji_filter_to_all_loggers()
        
        # 3. –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ü–†–û–°–¢–´–ú fallback loader
        logger.info("üìö Creating dataset with SimpleFallbackEmbeddingLoader...")
        dialogue_pairs = [
            {"question": "What is artificial intelligence?", "answer": "AI is the simulation of human intelligence."},
            {"question": "How do neural networks work?", "answer": "Neural networks process data through interconnected layers."},
            {"question": "What is machine learning?", "answer": "ML enables computers to learn from data without explicit programming."},
            {"question": "Explain deep learning.", "answer": "Deep learning uses multi-layered neural networks for complex pattern recognition."},
            {"question": "What are transformers in AI?", "answer": "Transformers are attention-based models for sequence processing."},
            {"question": "How does natural language processing work?", "answer": "NLP enables computers to understand and generate human language."},
            {"question": "What is computer vision?", "answer": "Computer vision enables machines to interpret visual information."},
            {"question": "Explain reinforcement learning.", "answer": "RL trains agents through interaction with an environment."},
            {"question": "What is supervised learning?", "answer": "Supervised learning uses labeled data to train models."},
            {"question": "How do convolutional networks work?", "answer": "CNNs use filters to detect features in spatial data."}
        ]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π fallback –≤–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
        self.dataset = create_dialogue_dataset_simple_fallback(
            dialogue_pairs,
            teacher_model="distilbert-base-uncased",
            normalize_embeddings=True
        )
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ
        sample = self.dataset[0]
        q_emb, a_emb = sample
        logger.info(f"‚úÖ Dataset created successfully:")
        logger.info(f"   Question embedding norm: {q_emb.norm().item():.6f}")
        logger.info(f"   Answer embedding norm: {a_emb.norm().item():.6f}")
        logger.info(f"   Dataset size: {len(self.dataset)}")
        
        if q_emb.norm().item() < 0.1 or a_emb.norm().item() < 0.1:
            raise ValueError("Dataset still contains zero embeddings!")
        
        logger.info("‚úÖ Training setup completed successfully")
    
    def run_training(self, max_epochs: int = 999999, batch_size: int = 1024):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üéØ Starting overnight training:")
        logger.info(f"   Max epochs: {max_epochs}")
        logger.info(f"   Batch size: {batch_size}")
        logger.info(f"   Device: {next(self.trainer.parameters()).device}")
        
        # –°–æ–∑–¥–∞–µ–º DataLoader
        from torch.utils.data import DataLoader
        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = torch.optim.AdamW(self.trainer.parameters(), lr=0.0001)
        
        epoch = 0
        start_time = time.time()
        
        try:
            while epoch < max_epochs and not self.should_stop:
                epoch_start_time = time.time()
                
                # Training epoch
                total_loss = 0.0
                total_similarity = 0.0
                num_batches = 0
                
                for batch_idx, (question_emb, answer_emb) in enumerate(dataloader):
                    if self.should_stop:
                        break
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ device
                    device = next(self.trainer.parameters()).device
                    question_emb = question_emb.to(device)
                    answer_emb = answer_emb.to(device)
                    
                    # Forward pass
                    optimizer.zero_grad()
                    outputs = self.trainer.forward(question_emb)
                    
                    # –ö–†–ò–¢–ò–ß–ù–û: –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º target embedding –∫ 225D —á–µ—Ä–µ–∑ —Ç–æ—Ç –∂–µ –∞–¥–∞–ø—Ç–µ—Ä
                    with torch.no_grad():
                        adapted_target = self.trainer.base_trainer.adapter(answer_emb)
                    
                    # Targets
                    targets = {
                        'target_embedding': adapted_target,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π target 225D
                        'target_surface': outputs['input_surface']
                    }
                    
                    # Loss computation
                    losses = self.trainer.compute_loss(outputs, targets)
                    
                    # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ loss'–æ–≤ –≤ —Å–∫–∞–ª—è—Ä
                    total_loss_tensor = torch.tensor(0.0, device=device, requires_grad=True)
                    for loss_name, loss_value in losses.items():
                        if torch.is_tensor(loss_value) and loss_value.requires_grad:
                            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ loss —Å–∫–∞–ª—è—Ä
                            if loss_value.dim() > 0:
                                loss_value = loss_value.mean()
                            total_loss_tensor = total_loss_tensor + loss_value
                    
                    # Backward pass
                    total_loss_tensor.backward()
                    optimizer.step()
                    
                    # Metrics - –∏—Å–ø–æ–ª—å–∑—É–µ–º adapted target –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                    with torch.no_grad():
                        similarity = torch.cosine_similarity(
                            outputs['final_output'], 
                            adapted_target,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π target 225D
                            dim=-1
                        ).mean().item()
                    
                    total_loss += total_loss_tensor.item()
                    total_similarity += similarity
                    num_batches += 1
                
                # Epoch metrics
                avg_loss = total_loss / max(num_batches, 1)
                avg_similarity = total_similarity / max(num_batches, 1)
                epoch_time = time.time() - epoch_start_time
                
                # Logging
                epoch += 1
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö –∏–ª–∏ –µ—Å–ª–∏ loss –∏–∑–º–µ–Ω–∏–ª—Å—è
                if epoch % 10 == 0 or epoch <= 5:
                    logger.info(f"Epoch {epoch:4d} | "
                              f"Loss: {avg_loss:.6f} | "
                              f"Similarity: {avg_similarity:.4f} | "
                              f"Time: {epoch_time:.1f}s")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                log_entry = {
                    'epoch': epoch,
                    'loss': avg_loss,
                    'similarity': avg_similarity,
                    'time': epoch_time,
                    'timestamp': datetime.now().isoformat()
                }
                self.training_log.append(log_entry)
                
                # Best model tracking
                if avg_similarity > self.best_similarity:
                    self.best_similarity = avg_similarity
                    logger.info(f"üèÜ New best similarity: {avg_similarity:.4f}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    self.weights_manager.save_latest_weights(
                        self.trainer, 
                        self.config.to_dict(),  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict –¥–ª—è JSON
                        metadata={
                            'epoch': epoch,
                            'loss': avg_loss,
                            'similarity': avg_similarity,
                            'training_type': 'overnight_fixed'
                        }
                    )
                
                # Checkpoint –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö
                if epoch % 50 == 0:
                    self.weights_manager.create_training_checkpoint(
                        self.trainer, self.config.to_dict(), epoch, avg_loss, avg_similarity,
                        metadata={'training_type': 'overnight_fixed'}
                    )
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Ç—Ä—è—Å–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                if avg_similarity > 0.6:
                    logger.info(f"üéâ EXCELLENT RESULTS! Similarity > 60%")
                elif avg_similarity > 0.45:
                    logger.info(f"üéØ GREAT PROGRESS! Similarity > 45%")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –∫–∞–∂–¥—ã–µ 100 —ç–ø–æ—Ö
                if epoch % 100 == 0:
                    self._save_training_log()
        
        except KeyboardInterrupt:
            logger.info("Training interrupted by user")
        except Exception as e:
            logger.error(f"Training error: {e}")
            raise
        finally:
            self._finalize_training(epoch, time.time() - start_time)
    
    def _save_training_log(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        log_path = f"logs/overnight_training_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(log_path, 'w') as f:
            json.dump(self.training_log, f, indent=2)
    
    def _finalize_training(self, final_epoch: int, total_time: float):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üèÅ Training completed:")
        logger.info(f"   Final epoch: {final_epoch}")
        logger.info(f"   Total time: {total_time/3600:.1f} hours")
        logger.info(f"   Best similarity: {self.best_similarity:.4f}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_training_log()
        
        # –°–æ–∑–¥–∞–µ–º milestone checkpoint
        self.weights_manager.create_milestone_checkpoint(
            self.trainer, self.config.to_dict(),  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict –¥–ª—è JSON
            f"overnight_fixed_final_{final_epoch}",
            {
                'final_epoch': final_epoch,
                'total_time_hours': total_time/3600,
                'best_similarity': self.best_similarity,
                'total_batches': len(self.training_log)
            }
        )
        
        logger.info("‚úÖ Training finalization completed")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üåô –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï OVERNIGHT TRAINING")
    print("="*50)
    print("Using SimpleFallbackEmbeddingLoader")
    print("No early stopping - run until manually stopped")
    print("Optimal batch_size 1024 for RTX 5090")
    print("="*50)
    
    trainer = FixedOvernightTrainer()
    
    try:
        trainer.setup_training()
        trainer.run_training(
            max_epochs=999999,  # Unlimited
            batch_size=1024     # Optimal for RTX 5090
        )
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main()) 