"""
[START] Dynamic Training Script —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Dynamic Configuration System –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∂–µ–ª–µ–∑–∞
"""

import os
import sys
import torch
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional
import gc
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def setup_environment():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PyTorch
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–º—è—Ç–∏
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.set_per_process_memory_fraction(0.9)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 90% GPU –ø–∞–º—è—Ç–∏

    logger.info("[CONFIG] Environment setup completed")


class DynamicTrainingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""

    def __init__(
        self,
        forced_mode: Optional[str] = None,
        custom_scale: Optional[float] = None,
        external_config: Optional[Dict] = None,
    ):
        """
        Args:
            forced_mode: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º ("development", "research", etc.)
            custom_scale: –ö–∞—Å—Ç–æ–º–Ω—ã–π –º–∞—Å—à—Ç–∞–± (–ø–µ—Ä–µ–∫—Ä—ã–≤–∞–µ—Ç —Ä–µ–∂–∏–º)
            external_config: –ì–æ—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ—Ç smart_resume_training)
        """
        self.forced_mode = forced_mode
        self.custom_scale_factor = custom_scale
        self.external_config = external_config
        self.trainer = None
        self.config_manager = None
        self.dynamic_config = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if self.external_config:
            logger.info(f"[CONFIG] Using external configuration")
            self.dynamic_config = self.external_config
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∫–∏–µ —Å–µ–∫—Ü–∏–∏ –ø–æ–ª—É—á–∏–ª–∏
            available_sections = list(self.external_config.keys())
            logger.info(f"   Available sections: {available_sections}")
            if "emergent_training" in self.external_config:
                logger.info(
                    f"   emergent_training.spatial_propagation_depth: {self.external_config['emergent_training'].get('spatial_propagation_depth', 'unknown')}"
                )
        else:
            self._load_dynamic_config()

    def _load_dynamic_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            from utils.config_manager.config_manager import (
                ConfigManager,
                ConfigManagerSettings,
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å –≤–∫–ª—é—á–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            settings = ConfigManagerSettings(
                enable_dynamic_config=True,
                dynamic_config_mode=self.forced_mode or "auto",
                auto_hardware_detection=True,
                enable_hot_reload=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
                # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤
                config_search_paths=[],  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ = —Ç–æ–ª—å–∫–æ main_config.yaml + –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞
                # –ü–µ—Ä–µ–¥–∞–µ–º custom scale —Å—Ä–∞–∑—É –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                custom_scale_factor=self.custom_scale_factor,
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ ConfigManager
            self.config_manager = ConfigManager(settings)

            # Custom scale —É–∂–µ –ø–µ—Ä–µ–¥–∞–Ω –≤ settings.custom_scale_factor
            # –î—É–±–ª–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤ _apply_custom_scale() —É–±—Ä–∞–Ω

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.dynamic_config = {
                "lattice": self.config_manager.get_config("lattice"),
                "embeddings": self.config_manager.get_config("embeddings"),
                "training": self.config_manager.get_config("training"),
                "gmlp": self.config_manager.get_config("gmlp"),
            }

            # –î–û–ë–ê–í–õ–Ø–ï–ú emergent_training —Å–µ–∫—Ü–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            try:
                emergent_training_config = self.config_manager.get_config(
                    "emergent_training"
                )
                if emergent_training_config:
                    self.dynamic_config["emergent_training"] = emergent_training_config
                    logger.info(
                        f"[CONFIG] Added emergent_training section to dynamic config"
                    )
                    logger.info(
                        f"   spatial_propagation_depth: {emergent_training_config.get('spatial_propagation_depth', 'unknown')}"
                    )
                else:
                    logger.warning(
                        f"[WARNING] emergent_training section is empty in dynamic config"
                    )
            except Exception as e:
                logger.warning(
                    f"[WARNING] Failed to load emergent_training section: {e}"
                )
                logger.warning(f"   Will use fallback approach in create_trainer()")

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∂–∏–º–µ
            dynamic_info = self.config_manager.get_dynamic_config_info()
            if dynamic_info:
                logger.info(
                    f"[TARGET] Loaded dynamic config: {dynamic_info['mode']} mode"
                )
                logger.info(f"   Scale factor: {dynamic_info['scale_factor']}")

            # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            lattice = self.dynamic_config["lattice"]
            embeddings = self.dynamic_config["embeddings"]
            training = self.dynamic_config["training"]

            logger.info(f"[DATA] Configuration loaded:")
            logger.info(f"   Lattice: {lattice['xs']}x{lattice['ys']}x{lattice['zs']}")
            logger.info(f"   Total neurons: {lattice['total_neurons']:,}")
            logger.info(f"   Embedding dim: {embeddings['embedding_dim']:,}")
            logger.info(f"   Batch size: {training['batch_size']}")
            logger.info(f"   Learning rate: {training['learning_rate']}")

        except Exception as e:
            logger.error(f"[ERROR] Failed to load dynamic config: {e}")
            raise

    # –ú–µ—Ç–æ–¥ _apply_custom_scale() —É–¥–∞–ª–µ–Ω - —Ç–µ–ø–µ—Ä—å custom scale
    # –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ ConfigManagerSettings.custom_scale_factor

    def create_trainer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ trainer —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        try:
            from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
                EmergentCubeTrainer,
                EmergentTrainingConfig,
            )
            from training.embedding_trainer.neural_cellular_automata import (
                create_nca_config,
            )

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º emergent_training —Å–µ–∫—Ü–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ fallback
            if "emergent_training" in self.dynamic_config:
                emergent_config = self.dynamic_config["emergent_training"]

                # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ emergent_training –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                logger.info(
                    f"[CONFIG] Using emergent_training section from dynamic config"
                )
                logger.info(f"   cube_dimensions: {emergent_config['cube_dimensions']}")
                logger.info(
                    f"   spatial_propagation_depth: {emergent_config['spatial_propagation_depth']}"
                )

                # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è EmergentTrainingConfig –∏–∑ emergent_training —Å–µ–∫—Ü–∏–∏
                trainer_config = EmergentTrainingConfig(
                    teacher_model=emergent_config.get(
                        "teacher_model", "distilbert-base-uncased"
                    ),
                    cube_dimensions=tuple(emergent_config["cube_dimensions"]),
                    # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º spatial_propagation_depth –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    spatial_propagation_depth=emergent_config[
                        "spatial_propagation_depth"
                    ],
                    enable_full_cube_gradient=emergent_config.get(
                        "enable_full_cube_gradient", True
                    ),
                    emergent_specialization=emergent_config.get(
                        "emergent_specialization", True
                    ),
                    # gMLP –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã
                    gmlp_config=emergent_config["gmlp_config"],
                    # Loss weights
                    loss_weights=emergent_config.get(
                        "loss_weights",
                        {
                            "surface_reconstruction": 0.3,
                            "internal_consistency": 0.3,
                            "dialogue_similarity": 0.4,
                        },
                    ),
                    # Training –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    learning_rate=emergent_config["learning_rate"],
                    batch_size=emergent_config["batch_size"],
                    epochs=emergent_config["epochs"],
                    warmup_epochs=emergent_config.get("warmup_epochs", 3),
                    # Optimization settings
                    gradient_balancing=emergent_config.get("gradient_balancing", True),
                    adaptive_loss_weighting=emergent_config.get(
                        "adaptive_loss_weighting", True
                    ),
                    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                    mixed_precision=True,
                    gradient_checkpointing=True,
                    gradient_accumulation_steps=4,
                    # NCA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
                    enable_nca=True,
                    nca_config=create_nca_config(
                        update_probability=0.7,
                        residual_learning_rate=0.1,
                        enable_pattern_detection=True,
                    ),
                )

                logger.info(
                    f"[BRAIN] EmergentTrainingConfig from dynamic emergent_training:"
                )
                logger.info(f"   cube_dimensions: {trainer_config.cube_dimensions}")
                logger.info(
                    f"   spatial_propagation_depth: {trainer_config.spatial_propagation_depth}"
                )
                logger.info(
                    f"   gmlp_config.state_size: {trainer_config.gmlp_config.get('state_size')}"
                )
                logger.info(
                    f"   gmlp_config.target_params: {trainer_config.gmlp_config.get('target_params')}"
                )

            else:
                # FALLBACK: –°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± –µ—Å–ª–∏ emergent_training —Å–µ–∫—Ü–∏–∏ –Ω–µ—Ç
                logger.warning(
                    f"[WARNING] emergent_training section not found, using fallback approach"
                )

                lattice_config = self.dynamic_config["lattice"]
                gmlp_config = self.dynamic_config["gmlp"]
                training_config = self.dynamic_config["training"]

                # –õ–æ–≥–∏—Ä—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.info(f"[MAGNIFY] Dynamic gMLP config from generator:")
                logger.info(f"   target_params: {gmlp_config.get('target_params')}")
                logger.info(f"   state_size: {gmlp_config.get('state_size')}")
                logger.info(f"   hidden_dim: {gmlp_config.get('hidden_dim')}")
                logger.info(
                    f"   external_input_size: {gmlp_config.get('external_input_size')}"
                )
                logger.info(f"   memory_dim: {gmlp_config.get('memory_dim')}")

                # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è EmergentTrainingConfig
                trainer_config = EmergentTrainingConfig(
                    teacher_model="distilbert-base-uncased",
                    cube_dimensions=(
                        lattice_config["xs"],
                        lattice_config["ys"],
                        lattice_config["zs"],
                    ),
                    # –í–ê–ñ–ù–û: spatial_propagation_depth –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–µ–Ω zs
                    spatial_propagation_depth=lattice_config["zs"],
                    # gMLP –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã
                    gmlp_config={
                        "state_size": gmlp_config["state_size"],
                        "neighbor_count": gmlp_config["neighbor_count"],
                        "hidden_dim": gmlp_config["hidden_dim"],
                        "external_input_size": gmlp_config["external_input_size"],
                        "memory_dim": gmlp_config.get("memory_dim", 16),
                        "target_params": gmlp_config[
                            "target_params"
                        ],  # –ù–û–í–û–ï: –ü–µ—Ä–µ–¥–∞–µ–º –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π target
                        "use_memory": True,
                        "activation": "gelu",
                        "dropout": 0.1,
                        "spatial_connections": True,
                    },
                    # Training –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    learning_rate=training_config["learning_rate"],
                    batch_size=training_config["batch_size"],
                    epochs=training_config["epochs"],
                    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                    mixed_precision=True,
                    gradient_checkpointing=True,
                    gradient_accumulation_steps=4,
                    # NCA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
                    enable_nca=True,
                    nca_config=create_nca_config(
                        update_probability=0.7,
                        residual_learning_rate=0.1,
                        enable_pattern_detection=True,
                    ),
                )

            # –°–æ–∑–¥–∞–µ–º trainer
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.trainer = EmergentCubeTrainer(trainer_config, device=device)

            logger.info(f"[OK] Trainer created successfully")
            logger.info(f"   Device: {device}")
            logger.info(f"   Mixed precision: {trainer_config.mixed_precision}")
            logger.info(
                f"   Gradient checkpointing: {trainer_config.gradient_checkpointing}"
            )

            return self.trainer

        except Exception as e:
            logger.error(f"[ERROR] Failed to create trainer: {e}")
            raise

    def prepare_dataset(
        self, limit: Optional[int] = None, fixed_sampling: bool = False
    ):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º precomputed embeddings"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É PrecomputedEmbeddingLoader
            import sys
            from pathlib import Path

            sys.path.append(str(Path(__file__).parent))

            from precomputed_embedding_loader import PrecomputedEmbeddingLoader

            # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
            loader = PrecomputedEmbeddingLoader()

            # –ù–∞—Ö–æ–¥–∏–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
            datasets = loader.list_available_datasets()
            if not datasets:
                raise FileNotFoundError(
                    "No precomputed datasets found! Run generate_large_embedding_dataset.py first."
                )

            # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π –∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç
            latest_dataset = datasets[0]
            embeddings_file = latest_dataset["file_path"]

            logger.info(
                f"[FOLDER] Using precomputed dataset: {latest_dataset['filename']}"
            )
            logger.info(f"   Available size: {latest_dataset['size']} pairs")
            logger.info(f"   Teacher model: {latest_dataset['teacher_model']}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = loader.load_dataset(embeddings_file)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if limit and limit < len(dataset):
                from torch.utils.data import Subset
                import torch

                # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –≤—ã–±–æ—Ä–∫–∏
                if fixed_sampling:
                    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ (–ø–µ—Ä–≤—ã–µ N –ø—Ä–∏–º–µ—Ä–æ–≤)
                    indices = torch.arange(limit)
                    logger.info(f"   Using fixed sampling (first {limit} examples)")
                else:
                    # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
                    torch.manual_seed(42)
                    indices = torch.randperm(len(dataset))[:limit]
                    logger.info(f"   Using random sampling with seed=42")
                dataset = Subset(dataset, indices)
                logger.info(f"   Limited to: {limit} pairs (reproducible with seed=42)")

            logger.info(f"[FOLDER] Dataset prepared:")
            logger.info(f"   Final size: {len(dataset)} pairs")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–∑–µ—Ü
            sample = dataset[0]
            q_emb, a_emb = sample
            logger.info(f"   Question embedding shape: {q_emb.shape}")
            logger.info(f"   Answer embedding shape: {a_emb.shape}")
            logger.info(f"   Question norm: {q_emb.norm().item():.6f}")
            logger.info(f"   Answer norm: {a_emb.norm().item():.6f}")

            return dataset

        except Exception as e:
            logger.error(f"[ERROR] Failed to prepare dataset: {e}")
            raise

    def run_training(
        self,
        dataset_limit: Optional[int] = None,
        epochs: int = None,
        batch_size: int = None,
        resume_trainer: Optional[Any] = None,
        start_epoch: int = 0,
        fixed_sampling: bool = False,
    ):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
            setup_environment()

            # –°–æ–∑–¥–∞–Ω–∏–µ trainer –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ
            if resume_trainer is not None:
                trainer = resume_trainer
                logger.info(
                    f"[REFRESH] Using resumed trainer (starting from epoch {start_epoch + 1})"
                )
            else:
                trainer = self.create_trainer()
                logger.info(f"üÜï Created fresh trainer")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            dataset = self.prepare_dataset(
                limit=dataset_limit, fixed_sampling=fixed_sampling
            )

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
            if epochs is None:
                epochs = self.dynamic_config["training"]["epochs"]
            if batch_size is None:
                batch_size = self.dynamic_config["training"]["batch_size"]

            # –°–æ–∑–¥–∞–µ–º DataLoader
            from torch.utils.data import DataLoader

            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0,  # 0 –¥–ª—è Windows —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                pin_memory=True if torch.cuda.is_available() else False,
            )

            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            optimizer = torch.optim.AdamW(
                trainer.parameters(),
                lr=self.dynamic_config["training"]["learning_rate"],
            )

            # Warm-up scheduler (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–ª—è resume)
            from warmup_scheduler import create_warmup_scheduler

            warmup_scheduler = create_warmup_scheduler(
                optimizer=optimizer,
                is_resume=(resume_trainer is not None),
                warmup_epochs=3,  # 3 —ç–ø–æ—Ö–∏ warm-up –ø—Ä–∏ resume
            )

            # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            logger.info(f"[START] Starting dynamic training:")
            logger.info(f"   Dataset size: {len(dataset)}")
            logger.info(f"   Epochs: {epochs}")
            logger.info(f"   Batch size: {batch_size}")
            logger.info(
                f"   Estimated time: {self._estimate_training_time(len(dataset), epochs)}"
            )

            start_time = time.time()
            best_similarity = 0.0
            training_log = []

            for epoch in range(start_epoch, start_epoch + epochs):
                epoch_start = time.time()

                # Warm-up scheduler step (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if warmup_scheduler:
                    warmup_factor = warmup_scheduler.step(epoch + 1)
                    warmup_info = warmup_scheduler.get_warmup_info()

                    if warmup_info["is_warmup_phase"]:
                        logger.info(
                            f"[WARMUP] Epoch {epoch+1}: LR={warmup_info['current_lr']:.6f} "
                            f"(factor={warmup_factor:.2f})"
                        )

                # Training epoch
                total_loss = 0.0
                total_similarity = 0.0
                num_batches = 0

                for batch_idx, (question_emb, answer_emb) in enumerate(dataloader):
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ device
                    device = next(trainer.parameters()).device
                    question_emb = question_emb.to(device)
                    answer_emb = answer_emb.to(device)

                    # Forward pass
                    optimizer.zero_grad()
                    outputs = trainer.forward(question_emb)

                    # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º target embedding
                    with torch.no_grad():
                        adapted_target = trainer.base_trainer.adapter(answer_emb)

                    # Targets
                    targets = {
                        "target_embedding": adapted_target,
                        "target_surface": outputs["input_surface"],
                    }

                    # Loss computation
                    losses = trainer.compute_loss(outputs, targets)

                    # –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ loss'–æ–≤
                    total_loss_tensor = torch.tensor(
                        0.0, device=device, requires_grad=True
                    )
                    for loss_name, loss_value in losses.items():
                        if torch.is_tensor(loss_value) and loss_value.requires_grad:
                            if loss_value.dim() > 0:
                                loss_value = loss_value.mean()
                            total_loss_tensor = total_loss_tensor + loss_value

                    # Backward pass
                    total_loss_tensor.backward()
                    torch.nn.utils.clip_grad_norm_(trainer.parameters(), max_norm=1.0)
                    optimizer.step()

                    # Metrics
                    with torch.no_grad():
                        similarity = (
                            torch.cosine_similarity(
                                outputs["final_output"], adapted_target, dim=-1
                            )
                            .mean()
                            .item()
                        )

                    total_loss += total_loss_tensor.item()
                    total_similarity += similarity
                    num_batches += 1

                # Epoch metrics
                avg_loss = total_loss / max(num_batches, 1)
                avg_similarity = total_similarity / max(num_batches, 1)
                epoch_time = time.time() - epoch_start

                # –õ–æ–≥ –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö –∏–ª–∏ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
                if (
                    epoch % 5 == 0
                    or epoch <= start_epoch + 10
                    or avg_similarity > best_similarity
                ):
                    total_epochs = start_epoch + epochs

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è warm-up —Ñ–∞–∑—ã
                    warmup_info_str = ""
                    if (
                        warmup_scheduler
                        and warmup_scheduler.get_warmup_info()["is_warmup_phase"]
                    ):
                        current_lr = warmup_scheduler.get_current_lr()
                        warmup_info_str = f" | LR: {current_lr:.6f} [WARMUP]"

                    logger.info(
                        f"Epoch {epoch+1:3d}/{total_epochs} | "
                        f"Loss: {avg_loss:.6f} | "
                        f"Similarity: {avg_similarity:.4f} | "
                        f"Time: {epoch_time:.1f}s{warmup_info_str}"
                    )

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                training_log.append(
                    {
                        "epoch": epoch + 1,
                        "loss": avg_loss,
                        "similarity": avg_similarity,
                        "time": epoch_time,
                    }
                )

                # Best model tracking
                if avg_similarity > best_similarity:
                    best_similarity = avg_similarity
                    logger.info(
                        f"[BEST] New best similarity: {avg_similarity:.4f} (epoch {epoch+1})"
                    )

                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
                if (epoch + 1) % 10 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()

            total_time = time.time() - start_time

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º scale
            self._save_training_results(
                trainer, len(dataset), epochs, best_similarity, total_time, training_log
            )

            logger.info(f"[SUCCESS] Training completed:")
            logger.info(f"   Total time: {total_time/60:.1f} minutes")
            logger.info(f"   Final similarity: {avg_similarity:.4f}")
            logger.info(f"   Best similarity: {best_similarity:.4f}")

            return {
                "final_similarity": avg_similarity,
                "best_similarity": best_similarity,
                "total_time": total_time,
                "epochs": epochs,
                "dataset_size": len(dataset),
                # –£–±–∏—Ä–∞–µ–º training_log –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª
                "log_saved": True,
            }

        except Exception as e:
            logger.error(f"[ERROR] Training failed: {e}")
            raise

    def _save_training_results(
        self,
        trainer,
        dataset_size: int,
        epochs: int,
        best_similarity: float,
        total_time: float,
        training_log: list,
    ):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º scale –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏"""
        try:
            from model_weights_manager import ModelWeightsManager

            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            dynamic_info = self.config_manager.get_dynamic_config_info()
            mode = dynamic_info.get("mode", "unknown") if dynamic_info else "unknown"
            scale_factor = (
                dynamic_info.get("scale_factor", "unknown")
                if dynamic_info
                else "unknown"
            )

            # –°–æ–∑–¥–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ –∏–º—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º scale
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_name = f"dynamic_{mode}_scale{scale_factor}_{dataset_size}pairs_{epochs}epochs_{best_similarity:.3f}sim_{timestamp}"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
            if best_similarity > 0.1:
                weights_manager = ModelWeightsManager()

                logger.info(f"[SAVE] Saving dynamic training result: {result_name}")

                weights_manager.create_milestone_checkpoint(
                    trainer,
                    self.config_manager.get_config(),
                    result_name,
                    {
                        "training_type": "dynamic_training",
                        "mode": mode,
                        "scale_factor": scale_factor,
                        "cube_dimensions": f"{self.dynamic_config['lattice']['xs']}x{self.dynamic_config['lattice']['ys']}x{self.dynamic_config['lattice']['zs']}",
                        "total_neurons": self.dynamic_config["lattice"][
                            "total_neurons"
                        ],
                        "embedding_dim": self.dynamic_config["embeddings"][
                            "embedding_dim"
                        ],
                        "dataset_size": dataset_size,
                        "epochs": epochs,
                        "best_similarity": best_similarity,
                        "total_time_minutes": total_time / 60,
                        "timestamp": timestamp,
                        "description": f"Dynamic training {mode} mode (scale={scale_factor}), {dataset_size} pairs, {epochs} epochs, best similarity {best_similarity:.3f}",
                    },
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è
                import json

                # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é logs –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                logs_dir = Path("logs")
                logs_dir.mkdir(exist_ok=True)

                log_path = (
                    f"logs/dynamic_training_{mode}_scale{scale_factor}_{timestamp}.json"
                )
                with open(log_path, "w") as f:
                    json.dump(
                        {
                            "training_info": {
                                "type": "dynamic_training",
                                "mode": mode,
                                "scale_factor": scale_factor,
                                "dataset_size": dataset_size,
                                "epochs": epochs,
                                "best_similarity": best_similarity,
                                "total_time_minutes": total_time / 60,
                            },
                            "dynamic_config": self.dynamic_config,
                            "training_log": training_log,
                        },
                        f,
                        indent=2,
                    )

                logger.info(f"[INFO] Detailed training log saved: {log_path}")

                logger.info(f"[OK] Results saved with scale indication: {result_name}")
            else:
                logger.info(
                    f"[WARNING] Similarity too low ({best_similarity:.3f}), skipping checkpoint save"
                )

        except Exception as e:
            logger.error(f"[ERROR] Failed to save training results: {e}")

    def _estimate_training_time(self, dataset_size: int, epochs: int) -> str:
        """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è"""
        lattice = self.dynamic_config["lattice"]
        total_neurons = lattice["total_neurons"]

        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ —Ä–µ—à–µ—Ç–∫–∏
        if total_neurons < 1000:
            time_per_epoch = 5  # —Å–µ–∫—É–Ω–¥
        elif total_neurons < 50000:
            time_per_epoch = 15
        elif total_neurons < 500000:
            time_per_epoch = 60
        elif total_neurons < 2000000:
            time_per_epoch = 180  # 3 –º–∏–Ω—É—Ç—ã
        else:
            time_per_epoch = 300  # 5 –º–∏–Ω—É—Ç

        total_minutes = (time_per_epoch * epochs) / 60

        if total_minutes < 60:
            return f"~{total_minutes:.0f} minutes"
        else:
            return f"~{total_minutes/60:.1f} hours"


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Dynamic Training Script with Precomputed Embeddings"
    )
    parser.add_argument(
        "--mode",
        choices=["auto", "development", "research", "validation", "production"],
        default="development",
        help="Configuration mode (default: development for scale=0.01)",
    )
    parser.add_argument(
        "--dataset-limit",
        type=int,
        default=None,
        help="Limit dataset size (uses full dataset if not specified)",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="Number of epochs (uses config default if not specified)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Batch size (uses config default if not specified)",
    )
    parser.add_argument(
        "--scale",
        type=float,
        default=None,
        help="Custom scale factor (overrides mode default)",
    )
    parser.add_argument(
        "--fixed-sampling",
        action="store_true",
        help="Use fixed sampling instead of random (reproducible resume)",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –æ–±—É—á–µ–Ω–∏—è
        training_manager = DynamicTrainingManager(
            forced_mode=args.mode if args.mode != "auto" else None,
            custom_scale=args.scale,
        )

        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        results = training_manager.run_training(
            dataset_limit=args.dataset_limit,
            epochs=args.epochs,
            batch_size=args.batch_size,
            fixed_sampling=args.fixed_sampling,
        )

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("[CHART] Training Results:")
        for key, value in results.items():
            logger.info(f"   {key}: {value}")

        logger.info("[OK] Dynamic training completed successfully!")

    except KeyboardInterrupt:
        logger.info("[STOP] Training interrupted by user")
    except Exception as e:
        logger.error(f"[ERROR] Training failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
