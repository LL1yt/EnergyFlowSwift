"""
üöÄ Dynamic Training Script —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Dynamic Configuration System –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∂–µ–ª–µ–∑–∞
"""

import os
import sys
import torch
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional
import gc

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def setup_environment():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PyTorch
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–º—è—Ç–∏
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.set_per_process_memory_fraction(0.9)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 90% GPU –ø–∞–º—è—Ç–∏

    logger.info("üîß Environment setup completed")


class DynamicTrainingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""

    def __init__(
        self, forced_mode: Optional[str] = None, custom_scale: Optional[float] = None
    ):
        """
        Args:
            forced_mode: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (development, research, validation, production)
                        –ï—Å–ª–∏ None, –±—É–¥–µ—Ç –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            custom_scale: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π scale factor (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        """
        self.forced_mode = forced_mode
        self.custom_scale = custom_scale
        self.config_manager = None
        self.dynamic_config = None
        self.trainer = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self._load_dynamic_config()

    def _load_dynamic_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            from utils.config_manager.config_manager import (
                ConfigManager,
                ConfigManagerSettings,
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å –≤–∫–ª—é—á–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            settings = ConfigManagerSettings(
                enable_dynamic_config=True,
                dynamic_config_mode=self.forced_mode or "auto",
                auto_hardware_detection=True,
                enable_hot_reload=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ ConfigManager
            self.config_manager = ConfigManager(settings)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º custom scale –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if self.custom_scale:
                self._apply_custom_scale()

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.dynamic_config = {
                "lattice": self.config_manager.get_config("lattice"),
                "embeddings": self.config_manager.get_config("embeddings"),
                "training": self.config_manager.get_config("training"),
                "gmlp": self.config_manager.get_config("gmlp"),
            }

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∂–∏–º–µ
            dynamic_info = self.config_manager.get_dynamic_config_info()
            if dynamic_info:
                logger.info(f"üéØ Loaded dynamic config: {dynamic_info['mode']} mode")
                logger.info(f"   Scale factor: {dynamic_info['scale_factor']}")

            # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            lattice = self.dynamic_config["lattice"]
            embeddings = self.dynamic_config["embeddings"]
            training = self.dynamic_config["training"]

            logger.info(f"üìä Configuration loaded:")
            logger.info(f"   Lattice: {lattice['xs']}x{lattice['ys']}x{lattice['zs']}")
            logger.info(f"   Total neurons: {lattice['total_neurons']:,}")
            logger.info(f"   Embedding dim: {embeddings['embedding_dim']:,}")
            logger.info(f"   Batch size: {training['batch_size']}")
            logger.info(f"   Learning rate: {training['learning_rate']}")

        except Exception as e:
            logger.error(f"‚ùå Failed to load dynamic config: {e}")
            raise

    def _apply_custom_scale(self):
        """–ü—Ä–∏–º–µ–Ω–∏—Ç—å custom scale factor"""
        try:
            from utils.config_manager.dynamic_config import DynamicConfigManager

            # –°–æ–∑–¥–∞–µ–º dynamic config manager
            dynamic_manager = DynamicConfigManager()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è custom scale
            mode = self.forced_mode or "development"

            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏–∑–º–µ–Ω—è–µ–º scale –≤ dynamic manager
            original_scale = getattr(dynamic_manager.generator.scale_settings, mode)
            setattr(dynamic_manager.generator.scale_settings, mode, self.custom_scale)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            new_config = dynamic_manager.create_config_for_mode(mode)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π ConfigManager
            self.config_manager.merge_dynamic_config(new_config)

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π scale
            setattr(dynamic_manager.generator.scale_settings, mode, original_scale)

            logger.info(f"üéØ Applied custom scale factor: {self.custom_scale}")

        except Exception as e:
            logger.error(f"‚ùå Failed to apply custom scale: {e}")
            raise

    def create_trainer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ trainer —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        try:
            from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
                EmergentCubeTrainer,
                EmergentTrainingConfig,
            )
            from training.embedding_trainer.neural_cellular_automata import (
                create_nca_config,
            )

            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é trainer'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            lattice_config = self.dynamic_config["lattice"]
            gmlp_config = self.dynamic_config["gmlp"]
            training_config = self.dynamic_config["training"]

            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è EmergentTrainingConfig
            trainer_config = EmergentTrainingConfig(
                teacher_model="distilbert-base-uncased",
                cube_dimensions=(
                    lattice_config["xs"],
                    lattice_config["ys"],
                    lattice_config["zs"],
                ),
                # gMLP –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã
                gmlp_config={
                    "state_size": gmlp_config["state_size"],
                    "neighbor_count": gmlp_config["neighbor_count"],
                    "hidden_dim": gmlp_config["hidden_dim"],
                    "external_input_size": gmlp_config["external_input_size"],
                    "use_memory": True,
                    "activation": "gelu",
                    "dropout": 0.1,
                    "spatial_connections": True,
                },
                # Training –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                learning_rate=training_config["learning_rate"],
                batch_size=training_config["batch_size"],
                epochs=training_config["epochs"],
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                mixed_precision=True,
                gradient_checkpointing=True,
                gradient_accumulation_steps=4,
                # NCA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
                enable_nca=True,
                nca_config=create_nca_config(
                    update_probability=0.7,
                    residual_learning_rate=0.1,
                    enable_pattern_detection=True,
                ),
            )

            # –°–æ–∑–¥–∞–µ–º trainer
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.trainer = EmergentCubeTrainer(trainer_config, device=device)

            logger.info(f"‚úÖ Trainer created successfully")
            logger.info(f"   Device: {device}")
            logger.info(f"   Mixed precision: {trainer_config.mixed_precision}")
            logger.info(
                f"   Gradient checkpointing: {trainer_config.gradient_checkpointing}"
            )

            return self.trainer

        except Exception as e:
            logger.error(f"‚ùå Failed to create trainer: {e}")
            raise

    def prepare_dataset(self, limit: Optional[int] = None):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º precomputed embeddings"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É PrecomputedEmbeddingLoader
            import sys
            from pathlib import Path

            sys.path.append(str(Path(__file__).parent))

            from precomputed_embedding_loader import PrecomputedEmbeddingLoader

            # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
            loader = PrecomputedEmbeddingLoader()

            # –ù–∞—Ö–æ–¥–∏–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
            datasets = loader.list_available_datasets()
            if not datasets:
                raise FileNotFoundError(
                    "No precomputed datasets found! Run generate_large_embedding_dataset.py first."
                )

            # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π –∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç
            latest_dataset = datasets[0]
            embeddings_file = latest_dataset["file_path"]

            logger.info(f"üìÅ Using precomputed dataset: {latest_dataset['filename']}")
            logger.info(f"   Available size: {latest_dataset['size']} pairs")
            logger.info(f"   Teacher model: {latest_dataset['teacher_model']}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = loader.load_dataset(embeddings_file)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if limit and limit < len(dataset):
                from torch.utils.data import Subset
                import torch

                # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                indices = torch.randperm(len(dataset))[:limit]
                dataset = Subset(dataset, indices)
                logger.info(f"   Limited to: {limit} pairs")

            logger.info(f"üìÅ Dataset prepared:")
            logger.info(f"   Final size: {len(dataset)} pairs")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–∑–µ—Ü
            sample = dataset[0]
            q_emb, a_emb = sample
            logger.info(f"   Question embedding shape: {q_emb.shape}")
            logger.info(f"   Answer embedding shape: {a_emb.shape}")
            logger.info(f"   Question norm: {q_emb.norm().item():.6f}")
            logger.info(f"   Answer norm: {a_emb.norm().item():.6f}")

            return dataset

        except Exception as e:
            logger.error(f"‚ùå Failed to prepare dataset: {e}")
            raise

    def run_training(
        self,
        dataset_limit: Optional[int] = None,
        epochs: int = None,
        batch_size: int = None,
    ):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
            setup_environment()

            # –°–æ–∑–¥–∞–Ω–∏–µ trainer
            trainer = self.create_trainer()

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            dataset = self.prepare_dataset(limit=dataset_limit)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
            if epochs is None:
                epochs = self.dynamic_config["training"]["epochs"]
            if batch_size is None:
                batch_size = self.dynamic_config["training"]["batch_size"]

            # –°–æ–∑–¥–∞–µ–º DataLoader
            from torch.utils.data import DataLoader

            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0,  # 0 –¥–ª—è Windows —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                pin_memory=True if torch.cuda.is_available() else False,
            )

            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            optimizer = torch.optim.AdamW(
                trainer.parameters(),
                lr=self.dynamic_config["training"]["learning_rate"],
            )

            # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            logger.info(f"üöÄ Starting dynamic training:")
            logger.info(f"   Dataset size: {len(dataset)}")
            logger.info(f"   Epochs: {epochs}")
            logger.info(f"   Batch size: {batch_size}")
            logger.info(
                f"   Estimated time: {self._estimate_training_time(len(dataset), epochs)}"
            )

            start_time = time.time()
            best_similarity = 0.0
            training_log = []

            for epoch in range(epochs):
                epoch_start = time.time()

                # Training epoch
                total_loss = 0.0
                total_similarity = 0.0
                num_batches = 0

                for batch_idx, (question_emb, answer_emb) in enumerate(dataloader):
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ device
                    device = next(trainer.parameters()).device
                    question_emb = question_emb.to(device)
                    answer_emb = answer_emb.to(device)

                    # Forward pass
                    optimizer.zero_grad()
                    outputs = trainer.forward(question_emb)

                    # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º target embedding
                    with torch.no_grad():
                        adapted_target = trainer.base_trainer.adapter(answer_emb)

                    # Targets
                    targets = {
                        "target_embedding": adapted_target,
                        "target_surface": outputs["input_surface"],
                    }

                    # Loss computation
                    losses = trainer.compute_loss(outputs, targets)

                    # –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ loss'–æ–≤
                    total_loss_tensor = torch.tensor(
                        0.0, device=device, requires_grad=True
                    )
                    for loss_name, loss_value in losses.items():
                        if torch.is_tensor(loss_value) and loss_value.requires_grad:
                            if loss_value.dim() > 0:
                                loss_value = loss_value.mean()
                            total_loss_tensor = total_loss_tensor + loss_value

                    # Backward pass
                    total_loss_tensor.backward()
                    torch.nn.utils.clip_grad_norm_(trainer.parameters(), max_norm=1.0)
                    optimizer.step()

                    # Metrics
                    with torch.no_grad():
                        similarity = (
                            torch.cosine_similarity(
                                outputs["final_output"], adapted_target, dim=-1
                            )
                            .mean()
                            .item()
                        )

                    total_loss += total_loss_tensor.item()
                    total_similarity += similarity
                    num_batches += 1

                # Epoch metrics
                avg_loss = total_loss / max(num_batches, 1)
                avg_similarity = total_similarity / max(num_batches, 1)
                epoch_time = time.time() - epoch_start

                # –õ–æ–≥ –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö –∏–ª–∏ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
                if epoch % 5 == 0 or epoch <= 10 or avg_similarity > best_similarity:
                    logger.info(
                        f"Epoch {epoch+1:3d}/{epochs} | "
                        f"Loss: {avg_loss:.6f} | "
                        f"Similarity: {avg_similarity:.4f} | "
                        f"Time: {epoch_time:.1f}s"
                    )

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                training_log.append(
                    {
                        "epoch": epoch + 1,
                        "loss": avg_loss,
                        "similarity": avg_similarity,
                        "time": epoch_time,
                    }
                )

                # Best model tracking
                if avg_similarity > best_similarity:
                    best_similarity = avg_similarity
                    logger.info(
                        f"[BEST] New best similarity: {avg_similarity:.4f} (epoch {epoch+1})"
                    )

                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
                if (epoch + 1) % 10 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()

            total_time = time.time() - start_time

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º scale
            self._save_training_results(
                trainer, len(dataset), epochs, best_similarity, total_time, training_log
            )

            logger.info(f"üéâ Training completed:")
            logger.info(f"   Total time: {total_time/60:.1f} minutes")
            logger.info(f"   Final similarity: {avg_similarity:.4f}")
            logger.info(f"   Best similarity: {best_similarity:.4f}")

            return {
                "final_similarity": avg_similarity,
                "best_similarity": best_similarity,
                "total_time": total_time,
                "epochs": epochs,
                "dataset_size": len(dataset),
                "training_log": training_log,
            }

        except Exception as e:
            logger.error(f"‚ùå Training failed: {e}")
            raise

    def _save_training_results(
        self,
        trainer,
        dataset_size: int,
        epochs: int,
        best_similarity: float,
        total_time: float,
        training_log: list,
    ):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º scale –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏"""
        try:
            from datetime import datetime
            from model_weights_manager import ModelWeightsManager

            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            dynamic_info = self.config_manager.get_dynamic_config_info()
            mode = dynamic_info.get("mode", "unknown") if dynamic_info else "unknown"
            scale_factor = (
                dynamic_info.get("scale_factor", "unknown")
                if dynamic_info
                else "unknown"
            )

            # –°–æ–∑–¥–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ –∏–º—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º scale
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_name = f"dynamic_{mode}_scale{scale_factor}_{dataset_size}pairs_{epochs}epochs_{best_similarity:.3f}sim_{timestamp}"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
            if best_similarity > 0.1:
                weights_manager = ModelWeightsManager()

                logger.info(f"[SAVE] Saving dynamic training result: {result_name}")

                weights_manager.create_milestone_checkpoint(
                    trainer,
                    self.config_manager.get_config(),
                    result_name,
                    {
                        "training_type": "dynamic_training",
                        "mode": mode,
                        "scale_factor": scale_factor,
                        "cube_dimensions": f"{self.dynamic_config['lattice']['xs']}x{self.dynamic_config['lattice']['ys']}x{self.dynamic_config['lattice']['zs']}",
                        "total_neurons": self.dynamic_config["lattice"][
                            "total_neurons"
                        ],
                        "embedding_dim": self.dynamic_config["embeddings"][
                            "embedding_dim"
                        ],
                        "dataset_size": dataset_size,
                        "epochs": epochs,
                        "best_similarity": best_similarity,
                        "total_time_minutes": total_time / 60,
                        "timestamp": timestamp,
                        "description": f"Dynamic training {mode} mode (scale={scale_factor}), {dataset_size} pairs, {epochs} epochs, best similarity {best_similarity:.3f}",
                    },
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è
                import json

                log_path = (
                    f"logs/dynamic_training_{mode}_scale{scale_factor}_{timestamp}.json"
                )
                with open(log_path, "w") as f:
                    json.dump(
                        {
                            "training_info": {
                                "type": "dynamic_training",
                                "mode": mode,
                                "scale_factor": scale_factor,
                                "dataset_size": dataset_size,
                                "epochs": epochs,
                                "best_similarity": best_similarity,
                                "total_time_minutes": total_time / 60,
                            },
                            "dynamic_config": self.dynamic_config,
                            "training_log": training_log,
                        },
                        f,
                        indent=2,
                    )

                logger.info(f"‚úÖ Results saved with scale indication: {result_name}")
            else:
                logger.info(
                    f"‚ö†Ô∏è Similarity too low ({best_similarity:.3f}), skipping checkpoint save"
                )

        except Exception as e:
            logger.error(f"‚ùå Failed to save training results: {e}")

    def _estimate_training_time(self, dataset_size: int, epochs: int) -> str:
        """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è"""
        lattice = self.dynamic_config["lattice"]
        total_neurons = lattice["total_neurons"]

        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ —Ä–µ—à–µ—Ç–∫–∏
        if total_neurons < 1000:
            time_per_epoch = 5  # —Å–µ–∫—É–Ω–¥
        elif total_neurons < 50000:
            time_per_epoch = 15
        elif total_neurons < 500000:
            time_per_epoch = 60
        elif total_neurons < 2000000:
            time_per_epoch = 180  # 3 –º–∏–Ω—É—Ç—ã
        else:
            time_per_epoch = 300  # 5 –º–∏–Ω—É—Ç

        total_minutes = (time_per_epoch * epochs) / 60

        if total_minutes < 60:
            return f"~{total_minutes:.0f} minutes"
        else:
            return f"~{total_minutes/60:.1f} hours"


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Dynamic Training Script with Precomputed Embeddings"
    )
    parser.add_argument(
        "--mode",
        choices=["auto", "development", "research", "validation", "production"],
        default="development",
        help="Configuration mode (default: development for scale=0.01)",
    )
    parser.add_argument(
        "--dataset-limit",
        type=int,
        default=None,
        help="Limit dataset size (uses full dataset if not specified)",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=None,
        help="Number of epochs (uses config default if not specified)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Batch size (uses config default if not specified)",
    )
    parser.add_argument(
        "--scale",
        type=float,
        default=None,
        help="Custom scale factor (overrides mode default)",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –æ–±—É—á–µ–Ω–∏—è
        training_manager = DynamicTrainingManager(
            forced_mode=args.mode if args.mode != "auto" else None,
            custom_scale=args.scale,
        )

        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        results = training_manager.run_training(
            dataset_limit=args.dataset_limit,
            epochs=args.epochs,
            batch_size=args.batch_size,
        )

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("üìà Training Results:")
        for key, value in results.items():
            logger.info(f"   {key}: {value}")

        logger.info("‚úÖ Dynamic training completed successfully!")

    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Training interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
