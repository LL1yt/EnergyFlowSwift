#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ Data Pipeline - –ø–æ—á–µ–º—É embeddings –ø—Ä–∏—Ö–æ–¥—è—Ç –∫–∞–∫ –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
"""

import torch
import numpy as np
from pathlib import Path
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.append(str(Path(__file__).parent))

from training.embedding_trainer.dialogue_dataset import create_dialogue_dataset

class DataPipelineDiagnostics:
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å data pipeline"""
    
    def __init__(self):
        print(f"üîç Data Pipeline Diagnostics")
    
    def run_diagnostics(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ data pipeline"""
        print("\n" + "="*60)
        print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê DATA PIPELINE")
        print("="*60)
        
        # 1. –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ teacher models
        self._test_teacher_models()
        
        # 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self._test_dataset_settings()
        
        # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º manual embedding creation
        self._test_manual_embeddings()
        
        print("\n" + "="*60)
        print("‚úÖ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print("="*60)
    
    def _test_teacher_models(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö teacher models"""
        print("\nü§ñ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TEACHER MODELS:")
        
        test_data = [
            {"question": "What is AI?", "answer": "AI is artificial intelligence."}
        ]
        
        models_to_test = [
            "distilbert-base-uncased",
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/all-mpnet-base-v2"
        ]
        
        for model_name in models_to_test:
            print(f"\n   üìö Testing {model_name}:")
            
            try:
                dataset = create_dialogue_dataset(
                    test_data,
                    teacher_model=model_name,
                    cache_embeddings=False,
                    validation_split=0.0,
                    normalize_embeddings=True
                )
                
                sample = dataset[0]
                if isinstance(sample, tuple):
                    question_emb, answer_emb = sample
                else:
                    question_emb = sample['question_embedding']
                    answer_emb = sample['answer_embedding']
                
                print(f"      ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω dataset")
                print(f"      Question embedding: shape={question_emb.shape}, norm={question_emb.norm().item():.6f}")
                print(f"      Answer embedding: shape={answer_emb.shape}, norm={answer_emb.norm().item():.6f}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if question_emb.norm().item() == 0.0:
                    print(f"      üö® –ü–†–û–ë–õ–ï–ú–ê: Question embedding = –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä!")
                    print(f"         –ü–µ—Ä–≤—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {question_emb[:10]}")
                    print(f"         –í—Å–µ –ª–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω—É–ª–∏? {torch.all(question_emb == 0).item()}")
                else:
                    print(f"      ‚úÖ Question embedding –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
                    print(f"         –ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {question_emb[:5]}")
                
                if answer_emb.norm().item() == 0.0:
                    print(f"      üö® –ü–†–û–ë–õ–ï–ú–ê: Answer embedding = –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä!")
                    print(f"         –ü–µ—Ä–≤—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {answer_emb[:10]}")
                    print(f"         –í—Å–µ –ª–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω—É–ª–∏? {torch.all(answer_emb == 0).item()}")
                else:
                    print(f"      ‚úÖ Answer embedding –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
                    print(f"         –ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {answer_emb[:5]}")
                
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ —Å {model_name}: {e}")
    
    def _test_dataset_settings(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ dataset"""
        print("\n‚öôÔ∏è –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê–°–¢–†–û–ï–ö DATASET:")
        
        test_data = [
            {"question": "What is machine learning?", "answer": "ML is a subset of AI."},
            {"question": "How do neural networks work?", "answer": "They process data through layers."}
        ]
        
        settings_to_test = [
            {"normalize_embeddings": True, "cache_embeddings": False},
            {"normalize_embeddings": False, "cache_embeddings": False},
            {"normalize_embeddings": True, "cache_embeddings": True},
            {"normalize_embeddings": False, "cache_embeddings": True}
        ]
        
        for i, settings in enumerate(settings_to_test):
            print(f"\n   ‚öôÔ∏è Settings {i+1}: {settings}")
            
            try:
                dataset = create_dialogue_dataset(
                    test_data,
                    teacher_model="distilbert-base-uncased",
                    validation_split=0.0,
                    **settings
                )
                
                print(f"      Dataset size: {len(dataset)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ samples
                for j in range(min(2, len(dataset))):
                    sample = dataset[j]
                    if isinstance(sample, tuple):
                        question_emb, answer_emb = sample
                    else:
                        question_emb = sample['question_embedding']
                        answer_emb = sample['answer_embedding']
                    
                    q_norm = question_emb.norm().item()
                    a_norm = answer_emb.norm().item()
                    
                    print(f"      Sample {j}: Q_norm={q_norm:.6f}, A_norm={a_norm:.6f}")
                    
                    if q_norm == 0.0 or a_norm == 0.0:
                        print(f"         üö® –ù—É–ª–µ–≤—ã–µ embeddings –≤ sample {j}!")
                        if q_norm == 0.0:
                            print(f"            Question text: '{test_data[j]['question']}'")
                        if a_norm == 0.0:
                            print(f"            Answer text: '{test_data[j]['answer']}'")
                
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ —Å settings {settings}: {e}")
    
    def _test_manual_embeddings(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ manual —Å–æ–∑–¥–∞–Ω–∏—è embeddings"""
        print("\nüîß –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï MANUAL EMBEDDINGS:")
        
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch.nn.functional as F
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é
            model_name = "distilbert-base-uncased"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            
            print(f"   üìö Manual loading {model_name}:")
            print(f"      Model loaded: {type(model)}")
            print(f"      Tokenizer loaded: {type(tokenizer)}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º —Ç–µ–∫—Å—Ç–µ
            test_texts = [
                "What is AI?",
                "AI is artificial intelligence.",
                "This is a test sentence.",
                ""  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
            ]
            
            for text in test_texts:
                print(f"\n      Testing text: '{text}'")
                
                if not text.strip():
                    print(f"         ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç - –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã")
                    continue
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                print(f"         Tokens: {inputs['input_ids'].shape}")
                
                # –ü–æ–ª—É—á–∞–µ–º embeddings
                with torch.no_grad():
                    outputs = model(**inputs)
                    embeddings = outputs.last_hidden_state
                    
                    # Pooling (mean)
                    pooled_embedding = embeddings.mean(dim=1).squeeze()
                    
                print(f"         Raw embedding shape: {embeddings.shape}")
                print(f"         Pooled embedding shape: {pooled_embedding.shape}")
                print(f"         Pooled embedding norm: {pooled_embedding.norm().item():.6f}")
                
                if pooled_embedding.norm().item() == 0.0:
                    print(f"         üö® –ü–†–û–ë–õ–ï–ú–ê: Manual embedding —Ç–æ–∂–µ –Ω—É–ª–µ–≤–æ–π!")
                    print(f"            –ü–µ—Ä–≤—ã–µ 10 –∑–Ω–∞—á–µ–Ω–∏–π: {pooled_embedding[:10]}")
                    print(f"            –í—Å–µ –Ω—É–ª–∏? {torch.all(pooled_embedding == 0).item()}")
                else:
                    print(f"         ‚úÖ Manual embedding –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
                    print(f"            –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {pooled_embedding[:5]}")
                    print(f"            Min: {pooled_embedding.min().item():.6f}")
                    print(f"            Max: {pooled_embedding.max().item():.6f}")
                    print(f"            Mean: {pooled_embedding.mean().item():.6f}")
                    print(f"            Std: {pooled_embedding.std().item():.6f}")
                
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ manual embedding: {e}")
    
    def _test_dialogue_dataset_internals(self):
        """–ì–ª—É–±–æ–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–æ—Å—Ç–µ–π dialogue_dataset"""
        print("\nüî¨ –ì–õ–£–ë–û–ö–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï DIALOGUE_DATASET:")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        try:
            from training.embedding_trainer.dialogue_dataset import DialogueDataset
            from training.embedding_trainer.embedding_loader import EmbeddingLoader
            
            # –°–æ–∑–¥–∞–µ–º embedding loader –Ω–∞–ø—Ä—è–º—É—é
            loader = EmbeddingLoader("distilbert-base-uncased")
            
            print(f"   üìö EmbeddingLoader —Å–æ–∑–¥–∞–Ω:")
            print(f"      Model: {type(loader.model)}")
            print(f"      Tokenizer: {type(loader.tokenizer)}")
            print(f"      Device: {loader.device}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º loader –Ω–∞–ø—Ä—è–º—É—é
            test_texts = ["What is AI?", "AI is artificial intelligence."]
            
            for text in test_texts:
                embedding = loader.encode_text(text)
                print(f"      Text: '{text}'")
                print(f"         Embedding shape: {embedding.shape}")
                print(f"         Embedding norm: {embedding.norm().item():.6f}")
                
                if embedding.norm().item() == 0.0:
                    print(f"         üö® Loader –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω—É–ª–µ–≤–æ–π embedding!")
                else:
                    print(f"         ‚úÖ Loader —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è internals: {e}")

def main():
    """–ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ data pipeline"""
    diagnostics = DataPipelineDiagnostics()
    diagnostics.run_diagnostics()
    
    print("\nüéØ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print("1. –ï—Å–ª–∏ –≤—Å–µ teacher models –¥–∞—é—Ç –Ω—É–ª–µ–≤—ã–µ embeddings - –ø—Ä–æ–±–ª–µ–º–∞ –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
    print("2. –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ - –ø—Ä–æ–±–ª–µ–º–∞ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏")
    print("3. –ï—Å–ª–∏ manual embeddings —Ä–∞–±–æ—Ç–∞—é—Ç - –ø—Ä–æ–±–ª–µ–º–∞ –≤ dialogue_dataset –∫–æ–¥–µ")

if __name__ == "__main__":
    main() 