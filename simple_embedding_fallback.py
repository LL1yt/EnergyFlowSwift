#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π Fallback Embedding Loader - –æ–±—Ö–æ–¥–∏—Ç —Å–ª–æ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
"""

import torch
import numpy as np
from typing import List
from transformers import AutoTokenizer, AutoModel
import torch.nn.functional as F

class SimpleFallbackEmbeddingLoader:
    """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–≥—Ä—É–∑—á–∏–∫ embeddings –≤ –æ–±—Ö–æ–¥ —Å–ª–æ–∂–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, model_name: str = "distilbert-base-uncased"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üîß SimpleFallbackEmbeddingLoader: {model_name} –Ω–∞ {self.device}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ tokenizer"""
        if self.model is None:
            print(f"üìö Loading {self.model_name}...")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModel.from_pretrained(self.model_name)
                self.model.to(self.device)
                self.model.eval()
                print(f"‚úÖ Model loaded successfully")
            except Exception as e:
                print(f"‚ùå Failed to load model: {e}")
                raise
    
    def encode_text(self, text: str) -> torch.Tensor:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ embedding"""
        return self.encode_texts([text])[0]
    
    def encode_texts(self, texts: List[str]) -> torch.Tensor:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ embeddings"""
        self.load_model()
        
        if not texts:
            raise ValueError("Empty text list")
        
        # –î–æ–±–∞–≤–ª—è–µ–º padding token –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ embeddings
        with torch.no_grad():
            outputs = self.model(**inputs)
            hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden]
            
            # Mean pooling —Å —É—á–µ—Ç–æ–º attention mask
            attention_mask = inputs['attention_mask'].unsqueeze(-1)
            embeddings = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)
        
        embeddings = embeddings.cpu()
        return embeddings


def create_dialogue_dataset_simple_fallback(dialogue_pairs: List[dict], 
                                           teacher_model: str = "distilbert-base-uncased",
                                           normalize_embeddings: bool = True,
                                           **kwargs):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ dataset —Å –ø—Ä–æ—Å—Ç—ã–º fallback loader
    """
    print(f"üîß Creating dataset with SimpleFallbackEmbeddingLoader")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π loader
    loader = SimpleFallbackEmbeddingLoader(teacher_model)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
    questions = [pair['question'] for pair in dialogue_pairs]
    answers = [pair['answer'] for pair in dialogue_pairs]
    
    print(f"üìù Processing {len(questions)} question-answer pairs...")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
    question_embeddings = loader.encode_texts(questions)
    answer_embeddings = loader.encode_texts(answers)
    
    print(f"üìä Generated embeddings:")
    print(f"   Questions: {question_embeddings.shape}, norm={question_embeddings.norm(dim=1).mean().item():.6f}")
    print(f"   Answers: {answer_embeddings.shape}, norm={answer_embeddings.norm(dim=1).mean().item():.6f}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    if normalize_embeddings:
        question_embeddings = F.normalize(question_embeddings, p=2, dim=1)
        answer_embeddings = F.normalize(answer_embeddings, p=2, dim=1)
        print(f"‚úÖ Embeddings normalized")
    
    return SimpleFallbackDataset(question_embeddings, answer_embeddings)


class SimpleFallbackDataset:
    """–ü—Ä–æ—Å—Ç–æ–π dataset –∫–ª–∞—Å—Å –¥–ª—è fallback loader"""
    
    def __init__(self, question_embeddings, answer_embeddings):
        self.question_embeddings = question_embeddings
        self.answer_embeddings = answer_embeddings
    
    def __len__(self):
        return len(self.question_embeddings)
    
    def __getitem__(self, idx):
        return self.question_embeddings[idx], self.answer_embeddings[idx]


def test_simple_fallback():
    """–¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ fallback loader"""
    print("üß™ Testing SimpleFallbackEmbeddingLoader")
    
    test_data = [
        {"question": "What is AI?", "answer": "AI is artificial intelligence."},
        {"question": "How does ML work?", "answer": "ML uses algorithms to learn patterns."}
    ]
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    models_to_test = [
        "distilbert-base-uncased",
        # "roberta-base",  # —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å
        # "gpt2"           # —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å
    ]
    
    for model_name in models_to_test:
        print(f"\nüìö Testing {model_name}:")
        
        try:
            dataset = create_dialogue_dataset_simple_fallback(
                test_data,
                teacher_model=model_name,
                normalize_embeddings=True
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π sample
            sample = dataset[0]
            q_emb, a_emb = sample
            
            print(f"   Sample 0:")
            print(f"      Question embedding: norm={q_emb.norm().item():.6f}")
            print(f"      Answer embedding: norm={a_emb.norm().item():.6f}")
            
            if q_emb.norm().item() > 0.1 and a_emb.norm().item() > 0.1:
                print(f"      ‚úÖ SUCCESS: Working embeddings!")
            else:
                print(f"      ‚ùå FAILED: Still zero embeddings")
            
        except Exception as e:
            print(f"      ‚ùå ERROR: {e}")


if __name__ == "__main__":
    test_simple_fallback() 