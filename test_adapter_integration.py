#!/usr/bin/env python3
"""
üß™ TEST: AdapterCubeTrainer Integration with EmbeddingProcessor.SURFACE_ONLY
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Universal Adapter + EmbeddingProcessor

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. –°–æ–∑–¥–∞–Ω–∏–µ AdapterCubeTrainer —Å –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
2. Universal Adapter ‚Üí EmbeddingProcessor.SURFACE_ONLY pipeline
3. Training workflow (joint & separate training)
4. Gradient flow –∏ backpropagation
5. Performance metrics
6. End-to-end functionality

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–î–∞—Ç–∞: 7 –∏—é–Ω—è 2025
"""

import torch
import numpy as np
import logging
from typing import Dict, Any
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def test_adapter_cube_trainer_creation():
    """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ AdapterCubeTrainer"""
    print("üîß Testing AdapterCubeTrainer creation with EmbeddingProcessor.SURFACE_ONLY...")
    
    try:
        from training.embedding_trainer.adapter_integration import (
            AdapterCubeTrainer, 
            AdapterIntegrationConfig,
            create_llama3_cube_trainer
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = AdapterIntegrationConfig(
            teacher_model="Meta-Llama-3-8B",
            cube_dimensions=(15, 15, 11),
            surface_strategy="single",
            adapter_strategy="learned_linear",
            joint_training=True
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = AdapterCubeTrainer(config=config, device="cpu")
        trainer.initialize_components()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        assert trainer.adapter is not None, "UniversalEmbeddingAdapter –Ω–µ —Å–æ–∑–¥–∞–Ω"
        assert trainer.embedding_processor is not None, "EmbeddingProcessor –Ω–µ —Å–æ–∑–¥–∞–Ω"
        assert trainer.joint_optimizer is not None, "Joint optimizer –Ω–µ —Å–æ–∑–¥–∞–Ω"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
        expected_surface_size = 15 * 15  # single strategy
        assert trainer.adapter.output_dim == expected_surface_size, f"–ù–µ–≤–µ—Ä–Ω—ã–π adapter output: {trainer.adapter.output_dim}"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        info = trainer.get_info()
        print("‚úÖ AdapterCubeTrainer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ:")
        print(f"   Teacher: {info['teacher_model']}")
        print(f"   Surface size: {info['processor']['surface_size']}D")
        print(f"   Processing mode: {info['processor']['mode']}")
        print(f"   Total parameters: {info['total_parameters']:,}")
        
        return trainer
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è AdapterCubeTrainer: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_end_to_end_pipeline(trainer):
    """–¢–µ—Å—Ç end-to-end pipeline: teacher embeddings ‚Üí surface processing ‚Üí output"""
    print("\nüöÄ Testing end-to-end pipeline...")
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö teacher embeddings (LLaMA-3-8B: 4096D)
        batch_size = 4
        teacher_dim = 4096
        teacher_embeddings = torch.randn(batch_size, teacher_dim, dtype=torch.float32)
        
        print(f"üì• Input: {teacher_embeddings.shape} (teacher embeddings)")
        
        # Forward pass
        start_time = time.time()
        
        # –¢–µ—Å—Ç –æ–±—ã—á–Ω–æ–≥–æ forward
        output = trainer.forward(teacher_embeddings)
        
        # –¢–µ—Å—Ç —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results = trainer.forward(teacher_embeddings, return_intermediate=True)
        
        processing_time = time.time() - start_time
        
        print(f"üì§ Output: {output.shape}")
        print(f"‚è±Ô∏è  Processing time: {processing_time:.4f}s")
        print(f"‚ö° Throughput: {batch_size / processing_time:.1f} samples/sec")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        expected_surface_size = 15 * 15  # single surface strategy
        assert output.shape == (batch_size, expected_surface_size), f"–ù–µ–≤–µ—Ä–Ω–∞—è output shape: {output.shape}"
        assert output.requires_grad, "Output –¥–æ–ª–∂–µ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        assert "surface_embeddings" in results, "surface_embeddings –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
        assert "output" in results, "output –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        
        surface_embeddings = results["surface_embeddings"]
        assert surface_embeddings.shape == (batch_size, expected_surface_size), f"–ù–µ–≤–µ—Ä–Ω–∞—è surface shape: {surface_embeddings.shape}"
        
        print("‚úÖ End-to-end pipeline —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        print(f"   Teacher ({teacher_dim}D) ‚Üí Adapter ({expected_surface_size}D) ‚Üí Processor ({expected_surface_size}D)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ end-to-end pipeline: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_training_workflows(trainer):
    """–¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    print("\nüéØ Testing training workflows...")
    
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        batch_size = 2
        teacher_dim = 4096
        
        question_embeddings = torch.randn(batch_size, teacher_dim, dtype=torch.float32)
        answer_embeddings = torch.randn(batch_size, teacher_dim, dtype=torch.float32)
        
        print(f"üì• Training data: questions {question_embeddings.shape}, answers {answer_embeddings.shape}")
        
        # –¢–µ—Å—Ç joint training step
        print("\nüîó Testing joint training step...")
        trainer.config.joint_training = True
        
        joint_metrics = trainer.train_step(question_embeddings, answer_embeddings)
        
        assert "total_loss" in joint_metrics, "total_loss –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        assert "main_loss" in joint_metrics, "main_loss –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        assert "qa_similarity" in joint_metrics, "qa_similarity –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        
        print(f"   Loss: {joint_metrics['total_loss']:.6f}")
        print(f"   QA similarity: {joint_metrics['qa_similarity']:.4f}")
        
        # –¢–µ—Å—Ç separate training
        print("\nüîÄ Testing separate training workflow...")
        trainer.config.joint_training = False
        trainer.current_epoch = 0  # Reset –¥–ª—è warmup
        trainer.adapter_warmup_complete = False
        
        # Adapter warmup step
        warmup_metrics = trainer.train_step(question_embeddings, answer_embeddings)
        
        assert "phase" in warmup_metrics, "phase –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        assert warmup_metrics["phase"] == "adapter_warmup", f"–ù–µ–≤–µ—Ä–Ω–∞—è —Ñ–∞–∑–∞: {warmup_metrics['phase']}"
        
        print(f"   Warmup loss: {warmup_metrics['total_loss']:.6f}")
        
        # Processor training step
        trainer.current_epoch = 5  # Skip warmup
        processor_metrics = trainer.train_step(question_embeddings, answer_embeddings)
        
        assert processor_metrics["phase"] == "processor_training", f"–ù–µ–≤–µ—Ä–Ω–∞—è —Ñ–∞–∑–∞: {processor_metrics['phase']}"
        
        print(f"   Processor loss: {processor_metrics['total_loss']:.6f}")
        print(f"   Processor QA similarity: {processor_metrics['qa_similarity']:.4f}")
        
        print("‚úÖ –í—Å–µ —Ä–µ–∂–∏–º—ã –æ–±—É—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ training workflows: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gradient_flow(trainer):
    """–¢–µ—Å—Ç gradient flow –¥–ª—è training –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏"""
    print("\nüîÑ Testing gradient flow...")
    
    try:
        # –í–∫–ª—é—á–∞–µ–º anomaly detection –¥–ª—è debugging
        torch.autograd.set_detect_anomaly(True)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
        question_embeddings = torch.randn(2, 4096, dtype=torch.float32, requires_grad=True)
        answer_embeddings = torch.randn(2, 4096, dtype=torch.float32)
        
        # Forward pass
        results = trainer.forward(question_embeddings, return_intermediate=True)
        
        # Loss computation
        target_surface = trainer.adapter(answer_embeddings)
        loss = torch.nn.functional.mse_loss(results["output"], target_surface)
        
        print(f"üìä Test loss: {loss.item():.6f}")
        
        # Backward pass
        loss.backward()
        
        # –í—ã–∫–ª—é—á–∞–µ–º anomaly detection
        torch.autograd.set_detect_anomaly(False)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤ adapter
        adapter_grad_norm = 0.0
        adapter_params_with_grad = 0
        for param in trainer.adapter.parameters():
            if param.grad is not None:
                adapter_grad_norm += param.grad.norm().item() ** 2
                adapter_params_with_grad += 1
        adapter_grad_norm = np.sqrt(adapter_grad_norm)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤ embedding_processor
        processor_grad_norm = 0.0
        processor_params_with_grad = 0
        for param in trainer.embedding_processor.parameters():
            if param.grad is not None:
                processor_grad_norm += param.grad.norm().item() ** 2
                processor_params_with_grad += 1
        processor_grad_norm = np.sqrt(processor_grad_norm)
        
        print(f"üîó Adapter gradients: norm={adapter_grad_norm:.6f}, params={adapter_params_with_grad}")
        print(f"üß† Processor gradients: norm={processor_grad_norm:.6f}, params={processor_params_with_grad}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –µ—Å—Ç—å
        assert adapter_grad_norm > 0, "Adapter –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
        assert processor_grad_norm > 0, "EmbeddingProcessor –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
        assert adapter_params_with_grad > 0, "–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –≤ adapter"
        assert processor_params_with_grad > 0, "–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –≤ processor"
        
        print("‚úÖ Gradient flow —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ gradient flow: {e}")
        # –í—ã–∫–ª—é—á–∞–µ–º anomaly detection –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        torch.autograd.set_detect_anomaly(False)
        import traceback
        traceback.print_exc()
        return False


def test_performance_benchmark(trainer):
    """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    print("\n‚ö° Performance benchmark...")
    
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ batch
        teacher_dim = 4096
        batch_sizes = [1, 4, 8, 16]
        
        print("Batch Size | Forward Time | Throughput")
        print("-" * 40)
        
        for batch_size in batch_sizes:
            teacher_embeddings = torch.randn(batch_size, teacher_dim, dtype=torch.float32)
            
            # Warm-up
            for _ in range(3):
                _ = trainer.forward(teacher_embeddings)
            
            # Benchmark
            start_time = time.time()
            for _ in range(10):
                output = trainer.forward(teacher_embeddings)
            total_time = time.time() - start_time
            
            avg_time_per_batch = total_time / 10
            throughput = batch_size / avg_time_per_batch
            
            print(f"{batch_size:>9} | {avg_time_per_batch:>11.4f}s | {throughput:>9.1f} smp/s")
        
        print("‚úÖ Performance benchmark –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ performance benchmark: {e}")
        return False


def test_convenience_functions():
    """–¢–µ—Å—Ç convenience functions –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–µ–Ω–µ—Ä–æ–≤"""
    print("\nüõ†Ô∏è  Testing convenience functions...")
    
    try:
        from training.embedding_trainer.adapter_integration import (
            create_llama3_cube_trainer,
            create_distilbert_cube_trainer
        )
        
        # –¢–µ—Å—Ç LLaMA-3 —Ç—Ä–µ–Ω–µ—Ä–∞
        llama_trainer = create_llama3_cube_trainer(
            cube_dimensions=(15, 15, 11),
            adapter_strategy="learned_linear",
            device="cpu"
        )
        
        llama_info = llama_trainer.get_info()
        assert llama_info["teacher_model"] == "Meta-Llama-3-8B", "–ù–µ–≤–µ—Ä–Ω–∞—è teacher –º–æ–¥–µ–ª—å –¥–ª—è LLaMA"
        
        print(f"‚úÖ LLaMA-3 trainer: {llama_info['teacher_model']}, {llama_info['total_parameters']:,} params")
        
        # –¢–µ—Å—Ç DistilBERT —Ç—Ä–µ–Ω–µ—Ä–∞
        bert_trainer = create_distilbert_cube_trainer(
            cube_dimensions=(15, 15, 11),
            adapter_strategy="learned_linear",
            device="cpu"
        )
        
        bert_info = bert_trainer.get_info()
        assert bert_info["teacher_model"] == "DistilBERT", "–ù–µ–≤–µ—Ä–Ω–∞—è teacher –º–æ–¥–µ–ª—å –¥–ª—è BERT"
        
        print(f"‚úÖ DistilBERT trainer: {bert_info['teacher_model']}, {bert_info['total_parameters']:,} params")
        
        print("‚úÖ Convenience functions —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ convenience functions: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_comprehensive_test():
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤"""
    print("=" * 80)
    print("üß™ COMPREHENSIVE TEST: AdapterCubeTrainer + EmbeddingProcessor.SURFACE_ONLY")
    print("=" * 80)
    
    test_results = []
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞  
    trainer = test_adapter_cube_trainer_creation()
    test_results.append(("Trainer Creation", trainer is not None))
    
    if trainer is not None:
        # 2. End-to-end pipeline
        pipeline_success = test_end_to_end_pipeline(trainer)
        test_results.append(("End-to-End Pipeline", pipeline_success))
        
        # 3. Training workflows
        training_success = test_training_workflows(trainer)
        test_results.append(("Training Workflows", training_success))
        
        # 4. Gradient flow
        gradient_success = test_gradient_flow(trainer)
        test_results.append(("Gradient Flow", gradient_success))
        
        # 5. Performance benchmark
        performance_success = test_performance_benchmark(trainer)
        test_results.append(("Performance Benchmark", performance_success))
    
    # 6. Convenience functions
    convenience_success = test_convenience_functions()
    test_results.append(("Convenience Functions", convenience_success))
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üìä TEST RESULTS:")
    print("=" * 80)
    
    passed = 0
    total = len(test_results)
    
    for test_name, success in test_results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if success:
            passed += 1
    
    print(f"\nüìà Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("üéâ ALL TESTS PASSED! AdapterCubeTrainer integration –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
        return True
    else:
        print("‚ö†Ô∏è  Some tests failed. –¢—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.")
        return False


if __name__ == "__main__":
    run_comprehensive_test() 