#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
==============================================

–û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —Å experiment –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏ —É–º–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç experiment –¥–∞—Ç–∞—Å–µ—Ç (5021 pairs) –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- Experiment –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (50x50x20 —Ä–µ—à–µ—Ç–∫–∞)
- –£–º–Ω–æ–µ –∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
- –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ command line –∞—Ä–≥—É–º–µ–Ω—Ç—ã
"""

import sys
import argparse
from pathlib import Path
import torch
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from energy_flow.config import create_experiment_config, set_energy_config
from energy_flow.training import EnergyTrainer
from energy_flow.training.checkpoint_loader import create_checkpoint_loader
from energy_flow.utils.logging import DEBUG_ENERGY, get_logger, DEBUG_TRAINING, setup_logging
from energy_flow.utils.checkpoint_utils import list_checkpoints

logger = get_logger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å convergence –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
setup_logging(debug_mode=True, level="DEBUG_FORWARD")

# –ü—É—Ç—å –∫ experiment –¥–∞—Ç–∞—Å–µ—Ç—É
EXPERIMENT_DATASET_PATH = "data/energy_flow/active/experiment_mixed_5021pairs_20250729_121801.pt"


def load_experiment_dataset(dataset_path: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ experiment –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    logger.info(f"üìÅ Loading experiment dataset from {dataset_path}")
    
    if not Path(dataset_path).exists():
        raise FileNotFoundError(f"Experiment dataset not found: {dataset_path}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = torch.load(dataset_path, map_location='cuda', weights_only=False)
    
    total_pairs = len(dataset['text_pairs'])
    embedding_dim = dataset['input_embeddings'].shape[1]
    generation_time = dataset['generation_info']['generation_timestamp']
    sources = ', '.join(dataset['generation_info']['sources'])
    
    logger.info(f"‚úÖ Experiment dataset loaded:")
    logger.info(f"   üìä Total pairs: {total_pairs:,}")
    logger.info(f"   üî¢ Embedding dimension: {embedding_dim}")
    logger.info(f"   üìÖ Generated: {generation_time}")
    logger.info(f"   üìö Sources: {sources}")
    
    return dataset


def create_dataloader_from_experiment_dataset(dataset, batch_size: int = 16, shuffle: bool = True):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader –∏–∑ experiment –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    from torch.utils.data import DataLoader
    
    class ExperimentDatasetWrapper:
        def __init__(self, dataset):
            self.dataset = dataset
            self.length = len(dataset['text_pairs'])
        
        def __len__(self):
            return self.length
        
        def __getitem__(self, idx):
            input_text, target_text = self.dataset['text_pairs'][idx]
            return {
                'input_embedding': self.dataset['input_embeddings'][idx],
                'target_embedding': self.dataset['target_embeddings'][idx],
                'input_text': input_text,
                'target_text': target_text
            }
    
    wrapped_dataset = ExperimentDatasetWrapper(dataset)
    dataloader = DataLoader(
        wrapped_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        generator=torch.Generator(device=torch.get_default_device()) if shuffle else None,
        collate_fn=lambda batch: {
            'input_embedding': torch.stack([item['input_embedding'] for item in batch]),
            'target_embedding': torch.stack([item['target_embedding'] for item in batch]),
            'input_text': [item['input_text'] for item in batch],
            'target_text': [item['target_text'] for item in batch]
        }
    )
    
    return dataloader


def setup_experiment_trainer(resume_from: str = None):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ experiment —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    logger.info("üîß Setting up experiment trainer...")
    
    # Experiment –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = create_experiment_config()
    set_energy_config(config)
    
    logger.info(f"üìê Experiment config loaded:")
    logger.info(f"   üî≤ Lattice: {config.lattice_width}x{config.lattice_height}x{config.lattice_depth}")
    logger.info(f"   üì¶ Batch size: {config.batch_size}")
    logger.info(f"   üìà Learning rate: {config.learning_rate}")
    logger.info(f"   üåâ Text bridge: {config.text_bridge_enabled}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = EnergyTrainer(config)
    
    # –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if resume_from:
        logger.info(f"üîÑ Attempting to resume from: {resume_from}")
        
        if resume_from == "latest":
            success = trainer.load_smart_checkpoint(load_latest=True)
        elif resume_from == "best":
            success = trainer.load_smart_checkpoint(load_best=True)
        else:
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª –∏–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω
            checkpoint_path = Path(resume_from)
            if checkpoint_path.exists():
                success = trainer.load_smart_checkpoint(checkpoint_path=checkpoint_path)
            else:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∫–∞–∫ –ø–∞—Ç—Ç–µ—Ä–Ω
                success = trainer.checkpoint_loader.load_checkpoint_by_pattern(resume_from)
                if success:
                    success = trainer.load_smart_checkpoint(load_latest=True)  # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
        
        if success:
            logger.info("‚úÖ Resume successful - continuing from loaded checkpoint")
        else:
            logger.warning("‚ö†Ô∏è Resume failed - starting fresh training")
    
    return config, trainer


def run_training_session(
    trainer: EnergyTrainer,
    dataloader,
    num_epochs: int = 10,
    validate_every: int = 5,
    save_every: int = 2
):
    """–ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏"""
    logger.info(f"üöÄ Starting training session:")
    logger.info(f"   üìä Epochs: {num_epochs}")
    logger.info(f"   üîç Validation every: {validate_every} epochs")
    logger.info(f"   üíæ Save every: {save_every} epochs")
    logger.info(f"   üì¶ Batches per epoch: {len(dataloader)}")
    
    start_epoch = trainer.epoch
    session_start_time = datetime.now()
    
    try:
        for epoch in range(num_epochs):
            current_epoch = start_epoch + epoch
            logger.info(f"\n{'='*60}")
            logger.info(f"üéØ EPOCH {current_epoch + 1} (session {epoch + 1}/{num_epochs})")
            logger.info(f"{'='*60}")
            
            epoch_start_time = datetime.now()
            
            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è —ç–ø–æ—Ö–∏
            epoch_metrics = {
                'total_loss': 0.0,
                'energy_loss': 0.0,
                'text_loss': 0.0,
                'step_time': 0.0,
                'flow_time': 0.0,
                'batches_processed': 0
            }
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –±–∞—Ç—á–∞–º
            for batch_idx, batch in enumerate(dataloader):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                input_texts = batch['input_text']
                target_texts = batch['target_text']
                input_embeddings = batch['input_embedding']
                target_embeddings = batch['target_embedding']
                
                # –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                step_metrics = trainer.train_step(
                    input_texts=input_texts,
                    target_texts=target_texts,
                    teacher_input_embeddings=input_embeddings,
                    teacher_target_embeddings=target_embeddings
                )
                
                # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                for key in ['total_loss', 'energy_loss', 'text_loss', 'step_time', 'flow_time']:
                    if key in step_metrics:
                        epoch_metrics[key] += step_metrics[key]
                epoch_metrics['batches_processed'] += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –±–∞—Ç—á–µ–π
                if batch_idx % 10 == 0:
                    progress = (batch_idx + 1) / len(dataloader) * 100
                    logger.info(f"  üìà Batch {batch_idx + 1}/{len(dataloader)} ({progress:.1f}%): "
                              f"loss={step_metrics.get('total_loss', 0):.4f}, "
                              f"time={step_metrics.get('step_time', 0):.2f}s")
                
                # –î–ª—è debug —Ä–µ–∂–∏–º–∞ - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π
                if batch_idx >= 50:  # –ú–∞–∫—Å–∏–º—É–º 50 –±–∞—Ç—á–µ–π –∑–∞ —ç–ø–æ—Ö—É –¥–ª—è experiment
                    logger.info(f"  ‚èπÔ∏è Limited to {batch_idx + 1} batches for experiment session")
                    break
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–µ
            if epoch_metrics['batches_processed'] > 0:
                for key in ['total_loss', 'energy_loss', 'text_loss', 'step_time', 'flow_time']:
                    epoch_metrics[key] /= epoch_metrics['batches_processed']
            
            epoch_time = (datetime.now() - epoch_start_time).total_seconds()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–Ω–µ—Ä
            trainer.epoch = current_epoch + 1
            
            # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
            trainer.scheduler.step(epoch_metrics['total_loss'])
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if epoch_metrics['total_loss'] < trainer.best_loss:
                trainer.best_loss = epoch_metrics['total_loss']
                trainer.save_smart_checkpoint(
                    current_loss=epoch_metrics['total_loss'],
                    is_best=True,
                    custom_suffix=f"session_{session_start_time.strftime('%H%M%S')}"
                )
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if (epoch + 1) % save_every == 0:
                trainer.save_smart_checkpoint(
                    current_loss=epoch_metrics['total_loss'],
                    is_best=False,
                    custom_suffix=f"session_{session_start_time.strftime('%H%M%S')}_periodic"
                )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–æ—Ö–∏
            logger.info(f"‚úÖ Epoch {current_epoch + 1} completed:")
            logger.info(f"   üìä Total loss: {epoch_metrics['total_loss']:.4f}")
            logger.info(f"   ‚ö° Energy loss: {epoch_metrics['energy_loss']:.4f}")
            logger.info(f"   üìù Text loss: {epoch_metrics['text_loss']:.4f}")
            logger.info(f"   ‚è±Ô∏è Epoch time: {epoch_time:.1f}s")
            logger.info(f"   üîÑ Learning rate: {trainer.optimizer.param_groups[0]['lr']:.2e}")
            logger.info(f"   üì¶ Batches processed: {epoch_metrics['batches_processed']}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if (epoch + 1) % validate_every == 0:
                logger.info(f"\nüîç Running validation...")
                
                # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                val_batches = []
                val_dataloader_iter = iter(dataloader)
                for _ in range(min(3, len(dataloader))):  # 3 –±–∞—Ç—á–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    try:
                        val_batches.append(next(val_dataloader_iter))
                    except StopIteration:
                        break
                
                if val_batches:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–∞—Ç—á–∏
                    val_input_texts = []
                    val_target_texts = []
                    val_input_embeddings = []
                    val_target_embeddings = []
                    
                    for batch in val_batches:
                        val_input_texts.extend(batch['input_text'])
                        val_target_texts.extend(batch['target_text'])
                        val_input_embeddings.append(batch['input_embedding'])
                        val_target_embeddings.append(batch['target_embedding'])
                    
                    val_input_embeddings = torch.cat(val_input_embeddings, dim=0)
                    val_target_embeddings = torch.cat(val_target_embeddings, dim=0)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                    max_val_samples = 12
                    if len(val_input_texts) > max_val_samples:
                        val_input_texts = val_input_texts[:max_val_samples]
                        val_target_texts = val_target_texts[:max_val_samples]
                        val_input_embeddings = val_input_embeddings[:max_val_samples]
                        val_target_embeddings = val_target_embeddings[:max_val_samples]
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
                    val_results = trainer.validate(
                        input_texts=val_input_texts,
                        target_texts=val_target_texts,
                        teacher_input_embeddings=val_input_embeddings,
                        teacher_target_embeddings=val_target_embeddings
                    )
                    
                    logger.info(f"üìä Validation results:")
                    logger.info(f"   üìâ Validation loss: {val_results.get('total_loss', 'N/A'):.4f}")
                    logger.info(f"   üìù Examples generated: {len(val_results.get('examples', []))}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
                    examples = val_results.get('examples', [])
                    if examples:
                        logger.info(f"üìù Sample predictions:")
                        for i, example in enumerate(examples[:2]):  # –ü–µ—Ä–≤—ã–µ 2 –ø—Ä–∏–º–µ—Ä–∞
                            logger.info(f"   {i+1}. Input: '{example['input'][:60]}...'")
                            logger.info(f"      Target: '{example['target'][:60]}...'")
                            logger.info(f"      Predicted: '{example['predicted'][:60]}...'")
        
        session_time = (datetime.now() - session_start_time).total_seconds()
        
        logger.info(f"\nüéâ Training session completed successfully!")
        logger.info(f"   ‚è±Ô∏è Total session time: {session_time/60:.1f} minutes")
        logger.info(f"   üìä Final epoch: {trainer.epoch}")
        logger.info(f"   üéØ Best loss achieved: {trainer.best_loss:.4f}")
        
        return True
        
    except KeyboardInterrupt:
        logger.info(f"\nüõë Training interrupted by user")
        logger.info(f"   üíæ Saving interruption checkpoint...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
        trainer.save_smart_checkpoint(
            current_loss=epoch_metrics.get('total_loss', float('inf')),
            is_best=False,
            custom_suffix=f"interrupted_{datetime.now().strftime('%H%M%S')}"
        )
        
        return False
        
    except Exception as e:
        logger.error(f"‚ùå Training session failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def list_available_checkpoints():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã"""
    logger.info("üìã Available checkpoints:")
    
    checkpoint_loader = create_checkpoint_loader()
    checkpoints = checkpoint_loader.list_available_checkpoints()
    
    if not checkpoints:
        logger.info("   (No checkpoints found)")
        return
    
    logger.info(f"\nüìÅ Found {len(checkpoints)} checkpoints:")
    for i, (path, metadata) in enumerate(checkpoints):
        status = "üèÜ BEST" if metadata['is_best'] else "üìÑ REG"
        timestamp = metadata['timestamp'].strftime("%Y-%m-%d %H:%M")
        logger.info(f"   {i+1:2d}. {status} {path.name}")
        logger.info(f"       üìÖ {timestamp} | üìä Epoch {metadata['epoch']} | üìâ Loss {metadata['loss']:.4f}")


def main():
    parser = argparse.ArgumentParser(description="Full Energy Flow Trainer")
    parser.add_argument("--dataset", type=str, default=EXPERIMENT_DATASET_PATH,
                       help="Path to experiment dataset file")
    parser.add_argument("--epochs", type=int, default=20,
                       help="Number of training epochs")
    parser.add_argument("--resume", type=str, default=None,
                       help="Resume from checkpoint: 'latest', 'best', filename, or pattern")
    parser.add_argument("--validate-every", type=int, default=5,
                       help="Run validation every N epochs")
    parser.add_argument("--save-every", type=int, default=2,
                       help="Save checkpoint every N epochs")
    parser.add_argument("--list-checkpoints", action="store_true",
                       help="List available checkpoints and exit")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="Override default batch size")
    
    args = parser.parse_args()
    
    try:
        # –°–ø–∏—Å–æ–∫ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        if args.list_checkpoints:
            list_available_checkpoints()
            return
        
        logger.info(f"üåü Full Energy Flow Training Session")
        logger.info(f"{'='*80}")
        logger.info(f"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"üìÅ Dataset: {args.dataset}")
        logger.info(f"üìä Epochs: {args.epochs}")
        if args.resume:
            logger.info(f"üîÑ Resume from: {args.resume}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        logger.info(f"\n1Ô∏è‚É£ Loading experiment dataset...")
        dataset = load_experiment_dataset(args.dataset)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞
        logger.info(f"\n2Ô∏è‚É£ Setting up experiment trainer...")
        config, trainer = setup_experiment_trainer(resume_from=args.resume)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º batch_size –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if args.batch_size:
            logger.info(f"üîß Overriding batch size: {config.batch_size} -> {args.batch_size}")
            config.batch_size = args.batch_size
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        logger.info(f"\n3Ô∏è‚É£ Creating experiment DataLoader...")
        dataloader = create_dataloader_from_experiment_dataset(
            dataset, 
            batch_size=config.batch_size,
            shuffle=True
        )
        logger.info(f"‚úÖ DataLoader ready: {len(dataloader)} batches, batch_size={config.batch_size}")
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        logger.info(f"\n4Ô∏è‚É£ Starting experiment training session...")
        success = run_training_session(
            trainer=trainer,
            dataloader=dataloader,
            num_epochs=args.epochs,
            validate_every=args.validate_every,
            save_every=args.save_every
        )
        
        if success:
            logger.info(f"\n‚ú® Experiment training completed successfully!")
            logger.info(f"üíæ Checkpoints saved to: checkpoints/energy_flow/active/")
            logger.info(f"üéØ Best loss achieved: {trainer.best_loss:.4f}")
        else:
            logger.info(f"\n‚ö†Ô∏è Training session ended early")
        
    except KeyboardInterrupt:
        logger.info(f"\nüõë Interrupted by user")
    except Exception as e:
        logger.error(f"\nüí• Unexpected error: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()