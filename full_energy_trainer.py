#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
==============================================

–û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —Å experiment –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏ —É–º–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç experiment –¥–∞—Ç–∞—Å–µ—Ç (5021 pairs) –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- Experiment –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (50x50x20 —Ä–µ—à–µ—Ç–∫–∞)
- –£–º–Ω–æ–µ –∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
- –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ command line –∞—Ä–≥—É–º–µ–Ω—Ç—ã
"""

import sys
import argparse
from pathlib import Path
import torch
from datetime import datetime
from contextlib import nullcontext

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from energy_flow.config import create_experiment_config, set_energy_config
from energy_flow.training import EnergyTrainer
from energy_flow.training.checkpoint_loader import create_checkpoint_loader
from energy_flow.utils.logging import DEBUG_ENERGY, get_logger, DEBUG_TRAINING, setup_logging, DEBUG_PERFORMANCE, DEBUG_MEMORY
from energy_flow.utils.checkpoint_utils import list_checkpoints
from energy_flow.utils import (
    MetricsConfig,
    MetricsCollector,
    GPUMonitor,
    ProfilerManager,
    memory_guard,
)

logger = get_logger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å convergence –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
setup_logging(debug_mode=True, level="DEBUG_FORWARD")

# –ü—É—Ç—å –∫ experiment –¥–∞—Ç–∞—Å–µ—Ç—É
EXPERIMENT_DATASET_PATH = "data/energy_flow/active/experiment_mixed_5021pairs_20250729_121801.pt"

# torch.autograd.set_detect_anomaly(True)


def load_experiment_dataset(dataset_path: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ experiment –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    logger.info(f"üìÅ Loading experiment dataset from {dataset_path}")
    
    if not Path(dataset_path).exists():
        raise FileNotFoundError(f"Experiment dataset not found: {dataset_path}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    # Load dataset on CPU to avoid permanently reserving GPU memory
    dataset = torch.load(dataset_path, map_location='cpu', weights_only=False)
    
    total_pairs = len(dataset['text_pairs'])
    embedding_dim = dataset['input_embeddings'].shape[1]
    generation_time = dataset['generation_info']['generation_timestamp']
    sources = ', '.join(dataset['generation_info']['sources'])
    
    logger.info(f"‚úÖ Experiment dataset loaded:")
    logger.info(f"   üìä Total pairs: {total_pairs:,}")
    logger.info(f"   üî¢ Embedding dimension: {embedding_dim}")
    logger.info(f"   üìÖ Generated: {generation_time}")
    logger.info(f"   üìö Sources: {sources}")
    
    return dataset


# –¢–æ–ø-—É—Ä–æ–≤–Ω–µ–≤—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Windows multiprocessing
from torch.utils.data import DataLoader


def experiment_collate_fn(batch):
    return {
        'input_embedding': torch.stack([item['input_embedding'].cpu() for item in batch]),
        'target_embedding': torch.stack([item['target_embedding'].cpu() for item in batch]),
        'input_text': [item['input_text'] for item in batch],
        'target_text': [item['target_text'] for item in batch]
    }


class ExperimentDatasetWrapper:
    def __init__(self, dataset):
        self.dataset = dataset
        self.length = len(dataset['text_pairs'])
    
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        input_text, target_text = self.dataset['text_pairs'][idx]
        return {
            'input_embedding': self.dataset['input_embeddings'][idx],
            'target_embedding': self.dataset['target_embeddings'][idx],
            'input_text': input_text,
            'target_text': target_text
        }


def create_dataloader_from_experiment_dataset(dataset, batch_size: int = 16, shuffle: bool = True):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader –∏–∑ experiment –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    wrapped_dataset = ExperimentDatasetWrapper(dataset)
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ:
    # - –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ CUDA –∏ –≤–∫–ª—é—á–µ–Ω shuffle, –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    # - –ò–Ω–∞—á–µ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (–∏–ª–∏ CPU –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    cuda_available = torch.cuda.is_available()
    dl_generator = torch.Generator(device='cuda') if (shuffle and cuda_available) else (torch.Generator(device='cpu') if shuffle else None)

    dataloader = DataLoader(
        wrapped_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        generator=dl_generator,
        pin_memory=cuda_available,
        num_workers=2,
        persistent_workers=True,
        collate_fn=experiment_collate_fn,
    )
    
    return dataloader


def setup_experiment_trainer(resume_from: str = None):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ experiment —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    logger.info("üîß Setting up experiment trainer...")
    
    # Experiment –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = create_experiment_config()
    set_energy_config(config)

    if torch.cuda.is_available():
        torch.cuda.set_device(0)
        torch.backends.cudnn.benchmark = True
    
    logger.info(f"üìê Experiment config loaded:")
    logger.info(f"   üî≤ Lattice: {config.lattice_width}x{config.lattice_height}x{config.lattice_depth}")
    logger.info(f"   üì¶ Batch size: {config.batch_size}")
    logger.info(f"   üìà Learning rate: {config.learning_rate}")
    logger.info(f"   üåâ Text bridge: {config.text_bridge_enabled}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = EnergyTrainer(config)
    
    # –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if resume_from:
        logger.info(f"üîÑ Attempting to resume from: {resume_from}")
        
        if resume_from == "latest":
            success = trainer.load_smart_checkpoint(load_latest=True)
        elif resume_from == "best":
            success = trainer.load_smart_checkpoint(load_best=True)
        else:
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª –∏–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω
            checkpoint_path = Path(resume_from)
            if checkpoint_path.exists():
                success = trainer.load_smart_checkpoint(checkpoint_path=checkpoint_path)
            else:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∫–∞–∫ –ø–∞—Ç—Ç–µ—Ä–Ω
                success = trainer.checkpoint_loader.load_checkpoint_by_pattern(resume_from)
                if success:
                    success = trainer.load_smart_checkpoint(load_latest=True)  # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
        
        if success:
            logger.info("‚úÖ Resume successful - continuing from loaded checkpoint")
        else:
            logger.warning("‚ö†Ô∏è Resume failed - starting fresh training")
    
    return config, trainer


def run_training_session(
    trainer: EnergyTrainer,
    dataloader,
    num_epochs: int = 10,
    validate_every: int = 5,
    save_every: int = 2,
    *,
    metrics: MetricsCollector | None = None,
    gpu_monitor: GPUMonitor | None = None,
    profiler: ProfilerManager | None = None,
    mem_guard_gb: float | None = None,
):
    """–ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏"""
    logger.info(f"üöÄ Starting training session:")
    logger.info(f"   üìä Epochs: {num_epochs}")
    logger.info(f"   üîç Validation every: {validate_every} epochs")
    logger.info(f"   üíæ Save every: {save_every} epochs")
    logger.info(f"   üì¶ Batches per epoch: {len(dataloader)}")
    
    start_epoch = trainer.epoch
    session_start_time = datetime.now()
    
    try:
        for epoch in range(num_epochs):
            current_epoch = start_epoch + epoch
            logger.info(f"\n{'='*60}")
            logger.info(f"üéØ EPOCH {current_epoch + 1} (session {epoch + 1}/{num_epochs})")
            logger.info(f"{'='*60}")
            
            epoch_start_time = datetime.now()
            
            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è —ç–ø–æ—Ö–∏
            epoch_metrics = {
                'total_loss': 0.0,
                'energy_loss': 0.0,
                'text_loss': 0.0,
                'step_time': 0.0,
                'flow_time': 0.0,
                'batches_processed': 0
            }
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –±–∞—Ç—á–∞–º
            for batch_idx, batch in enumerate(dataloader):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                input_texts = batch['input_text']
                target_texts = batch['target_text']
                input_embeddings = batch['input_embedding']
                target_embeddings = batch['target_embedding']

                # Guards and profiling contexts (no-op if disabled)
                mg_ctx = memory_guard(mem_guard_gb) if (mem_guard_gb is not None) else nullcontext()
                pf_ctx = profiler.profile_step("train_step") if profiler is not None else nullcontext()
                tm_ctx = metrics.time_component("total_step") if metrics is not None else nullcontext()
                flow_ctx = metrics.time_component("flow_processor") if metrics is not None else nullcontext()

                with mg_ctx:
                    with pf_ctx:
                        with tm_ctx:
                            # –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                            with flow_ctx:
                                pass  # timing wrapper around flow; actual call below includes total time
                            step_metrics = trainer.train_step(
                                input_texts=input_texts,
                                target_texts=target_texts,
                                teacher_input_embeddings=input_embeddings,
                                teacher_target_embeddings=target_embeddings
                            )

                # Optional metrics
                if metrics is not None:
                    st = float(step_metrics.get('step_time', 0.0) or 0.0)
                    bs = input_embeddings.shape[0] if hasattr(input_embeddings, 'shape') else 0
                    if st > 0 and bs > 0:
                        metrics.record_throughput("samples", items=int(bs), elapsed_s=st)
                    # Snapshot GPU memory occasionally
                    if batch_idx % max(1, metrics.config.metrics_log_interval) == 0:
                        metrics.snapshot_gpu_memory(label=f"epoch{current_epoch+1}_batch{batch_idx+1}")

                # Optional GPU utilization log (rate-limited inside monitor)
                if gpu_monitor is not None and batch_idx % max(1, (metrics.config.metrics_log_interval if metrics else 10)) == 0:
                    util = gpu_monitor.gpu_utilization
                    logger.log(DEBUG_PERFORMANCE, f"GPU util (cached): {util:.0f}%")
                
                # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                for key in ['total_loss', 'energy_loss', 'text_loss', 'step_time', 'flow_time']:
                    if key in step_metrics:
                        epoch_metrics[key] += step_metrics[key]
                epoch_metrics['batches_processed'] += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –±–∞—Ç—á–µ–π
                if batch_idx % 10 == 0:
                    progress = (batch_idx + 1) / len(dataloader) * 100
                    logger.info(f"  üìà Batch {batch_idx + 1}/{len(dataloader)} ({progress:.1f}%): "
                              f"loss={step_metrics.get('total_loss', 0):.4f}, "
                              f"time={step_metrics.get('step_time', 0):.2f}s")
                
                # –î–ª—è debug —Ä–µ–∂–∏–º–∞ - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π
                if batch_idx >= 50:  # –ú–∞–∫—Å–∏–º—É–º 50 –±–∞—Ç—á–µ–π –∑–∞ —ç–ø–æ—Ö—É –¥–ª—è experiment
                    logger.info(f"  ‚èπÔ∏è Limited to {batch_idx + 1} batches for experiment session")
                    break
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–µ
            if epoch_metrics['batches_processed'] > 0:
                for key in ['total_loss', 'energy_loss', 'text_loss', 'step_time', 'flow_time']:
                    epoch_metrics[key] /= epoch_metrics['batches_processed']
            
            epoch_time = (datetime.now() - epoch_start_time).total_seconds()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–Ω–µ—Ä
            trainer.epoch = current_epoch + 1
            
            # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
            trainer.scheduler.step(epoch_metrics['total_loss'])
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if epoch_metrics['total_loss'] < trainer.best_loss:
                trainer.best_loss = epoch_metrics['total_loss']
                trainer.save_smart_checkpoint(
                    current_loss=epoch_metrics['total_loss'],
                    is_best=True,
                    custom_suffix=f"session_{session_start_time.strftime('%H%M%S')}"
                )
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if (epoch + 1) % save_every == 0:
                trainer.save_smart_checkpoint(
                    current_loss=epoch_metrics['total_loss'],
                    is_best=False,
                    custom_suffix=f"session_{session_start_time.strftime('%H%M%S')}_periodic"
                )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–æ—Ö–∏
            logger.info(f"‚úÖ Epoch {current_epoch + 1} completed:")
            logger.info(f"   üìä Total loss: {epoch_metrics['total_loss']:.4f}")
            logger.info(f"   ‚ö° Energy loss: {epoch_metrics['energy_loss']:.4f}")
            logger.info(f"   üìù Text loss: {epoch_metrics['text_loss']:.4f}")
            logger.info(f"   ‚è±Ô∏è Epoch time: {epoch_time:.1f}s")
            logger.info(f"   üîÑ Learning rate: {trainer.optimizer.param_groups[0]['lr']:.2e}")
            logger.info(f"   üì¶ Batches processed: {epoch_metrics['batches_processed']}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if (epoch + 1) % validate_every == 0:
                logger.info(f"\nüîç Running validation...")
                
                # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                val_batches = []
                val_dataloader_iter = iter(dataloader)
                for _ in range(min(3, len(dataloader))):  # 3 –±–∞—Ç—á–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    try:
                        val_batches.append(next(val_dataloader_iter))
                    except StopIteration:
                        break
                
                if val_batches:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–∞—Ç—á–∏
                    val_input_texts = []
                    val_target_texts = []
                    val_input_embeddings = []
                    val_target_embeddings = []
                    
                    for batch in val_batches:
                        val_input_texts.extend(batch['input_text'])
                        val_target_texts.extend(batch['target_text'])
                        val_input_embeddings.append(batch['input_embedding'])
                        val_target_embeddings.append(batch['target_embedding'])
                    
                    val_input_embeddings = torch.cat(val_input_embeddings, dim=0)
                    val_target_embeddings = torch.cat(val_target_embeddings, dim=0)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                    max_val_samples = 12
                    if len(val_input_texts) > max_val_samples:
                        val_input_texts = val_input_texts[:max_val_samples]
                        val_target_texts = val_target_texts[:max_val_samples]
                        val_input_embeddings = val_input_embeddings[:max_val_samples]
                        val_target_embeddings = val_target_embeddings[:max_val_samples]
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
                    val_results = trainer.validate(
                        input_texts=val_input_texts,
                        target_texts=val_target_texts,
                        teacher_input_embeddings=val_input_embeddings,
                        teacher_target_embeddings=val_target_embeddings
                    )
                    
                    logger.info(f"üìä Validation results:")
                    logger.info(f"   üìâ Validation loss: {val_results.get('total_loss', 'N/A'):.4f}")
                    logger.info(f"   üìù Examples generated: {len(val_results.get('examples', []))}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
                    examples = val_results.get('examples', [])
                    if examples:
                        logger.info(f"üìù Sample predictions:")
                        for i, example in enumerate(examples[:2]):  # –ü–µ—Ä–≤—ã–µ 2 –ø—Ä–∏–º–µ—Ä–∞
                            logger.info(f"   {i+1}. Input: '{example['input'][:60]}...'")
                            logger.info(f"      Target: '{example['target'][:60]}...'")
                            logger.info(f"      Predicted: '{example['predicted'][:60]}...'")
        
        session_time = (datetime.now() - session_start_time).total_seconds()
        
        logger.info(f"\nüéâ Training session completed successfully!")
        logger.info(f"   ‚è±Ô∏è Total session time: {session_time/60:.1f} minutes")
        logger.info(f"   üìä Final epoch: {trainer.epoch}")
        logger.info(f"   üéØ Best loss achieved: {trainer.best_loss:.4f}")
        
        return True
        
    except KeyboardInterrupt:
        logger.info(f"\nüõë Training interrupted by user")
        logger.info(f"   üíæ Saving interruption checkpoint...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
        trainer.save_smart_checkpoint(
            current_loss=epoch_metrics.get('total_loss', float('inf')),
            is_best=False,
            custom_suffix=f"interrupted_{datetime.now().strftime('%H%M%S')}"
        )
        
        return False
        
    except Exception as e:
        logger.error(f"‚ùå Training session failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def list_available_checkpoints():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã"""
    logger.info("üìã Available checkpoints:")
    
    checkpoint_loader = create_checkpoint_loader()
    checkpoints = checkpoint_loader.list_available_checkpoints()
    
    if not checkpoints:
        logger.info("   (No checkpoints found)")
        return
    
    logger.info(f"\nüìÅ Found {len(checkpoints)} checkpoints:")
    for i, (path, metadata) in enumerate(checkpoints):
        status = "üèÜ BEST" if metadata['is_best'] else "üìÑ REG"
        timestamp = metadata['timestamp'].strftime("%Y-%m-%d %H:%M")
        logger.info(f"   {i+1:2d}. {status} {path.name}")
        logger.info(f"       üìÖ {timestamp} | üìä Epoch {metadata['epoch']} | üìâ Loss {metadata['loss']:.4f}")


def main():
    parser = argparse.ArgumentParser(description="Full Energy Flow Trainer")
    parser.add_argument("--dataset", type=str, default=EXPERIMENT_DATASET_PATH,
                       help="Path to experiment dataset file")
    parser.add_argument("--epochs", type=int, default=20,
                       help="Number of training epochs")
    parser.add_argument("--resume", type=str, default=None,
                       help="Resume from checkpoint: 'latest', 'best', filename, or pattern")
    parser.add_argument("--validate-every", type=int, default=5,
                       help="Run validation every N epochs")
    parser.add_argument("--save-every", type=int, default=2,
                       help="Save checkpoint every N epochs")
    parser.add_argument("--list-checkpoints", action="store_true",
                       help="List available checkpoints and exit")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="Override default batch size")

    # Optional metrics/profiling/memory flags (opt-in; default off)
    parser.add_argument("--enable-metrics", action="store_true", help="Enable lightweight metrics collection")
    parser.add_argument("--enable-profiler", action="store_true", help="Enable torch.profiler (heavier)")
    parser.add_argument("--enable-gpu-monitor", action="store_true", help="Enable GPU utilization monitor (NVML)")
    parser.add_argument("--metrics-log-interval", type=int, default=10, help="Metrics logging interval (batches)")
    parser.add_argument("--mem-guard-gb", type=float, default=None, help="Enable memory guard with threshold in GB (e.g., 28.0)")
    
    args = parser.parse_args()
    
    try:
        # –°–ø–∏—Å–æ–∫ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        if args.list_checkpoints:
            list_available_checkpoints()
            return
        
        logger.info(f"üåü Full Energy Flow Training Session")
        logger.info(f"{'='*80}")
        logger.info(f"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"üìÅ Dataset: {args.dataset}")
        logger.info(f"üìä Epochs: {args.epochs}")
        if args.resume:
            logger.info(f"üîÑ Resume from: {args.resume}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        logger.info(f"\n1Ô∏è‚É£ Loading experiment dataset...")
        dataset = load_experiment_dataset(args.dataset)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞
        logger.info(f"\n2Ô∏è‚É£ Setting up experiment trainer...")
        config, trainer = setup_experiment_trainer(resume_from=args.resume)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º batch_size –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if args.batch_size:
            logger.info(f"üîß Overriding batch size: {config.batch_size} -> {args.batch_size}")
            config.batch_size = args.batch_size
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        logger.info(f"\n3Ô∏è‚É£ Creating experiment DataLoader...")
        dataloader = create_dataloader_from_experiment_dataset(
            dataset, 
            batch_size=config.batch_size,
            shuffle=True
        )
        logger.info(f"‚úÖ DataLoader ready: {len(dataloader)} batches, batch_size={config.batch_size}")
        
        # Optional metrics components (zero-overhead if not enabled)
        mcfg = MetricsConfig(
            enable_metrics=bool(args.enable_metrics),
            enable_profiler=bool(args.enable_profiler),
            enable_gpu_monitoring=bool(args.enable_gpu_monitor),
            metrics_log_interval=int(args.metrics_log_interval),
        )
        metrics = MetricsCollector(mcfg) if mcfg.enable_metrics else None
        profiler = ProfilerManager(mcfg) if mcfg.enable_profiler else None
        gpu_monitor = GPUMonitor(mcfg) if mcfg.enable_gpu_monitoring else None

        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        logger.info(f"\n4Ô∏è‚É£ Starting experiment training session...")
        success = run_training_session(
            trainer=trainer,
            dataloader=dataloader,
            num_epochs=args.epochs,
            validate_every=args.validate_every,
            save_every=args.save_every,
            metrics=metrics,
            gpu_monitor=gpu_monitor,
            profiler=profiler,
            mem_guard_gb=args.mem_guard_gb,
        )
        
        if success:
            logger.info(f"\n‚ú® Experiment training completed successfully!")
            logger.info(f"üíæ Checkpoints saved to: checkpoints/energy_flow/active/")
            logger.info(f"üéØ Best loss achieved: {trainer.best_loss:.4f}")
        else:
            logger.info(f"\n‚ö†Ô∏è Training session ended early")
        
    except KeyboardInterrupt:
        logger.info(f"\nüõë Interrupted by user")
    except Exception as e:
        logger.error(f"\nüí• Unexpected error: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()