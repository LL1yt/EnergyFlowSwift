# Examples: Tokenizer Module - 3D Cellular Neural Network

**–ú–æ–¥—É–ª—å:** `data/tokenizer/`  
**–í–µ—Ä—Å–∏—è:** 1.0.0  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é  
**–î–∞—Ç–∞:** 5 –∏—é–Ω—å 2025

---

## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TokenizerManager

```python
from data.tokenizer import TokenizerManager

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é basic)
tokenizer = TokenizerManager(tokenizer_type='basic')

# –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
text = "Hello world! This is a test."
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")
# Output: ['hello', 'world', 'this', 'is', 'a', 'test']

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ ID
token_ids = tokenizer.encode(text)
print(f"Token IDs: {token_ids}")
# Output: [2, 7179, 9692, 9477, 58, 838, 8449, 3]

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
decoded = tokenizer.decode(token_ids)
print(f"Decoded: {decoded}")
```

### 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π

```python
# –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
config = {
    'tokenizer': {
        'type': 'basic',
        'max_length': 50,
        'padding': True,
        'add_special_tokens': True
    },
    'text_processing': {
        'lowercase': True,
        'remove_punctuation': False,
        'remove_urls': True
    },
    'caching': {
        'enabled': True,
        'max_size': 5000
    }
}

tokenizer = TokenizerManager(tokenizer_type='basic', config=config)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
text = "This is a longer text that needs to be limited in length"
tokens = tokenizer.encode(text, max_length=10, padding='max_length')
print(f"Limited tokens: {tokens}")
```

### 3. Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ä–∞–∑—É
texts = [
    "First example text",
    "Second text is longer than the first one",
    "Third and final text"
]

# Batch –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
batch_encoded = tokenizer.batch_encode(texts, max_length=15)
print(f"Batch encoded: {batch_encoded}")

# Batch –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
batch_decoded = tokenizer.batch_decode(batch_encoded)
print(f"Batch decoded: {batch_decoded}")
```

### 4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å 3D —Ä–µ—à–µ—Ç–∫–æ–π

```python
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 3D –∫–ª–µ—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
text = "Input data for neural network processing"
lattice_size = (8, 8, 8)  # –†–∞–∑–º–µ—Ä 3D —Ä–µ—à–µ—Ç–∫–∏

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–∞ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–π –≥—Ä–∞–Ω–∏ —Ä–µ—à–µ—Ç–∫–∏
lattice_input = tokenizer.prepare_for_lattice(text, lattice_size)
print(f"Lattice input shape: {lattice_input.shape}")  # torch.Size([8, 8])
print(f"Ready for lattice: {lattice_input}")

# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–æ–¥–∞—Ç—å –Ω–∞ —Ä–µ—à–µ—Ç–∫—É
# from core.lattice_3d import Lattice3D
# lattice = Lattice3D(size=lattice_size)
# lattice.set_input_face(lattice_input)
```

---

## üîß –†–∞–±–æ—Ç–∞ —Å TextProcessor

### 1. –ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
from data.tokenizer.text_processor import TextProcessor

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
processor = TextProcessor()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
messy_text = "  HELLO    World!!!   This is   MESSY text.  "
clean_text = processor.preprocess(messy_text)
print(f"Original: '{messy_text}'")
print(f"Cleaned:  '{clean_text}'")
# Output: 'hello world!!! this is messy text.'
```

### 2. –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏

```python
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏
aggressive_config = {
    'lowercase': True,
    'remove_punctuation': True,
    'remove_numbers': True,
    'remove_urls': True,
    'remove_emails': True,
    'normalize_unicode': True
}

processor = TextProcessor(aggressive_config)

text_with_issues = "Check out https://example.com! Email: test@example.com. Number: 123."
processed = processor.preprocess(text_with_issues)
print(f"Aggressively cleaned: '{processed}'")
# Output: 'check out email number'
```

### 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏

```python
original = "This is the ORIGINAL text with CAPS and numbers 123!"
processed = processor.preprocess(original)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
stats = processor.get_processing_stats(original, processed)
print(f"Processing stats: {stats}")
# Output: {
#   'original_length': 52,
#   'processed_length': 45,
#   'original_words': 9,
#   'processed_words': 8,
#   'reduction_ratio': 0.134,
#   'config': {...}
# }
```

---

## üèóÔ∏è –†–∞–±–æ—Ç–∞ —Å –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ (Advanced)

### 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ BERT —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)

```python
try:
    # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BERT (—Ç—Ä–µ–±—É–µ—Ç transformers)
    bert_tokenizer = TokenizerManager(tokenizer_type='bert-base-uncased')

    text = "Hello world! This is BERT tokenization."
    bert_tokens = bert_tokenizer.encode(text)
    print(f"BERT tokens: {bert_tokens}")

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã BERT
    special_tokens = bert_tokenizer.get_special_tokens()
    print(f"BERT special tokens: {special_tokens}")

except ImportError:
    print("BERT tokenizer not available (transformers not installed)")
    # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—É
    tokenizer = TokenizerManager(tokenizer_type='basic')
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤
tokenizer_types = ['bert-base-uncased', 'gpt2', 'basic']

for tokenizer_type in tokenizer_types:
    try:
        test_tokenizer = TokenizerManager(tokenizer_type=tokenizer_type)
        if test_tokenizer.is_available():
            vocab_size = test_tokenizer.get_vocab_size()
            print(f"‚úÖ {tokenizer_type}: Available (vocab size: {vocab_size})")
        else:
            print(f"‚ùå {tokenizer_type}: Not available")
    except Exception as e:
        print(f"‚ùå {tokenizer_type}: Failed to load ({str(e)})")
```

---

## üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥—É–ª—è–º–∏

### 1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å EmbeddingLoader

```python
from data.tokenizer import TokenizerManager
from data.embedding_loader import EmbeddingLoader

# –°–æ–∑–¥–∞–Ω–∏–µ pipeline
tokenizer = TokenizerManager(tokenizer_type='basic')
embedding_loader = EmbeddingLoader()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π pipeline
text = "Sample text for embedding processing"

# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")

# 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ integration –≥–æ—Ç–æ–≤–∞)
# embeddings = embedding_loader.get_embeddings_for_tokens(tokens)
# print(f"Embeddings shape: {embeddings.shape}")
```

### 2. –ü–æ–ª–Ω—ã–π pipeline –¥–ª—è 3D CNN

```python
import torch
from data.tokenizer import TokenizerManager

def text_to_3d_cnn_input(text: str, lattice_size: tuple) -> torch.Tensor:
    """
    –ü–æ–ª–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤—Ö–æ–¥ –¥–ª—è 3D CNN.

    Args:
        text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        lattice_size: –†–∞–∑–º–µ—Ä 3D —Ä–µ—à–µ—Ç–∫–∏ (x, y, z)

    Returns:
        –¢–µ–Ω–∑–æ—Ä –≥–æ—Ç–æ–≤—ã–π –¥–ª—è –ø–æ–¥–∞—á–∏ –Ω–∞ 3D —Ä–µ—à–µ—Ç–∫—É
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
    tokenizer = TokenizerManager(tokenizer_type='basic')

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞
    lattice_input = tokenizer.prepare_for_lattice(text, lattice_size)

    return lattice_input

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
text = "Input for 3D cellular neural network"
lattice_size = (10, 10, 10)
cnn_input = text_to_3d_cnn_input(text, lattice_size)
print(f"Ready for 3D CNN: {cnn_input.shape}")
```

---

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞

### 1. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
tokenizer = TokenizerManager(tokenizer_type='basic')

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
test_texts = [
    "First test text",
    "Second longer test text with more words",
    "Third text for testing performance"
]

for text in test_texts:
    tokens = tokenizer.encode(text)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
metrics = tokenizer.get_metrics()
print("üìä Performance Metrics:")
print(f"  Total tokenizations: {metrics['total_tokenizations']}")
print(f"  Average tokens per text: {metrics['avg_tokens_per_text']:.2f}")
print(f"  Cache hit rate: {metrics['cache_hit_rate']:.2%}")
print(f"  Cache size: {metrics['cache_size']}")
```

### 2. –û—Ç–ª–∞–¥–∫–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.DEBUG)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ —Å –æ—Ç–ª–∞–¥–∫–æ–π
tokenizer = TokenizerManager(tokenizer_type='basic')

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏
text = "Debug tokenization process"
tokens = tokenizer.encode(text)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
print(f"Tokenizer type: {tokenizer.tokenizer_type}")
print(f"Config: {tokenizer.config}")
print(f"Cache enabled: {tokenizer._cache_enabled}")
```

### 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

```python
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
configs = [
    {'text_processing': {'lowercase': True, 'remove_punctuation': False}},
    {'text_processing': {'lowercase': False, 'remove_punctuation': True}},
    {'caching': {'enabled': False}},
    {'tokenizer': {'max_length': 20, 'padding': True}}
]

test_text = "Testing DIFFERENT configurations! With punctuation."

for i, config in enumerate(configs):
    print(f"\nüß™ Configuration {i+1}:")
    tokenizer = TokenizerManager(tokenizer_type='basic', config=config)

    tokens = tokenizer.encode(test_text, max_length=15)
    print(f"Result: {tokens}")

    metrics = tokenizer.get_metrics()
    print(f"Cache enabled: {metrics.get('cache_enabled', 'N/A')}")
```

---

## üéØ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏

```python
def process_text_file(file_path: str, output_path: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""

    tokenizer = TokenizerManager(tokenizer_type='basic')

    with open(file_path, 'r', encoding='utf-8') as f:
        texts = f.readlines()

    # Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞
    processed_texts = []
    for text in texts:
        tokens = tokenizer.encode(text.strip())
        processed_texts.append(tokens)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    import json
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(processed_texts, f, indent=2)

    print(f"Processed {len(texts)} texts to {output_path}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
# process_text_file('input_texts.txt', 'tokenized_output.json')
```

### 2. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

```python
def create_training_dataset(texts: list, lattice_size: tuple) -> list:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D CNN."""

    tokenizer = TokenizerManager(tokenizer_type='basic')
    dataset = []

    for text in texts:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ—Ç–∫–∏
        lattice_input = tokenizer.prepare_for_lattice(text, lattice_size)
        dataset.append({
            'original_text': text,
            'lattice_input': lattice_input.tolist(),  # –î–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            'shape': lattice_input.shape
        })

    return dataset

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
sample_texts = [
    "First training example",
    "Second training sample with more content",
    "Third example for neural network"
]

dataset = create_training_dataset(sample_texts, (5, 5, 5))
print(f"Created dataset with {len(dataset)} samples")
```

---

**üéØ –ò—Ç–æ–≥–∏ –ø—Ä–∏–º–µ—Ä–æ–≤:**

- ‚úÖ –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –≥–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å 3D —Ä–µ—à–µ—Ç–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞
- ‚úÖ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞
- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç–ª–∞–¥–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
- ‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–µ–∞–ª–∏–∑—É–µ–º—ã

**üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**

- `README.md` - –æ–±—â–∏–π –æ–±–∑–æ—Ä –º–æ–¥—É–ª—è
- `meta.md` - –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ API
- `plan.md` - –ø–ª–∞–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
- `diagram.mmd` - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
