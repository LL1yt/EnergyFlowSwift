#!/usr/bin/env python3
"""
–¢–µ—Å—Ç CubeToTextDecoder –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
"""

import torch
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ energy_flow
sys.path.append('energy_flow')

from energy_flow.config import create_debug_config
from energy_flow.text_bridge import TextToCubeEncoder
from energy_flow.text_bridge.cube_to_text_decoder import (
    CubeToTextDecoder, 
    create_cube_to_text_decoder,
    SyntheticTrainingDataGenerator
)

def test_cube_to_text_decoder():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CubeToTextDecoder...")
    
    # –°–æ–∑–¥–∞–µ–º debug –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = create_debug_config()
    print(f"üìê –†–∞–∑–º–µ—Ä—ã –∫—É–±–∞: {config.lattice_width}√ó{config.lattice_height}√ó{config.lattice_depth}")
    print(f"üìè Surface dim: {config.lattice_width * config.lattice_height}")
    
    # –°–æ–∑–¥–∞–µ–º decoder
    decoder = create_cube_to_text_decoder(config)
    
    print("\n1Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏:")
    total_params = sum(p.numel() for p in decoder.parameters() if p.requires_grad)
    frozen_params = sum(p.numel() for p in decoder.parameters() if not p.requires_grad)
    print(f"   –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params:,}")
    print(f"   –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {frozen_params:,}")
    print(f"   –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params + frozen_params:,}")
    print(f"   –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (~60M): {'‚úÖ OK' if total_params < 70_000_000 else '‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ'}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ surface embeddings
    print("\n2Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö surface embeddings:")
    batch_size = 3
    surface_dim = config.lattice_width * config.lattice_height
    
    # –°–ª—É—á–∞–π–Ω—ã–µ embeddings –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1]
    test_surface_embeddings = torch.randn(batch_size, surface_dim) * 0.5
    test_surface_embeddings = torch.clamp(test_surface_embeddings, -1, 1)
    
    print(f"   –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö embeddings: {test_surface_embeddings.shape}")
    print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={test_surface_embeddings.mean():.4f}, std={test_surface_embeddings.std():.4f}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: [{test_surface_embeddings.min():.4f}, {test_surface_embeddings.max():.4f}]")
    
    # –¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    print("\n3Ô∏è‚É£ –¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:")
    try:
        decoded_texts = decoder.decode_surface(test_surface_embeddings, max_length=32)
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(decoded_texts)}")
        for i, text in enumerate(decoded_texts):
            print(f"   –¢–µ–∫—Å—Ç {i+1}: '{text}'")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞–∑–æ–≤–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # –¢–µ—Å—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    print("\n4Ô∏è‚É£ –¢–µ—Å—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:")
    try:
        iterative_texts = decoder.iterative_decode(
            test_surface_embeddings, 
            max_length=32, 
            correction_steps=2
        )
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(iterative_texts)}")
        for i, text in enumerate(iterative_texts):
            print(f"   –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç {i+1}: '{text}'")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
    
    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\n5Ô∏è‚É£ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:")
    try:
        generator = SyntheticTrainingDataGenerator(config)
        synthetic_pairs = generator.generate_synthetic_pairs(5)
        
        print(f"   –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ä: {len(synthetic_pairs)}")
        for i, (emb, text) in enumerate(synthetic_pairs[:3]):
            print(f"   –ü–∞—Ä–∞ {i+1}: embedding {emb.shape} ‚Üí '{text}'")
            print(f"            —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={emb.mean():.3f}, std={emb.std():.3f}")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    # –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å TextToCubeEncoder
    print("\n6Ô∏è‚É£ –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å TextToCubeEncoder:")
    try:
        # –°–æ–∑–¥–∞–µ–º encoder
        encoder = TextToCubeEncoder(config)
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
        test_texts = [
            "Hello world!",
            "This is a test for integration.",
            "Neural networks are fascinating."
        ]
        
        # –ö–æ–¥–∏—Ä—É–µ–º –≤ surface embeddings
        surface_embeddings = encoder.encode_text(test_texts)
        print(f"   –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings: {surface_embeddings.shape}")
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
        reconstructed_texts = decoder.decode_surface(surface_embeddings, max_length=32)
        
        print(f"   –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª ‚Üí reconstruction:")
        for orig, recon in zip(test_texts, reconstructed_texts):
            print(f"   '{orig}' ‚Üí '{recon}'")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    print(f"\n7Ô∏è‚É£ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ:")
    device = next(decoder.parameters()).device
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {device}")
    print(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    print(f"   Default device: {torch.get_default_device()}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ
    print(f"\n8Ô∏è‚É£ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ:")
    tokenizer = decoder.get_tokenizer()
    print(f"   –¢–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: T5Tokenizer")
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(tokenizer)}")
    print(f"   Pad token: {tokenizer.pad_token}")
    print(f"   EOS token: {tokenizer.eos_token}")
    
    print("\n‚úÖ –¢–µ—Å—Ç CubeToTextDecoder –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return True

if __name__ == "__main__":
    try:
        test_cube_to_text_decoder()
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}")
        import traceback
        traceback.print_exc()