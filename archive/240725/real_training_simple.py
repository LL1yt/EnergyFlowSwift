#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 3D Cellular Neural Network
==================================================================

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–û–õ–¨–ö–û —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ new_rebuild.config
–°–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∞–º CLAUDE.md - –Ω–∏–∫–∞–∫–∏—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
"""

import torch
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any


def convert_tensors_to_json(obj):
    """Convert PyTorch tensors to JSON-serializable values."""
    if isinstance(obj, torch.Tensor):
        return obj.item() if obj.numel() == 1 else obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_tensors_to_json(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_tensors_to_json(item) for item in obj]
    else:
        return obj


from new_rebuild.config import SimpleProjectConfig
from new_rebuild.core.training import EmbeddingTrainer
from new_rebuild.core.training.utils import create_training_dataloader
from new_rebuild.utils.logging import setup_logging, get_logger

# === –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–õ–¨–ö–û –¶–ï–ù–¢–†–ê–õ–¨–ù–£–Æ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ ===
config = SimpleProjectConfig()

# Initialize centralized logging system
setup_logging(
    debug_mode=config.logging.debug_mode,
    level=config.logging.level,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º level –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    log_file=(
        config.logging.log_file
        if getattr(config.logging, "log_to_file", False)
        else None
    ),
    enable_context=True,
)

logger = get_logger(__name__)
logger.info("‚öôÔ∏è Loading central configuration...")


def setup_experiment_tracking(experiment_name: str) -> Path:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = Path(f"experiments/{experiment_name}_{timestamp}")
    experiment_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    (experiment_dir / "checkpoints").mkdir(exist_ok=True)
    (experiment_dir / "logs").mkdir(exist_ok=True)
    (experiment_dir / "metrics").mkdir(exist_ok=True)

    logger.info(f"üìä Experiment tracking setup: {experiment_dir}")
    return experiment_dir


def run_training_epoch(
    trainer: EmbeddingTrainer, dataloader, epoch: int, experiment_dir: Path
) -> Dict[str, float]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è"""

    logger.info(f"\nüöÄ Starting Epoch {epoch + 1}")
    epoch_start_time = time.time()

    # Use trainer's train_epoch method directly
    train_losses = trainer.train_epoch(dataloader)

    epoch_time = time.time() - epoch_start_time

    logger.info(f"‚úÖ Epoch {epoch + 1} completed in {epoch_time:.2f}s")
    logger.info(
        f"   Average losses: {', '.join([f'{k}={v:.6f}' for k, v in train_losses.items()])}"
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics_file = experiment_dir / "metrics" / f"epoch_{epoch + 1}_metrics.json"
    metrics_file.parent.mkdir(exist_ok=True)

    metrics = {
        "epoch": epoch + 1,
        "train_losses": convert_tensors_to_json(train_losses),
        "epoch_time": epoch_time,
        "timestamp": datetime.now().isoformat(),
    }

    with open(metrics_file, "w") as f:
        json.dump(metrics, f, indent=2)

    return train_losses


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

    logger.info("üöÄ STARTING REAL 3D CELLULAR NEURAL NETWORK TRAINING")
    logger.info("Using CENTRAL CONFIG ONLY (new_rebuild.config)")
    logger.info("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    if config.training_embedding.test_mode:
        logger.warning(
            "‚ö†Ô∏è test_mode=True in config! Switch to real training mode in config_components.py"
        )
        logger.error("\n‚ùå CONFIGURATION ERROR:")
        logger.error("test_mode=True in central config!")
        logger.error("Edit new_rebuild/config/config_components.py:")
        logger.error("  Change: test_mode: bool = False")
        return

    logger.info("‚úÖ Real training mode enabled")
    logger.info(f"üìè Lattice size: {config.lattice.dimensions}")
    logger.info(
        f"üéØ Target embedding dim: {config.training_embedding.target_embedding_dim}"
    )
    logger.info(f"üìä Epochs: {config.training_embedding.num_epochs}")
    logger.info(f"üî• Batch size: {config.training_embedding.embedding_batch_size}")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment_name = f"real_training_{config.lattice.dimensions[0]}x{config.lattice.dimensions[1]}x{config.lattice.dimensions[2]}"
    experiment_dir = setup_experiment_tracking(experiment_name)

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
    logger.info("üìÇ Loading unified dataset...")
    # –î–ª—è –ø—Ä–æ–≥–æ–Ω–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 658 —Å—ç–º–ø–ª–æ–≤ (–∏–∑ dialogue cache)
    max_samples = config.training_embedding.test_dataset_size

    dataloader, dataset_stats = create_training_dataloader(config=config, shuffle=True)

    logger.info(f"üìä Dataset loaded: {dataset_stats.total_samples} total samples")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞
    with open(experiment_dir / "dataset_stats.json", "w") as f:
        stats_dict = {
            "total_samples": dataset_stats.total_samples,
            "embedding_dim": dataset_stats.embedding_dim,
            "source_distribution": dataset_stats.source_distribution,
            "type_distribution": dataset_stats.type_distribution,
        }
        json.dump(stats_dict, f, indent=2)

    # –°–æ–∑–¥–∞–Ω–∏–µ trainer'–∞
    logger.info("üß† Initializing EmbeddingTrainer...")
    trainer = EmbeddingTrainer(config)

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    num_epochs = config.training_embedding.num_epochs
    logger.info(f"üéØ Starting training for {num_epochs} epochs...")

    best_loss = float("inf")
    patience_counter = 0
    early_stopping_patience = config.training_embedding.early_stopping_patience

    for epoch in range(num_epochs):

        # –û–±—É—á–µ–Ω–∏–µ
        train_losses = run_training_epoch(trainer, dataloader, epoch, experiment_dir)
        current_loss = train_losses["total"]

        # Early stopping check
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            best_model_path = experiment_dir / "checkpoints" / "best_model.pth"
            trainer.save_checkpoint(
                str(best_model_path), epoch=epoch + 1, loss=current_loss
            )
            logger.info(f"üíæ New best model saved: loss={current_loss:.6f}")

        else:
            patience_counter += 1
            logger.info(f"‚ö†Ô∏è No improvement for {patience_counter} epochs")

        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ checkpoint'—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
        save_interval = config.training_embedding.save_checkpoint_every
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = experiment_dir / "checkpoints" / f"epoch_{epoch + 1}.pth"
            trainer.save_checkpoint(
                str(checkpoint_path), epoch=epoch + 1, loss=current_loss
            )
            logger.info(f"üíæ Regular checkpoint saved: epoch_{epoch + 1}.pth")

        # Early stopping
        if patience_counter >= early_stopping_patience:
            logger.info(
                f"üõë Early stopping triggered after {patience_counter} epochs without improvement"
            )
            break

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_model_path = experiment_dir / "checkpoints" / "final_model.pth"
    trainer.save_checkpoint(str(final_model_path), epoch=epoch + 1, loss=current_loss)

    # –°–≤–æ–¥–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    summary = {
        "experiment_name": experiment_name,
        "total_epochs": epoch + 1,
        "best_loss": convert_tensors_to_json(best_loss),
        "final_loss": convert_tensors_to_json(current_loss),
        "dataset_samples": dataset_stats.total_samples,
        "config_summary": {
            "lattice_size": config.lattice.dimensions,
            "state_size": config.model.state_size,
            "batch_size": config.training_embedding.embedding_batch_size,
            "learning_rate": config.training.learning_rate,
            "test_mode": config.training_embedding.test_mode,
        },
        "completion_time": datetime.now().isoformat(),
    }

    with open(experiment_dir / "experiment_summary.json", "w") as f:
        json.dump(summary, f, indent=2)

    logger.info(f"\nüéâ TRAINING COMPLETED!")
    logger.info(f"üìä Experiment results saved to: {experiment_dir}")
    logger.info(f"üèÜ Best loss achieved: {best_loss:.6f}")
    logger.info(f"üìà Total samples processed: {dataset_stats.total_samples}")
    logger.info(f"\nüöÄ Ready for analysis and next steps!")


if __name__ == "__main__":
    main()
