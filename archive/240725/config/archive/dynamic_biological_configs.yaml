# ðŸ§  DYNAMIC BIOLOGICAL CONFIGURATION SYSTEM
# All dimensions and parameters are configurable and biologically derived

# =============================================================================
# DYNAMIC BROCA'S AREA CONFIGURATION
# =============================================================================

broca_area_dynamic:
  name: "Broca Area Dynamic Configuration"
  description: "Fully configurable biologically accurate architecture"

  # === PIPELINE CONFIGURATION ===
  pipeline:
    mode: "direct_embedding" # direct_embedding, text_to_text, hybrid_embedding
    skip_tokenization: true # Skip tokenization for embedding-based modes
    use_existing_components: true # Leverage phrase_bank_decoder, universal_adapter

  # === CORE BIOLOGICAL PARAMETERS ===
  biological:
    brain_region: "broca" # broca, wernicke, custom
    neural_accuracy: "high" # low, medium, high, research
    biological_constraints: true # Enable biological plausibility

  # === LATTICE CONFIGURATION (DYNAMIC) ===
  lattice:
    # Primary dimensions (configurable)
    x: 333 # Width (neurons)
    y: 333 # Height (neurons)
    z: 166 # Depth (â‰ˆ0.5 * width, biologically accurate)

    # Scaling system
    scale_factor: 0.1 # 0.1 for dev, 0.3 for research, 1.0 for production
    min_scale: 0.01 # Minimum allowed scale
    max_scale: 1.0 # Maximum allowed scale
    xs: x*scale_factor
    ys: y*scale_factor
    zs: z*scale_factor

    # Computed values (auto-calculated)
    total_neurons: "{xs * ys * zs}" # Auto-computed from dimensions
    surface_size: "{xs * ys}" # Surface area calculation
    volume: "{xs * ys * zs}" # Total volume

    # Local processing
    gmlp_params: 10000 # Per-region parameters
    connectivity_pattern: "small_world" # biological, small_world, scale_free
    connectivity_radius: 3 # Connection radius for local interactions

  # === EMBEDDING CONFIGURATION (DYNAMIC) ===
  embeddings:
    # Primary embedding dimension (configurable)
    embedding_dim: "{xs*ys}"
    teacher_embedding_dim: 768 # Can be 512, 768, 1024, 1536, 2048, etc.

    # Auto-adaptation settings
    adaptive_embedding: true # Auto-adapt to teacher model
    teacher_compatibility: true # Ensure teacher model compatibility

    # Embedding strategies
    compression_strategy: "learned_linear" # learned_linear, hierarchical, attention_based
    reconstruction_loss_weight: 0.1 # Weight for reconstruction loss

  # === HARDWARE OPTIMIZATION (DYNAMIC) ===
  hardware:
    # Memory management
    memory:
      training_allocation: "auto" # Auto-calculate based on scale
      inference_allocation: "auto" # Auto-calculate for inference
      gradient_checkpointing: true # Enable gradient checkpointing
      memory_efficient_attention: true # Memory-efficient attention

    # Performance optimization
    performance:
      batch_size: 64 # Configurable batch size
      precision: "fp16" # fp16, fp32, bf4
      tensor_cores: true # Enable Tensor Cores
      jax_acceleration: true # Enable JAX acceleration

    # Scaling configuration
    scaling:
      training_scale: 0.3 # Scale for training (memory optimization)
      inference_scale: 1.0 # Scale for inference (full accuracy)
      adaptive_scaling: true # Enable adaptive scaling
      scale_strategy: "memory_aware" # Scaling strategy

  # === TRAINING CONFIGURATION (BIOLOGICALLY INSPIRED) ===
  training:
    # Learning parameters
    learning:
      rate: 0.0001 # Learning rate
      scheduler: "cosine_annealing" # Learning rate scheduler
      warmup_steps: 1000 # Warmup steps

    # Biological learning
    biological_learning:
      stdp_like: true # STDP-like plasticity
      local_learning: true # Local learning rules
      homeostatic: true # Homeostatic plasticity

    # Training strategy
    strategy:
      incremental_complexity: true # Start simple, increase complexity
      stage_progression: true # Word -> phrase -> sentence progression
      biological_curriculum: true # Biologically inspired curriculum

  # === INTEGRATION CONFIGURATION ===
  integration:
    # Component integration
    components:
      phrase_bank_decoder: true # Enable phrase_bank_decoder integration
      universal_adapter: true # Enable universal_adapter compatibility
      computational_graph_stability: true # Enable graph stability measures

    # CAX acceleration
    cax:
      enabled: true # Enable CAX acceleration
      speedup_target: "2000x" # Target speedup for CA processing
      biological_models: ["Lenia", "NCA"] # Supported biological models

    # Error handling
    error_handling:
      graph_stabilization: true # Mamba-based graph stabilization
      memory_management: true # Advanced memory management
      fallback_strategies: true # Enable fallback strategies
