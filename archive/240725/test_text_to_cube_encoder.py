#!/usr/bin/env python3
"""
–¢–µ—Å—Ç TextToCubeEncoder –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
"""

import torch
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ energy_flow
sys.path.append('energy_flow')

from energy_flow.config import create_debug_config
from energy_flow.text_bridge import TextToCubeEncoder, create_text_to_cube_encoder

def test_text_to_cube_encoder():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TextToCubeEncoder...")
    
    # –°–æ–∑–¥–∞–µ–º debug –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = create_debug_config()
    print(f"üìê –†–∞–∑–º–µ—Ä—ã –∫—É–±–∞: {config.lattice_width}√ó{config.lattice_height}√ó{config.lattice_depth}")
    print(f"üìè Surface dim: {config.lattice_width * config.lattice_height}")
    
    # –°–æ–∑–¥–∞–µ–º encoder
    encoder = create_text_to_cube_encoder(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
    test_texts = [
        "Hello world!",
        "This is a test sentence for energy flow cube.",
        "Machine learning and neural networks are fascinating.",
        "–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç",
        "A much longer text that should test the tokenization and encoding capabilities of our TextToCubeEncoder model implementation."
    ]
    
    print(f"\nüî§ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_texts)} —Ç–µ–∫—Å—Ç–∞—Ö...")
    
    # –¢–µ—Å—Ç –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    print("\n1Ô∏è‚É£ –¢–µ—Å—Ç –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:")
    single_result = encoder.encode_text(test_texts[0])
    print(f"   –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç: '{test_texts[0]}'")
    print(f"   –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {single_result.shape}")
    print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={single_result.mean():.4f}, std={single_result.std():.4f}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: [{single_result.min():.4f}, {single_result.max():.4f}]")
    
    # –¢–µ—Å—Ç –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤
    print("\n2Ô∏è‚É£ –¢–µ—Å—Ç –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤:")
    batch_result = encoder.encode_text(test_texts)
    print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {len(test_texts)}")
    print(f"   –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {batch_result.shape}")
    print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–∞: mean={batch_result.mean():.4f}, std={batch_result.std():.4f}")
    
    # –¢–µ—Å—Ç reshape –≤ 2D –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å
    print("\n3Ô∏è‚É£ –¢–µ—Å—Ç reshape –≤ 2D –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å:")
    surface_2d = encoder.reshape_to_surface(batch_result)
    print(f"   –†–∞–∑–º–µ—Ä 2D –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏: {surface_2d.shape}")
    print(f"   –û–∂–∏–¥–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä: [{len(test_texts)}, {config.lattice_height}, {config.lattice_width}]")
    
    # –¢–µ—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\n4Ô∏è‚É£ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ:")
    tokenizer = encoder.get_tokenizer()
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {encoder.get_vocab_size():,}")
    print(f"   Pad token: {tokenizer.pad_token}")
    
    # –¢–µ—Å—Ç —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–∞
    print("\n5Ô∏è‚É£ –¢–µ—Å—Ç —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–∞:")
    for i, text in enumerate(test_texts):
        tokens = tokenizer.encode(text)
        result = encoder.encode_text(text)
        print(f"   –¢–µ–∫—Å—Ç {i+1}: {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí {result.shape} —ç–º–±–µ–¥–¥–∏–Ω–≥")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
    print("\n6Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏:")
    total_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"   –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (~5M): {'‚úÖ OK' if total_params < 7_000_000 else '‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ'}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å CUDA –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    print(f"\n7Ô∏è‚É£ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ:")
    device = next(encoder.parameters()).device
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {device}")
    print(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    print(f"   Default device: {torch.get_default_device()}")
    
    print("\n‚úÖ –¢–µ—Å—Ç TextToCubeEncoder –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return True

if __name__ == "__main__":
    try:
        test_text_to_cube_encoder()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}")
        import traceback
        traceback.print_exc()