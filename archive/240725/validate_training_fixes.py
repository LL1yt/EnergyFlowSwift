#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è gradient flow –≤ energy_flow training
===========================================================================

–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏:
- FlowProcessor gradient flow
- Individual components testing  
- Loss computation analysis
- In-place operations detection
"""

import torch
import torch.nn as nn
import sys
from pathlib import Path
import traceback

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from energy_flow.config import create_debug_config, set_energy_config
from energy_flow.core import FlowProcessor, EnergyLattice, SimpleNeuron, EnergyCarrier
from energy_flow.text_bridge import TextToCubeEncoder, CubeToTextDecoder
from energy_flow.utils.logging import get_logger, DEBUG_TRAINING

logger = get_logger(__name__)

def setup_test_environment():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    print("üîß Setting up test environment...")
    
    # Debug –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    config = create_debug_config()
    set_energy_config(config)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º CUDA –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    if torch.cuda.is_available():
        torch.set_default_device('cuda')
        print(f"‚úÖ Using CUDA device: {torch.cuda.get_device_name()}")
    else:
        print("‚ö†Ô∏è CUDA not available, using CPU")
    
    return config

def test_individual_components(config):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –æ—Ç–¥–µ–ª—å–Ω–æ –Ω–∞ gradient flow"""
    print("\nüß™ Testing individual components...")
    
    batch_size = 4
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    results = {}
    
    # 1. –¢–µ—Å—Ç SimpleNeuron
    print("üìç Testing SimpleNeuron...")
    try:
        neuron = SimpleNeuron(config).to(device)
        positions = torch.randn(batch_size, 3, device=device, requires_grad=True)
        energies = torch.randn(batch_size, 1, device=device, requires_grad=True)
        
        output = neuron(positions, energies)
        loss = output.sum()
        loss.backward()
        
        has_grads = any(p.grad is not None for p in neuron.parameters())
        results['SimpleNeuron'] = f"‚úÖ Gradients: {has_grads}"
        print(f"  ‚úÖ SimpleNeuron: gradients={has_grads}, output_shape={output.shape}")
        
    except Exception as e:
        results['SimpleNeuron'] = f"‚ùå Error: {e}"
        print(f"  ‚ùå SimpleNeuron failed: {e}")
    
    # 2. –¢–µ—Å—Ç EnergyCarrier
    print("üìç Testing EnergyCarrier...")
    try:
        carrier = EnergyCarrier(config).to(device)
        neuron_output = torch.randn(batch_size, config.neuron_output_dim, device=device, requires_grad=True)
        energies = torch.randn(batch_size, 1, device=device, requires_grad=True)
        hidden_states = torch.randn(config.carrier_num_layers, batch_size, config.carrier_hidden_size, device=device, requires_grad=True)
        positions = torch.randn(batch_size, 3, device=device, requires_grad=True)
        ages = torch.randn(batch_size, device=device)
        
        output, new_hidden = carrier(neuron_output, energies, hidden_states, positions, ages)
        loss = output.energy_value.sum() + new_hidden.sum()
        loss.backward()
        
        has_grads = any(p.grad is not None for p in carrier.parameters())
        results['EnergyCarrier'] = f"‚úÖ Gradients: {has_grads}"
        print(f"  ‚úÖ EnergyCarrier: gradients={has_grads}, energy_shape={output.energy_value.shape}")
        
    except Exception as e:
        results['EnergyCarrier'] = f"‚ùå Error: {e}"
        print(f"  ‚ùå EnergyCarrier failed: {e}")
    
    # 3. –¢–µ—Å—Ç TextToCubeEncoder
    print("üìç Testing TextToCubeEncoder...")
    try:
        encoder = TextToCubeEncoder(config).to(device)
        texts = ["Test text for encoding", "Another test sentence", "Machine learning works", "Neural networks process"]
        
        output = encoder.encode_text(texts)
        loss = output.sum()
        loss.backward()
        
        has_grads = any(p.grad is not None for p in encoder.parameters())
        results['TextToCubeEncoder'] = f"‚úÖ Gradients: {has_grads}"
        print(f"  ‚úÖ TextToCubeEncoder: gradients={has_grads}, output_shape={output.shape}")
        
    except Exception as e:
        results['TextToCubeEncoder'] = f"‚ùå Error: {e}"
        print(f"  ‚ùå TextToCubeEncoder failed: {e}")
    
    return results

def test_flow_processor_gradients(config):
    """–î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ gradient flow —á–µ—Ä–µ–∑ FlowProcessor"""
    print("\nüî¨ Testing FlowProcessor gradient flow...")
    
    batch_size = 2  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # –°–æ–∑–¥–∞–µ–º FlowProcessor
        flow_processor = FlowProcessor(config).to(device)
        
        # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
        input_embeddings = torch.randn(batch_size, 768, device=device, requires_grad=True)
        print(f"üìä Input embeddings: {input_embeddings.shape}, requires_grad={input_embeddings.requires_grad}")
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        print("üîÑ Forward pass through FlowProcessor...")
        output_surface = flow_processor.forward(input_embeddings, max_steps=5)  # –ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ—Ö–æ–¥
        
        print(f"üìä Output surface: {output_surface.shape}, requires_grad={output_surface.requires_grad}")
        print(f"üìä Output stats: mean={output_surface.mean():.4f}, std={output_surface.std():.4f}")
        
        # –ü—Ä–æ—Å—Ç–æ–π loss –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        loss = output_surface.sum()
        print(f"üìä Loss: {loss:.4f}, requires_grad={loss.requires_grad}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ loss –∏–º–µ–µ—Ç grad_fn
        if loss.grad_fn is not None:
            print(f"‚úÖ Loss has grad_fn: {type(loss.grad_fn).__name__}")
        else:
            print("‚ùå Loss has no grad_fn!")
            return {"FlowProcessor": "‚ùå No grad_fn in loss"}
        
        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        print("üîÑ Backward pass...")
        loss.backward()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
        components_with_grads = {}
        
        # FlowProcessor parameters
        fp_params = sum(1 for p in flow_processor.parameters() if p.grad is not None)
        fp_total = sum(1 for p in flow_processor.parameters())
        components_with_grads['FlowProcessor'] = f"{fp_params}/{fp_total}"
        
        # Lattice parameters  
        lattice_params = sum(1 for p in flow_processor.lattice.parameters() if p.grad is not None)
        lattice_total = sum(1 for p in flow_processor.lattice.parameters())
        components_with_grads['EnergyLattice'] = f"{lattice_params}/{lattice_total}"
        
        # Neuron parameters
        neuron_params = sum(1 for p in flow_processor.neuron.parameters() if p.grad is not None)
        neuron_total = sum(1 for p in flow_processor.neuron.parameters())
        components_with_grads['SimpleNeuron'] = f"{neuron_params}/{neuron_total}"
        
        # Carrier parameters
        carrier_params = sum(1 for p in flow_processor.carrier.parameters() if p.grad is not None)
        carrier_total = sum(1 for p in flow_processor.carrier.parameters())
        components_with_grads['EnergyCarrier'] = f"{carrier_params}/{carrier_total}"
        
        # Mapper parameters
        mapper_params = sum(1 for p in flow_processor.mapper.parameters() if p.grad is not None)
        mapper_total = sum(1 for p in flow_processor.mapper.parameters())
        components_with_grads['EmbeddingMapper'] = f"{mapper_params}/{mapper_total}"
        
        print("üìä Components with gradients:")
        for comp, ratio in components_with_grads.items():
            print(f"  {comp}: {ratio}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º input gradients
        input_grad_–Ω–æ—Ä–º = 0
        if input_embeddings.grad is not None:
            input_grad_–Ω–æ—Ä–º = input_embeddings.grad.norm().item()
            print(f"‚úÖ Input gradients: norm={input_grad_–Ω–æ—Ä–º:.6f}")
        else:
            print("‚ùå No gradients in input_embeddings")
        
        return {
            "FlowProcessor": "‚úÖ Gradient flow successful", 
            "components": components_with_grads,
            "input_grad_norm": input_grad_–Ω–æ—Ä–º
        }
        
    except Exception as e:
        print(f"‚ùå FlowProcessor gradient test failed: {e}")
        traceback.print_exc()
        return {"FlowProcessor": f"‚ùå Error: {e}"}

def debug_gradient_flow(config):
    """–ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ gradient flow –≤ —Ä–µ–∞–ª—å–Ω–æ–º training setup"""
    print("\nüêõ Debugging real training setup...")
    
    batch_size = 2
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º training
        flow_processor = FlowProcessor(config).to(device)
        text_encoder = TextToCubeEncoder(config).to(device)
        
        # Teacher embeddings (–∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º training)
        teacher_input = torch.randn(batch_size, 768, device=device, requires_grad=False)  # Teacher –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤!
        teacher_target = torch.randn(batch_size, 768, device=device, requires_grad=False)
        
        # –¢–µ–∫—Å—Ç—ã
        input_texts = ["Test input text", "Another input"]
        target_texts = ["Target output text", "Another target"]
        
        print(f"üìä Teacher embeddings: input={teacher_input.requires_grad}, target={teacher_target.requires_grad}")
        
        # 1. –û—Å–Ω–æ–≤–Ω–æ–π energy flow
        print("üîÑ Energy flow forward pass...")
        cube_output = flow_processor.forward(teacher_input, max_steps=3)
        print(f"üìä Cube output: {cube_output.shape}, requires_grad={cube_output.requires_grad}")
        
        # 2. Target mapping 
        print("üîÑ Target mapping...")
        target_surface = flow_processor.mapper.input_mapper.forward(teacher_target)
        if target_surface.dim() == 3:
            target_surface = target_surface.view(batch_size, -1)
        print(f"üìä Target surface: {target_surface.shape}, requires_grad={target_surface.requires_grad}")
        
        # 3. Energy loss
        print("üîÑ Computing energy loss...")
        energy_loss = nn.functional.mse_loss(cube_output, target_surface)
        print(f"üìä Energy loss: {energy_loss:.4f}, requires_grad={energy_loss.requires_grad}")
        
        # 4. Text encoding
        print("üîÑ Text encoding...")
        encoder_outputs = text_encoder.encode_text(input_texts)
        print(f"üìä Encoder outputs: {encoder_outputs.shape}, requires_grad={encoder_outputs.requires_grad}")
        
        # 5. Text loss
        print("üîÑ Computing text loss...")
        text_loss = nn.functional.mse_loss(encoder_outputs, target_surface)
        print(f"üìä Text loss: {text_loss:.4f}, requires_grad={text_loss.requires_grad}")
        
        # 6. Combined loss
        print("üîÑ Computing combined loss...")
        total_loss = energy_loss + 0.1 * text_loss
        print(f"üìä Total loss: {total_loss:.4f}, requires_grad={total_loss.requires_grad}")
        
        # 7. –ü—Ä–æ–≤–µ—Ä—è–µ–º grad_fn —Ü–µ–ø–æ—á–∫—É
        print("üîó Gradient function chain:")
        if total_loss.grad_fn:
            print(f"  total_loss.grad_fn: {type(total_loss.grad_fn).__name__}")
            if hasattr(total_loss.grad_fn, 'next_functions'):
                for i, (fn, _) in enumerate(total_loss.grad_fn.next_functions):
                    if fn:
                        print(f"    [{i}] {type(fn).__name__}")
        
        # 8. Backward pass
        print("üîÑ Backward pass...")
        total_loss.backward()
        
        # 9. –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        print("üìä Gradient summary:")
        
        # FlowProcessor gradients
        fp_grads = [(name, p.grad.norm().item()) for name, p in flow_processor.named_parameters() if p.grad is not None]
        print(f"  FlowProcessor: {len(fp_grads)} params with gradients")
        if fp_grads:
            avg_fp_grad = sum(grad for _, grad in fp_grads) / len(fp_grads)
            print(f"    Average gradient norm: {avg_fp_grad:.6f}")
        
        # TextEncoder gradients
        te_grads = [(name, p.grad.norm().item()) for name, p in text_encoder.named_parameters() if p.grad is not None]
        print(f"  TextEncoder: {len(te_grads)} params with gradients")
        if te_grads:
            avg_te_grad = sum(grad for _, grad in te_grads) / len(te_grads)
            print(f"    Average gradient norm: {avg_te_grad:.6f}")
        
        return {
            "status": "‚úÖ Debug successful",
            "flow_processor_grads": len(fp_grads),
            "text_encoder_grads": len(te_grads),
            "energy_loss": energy_loss.item(),
            "text_loss": text_loss.item(),
            "total_loss": total_loss.item()
        }
        
    except Exception as e:
        print(f"‚ùå Debug failed: {e}")
        traceback.print_exc()
        return {"status": f"‚ùå Error: {e}"}

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    print("üî¨ Energy Flow Training Diagnostics")
    print("=" * 50)
    
    config = setup_test_environment()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    tests = [
        ("Individual Components", lambda: test_individual_components(config)),
        ("FlowProcessor Gradients", lambda: test_flow_processor_gradients(config)),
        ("Gradient Flow Debug", lambda: debug_gradient_flow(config))
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results[test_name] = result
        except Exception as e:
            print(f"‚ùå Test '{test_name}' crashed: {e}")
            results[test_name] = {"error": str(e)}
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"\n{'='*20} SUMMARY {'='*20}")
    for test_name, result in results.items():
        print(f"üìã {test_name}:")
        if isinstance(result, dict):
            for key, value in result.items():
                print(f"  {key}: {value}")
        else:
            print(f"  {result}")
    
    print(f"\nüèÅ Diagnostics completed!")

if __name__ == "__main__":
    main()