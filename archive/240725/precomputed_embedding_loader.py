#!/usr/bin/env python3
"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import torch
from pathlib import Path
import json
import logging
from typing import Optional, Dict, Any
from torch.utils.data import Dataset

logger = logging.getLogger(__name__)


class PrecomputedEmbeddingDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""

    def __init__(
        self, question_embeddings: torch.Tensor, answer_embeddings: torch.Tensor
    ):
        assert len(question_embeddings) == len(
            answer_embeddings
        ), f"Mismatch: {len(question_embeddings)} questions vs {len(answer_embeddings)} answers"

        self.question_embeddings = question_embeddings
        self.answer_embeddings = answer_embeddings

    def __len__(self):
        return len(self.question_embeddings)

    def __getitem__(self, idx):
        return self.question_embeddings[idx], self.answer_embeddings[idx]


class PrecomputedEmbeddingLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""

    def __init__(self):
        self.dataset_cache = {}  # –ö—ç—à –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

    def load_dataset(
        self, embeddings_file: str, use_cache: bool = True
    ) -> PrecomputedEmbeddingDataset:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞

        Args:
            embeddings_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ (.pt)
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫

        Returns:
            PrecomputedEmbeddingDataset
        """
        embeddings_path = Path(embeddings_file)

        if not embeddings_path.exists():
            raise FileNotFoundError(f"Embedding file not found: {embeddings_file}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = str(embeddings_path.absolute())
        if use_cache and cache_key in self.dataset_cache:
            logger.info(f"[CACHE] Loading dataset from cache: {embeddings_path.name}")
            return self.dataset_cache[cache_key]

        logger.info(f"[LOAD] Loading embedding dataset from: {embeddings_path.name}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            data = torch.load(embeddings_file, map_location=device)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
            required_keys = ["question_embeddings", "answer_embeddings"]
            missing_keys = [key for key in required_keys if key not in data]
            if missing_keys:
                raise ValueError(f"Missing keys in embedding file: {missing_keys}")

            question_embeddings = data["question_embeddings"]
            answer_embeddings = data["answer_embeddings"]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if question_embeddings.shape[0] != answer_embeddings.shape[0]:
                raise ValueError(
                    f"Size mismatch: {question_embeddings.shape[0]} questions vs {answer_embeddings.shape[0]} answers"
                )

            if question_embeddings.shape[1] != answer_embeddings.shape[1]:
                raise ValueError(
                    f"Dimension mismatch: {question_embeddings.shape[1]} vs {answer_embeddings.shape[1]}"
                )

            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = PrecomputedEmbeddingDataset(
                question_embeddings, answer_embeddings
            )

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            logger.info(f"‚úÖ Dataset loaded successfully:")
            logger.info(f"   Size: {len(dataset):,} pairs")
            logger.info(f"   Embedding dimension: {question_embeddings.shape[1]}")
            logger.info(f"   Teacher model: {data.get('teacher_model', 'unknown')}")
            logger.info(
                f"   File size: {embeddings_path.stat().st_size / 1024 / 1024:.1f} MB"
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
            q_norms = question_embeddings.norm(dim=1)
            a_norms = answer_embeddings.norm(dim=1)
            logger.info(
                f"   Question embeddings norm: mean={q_norms.mean():.4f}, std={q_norms.std():.4f}"
            )
            logger.info(
                f"   Answer embeddings norm: mean={a_norms.mean():.4f}, std={a_norms.std():.4f}"
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if use_cache:
                self.dataset_cache[cache_key] = dataset

            return dataset

        except Exception as e:
            logger.error(f"‚ùå Failed to load embedding dataset: {e}")
            raise

    def list_available_datasets(self, data_dir: str = "data/embeddings") -> list:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        data_path = Path(data_dir)

        if not data_path.exists():
            logger.warning(f"Data directory not found: {data_dir}")
            return []

        # –ò—â–µ–º .pt —Ñ–∞–π–ª—ã
        embedding_files = list(data_path.glob("*.pt"))

        datasets_info = []
        for file_path in embedding_files:
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                data = torch.load(file_path, map_location=device)

                info = {
                    "file_path": str(file_path),
                    "filename": file_path.name,
                    "size": data.get("size", "unknown"),
                    "teacher_model": data.get("teacher_model", "unknown"),
                    "timestamp": data.get("timestamp", "unknown"),
                    "file_size_mb": file_path.stat().st_size / 1024 / 1024,
                }

                datasets_info.append(info)

            except Exception as e:
                logger.warning(f"Failed to read metadata from {file_path.name}: {e}")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        datasets_info.sort(key=lambda x: x["timestamp"], reverse=True)

        return datasets_info

    def get_latest_dataset(self, data_dir: str = "data/embeddings") -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–∞–º–æ–º—É –Ω–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É"""
        datasets = self.list_available_datasets(data_dir)

        if not datasets:
            return None

        return datasets[0]["file_path"]

    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        self.dataset_cache.clear()
        logger.info("[CLEAR] Dataset cache cleared")


def create_precomputed_dataset(embeddings_file: str) -> PrecomputedEmbeddingDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    –°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º simple_embedding_fallback
    """
    loader = PrecomputedEmbeddingLoader()
    return loader.load_dataset(embeddings_file)


def test_precomputed_loader():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    print("üß™ Testing PrecomputedEmbeddingLoader")

    loader = PrecomputedEmbeddingLoader()

    # 1. –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    print("\n[LIST] Available datasets:")
    datasets = loader.list_available_datasets()

    if not datasets:
        print("   No datasets found. Run generate_large_embedding_dataset.py first!")
        return

    for i, dataset_info in enumerate(datasets):
        print(f"   {i+1}. {dataset_info['filename']}")
        print(f"      Size: {dataset_info['size']:,} pairs")
        print(f"      Model: {dataset_info['teacher_model']}")
        print(f"      File size: {dataset_info['file_size_mb']:.1f} MB")
        print(f"      Created: {dataset_info['timestamp']}")
        print()

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    latest_file = loader.get_latest_dataset()
    if latest_file:
        print(f"[LOAD] Loading latest dataset: {Path(latest_file).name}")
        dataset = loader.load_dataset(latest_file)

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        sample_q, sample_a = dataset[0]
        print(f"‚úÖ Sample loaded:")
        print(f"   Question embedding shape: {sample_q.shape}")
        print(f"   Answer embedding shape: {sample_a.shape}")
        print(f"   Question norm: {sample_q.norm().item():.6f}")
        print(f"   Answer norm: {sample_a.norm().item():.6f}")

        # 4. –¢–µ—Å—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ (–∫—ç—à)
        print(f"\n[CACHE] Testing cache...")
        dataset2 = loader.load_dataset(latest_file)
        print(f"   Cache working: {dataset is dataset2}")


if __name__ == "__main__":
    test_precomputed_loader()
