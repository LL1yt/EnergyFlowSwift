"""
AutoencoderDataset - –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫ –æ–±—É—á–µ–Ω–∏—é –∫—É–±–∞ –≤ autoencoder —Ä–µ–∂–∏–º–µ

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core
–Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (autoencoder mode).

–ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å EmbeddingLoader –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
- Smart caching –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
- Batch processing —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã, –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏)
- Adaptive sampling –¥–ª—è balanced training

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: v1.0.0 (Phase 3.1 - Stage 1.2)
–î–∞—Ç–∞: 6 –∏—é–Ω—è 2025
"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
import logging
from typing import Dict, Any, Optional, Union, List, Tuple
import json
import pickle
import hashlib
from dataclasses import dataclass
import random

# –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from data.embedding_loader import EmbeddingLoader
    EMBEDDING_LOADER_AVAILABLE = True
except ImportError as e:
    print(f"[WARNING]  Warning: EmbeddingLoader not available: {e}")
    EMBEDDING_LOADER_AVAILABLE = False


@dataclass
class DatasetConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è AutoencoderDataset"""
    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    data_sources: List[str] = None  # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –¥–∞–Ω–Ω—ã–º–∏
    embedding_format: str = "llm"   # word2vec | glove | bert | llm
    llm_model: str = "distilbert"   # –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    
    # –†–∞–∑–º–µ—Ä—ã –∏ —Ñ–æ—Ä–º–∞—Ç
    embedding_dim: int = 768        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    max_samples: int = 10000       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–º–ø–ª–æ–≤
    min_samples: int = 100         # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–º–ø–ª–æ–≤
    
    # Preprocessing
    normalize_embeddings: bool = True
    center_embeddings: bool = True
    add_noise: bool = False        # –î–æ–±–∞–≤–ª—è—Ç—å –ª–∏ —à—É–º –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    noise_std: float = 0.01        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —à—É–º–∞
    
    # Caching
    cache_dir: str = "cache/autoencoder_dataset"
    use_cache: bool = True
    cache_embeddings: bool = True
    cache_batch_size: int = 1000   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    
    # Validation split
    validation_split: float = 0.2  # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    shuffle_data: bool = True      # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
    random_seed: int = 42          # Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    def __post_init__(self):
        if self.data_sources is None:
            self.data_sources = []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫—ç—à–∞
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)


class AutoencoderDataset(Dataset):
    """
    Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D Cubic Core –≤ autoencoder —Ä–µ–∂–∏–º–µ
    
    –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä—ã (embedding, embedding) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏,
    —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π EmbeddingLoader –∏ smart caching —Å–∏—Å—Ç–µ–º–æ–π.
    """
    
    def __init__(self, 
                 config: Optional[Union[DatasetConfig, Dict, str]] = None,
                 texts: Optional[List[str]] = None,
                 embeddings: Optional[torch.Tensor] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AutoencoderDataset
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è dataset (DatasetConfig, dict –∏–ª–∏ –ø—É—Ç—å –∫ JSON)
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            embeddings: –ì–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info("üöÄ Initializing AutoencoderDataset...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        if not EMBEDDING_LOADER_AVAILABLE:
            raise ImportError("EmbeddingLoader is required for AutoencoderDataset. "
                            "Make sure data.embedding_loader is implemented.")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(config)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed
        random.seed(self.config.random_seed)
        np.random.seed(self.config.random_seed)
        torch.manual_seed(self.config.random_seed)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EmbeddingLoader
        self.embedding_loader = EmbeddingLoader(
            cache_dir=str(Path(self.config.cache_dir) / "embedding_loader_cache")
        )
        
        # –î–∞–Ω–Ω—ã–µ
        self.embeddings: torch.Tensor = None
        self.train_embeddings: torch.Tensor = None
        self.val_embeddings: torch.Tensor = None
        self.is_validation_mode: bool = False
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.dataset_info = {}
        self.cache_stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'total_loads': 0
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if embeddings is not None:
            self.logger.info("Using provided embeddings")
            self._load_from_embeddings(embeddings)
        elif texts is not None:
            self.logger.info("Generating embeddings from texts")
            self._load_from_texts(texts)
        elif self.config.data_sources:
            self.logger.info("Loading data from configured sources")
            self._load_from_sources()
        else:
            raise ValueError("No data source provided. Specify embeddings, texts, or data_sources in config.")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ train/val split
        self._create_train_val_split()
        
        self.logger.info(f"‚úÖ AutoencoderDataset initialized successfully")
        self.logger.info(f"   Total samples: {len(self.embeddings)}")
        self.logger.info(f"   Train samples: {len(self.train_embeddings)}")
        self.logger.info(f"   Val samples: {len(self.val_embeddings)}")
        self.logger.info(f"   Embedding dim: {self.embeddings.shape[1] if len(self.embeddings.shape) > 1 else 'Unknown'}")
    
    def _load_config(self, config: Optional[Union[DatasetConfig, Dict, str]]) -> DatasetConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if config is None:
            return DatasetConfig()
        
        elif isinstance(config, DatasetConfig):
            return config
        
        elif isinstance(config, dict):
            return DatasetConfig(**config)
        
        elif isinstance(config, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ JSON —Ñ–∞–π–ª–∞
            try:
                with open(config, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                return DatasetConfig(**config_data)
            except Exception as e:
                self.logger.error(f"Failed to load config from {config}: {e}")
                return DatasetConfig()
        
        else:
            self.logger.warning(f"Unknown config type: {type(config)}. Using default config.")
            return DatasetConfig()
    
    def _load_from_embeddings(self, embeddings: torch.Tensor):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤"""
        if not isinstance(embeddings, torch.Tensor):
            embeddings = torch.from_numpy(np.array(embeddings)).float()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if len(embeddings.shape) != 2:
            raise ValueError(f"Embeddings should be 2D tensor, got shape: {embeddings.shape}")
        
        if embeddings.shape[1] != self.config.embedding_dim:
            self.logger.warning(f"Embedding dimension mismatch: got {embeddings.shape[1]}, "
                              f"expected {self.config.embedding_dim}")
            self.config.embedding_dim = embeddings.shape[1]
        
        self.embeddings = embeddings
        self._update_dataset_info("provided_embeddings", embeddings.shape[0])
    
    def _load_from_texts(self, texts: List[str]):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ EmbeddingLoader"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = self._create_cache_key_for_texts(texts)
        cached_embeddings = self._load_from_cache(cache_key)
        
        if cached_embeddings is not None and self.config.use_cache:
            self.logger.info("Loading embeddings from cache")
            self.embeddings = cached_embeddings
            self.cache_stats['cache_hits'] += 1
        else:
            self.logger.info(f"Generating embeddings from {len(texts)} texts using {self.config.llm_model}")
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤
            if len(texts) > self.config.max_samples:
                texts = texts[:self.config.max_samples]
                self.logger.info(f"Limited to {self.config.max_samples} texts")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
            embeddings = self.embedding_loader.batch_load_from_llm(
                texts=texts,
                model_key=self.config.llm_model,
                batch_size=self.config.cache_batch_size
            )
            
            # Preprocessing
            if self.config.normalize_embeddings or self.config.center_embeddings:
                embeddings = self.embedding_loader.preprocess_embeddings(
                    embeddings,
                    normalize=self.config.normalize_embeddings,
                    center=self.config.center_embeddings
                )
            
            self.embeddings = embeddings
            self.cache_stats['cache_misses'] += 1
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            if self.config.cache_embeddings:
                self._save_to_cache(cache_key, embeddings)
        
        self._update_dataset_info("generated_from_texts", len(texts))
    
    def _load_from_sources(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_embeddings = []
        
        for source_path in self.config.data_sources:
            source_path = Path(source_path)
            
            if not source_path.exists():
                self.logger.warning(f"Source file not found: {source_path}")
                continue
            
            try:
                if source_path.suffix in ['.txt']:
                    # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª - —á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ —Ç–µ–∫—Å—Ç—ã
                    with open(source_path, 'r', encoding='utf-8') as f:
                        texts = [line.strip() for line in f if line.strip()]
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–∏–Ω–≥–∏
                    embeddings = self.embedding_loader.batch_load_from_llm(
                        texts=texts,
                        model_key=self.config.llm_model,
                        batch_size=self.config.cache_batch_size
                    )
                    
                elif source_path.suffix in ['.pt', '.pth']:
                    # PyTorch —Ç–µ–Ω–∑–æ—Ä
                    embeddings = torch.load(source_path)
                    
                elif source_path.suffix in ['.npy']:
                    # NumPy –º–∞—Å—Å–∏–≤
                    embeddings = torch.from_numpy(np.load(source_path)).float()
                    
                elif source_path.suffix in ['.pkl', '.pickle']:
                    # Pickle —Ñ–∞–π–ª
                    with open(source_path, 'rb') as f:
                        data = pickle.load(f)
                    embeddings = torch.from_numpy(np.array(data)).float()
                    
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ EmbeddingLoader
                    embeddings = self.embedding_loader.load_embeddings(
                        path=source_path,
                        format_type=self.config.embedding_format,
                        preprocess=True
                    )
                
                all_embeddings.append(embeddings)
                self.logger.info(f"Loaded {embeddings.shape[0]} embeddings from {source_path}")
                
            except Exception as e:
                self.logger.error(f"Failed to load from {source_path}: {e}")
                continue
        
        if not all_embeddings:
            raise ValueError("No valid data sources found")
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        self.embeddings = torch.cat(all_embeddings, dim=0)
        self._update_dataset_info("loaded_from_sources", len(self.config.data_sources))
    
    def _create_train_val_split(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train/validation"""
        total_samples = len(self.embeddings)
        
        if self.config.validation_split > 0:
            val_size = int(total_samples * self.config.validation_split)
            train_size = total_samples - val_size
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            indices = torch.arange(total_samples)
            if self.config.shuffle_data:
                indices = indices[torch.randperm(total_samples)]
            
            train_indices = indices[:train_size]
            val_indices = indices[train_size:]
            
            self.train_embeddings = self.embeddings[train_indices]
            self.val_embeddings = self.embeddings[val_indices]
        else:
            # –ù–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            self.train_embeddings = self.embeddings
            self.val_embeddings = torch.empty(0, self.config.embedding_dim)
    
    def _create_cache_key_for_texts(self, texts: List[str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –æ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ + –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        text_content = "\n".join(texts[:100])  # –ü–µ—Ä–≤—ã–µ 100 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ö—ç—à–∞
        config_content = f"{self.config.llm_model}_{self.config.embedding_dim}_{len(texts)}"
        content = f"{text_content}_{config_content}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[torch.Tensor]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞"""
        if not self.config.use_cache:
            return None
        
        cache_path = Path(self.config.cache_dir) / f"{cache_key}.pt"
        
        if cache_path.exists():
            try:
                return torch.load(cache_path)
            except Exception as e:
                self.logger.warning(f"Failed to load from cache {cache_path}: {e}")
                return None
        
        return None
    
    def _save_to_cache(self, cache_key: str, embeddings: torch.Tensor):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ –∫—ç—à"""
        if not self.config.cache_embeddings:
            return
        
        cache_path = Path(self.config.cache_dir) / f"{cache_key}.pt"
        
        try:
            torch.save(embeddings, cache_path)
            self.logger.info(f"Cached embeddings to {cache_path}")
        except Exception as e:
            self.logger.warning(f"Failed to cache embeddings: {e}")
    
    def _update_dataset_info(self, source_type: str, sample_count: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö dataset"""
        self.dataset_info.update({
            'source_type': source_type,
            'sample_count': sample_count,
            'embedding_dim': self.config.embedding_dim,
            'config': self.config.__dict__
        })
    
    def set_validation_mode(self, is_validation: bool = True):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É train/validation —Ä–µ–∂–∏–º–∞–º–∏"""
        self.is_validation_mode = is_validation
    
    def __len__(self) -> int:
        """–†–∞–∑–º–µ—Ä dataset"""
        if self.is_validation_mode:
            return len(self.val_embeddings)
        else:
            return len(self.train_embeddings)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ dataset
        
        Args:
            idx: –ò–Ω–¥–µ–∫—Å —ç–ª–µ–º–µ–Ω—Ç–∞
            
        Returns:
            Tuple[torch.Tensor, torch.Tensor]: (input_embedding, target_embedding)
            –î–ª—è autoencoder —Ä–µ–∂–∏–º–∞ input_embedding == target_embedding
        """
        if self.is_validation_mode:
            embedding = self.val_embeddings[idx]
        else:
            embedding = self.train_embeddings[idx]
        
        # –î–ª—è autoencoder —Ä–µ–∂–∏–º–∞ —Ü–µ–ª—å = –≤—Ö–æ–¥
        input_embedding = embedding.clone()
        target_embedding = embedding.clone()
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (—Ç–æ–ª—å–∫–æ –¥–ª—è –≤—Ö–æ–¥–∞)
        if self.config.add_noise and not self.is_validation_mode:
            noise = torch.randn_like(input_embedding) * self.config.noise_std
            input_embedding = input_embedding + noise
        
        return input_embedding, target_embedding
    
    def get_dataloader(self, 
                      batch_size: int = 32, 
                      shuffle: bool = True,
                      num_workers: int = 0,
                      validation: bool = False) -> DataLoader:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ DataLoader –¥–ª—è dataset
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            shuffle: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
            num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ worker –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            validation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å validation data
            
        Returns:
            DataLoader: –ì–æ—Ç–æ–≤—ã–π DataLoader
        """
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º
        original_mode = self.is_validation_mode
        self.set_validation_mode(validation)
        
        dataloader = DataLoader(
            self,
            batch_size=batch_size,
            shuffle=shuffle and not validation,  # –ù–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º validation –¥–∞–Ω–Ω—ã–µ
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º
        self.set_validation_mode(original_mode)
        
        return dataloader
    
    def get_sample_embeddings(self, n_samples: int = 5) -> Dict[str, torch.Tensor]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            n_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
            
        Returns:
            Dict —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ train –∏ validation —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        """
        samples = {}
        
        if len(self.train_embeddings) > 0:
            train_indices = torch.randperm(len(self.train_embeddings))[:n_samples]
            samples['train'] = self.train_embeddings[train_indices]
        
        if len(self.val_embeddings) > 0:
            val_indices = torch.randperm(len(self.val_embeddings))[:n_samples]
            samples['validation'] = self.val_embeddings[val_indices]
        
        return samples
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ dataset"""
        stats = {
            'total_samples': len(self.embeddings),
            'train_samples': len(self.train_embeddings),
            'val_samples': len(self.val_embeddings),
            'embedding_dim': self.config.embedding_dim,
            'validation_split': self.config.validation_split,
            'cache_stats': self.cache_stats.copy(),
            'dataset_info': self.dataset_info.copy()
        }
        
        if len(self.embeddings) > 0:
            stats.update({
                'embedding_mean': self.embeddings.mean().item(),
                'embedding_std': self.embeddings.std().item(),
                'embedding_min': self.embeddings.min().item(),
                'embedding_max': self.embeddings.max().item()
            })
        
        return stats
    
    def save_dataset_info(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ dataset"""
        info = {
            'statistics': self.get_statistics(),
            'config': self.config.__dict__,
            'creation_time': str(Path().cwd()),  # Placeholder –¥–ª—è –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"Dataset info saved to {path}")
    
    def __repr__(self):
        return (f"AutoencoderDataset("
                f"samples={len(self.embeddings)}, "
                f"dim={self.config.embedding_dim}, "
                f"train={len(self.train_embeddings)}, "
                f"val={len(self.val_embeddings)}, "
                f"mode={'validation' if self.is_validation_mode else 'train'})")


def create_text_dataset(texts: List[str], 
                       llm_model: str = "distilbert",
                       validation_split: float = 0.2,
                       **kwargs) -> AutoencoderDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è dataset –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        llm_model: –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        validation_split: –î–æ–ª—è –¥–ª—è validation
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è DatasetConfig
        
    Returns:
        AutoencoderDataset: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π dataset
    """
    config = DatasetConfig(
        llm_model=llm_model,
        validation_split=validation_split,
        **kwargs
    )
    
    return AutoencoderDataset(config=config, texts=texts)


def create_file_dataset(file_paths: List[str],
                       embedding_format: str = "llm",
                       llm_model: str = "distilbert",
                       validation_split: float = 0.2,
                       **kwargs) -> AutoencoderDataset:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è dataset –∏–∑ —Ñ–∞–π–ª–æ–≤
    
    Args:
        file_paths: –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –¥–∞–Ω–Ω—ã–º–∏
        embedding_format: –§–æ—Ä–º–∞—Ç —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        llm_model: LLM –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ format=llm)
        validation_split: –î–æ–ª—è –¥–ª—è validation
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è DatasetConfig
        
    Returns:
        AutoencoderDataset: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π dataset
    """
    config = DatasetConfig(
        data_sources=file_paths,
        embedding_format=embedding_format,
        llm_model=llm_model,
        validation_split=validation_split,
        **kwargs
    )
    
    return AutoencoderDataset(config=config)