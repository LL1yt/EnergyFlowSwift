# Energy Flow Training Issues Analysis

Исследование трёх проблем при обучении `full_energy_trainer.py` с учётом рекомендаций из `energy_flow/CLAUDE.md`.

## 1. Переполнение памяти GPU на втором батче
- **Вероятная причина** — загрузка батчей сразу на GPU из-за глобального `torch.set_default_device('cuda')`; `collate_fn` создаёт тензоры на GPU, что не освобождается до конца эпохи【F:full_energy_trainer.py†L99-L106】【F:energy_flow/config/energy_config.py†L18-L21】
- **Дополнительный фактор** — лишнее удержание графа при градиентной аккумуляции (`retain_graph=True`) приводит к накоплению графов каждого батча【F:energy_flow/training/energy_trainer.py†L669-L696】
- **Рекомендации**
  - Формировать батчи на CPU (`torch.stack(..., device='cpu')`), использовать `pin_memory=True`, переносить на GPU внутри `train_step`.
  - Удалить `retain_graph` при накоплении градиентов; каждое `forward` строит новый граф, поэтому удержание не нужно при `gradient_accumulation_steps>1`.
  - Проверить отсутствующие освобождения (например, предсказания текста) и при необходимости вызывать `torch.cuda.empty_cache()` после больших блоков.

## 2. Замедление обучения после первого батча
- **Синхронизация CUDA** — вызов `torch.cuda.reset_peak_memory_stats()` в начале каждого шага заставляет устройство синхронизироваться【F:energy_flow/training/energy_trainer.py†L444-L449】
- **Логирование и метрики** — обилие детального логирования и сбор метрик на каждом шаге вызывает CPU↔GPU синхронизации, особенно при включённой аномалийной проверке (`set_detect_anomaly`).
- **Рекомендации**
  - Вызывать `reset_peak_memory_stats` реже или только в профилировочных режимах.
  - Ограничить детальные логи/метрики (`DEBUG_*` уровни) после отладки, использовать rate limiting.
  - Проверить, что DataLoader работает на CPU и не держит заранее загруженные GPU батчи.

## 3. `RuntimeError: CudnnRnnBackward0 returned nan values`
- **Контекст** — ошибка исходит из GRU в `EnergyCarrier`【F:energy_flow/core/energy_carrier.py†L78-L85】
- **Возможные источники**
  - Нефинитные входы/градиенты в потоках или эмбеддингах (отсутствует предварительная проверка).
  - Слишком высокий learning rate или недостаточный клиппинг градиентов при смешанной точности.
  - Всплески значений в скрытом состоянии GRU.
- **Рекомендации**
  - Проверять данные и выходы на `torch.isfinite` до вызова GRU; применять `torch.nan_to_num`/clamp при необходимости.
  - Усилить `gradient_clip`, снизить LR или использовать `torch.cuda.set_device(0)` + `torch.backends.cudnn.deterministic = True` для стабильности.
  - Включать `torch.autograd.detect_anomaly()` только для поиска источника NaN и отключать после выявления.

---
Отчёт подготовлен кратко и практично в духе проекта; предложения ориентированы на прямые исправления без лишних абстракций.
