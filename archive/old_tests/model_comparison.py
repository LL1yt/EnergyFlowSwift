"""
[BOT] Model Comparison Suite - Stage 3.1.3
–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö Teacher models
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
"""

import torch
import torch.nn as nn
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
import logging
import time
from pathlib import Path
import json
from collections import defaultdict

# –ò–º–ø–æ—Ä—Ç—ã —Å–∏—Å—Ç–µ–º—ã
from .adapter_integration import AdapterCubeTrainer, AdapterIntegrationConfig
from data.embedding_adapter.universal_adapter import KNOWN_MODELS, UniversalEmbeddingAdapter
from .cube_trainer import EmbeddingMetrics
from data.embedding_loader import EmbeddingLoader

logger = logging.getLogger(__name__)


@dataclass
class ModelTestConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model_name: str
    embedding_dim: int
    adapter_strategies: List[str] = field(default_factory=lambda: ["learned_linear", "hierarchical"])
    learning_rates: List[float] = field(default_factory=lambda: [0.001, 0.0005, 0.0001])
    batch_sizes: List[int] = field(default_factory=lambda: [4, 8])
    test_epochs: int = 5  # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    quality_threshold: float = 0.8  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π cosine similarity


@dataclass 
class ModelTestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    model_name: str
    adapter_strategy: str
    learning_rate: float
    batch_size: int
    
    # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    final_loss: float
    reconstruction_loss: float
    cosine_similarity: float
    semantic_preservation: float
    
    # Performance –º–µ—Ç—Ä–∏–∫–∏
    training_time: float
    memory_usage: float
    convergence_epochs: int
    parameter_count: int
    compression_ratio: float
    
    # –°—Ç–∞—Ç—É—Å
    converged: bool
    quality_passed: bool
    recommended: bool = False


class ModelDetectionSystem:
    """
    –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ –º–æ–¥–µ–ª—è—Ö
        self.model_database = KNOWN_MODELS.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.model_database.update({
            "Claude-3": {"embedding_dim": 2048},
            "GPT-4": {"embedding_dim": 1536},
            "Gemini": {"embedding_dim": 3072},
        })
        
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ compression ratio
        self.strategy_recommendations = {
            # High compression (>10x) - –Ω—É–∂–Ω—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            "high_compression": {
                "ratio_threshold": 10.0,
                "recommended_strategies": ["hierarchical", "attention_based"],
                "learning_rates": [0.0005, 0.0001],
                "batch_sizes": [4, 8]
            },
            # Medium compression (3-10x) - balance –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é  
            "medium_compression": {
                "ratio_threshold": 3.0,
                "recommended_strategies": ["learned_linear", "hierarchical"],
                "learning_rates": [0.001, 0.0005],
                "batch_sizes": [8, 16]
            },
            # Low compression (<3x) - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            "low_compression": {
                "ratio_threshold": 1.0,
                "recommended_strategies": ["learned_linear"],
                "learning_rates": [0.001, 0.002],
                "batch_sizes": [16, 32]
            }
        }
    
    def detect_model(self, model_name: str) -> Optional[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏"""
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if model_name in self.model_database:
            return self.model_database[model_name]
        
        # Fuzzy matching –¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–º–µ–Ω
        for known_model in self.model_database:
            if known_model.lower() in model_name.lower() or model_name.lower() in known_model.lower():
                self.logger.info(f"[MAGNIFY] Model '{model_name}' matched to '{known_model}'")
                return self.model_database[known_model]
        
        return None
    
    def get_recommended_config(self, model_name: str, target_surface_size: int = 225) -> ModelTestConfig:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        model_info = self.detect_model(model_name)
        
        if not model_info:
            raise ValueError(f"Unknown model: {model_name}")
        
        embedding_dim = model_info["embedding_dim"]
        compression_ratio = embedding_dim / target_surface_size
        
        # –í—ã–±–∏—Ä–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é compression
        if compression_ratio >= self.strategy_recommendations["high_compression"]["ratio_threshold"]:
            category = "high_compression"
        elif compression_ratio >= self.strategy_recommendations["medium_compression"]["ratio_threshold"]:
            category = "medium_compression"
        else:
            category = "low_compression"
        
        config_template = self.strategy_recommendations[category]
        
        self.logger.info(f"[TARGET] Model {model_name}: {embedding_dim}D ‚Üí {target_surface_size}D")
        self.logger.info(f"   Compression: {compression_ratio:.1f}x ({category})")
        self.logger.info(f"   Recommended strategies: {config_template['recommended_strategies']}")
        
        return ModelTestConfig(
            model_name=model_name,
            embedding_dim=embedding_dim,
            adapter_strategies=config_template["recommended_strategies"],
            learning_rates=config_template["learning_rates"],
            batch_sizes=config_template["batch_sizes"]
        )
    
    def list_supported_models(self) -> List[str]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return list(self.model_database.keys())


class ModelComparisonSuite:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è Teacher models
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ multiple models
    - Comparison –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ optimal configurations
    - Benchmarking suite –¥–ª—è production readiness
    """
    
    def __init__(self, 
                 cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
                 surface_strategy: str = "single",
                 output_dir: str = "results/model_comparison",
                 device: str = "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Model Comparison Suite
        
        Args:
            cube_dimensions: –†–∞–∑–º–µ—Ä—ã –∫—É–±–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            surface_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è surface processing 
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info("[BOT] Initializing ModelComparisonSuite...")
        
        self.cube_dimensions = cube_dimensions
        self.surface_strategy = surface_strategy
        self.surface_size = cube_dimensions[0] * cube_dimensions[1]  # 15*15 = 225
        self.device = torch.device(device)
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.model_detector = ModelDetectionSystem()
        self.test_results: List[ModelTestResult] = []
        self.comparison_metrics = {}
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ embeddings)
        self.test_embeddings = self._prepare_test_data()
        
        self.logger.info(f"[OK] ModelComparisonSuite –≥–æ—Ç–æ–≤:")
        self.logger.info(f"   Cube: {cube_dimensions} ‚Üí Surface: {self.surface_size}D")
        self.logger.info(f"   Supported models: {len(self.model_detector.list_supported_models())}")
        self.logger.info(f"   Output: {self.output_dir}")
    
    def _prepare_test_data(self) -> Dict[str, torch.Tensor]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏"""
        test_data = {}
        
        for model_name, model_info in self.model_detector.model_database.items():
            embedding_dim = model_info["embedding_dim"]
            
            # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ embeddings –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å–ª—É—á–∞–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ EmbeddingLoader
            test_data[model_name] = {
                "questions": torch.randn(20, embedding_dim),  # 20 –≤–æ–ø—Ä–æ—Å–æ–≤
                "answers": torch.randn(20, embedding_dim)     # 20 –æ—Ç–≤–µ—Ç–æ–≤
            }
        
        self.logger.info(f"[DATA] Test data prepared for {len(test_data)} models")
        return test_data
    
    def test_single_model(self, 
                         model_name: str, 
                         config: Optional[ModelTestConfig] = None) -> List[ModelTestResult]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∏–ª–∏ auto-detect)
            
        Returns:
            List[ModelTestResult]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        self.logger.info(f"üß™ Testing model: {model_name}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config is None:
            config = self.model_detector.get_recommended_config(model_name, self.surface_size)
        
        results = []
        test_data = self.test_embeddings[model_name]
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_tests = len(config.adapter_strategies) * len(config.learning_rates) * len(config.batch_sizes)
        current_test = 0
        
        for strategy in config.adapter_strategies:
            for lr in config.learning_rates:
                for batch_size in config.batch_sizes:
                    current_test += 1
                    self.logger.info(f"   Test {current_test}/{total_tests}: {strategy}, lr={lr}, batch={batch_size}")
                    
                    try:
                        result = self._run_single_test(
                            model_name=model_name,
                            embedding_dim=config.embedding_dim,
                            adapter_strategy=strategy,
                            learning_rate=lr,
                            batch_size=batch_size,
                            test_epochs=config.test_epochs,
                            test_data=test_data,
                            quality_threshold=config.quality_threshold
                        )
                        results.append(result)
                        
                    except Exception as e:
                        self.logger.error(f"[ERROR] Test failed: {e}")
                        # –°–æ–∑–¥–∞–µ–º failed result
                        failed_result = ModelTestResult(
                            model_name=model_name,
                            adapter_strategy=strategy,
                            learning_rate=lr,
                            batch_size=batch_size,
                            final_loss=999.0,
                            reconstruction_loss=999.0,
                            cosine_similarity=0.0,
                            semantic_preservation=0.0,
                            training_time=0.0,
                            memory_usage=0.0,
                            convergence_epochs=config.test_epochs,
                            parameter_count=0,
                            compression_ratio=0.0,
                            converged=False,
                            quality_passed=False
                        )
                        results.append(failed_result)
                        continue
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        best_result = min([r for r in results if r.converged], 
                         key=lambda x: x.final_loss, 
                         default=None)
        
        if best_result:
            best_result.recommended = True
            self.logger.info(f"[TROPHY] Best config for {model_name}: {best_result.adapter_strategy}, lr={best_result.learning_rate}")
        
        return results
    
    def _run_single_test(self, 
                        model_name: str,
                        embedding_dim: int,
                        adapter_strategy: str,
                        learning_rate: float,
                        batch_size: int,
                        test_epochs: int,
                        test_data: Dict[str, torch.Tensor],
                        quality_threshold: float) -> ModelTestResult:
        """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º trainer –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        trainer_config = AdapterIntegrationConfig(
            teacher_model=model_name,
            teacher_embedding_dim=embedding_dim,
            cube_dimensions=self.cube_dimensions,
            surface_strategy=self.surface_strategy,
            adapter_strategy=adapter_strategy,
            adapter_learning_rate=learning_rate,
            cube_learning_rate=learning_rate * 0.5,  # Cube LR –º–µ–Ω—å—à–µ adapter LR
            joint_training=True
        )
        
        # –°–æ–∑–¥–∞–µ–º trainer
        trainer = AdapterCubeTrainer(trainer_config, device=str(self.device))
        trainer.initialize_components()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        initial_memory = torch.cuda.memory_allocated() if self.device.type == 'cuda' else 0
        parameter_count = trainer.adapter.get_parameter_count()
        compression_ratio = trainer.adapter.get_compression_ratio()
        
        # Training loop (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)  
        losses = []
        
        question_embeddings = test_data["questions"][:batch_size].to(self.device)
        answer_embeddings = test_data["answers"][:batch_size].to(self.device)
        
        for epoch in range(test_epochs):
            metrics = trainer.train_step(question_embeddings, answer_embeddings)
            losses.append(metrics["total_loss"])
            
            # Early stopping –µ—Å–ª–∏ loss —Ä–∞—Å—Ç–µ—Ç
            if len(losses) > 2 and losses[-1] > losses[-2] > losses[-3]:
                break
        
        training_time = time.time() - start_time
        final_memory = torch.cuda.memory_allocated() if self.device.type == 'cuda' else 0
        memory_usage = final_memory - initial_memory
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        final_metrics = trainer.train_step(question_embeddings, answer_embeddings)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º convergence –∏ quality
        converged = len(losses) >= 3 and all(l < 1.0 for l in losses[-3:])
        quality_passed = final_metrics.get("cosine_similarity", 0.0) > quality_threshold
        
        return ModelTestResult(
            model_name=model_name,
            adapter_strategy=adapter_strategy,
            learning_rate=learning_rate,
            batch_size=batch_size,
            final_loss=final_metrics["total_loss"],
            reconstruction_loss=final_metrics.get("reconstruction_loss", 0.0),
            cosine_similarity=final_metrics.get("cosine_similarity", 0.0),
            semantic_preservation=final_metrics.get("semantic_preservation", 0.0),
            training_time=training_time,
            memory_usage=float(memory_usage) / 1024**2,  # –í MB
            convergence_epochs=len(losses),
            parameter_count=parameter_count,
            compression_ratio=compression_ratio,
            converged=converged,
            quality_passed=quality_passed
        )
    
    def compare_models(self, model_names: List[str]) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        
        Args:
            model_names: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            
        Returns:
            Dict: Comprehensive comparison report
        """
        self.logger.info(f"üèÅ Starting comparison of {len(model_names)} models...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        all_results = []
        for model_name in model_names:
            if model_name not in self.test_embeddings:
                self.logger.warning(f"[WARNING] Skipping {model_name} - no test data")
                continue
            
            model_results = self.test_single_model(model_name)
            all_results.extend(model_results)
            self.test_results.extend(model_results)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        comparison_report = self._analyze_results(all_results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        self._save_comparison_report(comparison_report)
        
        return comparison_report
    
    def _analyze_results(self, results: List[ModelTestResult]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º
        by_model = defaultdict(list)
        for result in results:
            by_model[result.model_name].append(result)
        
        analysis = {
            "summary": {
                "total_tests": len(results),
                "models_tested": len(by_model),
                "successful_tests": len([r for r in results if r.converged]),
                "quality_passed": len([r for r in results if r.quality_passed])
            },
            "models": {},
            "best_overall": None,
            "recommendations": {}
        }
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –º–æ–¥–µ–ª—è–º
        for model_name, model_results in by_model.items():
            converged_results = [r for r in model_results if r.converged]
            quality_results = [r for r in model_results if r.quality_passed]
            
            if converged_results:
                best_result = min(converged_results, key=lambda x: x.final_loss)
                
                analysis["models"][model_name] = {
                    "embedding_dim": best_result.compression_ratio * self.surface_size,  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º original dim
                    "compression_ratio": best_result.compression_ratio,
                    "best_strategy": best_result.adapter_strategy,
                    "best_learning_rate": best_result.learning_rate,
                    "best_batch_size": best_result.batch_size,
                    "final_loss": best_result.final_loss,
                    "cosine_similarity": best_result.cosine_similarity,
                    "training_time": best_result.training_time,
                    "parameter_count": best_result.parameter_count,
                    "quality_passed": len(quality_results) > 0,
                    "convergence_rate": len(converged_results) / len(model_results)
                }
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å overall
        best_models = [(name, data) for name, data in analysis["models"].items() 
                      if data["quality_passed"]]
        
        if best_models:
            best_model_name, best_model_data = min(best_models, 
                                                  key=lambda x: x[1]["final_loss"])
            analysis["best_overall"] = {
                "model": best_model_name,
                **best_model_data
            }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        analysis["recommendations"] = self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = {
            "production_ready": [],
            "development_suitable": [],
            "high_compression": [],
            "fast_training": [],
            "best_quality": []
        }
        
        for model_name, data in analysis["models"].items():
            # Production ready: quality + stable convergence
            if data["quality_passed"] and data["convergence_rate"] > 0.7:
                recommendations["production_ready"].append(model_name)
            
            # Development suitable: at least converges
            if data["convergence_rate"] > 0.5:
                recommendations["development_suitable"].append(model_name)
            
            # High compression: ratio > 10x
            if data["compression_ratio"] > 10.0:
                recommendations["high_compression"].append(model_name)
            
            # Fast training: < 30 seconds per test
            if data["training_time"] < 30.0:
                recommendations["fast_training"].append(model_name)
            
            # Best quality: cosine similarity > 0.8
            if data["cosine_similarity"] > 0.8:
                recommendations["best_quality"].append(model_name)
        
        return recommendations
    
    def _save_comparison_report(self, report: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏"""
        
        # JSON –æ—Ç—á–µ—Ç
        json_path = self.output_dir / "comparison_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_path = self.output_dir / "detailed_results.json"
        results_data = [
            {
                "model_name": r.model_name,
                "adapter_strategy": r.adapter_strategy,
                "learning_rate": r.learning_rate,
                "batch_size": r.batch_size,
                "final_loss": r.final_loss,
                "reconstruction_loss": r.reconstruction_loss,
                "cosine_similarity": r.cosine_similarity,
                "semantic_preservation": r.semantic_preservation,
                "training_time": r.training_time,
                "memory_usage": r.memory_usage,
                "convergence_epochs": r.convergence_epochs,
                "parameter_count": r.parameter_count,
                "compression_ratio": r.compression_ratio,
                "converged": r.converged,  
                "quality_passed": r.quality_passed,
                "recommended": r.recommended
            }
            for r in self.test_results
        ]
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"[DATA] Comparison report saved:")
        self.logger.info(f"   Summary: {json_path}")
        self.logger.info(f"   Details: {results_path}")
    
    def get_recommended_config_for_model(self, model_name: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        model_results = [r for r in self.test_results if r.model_name == model_name and r.recommended]
        
        if not model_results:
            return None
        
        best_result = model_results[0]  # recommended —É–∂–µ –≤—ã–±—Ä–∞–Ω –∫–∞–∫ –ª—É—á—à–∏–π
        
        return {
            "model_name": model_name,
            "adapter_strategy": best_result.adapter_strategy,
            "learning_rate": best_result.learning_rate,
            "batch_size": best_result.batch_size,
            "expected_quality": {
                "final_loss": best_result.final_loss,
                "cosine_similarity": best_result.cosine_similarity,
                "training_time": best_result.training_time
            },
            "compression_info": {
                "compression_ratio": best_result.compression_ratio,
                "parameter_count": best_result.parameter_count
            }
        }


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def quick_model_comparison(models: List[str] = None, 
                          cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
                          device: str = "cpu") -> Dict[str, Any]:
    """
    –ë—ã—Å—Ç—Ä–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –±–∞–∑–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    
    Args:
        models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π (–∏–ª–∏ default set)
        cube_dimensions: –†–∞–∑–º–µ—Ä—ã –∫—É–±–∞
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
    Returns:
        Comparison report
    """
    if models is None:
        # Default set –¥–ª—è Stage 3.1.3
        models = ["Meta-Llama-3-8B", "DistilBERT", "BERT-large", "GPT-3.5", "RoBERTa-large"]
    
    suite = ModelComparisonSuite(
        cube_dimensions=cube_dimensions,
        device=device
    )
    
    return suite.compare_models(models)


def test_single_model_quick(model_name: str, 
                           cube_dimensions: Tuple[int, int, int] = (15, 15, 11),
                           device: str = "cpu") -> List[ModelTestResult]:
    """–ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    suite = ModelComparisonSuite(
        cube_dimensions=cube_dimensions,
        device=device
    )
    
    return suite.test_single_model(model_name) 