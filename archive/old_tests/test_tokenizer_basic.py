#!/usr/bin/env python3
"""
Basic Test for Tokenizer Module - 3D Cellular Neural Network

–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥—É–ª—è tokenizer.
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç transformers.

–ê–≤—Ç–æ—Ä: 3D CNN Team
–î–∞—Ç–∞: –î–µ–∫–∞–±—Ä—å 2025
"""

import sys
import os
import logging
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_basic_tokenizer():
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞."""
    print("üß™ Testing Basic Tokenizer...")
    
    try:
        from data.tokenizer import TokenizerManager
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ —Å –±–∞–∑–æ–≤—ã–º —Ç–∏–ø–æ–º
        tokenizer = TokenizerManager(tokenizer_type='basic')
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        test_text = "Hello world! This is a test of the basic tokenizer."
        
        print(f"[WRITE] Input text: {test_text}")
        
        # –¢–µ—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
        tokens = tokenizer.tokenize(test_text)
        print(f"üî§ Tokens: {tokens}")
        
        # –¢–µ—Å—Ç –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        token_ids = tokenizer.encode(test_text)
        print(f"üî¢ Token IDs: {token_ids}")
        
        # –¢–µ—Å—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        decoded_text = tokenizer.decode(token_ids)
        print(f"[FILE] Decoded: {decoded_text}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç—Ä–∏–∫
        metrics = tokenizer.get_metrics()
        print(f"[DATA] Metrics: {metrics}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        special_tokens = tokenizer.get_special_tokens()
        print(f"[TARGET] Special tokens: {special_tokens}")
        
        print("[OK] Basic tokenizer test PASSED!")
        return True
        
    except Exception as e:
        print(f"[ERROR] Basic tokenizer test FAILED: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_text_processor():
    """–¢–µ—Å—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞."""
    print("\nüß™ Testing Text Processor...")
    
    try:
        from data.tokenizer.text_processor import TextProcessor
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = TextProcessor()
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏
        test_text = "  Hello   WORLD!!! This is a TEST with   extra spaces.  "
        
        print(f"[WRITE] Input text: '{test_text}'")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_text = processor.preprocess(test_text)
        print(f"[CONFIG] Processed: '{processed_text}'")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        stats = processor.get_processing_stats(test_text, processed_text)
        print(f"[DATA] Processing stats: {stats}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        is_valid = processor.validate_text(processed_text)
        print(f"[OK] Text valid: {is_valid}")
        
        print("[OK] Text processor test PASSED!")
        return True
        
    except Exception as e:
        print(f"[ERROR] Text processor test FAILED: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_batch_processing():
    """–¢–µ—Å—Ç batch –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    print("\nüß™ Testing Batch Processing...")
    
    try:
        from data.tokenizer import TokenizerManager
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
        tokenizer = TokenizerManager(tokenizer_type='basic')
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
        test_texts = [
            "First test sentence.",
            "Second test sentence with more words.",
            "Third sentence is here."
        ]
        
        print(f"[WRITE] Input texts: {test_texts}")
        
        # Batch –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        batch_encoded = tokenizer.batch_encode(test_texts)
        print(f"üî¢ Batch encoded: {batch_encoded}")
        
        # Batch –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        batch_decoded = tokenizer.batch_decode(batch_encoded)
        print(f"[FILE] Batch decoded: {batch_decoded}")
        
        print("[OK] Batch processing test PASSED!")
        return True
        
    except Exception as e:
        print(f"[ERROR] Batch processing test FAILED: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_lattice_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ä–µ—à–µ—Ç–∫–æ–π."""
    print("\nüß™ Testing Lattice Integration...")
    
    try:
        from data.tokenizer import TokenizerManager
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
        tokenizer = TokenizerManager(tokenizer_type='basic')
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        test_text = "Input for neural network processing"
        
        # –†–∞–∑–º–µ—Ä —Ä–µ—à–µ—Ç–∫–∏
        lattice_size = (5, 5, 5)
        
        print(f"[WRITE] Input text: {test_text}")
        print(f"üî≤ Lattice size: {lattice_size}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è —Ä–µ—à–µ—Ç–∫–∏
        lattice_input = tokenizer.prepare_for_lattice(test_text, lattice_size)
        print(f"[TARGET] Lattice input shape: {lattice_input.shape}")
        print(f"üî¢ Lattice input: {lattice_input}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        expected_shape = (lattice_size[0], lattice_size[1])
        if lattice_input.shape == expected_shape:
            print("[OK] Lattice shape is correct!")
        else:
            print(f"[ERROR] Expected shape {expected_shape}, got {lattice_input.shape}")
            return False
        
        print("[OK] Lattice integration test PASSED!")
        return True
        
    except Exception as e:
        print(f"[ERROR] Lattice integration test FAILED: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_configuration():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    print("\nüß™ Testing Configuration...")
    
    try:
        from data.tokenizer import TokenizerManager
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        test_config = {
            'tokenizer': {
                'type': 'basic',
                'max_length': 100,
                'padding': True
            },
            'text_processing': {
                'lowercase': False,
                'remove_punctuation': True
            },
            'caching': {
                'enabled': False
            }
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        tokenizer = TokenizerManager(tokenizer_type='basic', config=test_config)
        
        print(f"[GEAR] Config loaded: {tokenizer.config['tokenizer']['max_length']}")
        print(f"[SAVE] Cache enabled: {tokenizer._cache_enabled}")
        
        # –¢–µ—Å—Ç —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        test_text = "Test with CUSTOM config!"
        tokens = tokenizer.encode(test_text, max_length=10)
        print(f"üî¢ Tokens (max_length=10): {tokens}")
        
        print("[OK] Configuration test PASSED!")
        return True
        
    except Exception as e:
        print(f"[ERROR] Configuration test FAILED: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("[START] Starting Tokenizer Module Tests")
    print("=" * 50)
    
    tests = [
        test_basic_tokenizer,
        test_text_processor,
        test_batch_processing,
        test_lattice_integration,
        test_configuration
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        if test_func():
            passed += 1
    
    print("\n" + "=" * 50)
    print(f"[TARGET] Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("[SUCCESS] ALL TESTS PASSED! Tokenizer module is working correctly.")
        return True
    else:
        print("[ERROR] Some tests failed. Please check the errors above.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 