#!/usr/bin/env python3
"""
Comprehensive testing for AutoencoderDataset - Stage 1.2

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ–≥–æ AutoencoderDataset:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å EmbeddingLoader
- Smart caching –∏ batch processing
- Train/validation split
- –†–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: v1.0.0 (Phase 3.1 - Stage 1.2)
–î–∞—Ç–∞: 6 –∏—é–Ω—è 2025
"""

import sys
import os
import tempfile
import shutil
from pathlib import Path
import torch
import numpy as np
import json
import logging

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(str(Path(__file__).parent.absolute()))

def test_autoencoder_dataset():
    """Comprehensive testing of AutoencoderDataset functionality"""
    
    print("üß™ COMPREHENSIVE AUTOENCODER DATASET TESTING")
    print("=" * 60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    temp_dir = tempfile.mkdtemp()
    temp_path = Path(temp_dir)
    
    try:
        # ========== –¢–ï–°–¢ 1: –ò–ú–ü–û–†–¢ –ò –ë–ê–ó–û–í–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ==========
        print("\n[PACKAGE] –¢–ï–°–¢ 1: –ò–º–ø–æ—Ä—Ç –∏ –±–∞–∑–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è")
        print("-" * 40)
        
        try:
            from training.embedding_trainer import (
                AutoencoderDataset, 
                DatasetConfig, 
                create_text_dataset, 
                create_file_dataset
            )
            print("[OK] AutoencoderDataset –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except ImportError as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 2: –°–û–ó–î–ê–ù–ò–ï –ò–ó –ì–û–¢–û–í–´–• –≠–ú–ë–ï–î–ò–ù–ì–û–í ==========
        print("\n[TARGET] –¢–ï–°–¢ 2: –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤")
        print("-" * 40)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
        test_embeddings = torch.randn(100, 768)  # 100 samples, 768 dim
        
        try:
            config = DatasetConfig(
                validation_split=0.2,
                cache_dir=str(temp_path / "cache_test2"),
                random_seed=42
            )
            
            dataset = AutoencoderDataset(
                config=config,
                embeddings=test_embeddings
            )
            
            print(f"[OK] Dataset —Å–æ–∑–¥–∞–Ω: {dataset}")
            print(f"   Total samples: {len(dataset.embeddings)}")
            print(f"   Train samples: {len(dataset.train_embeddings)}")  
            print(f"   Val samples: {len(dataset.val_embeddings)}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ train/val split
            expected_val_size = int(100 * 0.2)  # 20% –¥–ª—è validation
            expected_train_size = 100 - expected_val_size
            
            if len(dataset.val_embeddings) == expected_val_size:
                print("[OK] Validation split –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π")
            else:
                print(f"[ERROR] Validation split –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π: {len(dataset.val_embeddings)} vs {expected_val_size}")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è dataset –∏–∑ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 3: –°–û–ó–î–ê–ù–ò–ï –ò–ó –¢–ï–ö–°–¢–û–í ==========
        print("\n[WRITE] –¢–ï–°–¢ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ (—Å EmbeddingLoader)")
        print("-" * 40)
        
        test_texts = [
            "The quick brown fox jumps over the lazy dog",
            "Machine learning is a subset of artificial intelligence", 
            "Neural networks can learn complex patterns",
            "Deep learning revolutionized computer vision",
            "Natural language processing helps understand text"
        ]
        
        try:
            text_dataset = create_text_dataset(
                texts=test_texts,
                llm_model="distilbert",
                validation_split=0.2,
                cache_dir=str(temp_path / "cache_test3"),
                use_cache=True,
                normalize_embeddings=True
            )
            
            print(f"[OK] Text dataset —Å–æ–∑–¥–∞–Ω: {text_dataset}")
            print(f"   Total samples: {len(text_dataset.embeddings)}")
            print(f"   Embedding dim: {text_dataset.config.embedding_dim}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ __getitem__
            sample_input, sample_target = text_dataset[0]
            print(f"   Sample shapes: input={sample_input.shape}, target={sample_target.shape}")
            
            if sample_input.shape == sample_target.shape:
                print("[OK] Autoencoder sample format –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π")
            else:
                print("[ERROR] Autoencoder sample format –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è dataset –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 4: DATALOADER INTEGRATION ==========
        print("\n[REFRESH] –¢–ï–°–¢ 4: DataLoader –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è")
        print("-" * 40)
        
        try:
            # Train DataLoader
            train_loader = dataset.get_dataloader(
                batch_size=16,
                shuffle=True,
                validation=False
            )
            
            # Validation DataLoader
            val_loader = dataset.get_dataloader(
                batch_size=16,
                shuffle=False,
                validation=True
            )
            
            print(f"[OK] Train DataLoader —Å–æ–∑–¥–∞–Ω: {len(train_loader)} batches")
            print(f"[OK] Val DataLoader —Å–æ–∑–¥–∞–Ω: {len(val_loader)} batches")
            
            # –¢–µ—Å—Ç –∏—Ç–µ—Ä–∞—Ü–∏–∏
            batch_input, batch_target = next(iter(train_loader))
            print(f"   Batch shapes: input={batch_input.shape}, target={batch_target.shape}")
            
            if batch_input.shape == batch_target.shape:
                print("[OK] DataLoader batch format –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π")
            else:
                print("[ERROR] DataLoader batch format –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ DataLoader –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 5: –§–ê–ô–õ–û–í–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò –î–ê–ù–ù–´–• ==========
        print("\n[FOLDER] –¢–ï–°–¢ 5: –§–∞–π–ª–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        print("-" * 40)
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
            test_file = temp_path / "test_texts.txt"
            with open(test_file, 'w', encoding='utf-8') as f:
                for text in test_texts:
                    f.write(text + '\n')
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π PyTorch —Ñ–∞–π–ª
            test_pt_file = temp_path / "test_embeddings.pt"
            torch.save(torch.randn(50, 768), test_pt_file)
            
            file_dataset = create_file_dataset(
                file_paths=[str(test_file), str(test_pt_file)],
                embedding_format="llm",
                llm_model="distilbert",
                validation_split=0.15,
                cache_dir=str(temp_path / "cache_test5")
            )
            
            print(f"[OK] File dataset —Å–æ–∑–¥–∞–Ω: {file_dataset}")
            print(f"   Total samples: {len(file_dataset.embeddings)}")
            print(f"   Train samples: {len(file_dataset.train_embeddings)}")
            print(f"   Val samples: {len(file_dataset.val_embeddings)}")
            
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è dataset –∏–∑ —Ñ–∞–π–ª–æ–≤: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 6: –†–ï–ñ–ò–ú –í–ê–õ–ò–î–ê–¶–ò–ò ==========
        print("\n[MAGNIFY] –¢–ï–°–¢ 6: –†–µ–∂–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        print("-" * 40)
        
        try:
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤
            original_len = len(dataset)
            dataset.set_validation_mode(True)
            val_len = len(dataset)
            dataset.set_validation_mode(False)
            train_len = len(dataset)
            
            print(f"   Train mode length: {train_len}")
            print(f"   Validation mode length: {val_len}")
            
            if train_len != val_len:
                print("[OK] –†–µ–∂–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                print("[ERROR] –†–µ–∂–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Ä–µ–∂–∏–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 7: –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê ==========
        print("\n[GEAR]  –¢–ï–°–¢ 7: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞")
        print("-" * 40)
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ dict
            config_dict = {
                'embedding_dim': 512,
                'validation_split': 0.3,
                'normalize_embeddings': False,
                'add_noise': True,
                'noise_std': 0.05,
                'cache_dir': str(temp_path / "cache_test7")
            }
            
            dict_dataset = AutoencoderDataset(
                config=config_dict,
                embeddings=torch.randn(80, 512)
            )
            
            print(f"[OK] Dataset –∏–∑ dict config: {dict_dataset}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ JSON
            config_file = temp_path / "test_config.json"
            with open(config_file, 'w') as f:
                json.dump(config_dict, f)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ JSON
            json_dataset = AutoencoderDataset(
                config=str(config_file),
                embeddings=torch.randn(60, 512)
            )
            
            print(f"[OK] Dataset –∏–∑ JSON config: {json_dataset}")
            
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 8: –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ú–ï–¢–ê–î–ê–ù–ù–´–ï ==========
        print("\n[DATA] –¢–ï–°–¢ 8: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
        print("-" * 40)
        
        try:
            stats = dataset.get_statistics()
            print("[OK] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞:")
            for key, value in stats.items():
                if isinstance(value, (int, float)):
                    print(f"   {key}: {value:.4f}" if isinstance(value, float) else f"   {key}: {value}")
                elif isinstance(value, dict):
                    print(f"   {key}: {len(value)} elements")
                else:
                    print(f"   {key}: {type(value).__name__}")
            
            # Sample embeddings
            samples = dataset.get_sample_embeddings(n_samples=3)
            print(f"[OK] Sample embeddings –ø–æ–ª—É—á–µ–Ω—ã:")
            for split, embs in samples.items():
                print(f"   {split}: {embs.shape}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ info
            info_file = temp_path / "dataset_info.json"
            dataset.save_dataset_info(str(info_file))
            
            if info_file.exists():
                print("[OK] Dataset info —Å–æ—Ö—Ä–∞–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            else:
                print("[ERROR] Dataset info –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 9: CACHING –°–ò–°–¢–ï–ú–ê ==========
        print("\n[SAVE] –¢–ï–°–¢ 9: Caching —Å–∏—Å—Ç–µ–º–∞")
        print("-" * 40)
        
        try:
            # –ü–µ—Ä–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å cache miss
            cache_texts = ["Test text for caching", "Another test text", "Third text"]
            
            cache_config = DatasetConfig(
                cache_dir=str(temp_path / "cache_test9"),
                use_cache=True,
                cache_embeddings=True,
                llm_model="distilbert"
            )
            
            cache_dataset1 = AutoencoderDataset(
                config=cache_config,
                texts=cache_texts
            )
            
            cache_miss_count = cache_dataset1.cache_stats['cache_misses']
            print(f"   First creation - cache misses: {cache_miss_count}")
            
            # –í—Ç–æ—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å cache hit
            cache_dataset2 = AutoencoderDataset(
                config=cache_config,
                texts=cache_texts
            )
            
            cache_hit_count = cache_dataset2.cache_stats['cache_hits']
            print(f"   Second creation - cache hits: {cache_hit_count}")
            
            if cache_hit_count > 0:
                print("[OK] Caching —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                print("[WARNING]  Caching —Å–∏—Å—Ç–µ–º–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, –∫—ç—à –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ caching —Å–∏—Å—Ç–µ–º—ã: {e}")
            return False
        
        # ========== –¢–ï–°–¢ 10: NOISE AUGMENTATION ==========
        print("\nüîä –¢–ï–°–¢ 10: Noise augmentation")
        print("-" * 40)
        
        try:
            noise_config = DatasetConfig(
                add_noise=True,
                noise_std=0.1,
                validation_split=0.0  # –ë–µ–∑ validation –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            )
            
            noise_dataset = AutoencoderDataset(
                config=noise_config,
                embeddings=torch.ones(10, 768)  # –í—Å–µ –µ–¥–∏–Ω–∏—Ü—ã –¥–ª—è –ª–µ–≥–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —à—É–º–∞
            )
            
            # –ü–æ–ª—É—á–∞–µ–º sample —Å —à—É–º–æ–º
            input_emb, target_emb = noise_dataset[0]
            
            # Target –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–≤—Å–µ –µ–¥–∏–Ω–∏—Ü—ã)
            # Input –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —à—É–º
            target_diff = torch.abs(target_emb - 1.0).max().item()
            input_diff = torch.abs(input_emb - 1.0).max().item()
            
            print(f"   Target difference from 1.0: {target_diff:.6f}")
            print(f"   Input difference from 1.0: {input_diff:.6f}")
            
            if target_diff < 1e-6 and input_diff > 1e-3:
                print("[OK] Noise augmentation —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                print("[ERROR] Noise augmentation –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                
        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ noise augmentation: {e}")
            return False
        
        # ========== –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ ==========
        print("\n" + "=" * 60)
        print("[SUCCESS] –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("[OK] AutoencoderDataset –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω")
        print()
        print("[START] –ì–û–¢–û–í–ù–û–°–¢–¨ –ö STAGE 1.3:")
        print("   ‚úì –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å EmbeddingLoader —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print("   ‚úì Smart caching —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")  
        print("   ‚úì Batch processing —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω")
        print("   ‚úì Train/validation split –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
        print("   ‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–∏–±–∫–∞—è")
        print("   ‚úì –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è")
        print("   ‚úì DataLoader –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")
        print("   ‚úì –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã")
        print("   ‚úì Noise augmentation —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print()
        print("[INFO] STAGE 1.2 - AutoencoderDataset: –ó–ê–í–ï–†–®–ï–ù!")
        
        return True
        
    except Exception as e:
        print(f"\nüí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        try:
            shutil.rmtree(temp_dir)
            print(f"\nüßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã: {temp_dir}")
        except:
            print(f"\n[WARNING]  –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {temp_dir}")


if __name__ == "__main__":
    success = test_autoencoder_dataset()
    exit(0 if success else 1) 