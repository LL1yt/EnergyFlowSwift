#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è Phase 4 —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import torch
import tempfile
import yaml
from pathlib import Path
import logging


def create_minimal_training_data():
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üìä –°–û–ó–î–ê–ù–ò–ï –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•")
    print("=" * 40)

    # –ü—Ä–æ—Å—Ç—ã–µ –¥–∏–∞–ª–æ–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    dialogue_pairs = [
        {
            "question": "Hello, how are you?",
            "answer": "I'm doing well, thank you!",
            "quality_score": 0.9,
        },
        {
            "question": "What is AI?",
            "answer": "AI is artificial intelligence.",
            "quality_score": 0.8,
        },
        {
            "question": "Tell me about neural networks.",
            "answer": "Neural networks are computational models inspired by the brain.",
            "quality_score": 0.85,
        },
    ]

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(dialogue_pairs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    return dialogue_pairs


def test_simple_lattice_creation():
    """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–π —Ä–µ—à–µ—Ç–∫–∏"""
    print("\nüß± –°–û–ó–î–ê–ù–ò–ï –ü–†–û–°–¢–û–ô –†–ï–®–ï–¢–ö–ò")
    print("=" * 40)

    try:
        from core.lattice_3d.config import LatticeConfig
        from core.lattice_3d.lattice import Lattice3D

        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = LatticeConfig(
            dimensions=(6, 6, 6),  # 216 –∫–ª–µ—Ç–æ–∫ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞–ª–æ –¥–ª—è —Ç–µ—Å—Ç–∞
            gpu_enabled=True,
            parallel_processing=True,
            enable_logging=True,
            batch_size=1,
        )

        print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ—à–µ—Ç–∫–∏:")
        print(f"  –†–∞–∑–º–µ—Ä—ã: {config.dimensions}")
        print(f"  –û–±—â–µ–µ –∫–ª–µ—Ç–æ–∫: {config.total_cells}")
        print(f"  GPU –≤–∫–ª—é—á–µ–Ω: {config.gpu_enabled}")

        # –°–æ–∑–¥–∞–µ–º —Ä–µ—à–µ—Ç–∫—É
        lattice = Lattice3D(config)

        print(f"‚úÖ –†–µ—à–µ—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∞:")
        print(f"  Device: {lattice.device}")
        print(f"  States shape: {lattice.states.shape}")
        print(f"  Cell prototype: {type(lattice.cell_prototype).__name__}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU –ø–∞–º—è—Ç—å
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1024**2
            print(f"  GPU –ø–∞–º—è—Ç—å: {memory_used:.1f} MB")

        return lattice, config

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ—à–µ—Ç–∫–∏: {e}")
        import traceback

        traceback.print_exc()
        return None, None


def test_simple_forward_pass():
    """–¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –≤–ø–µ—Ä–µ–¥"""
    print("\n‚ö° –¢–ï–°–¢ –ü–†–û–°–¢–û–ì–û –ü–†–û–•–û–î–ê")
    print("=" * 40)

    lattice, config = test_simple_lattice_creation()
    if lattice is None:
        return False

    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size = 1
        input_size = lattice.cell_prototype.external_input_size

        # –ü—Ä–æ—Å—Ç—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        inputs = torch.randn(batch_size, len(lattice.input_indices), input_size)
        if torch.cuda.is_available():
            inputs = inputs.cuda()

        print(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {inputs.shape}")
        print(f"–í—Ö–æ–¥–Ω—ã–µ —Ç–æ—á–∫–∏: {len(lattice.input_indices)}")
        print(f"–í—ã—Ö–æ–¥–Ω—ã–µ —Ç–æ—á–∫–∏: {len(lattice.output_indices)}")

        # –î–µ–ª–∞–µ–º –ø—Ä–æ—Ö–æ–¥ –≤–ø–µ—Ä–µ–¥
        initial_memory = (
            torch.cuda.memory_allocated(0) / 1024**2 if torch.cuda.is_available() else 0
        )

        outputs = lattice.forward(inputs)

        final_memory = (
            torch.cuda.memory_allocated(0) / 1024**2 if torch.cuda.is_available() else 0
        )

        print(f"‚úÖ –ü—Ä–æ—Ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω:")
        print(f"  –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {outputs.shape}")
        print(f"  Output device: {outputs.device}")
        if torch.cuda.is_available():
            print(f"  –ü–∞–º—è—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞: {final_memory - initial_memory:.1f} MB")

        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ—Ö–æ–¥–∞: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_simple_training_step():
    """–¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
    print("\nüéì –¢–ï–°–¢ –ü–†–û–°–¢–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 40)

    try:
        from utils.config_manager.dynamic_config import DynamicConfigGenerator

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
        generator = DynamicConfigGenerator()
        config = generator.generate_config("development")

        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
        config["lattice"]["xs"] = 8
        config["lattice"]["ys"] = 8
        config["lattice"]["zs"] = 8

        print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
        print(
            f"  Lattice: {config['lattice']['xs']}√ó{config['lattice']['ys']}√ó{config['lattice']['zs']}"
        )
        print(f"  Architecture: {config['architecture']['neuron_architecture']}")
        print(f"  Hybrid mode: {config['architecture']['hybrid_mode']}")
        print(f"  Cell arch: {config['emergent_training']['cell_architecture']}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        lattice_3d = config.get("lattice_3d", {})
        training = config.get("training", {})

        print(f"  GPU enabled: {lattice_3d.get('gpu_enabled')}")
        print(f"  Training device: {training.get('device')}")
        print(f"  Mixed precision: {training.get('mixed_precision')}")

        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_memory_optimization():
    """–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏"""
    print("\nüíæ –¢–ï–°–¢ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ü–ê–ú–Ø–¢–ò")
    print("=" * 40)

    if not torch.cuda.is_available():
        print("‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç –ø–∞–º—è—Ç–∏")
        return True

    try:
        # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated(0) / 1024**2

        print(f"–ù–∞—á–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å: {initial_memory:.1f} MB")

        # –°–æ–∑–¥–∞–µ–º —Ä–µ—à–µ—Ç–∫—É —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        from core.lattice_3d.config import LatticeConfig
        from core.lattice_3d.lattice import Lattice3D

        config = LatticeConfig(
            dimensions=(16, 16, 16),  # 4096 –∫–ª–µ—Ç–æ–∫
            gpu_enabled=True,
            mixed_precision=True,  # –í–∫–ª—é—á–∞–µ–º mixed precision
            memory_efficient=True,
        )

        lattice = Lattice3D(config)

        after_lattice_memory = torch.cuda.memory_allocated(0) / 1024**2
        lattice_memory = after_lattice_memory - initial_memory

        print(
            f"–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ—à–µ—Ç–∫–∏: {after_lattice_memory:.1f} MB (+{lattice_memory:.1f} MB)"
        )

        # –¢–µ—Å—Ç —Å mixed precision
        with torch.cuda.amp.autocast():
            inputs = torch.randn(
                1,
                len(lattice.input_indices),
                lattice.cell_prototype.external_input_size,
            ).cuda()
            outputs = lattice.forward(inputs)

        final_memory = torch.cuda.memory_allocated(0) / 1024**2
        forward_memory = final_memory - after_lattice_memory

        print(
            f"–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ forward pass: {final_memory:.1f} MB (+{forward_memory:.1f} MB)"
        )

        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
        del lattice, inputs, outputs
        torch.cuda.empty_cache()

        cleaned_memory = torch.cuda.memory_allocated(0) / 1024**2
        print(f"–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {cleaned_memory:.1f} MB")

        print("‚úÖ –¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ –ø–∞–º—è—Ç–∏: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ü–†–û–°–¢–û–ô –¢–ï–°–¢ –û–ë–£–ß–ï–ù–ò–Ø - PHASE 4")
    print("=" * 60)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO)

    results = []

    # –¢–µ—Å—Ç 1: –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    dialogue_data = create_minimal_training_data()
    results.append(("Data Creation", dialogue_data is not None))

    # –¢–µ—Å—Ç 2: –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ö–æ–¥
    results.append(("Forward Pass", test_simple_forward_pass()))

    # –¢–µ—Å—Ç 3: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    results.append(("Training Config", test_simple_training_step()))

    # –¢–µ—Å—Ç 4: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
    results.append(("Memory Optimization", test_memory_optimization()))

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–°–¢–û–ì–û –¢–ï–°–¢–ê:")
    print("=" * 60)

    passed = 0
    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"  {status} | {name}")
        if result:
            passed += 1

    print(f"\nüéØ –ò–¢–û–ì–û: {passed}/{len(results)} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ—à–ª–∏")

    if passed == len(results):
        print("üéâ –í–°–ï –ü–†–û–°–¢–´–ï –¢–ï–°–¢–´ –ü–†–û–®–õ–ò!")
        print("üöÄ –ì–û–¢–û–í–û –ö –ü–û–õ–ù–û–ú–£ –¢–ï–°–¢–£ –û–ë–£–ß–ï–ù–ò–Ø!")
        print("\n–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
        print("  python test_phase4_full_training_cycle.py")
    else:
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞.")


if __name__ == "__main__":
    main()
