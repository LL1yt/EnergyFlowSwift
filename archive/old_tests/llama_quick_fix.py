"""
ü¶ô Quick LLaMA Fix - Stage 3.1.3.2
–ë—ã—Å—Ç—Ä–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π DialogueDataset 
–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–æ–≤ tensor'–æ–≤
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import torch
import torch.nn.functional as F
import time
import json
from pathlib import Path
from typing import Dict, List, Any
import logging

# –ù–∞—à–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ!)
from training.embedding_trainer.adapter_integration import AdapterCubeTrainer, AdapterIntegrationConfig
from training.embedding_trainer.dialogue_dataset import DialogueDataset, create_dialogue_dataset

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def test_llama_with_real_data(strategy: str = "hierarchical", device: str = "cpu") -> Dict[str, Any]:
    """
    –¢–µ—Å—Ç LLaMA —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ª–æ–≥–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    
    Args:
        strategy: Adapter strategy  
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    logger.info(f"ü¶ô Testing {strategy} with REAL dialogue data...")
    
    start_time = time.time()
    
    # 1. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = AdapterIntegrationConfig(
        teacher_model="llama3-8b-local",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        teacher_embedding_dim=4096,
        cube_dimensions=(15, 15, 11),
        surface_strategy="single",
        adapter_strategy=strategy,
        adapter_learning_rate=0.001,
        cube_learning_rate=0.0005,
        joint_training=True,
        use_reconstruction_loss=True,
        reconstruction_weight=0.1
    )
    
    # 2. –°–æ–∑–¥–∞–µ–º trainer
    trainer = AdapterCubeTrainer(config, device=device)
    trainer.initialize_components()
    
    # 3. –°–æ–∑–¥–∞–µ–º –†–ï–ê–õ–¨–ù–´–ï –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é DialogueDataset!)
    dialogue_pairs = [
        {"question": "What is machine learning?", 
         "answer": "Machine learning is a method of data analysis that automates analytical model building."},
        {"question": "How does neural networks work?", 
         "answer": "Neural networks are computing systems inspired by biological neural networks that process information."},
        {"question": "What is deep learning?", 
         "answer": "Deep learning is a subset of machine learning based on artificial neural networks with multiple layers."},
        {"question": "Explain artificial intelligence", 
         "answer": "Artificial intelligence is the simulation of human intelligence in machines programmed to think and learn."},
        {"question": "What is computer vision?", 
         "answer": "Computer vision is a field of AI that trains computers to interpret and understand visual information."},
        {"question": "How does natural language processing work?", 
         "answer": "NLP combines computational linguistics with statistical machine learning and deep learning models."},
        {"question": "What is reinforcement learning?", 
         "answer": "Reinforcement learning is an area of ML where agents learn to make decisions by taking actions in an environment."},
        {"question": "Explain data science", 
         "answer": "Data science is an interdisciplinary field that uses scientific methods to extract knowledge from data."}
    ]
    
    # –°–æ–∑–¥–∞–µ–º dataset —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    dataset = create_dialogue_dataset(
        dialogue_pairs, 
        teacher_model="llama3-8b-local",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        cache_embeddings=True,
        embedding_dim=4096  # LLaMA-3-8B —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    )
    
    # 4. –ü–æ–ª—É—á–∞–µ–º batch –¥–∞–Ω–Ω—ã—Ö
    dataloader = dataset.get_dataloader(batch_size=8, shuffle=True)
    batch = next(iter(dataloader))
    
    questions = batch[0].to(device).float()  # 4096D (question embeddings) - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32
    answers = batch[1].to(device).float()    # 4096D (answer embeddings) - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32
    
    logger.info(f"üìä Real data loaded:")
    logger.info(f"   Questions: {questions.shape}")
    logger.info(f"   Answers: {answers.shape}")
    logger.info(f"   Baseline Q‚ÜíA similarity: {F.cosine_similarity(questions, answers, dim=1).mean().item():.3f}")
    
    # 5. –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    param_count = trainer.adapter.get_parameter_count()
    compression_ratio = trainer.adapter.get_compression_ratio()
    
    logger.info(f"   Parameters: {param_count:,}")
    logger.info(f"   Compression: {compression_ratio:.3f}")
    
    # 6. Training loop —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    losses = []
    qa_similarities = []
    surface_qa_similarities = []  # –ù–û–í–ê–Ø –º–µ—Ç—Ä–∏–∫–∞: similarity –Ω–∞ surface level
    
    num_epochs = 15
    baseline_qa = F.cosine_similarity(questions, answers, dim=1).mean().item()
    
    for epoch in range(num_epochs):
        # Training step
        metrics = trainer.train_step(questions, answers)
        losses.append(metrics["total_loss"])
        
        # –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ Q‚ÜíA similarity (–Ω–∞ surface level)
        with torch.no_grad():
            # –ü–æ–ª—É—á–∞–µ–º surface embeddings –¥–ª—è questions –∏ answers
            surface_questions = trainer.forward(questions, return_intermediate=False)  # 225D
            surface_answers = trainer.forward(answers, return_intermediate=False)      # 225D
            
            # –¢–µ–ø–µ—Ä—å —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º 225D —Å 225D (–ù–ï 225D —Å 4096D!)
            surface_qa_sim = F.cosine_similarity(surface_questions, surface_answers, dim=1).mean().item()
            surface_qa_similarities.append(surface_qa_sim)
            
            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º original Q‚ÜíA similarity –¥–ª—è reference
            qa_sim = F.cosine_similarity(questions, answers, dim=1).mean().item()
            qa_similarities.append(qa_sim)
        
        if epoch % 5 == 0:
            logger.info(f"   Epoch {epoch:2d}: loss={metrics['total_loss']:.4f}, "
                       f"surface_qa_sim={surface_qa_sim:.4f}, original_qa_sim={qa_sim:.4f}")
    
    training_time = time.time() - start_time
    
    # 7. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    final_loss = losses[-1]
    final_surface_qa = surface_qa_similarities[-1] 
    surface_improvement = final_surface_qa - surface_qa_similarities[0]  # –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ surface level
    
    # 8. –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—Ö–∞
    converged = final_loss < 2.0 and len(losses) >= 10
    positive_learning = surface_improvement > 0.01  # –£–ª—É—á—à–µ–Ω–∏–µ –º–∏–Ω–∏–º—É–º –Ω–∞ 0.01
    stable_training = all(l < 10.0 for l in losses[-5:])  # –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ losses
    
    success_count = sum([converged, positive_learning, stable_training])
    overall_success = success_count >= 2  # –ú–∏–Ω–∏–º—É–º 2 –∏–∑ 3 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    
    result = {
        "strategy": strategy,
        "model_info": {
            "parameter_count": param_count,
            "compression_ratio": compression_ratio
        },
        "data_info": {
            "real_dialogue_pairs": len(dialogue_pairs),
            "baseline_qa_similarity": baseline_qa
        },
        "training_results": {
            "training_time": training_time,
            "final_loss": final_loss,
            "final_surface_qa_similarity": final_surface_qa,
            "surface_similarity_improvement": surface_improvement,
            "epochs": num_epochs
        },
        "success_metrics": {
            "converged": converged,
            "positive_learning": positive_learning,
            "stable_training": stable_training,
            "overall_success": overall_success,
            "success_score": f"{success_count}/3"
        },
        "history": {
            "losses": losses,
            "surface_qa_similarities": surface_qa_similarities,
            "original_qa_similarities": qa_similarities
        }
    }
    
    logger.info(f"‚úÖ Test completed:")
    logger.info(f"   Final loss: {final_loss:.4f}")
    logger.info(f"   Surface Q‚ÜíA similarity: {surface_qa_similarities[0]:.3f} ‚Üí {final_surface_qa:.3f} (Œî{surface_improvement:+.3f})")
    logger.info(f"   Overall success: {overall_success} ({success_count}/3 criteria)")
    
    return result


def print_results(result: Dict[str, Any]):
    """–ü–µ—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    print("\n" + "ü¶ô" * 20)
    print("Meta-Llama-3-8B FIXED Test Results")
    print("ü¶ô" * 20)
    
    model = result["model_info"]
    data = result["data_info"]
    training = result["training_results"]
    success = result["success_metrics"]
    
    print(f"\nüìä MODEL INFO:")
    print(f"   Strategy: {result['strategy']}")
    print(f"   Parameters: {model['parameter_count']:,}")
    print(f"   Compression: {model['compression_ratio']:.3f}")
    
    print(f"\nüìö DATA INFO:")
    print(f"   Real dialogue pairs: {data['real_dialogue_pairs']}")
    print(f"   Baseline Q‚ÜíA similarity: {data['baseline_qa_similarity']:.3f}")
    
    print(f"\nüéØ TRAINING RESULTS:")
    print(f"   Training time: {training['training_time']:.1f}s")
    print(f"   Final loss: {training['final_loss']:.4f}")
    print(f"   Surface Q‚ÜíA similarity: {training['final_surface_qa_similarity']:.3f}")
    print(f"   Surface improvement: {training['surface_similarity_improvement']:+.3f}")
    
    print(f"\n‚úÖ SUCCESS EVALUATION:")
    print(f"   Converged: {success['converged']}")
    print(f"   Positive learning: {success['positive_learning']}")
    print(f"   Stable training: {success['stable_training']}")
    print(f"   Overall success: {success['overall_success']} ({success['success_score']})")
    
    if success["overall_success"]:
        print(f"\nüéâ SUCCESS! Architecture works with real Meta-Llama-3-8B data!")
    else:
        print(f"\nüîß Needs improvement, but basic functionality confirmed.")


if __name__ == "__main__":
    print("ü¶ô Starting FIXED Meta-Llama-3-8B Test with Real Data...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üîß Using device: {device}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    result = test_llama_with_real_data("hierarchical", device)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = Path("results/llama_fixed")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_dir / "fixed_test_results.json", 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    # –ü–µ—á–∞—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print_results(result)
    
    print(f"\nüìÅ Results saved to: {output_dir}/fixed_test_results.json") 