#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –º–∏–≥—Ä–∞—Ü–∏–∏ –º–æ–¥—É–ª—è —Å hardcoded –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞
=====================================================================

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –∫–æ–¥ —Å hardcoded –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
"""

import torch
import torch.nn as nn
from new_rebuild.config import get_project_config
from new_rebuild.utils import strict_no_hardcoded, no_hardcoded, HardcodedValueError


# ‚ùå –ü–õ–û–•–û: –°—Ç–∞—Ä—ã–π –∫–æ–¥ —Å hardcoded –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
class OldModelWithHardcoded(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        # Hardcoded –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ–∑–¥–µ!
        hidden_dim = 64  # ‚ùå
        dropout_rate = 0.1  # ‚ùå
        num_layers = 3  # ‚ùå
        
        self.layers = nn.ModuleList([
            nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)
            for i in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
            x = torch.relu(x)
            x = self.dropout(x)
        return x


# ‚úÖ –•–û–†–û–®–û: –ù–æ–≤—ã–π –∫–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥–∞
class NewModelWithConfig(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        config = get_project_config()
        
        # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞!
        hidden_dim = config.model.hidden_dim
        dropout_rate = config.architecture.cnf_dropout_rate
        num_layers = config.model.num_layers
        
        self.layers = nn.ModuleList([
            nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)
            for i in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
            x = torch.relu(x)
            x = self.dropout(x)
        return x


# ‚ùå –ü–õ–û–•–û: –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å hardcoded
def train_model_old(model, data_loader):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  # ‚ùå
    max_epochs = 10  # ‚ùå
    log_every = 100  # ‚ùå
    
    for epoch in range(max_epochs):
        for batch_idx, (data, target) in enumerate(data_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = nn.functional.mse_loss(output, target)
            loss.backward()
            
            # Gradient clipping —Å hardcoded –∑–Ω–∞—á–µ–Ω–∏–µ–º
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ‚ùå
            
            optimizer.step()
            
            if batch_idx % log_every == 0:  # ‚ùå
                print(f"Loss: {loss.item()}")


# ‚úÖ –•–û–†–û–®–û: –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –∫–æ–Ω—Ñ–∏–≥–æ–º
@no_hardcoded  # –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã
def train_model_new(model, data_loader):
    config = get_project_config()
    
    # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=config.training_optimizer.learning_rate,
        weight_decay=config.training_optimizer.weight_decay
    )
    
    max_epochs = config.training.num_epochs
    log_every = config.training_optimizer.log_batch_frequency
    
    for epoch in range(max_epochs):
        for batch_idx, (data, target) in enumerate(data_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = nn.functional.mse_loss(output, target)
            loss.backward()
            
            # Gradient clipping –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), 
                max_norm=config.training_optimizer.gradient_clip_max_norm
            )
            
            optimizer.step()
            
            if batch_idx % log_every == 0:
                print(f"Loss: {loss.item()}")


# –ü—Ä–∏–º–µ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º strict_no_hardcoded –¥–ª—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏
class TransitionModel(nn.Module):
    """–ú–æ–¥–µ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –º–∏–≥—Ä–∞—Ü–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç strict_no_hardcoded"""
    
    def __init__(self, input_dim):
        super().__init__()
        
        # strict_no_hardcoded –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–º–µ–Ω–∏—Ç –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        hidden_dim = strict_no_hardcoded(64, "model.hidden_dim")
        dropout_rate = strict_no_hardcoded(0.1, "architecture.cnf_dropout_rate")
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_dim, input_dim)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
def process_data_with_checks(data, threshold=0.8):  # 0.8 —ç—Ç–æ hardcoded!
    """–ü—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç hardcoded –∑–Ω–∞—á–µ–Ω–∏—è"""
    
    try:
        # –≠—Ç–æ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É
        check_hardcoded_value(threshold, "process_data threshold")
    except HardcodedValueError as e:
        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ hardcoded –∑–Ω–∞—á–µ–Ω–∏–µ! {e}")
        # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        config = get_project_config()
        threshold = config.embedding_mapping.surface_coverage
        
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º threshold
    return data[data > threshold]


def demonstrate_migration():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –º–∏–≥—Ä–∞—Ü–∏–∏"""
    print("üîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Å hardcoded –Ω–∞ config")
    print("=" * 60)
    
    from new_rebuild.config import create_experiment_config, set_project_config
    config = create_experiment_config()
    set_project_config(config)
    
    print("\n1. –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å —Å hardcoded (–ø–ª–æ—Ö–æ):")
    old_model = OldModelWithHardcoded(100)
    print(f"   ‚ùå –ò—Å–ø–æ–ª—å–∑—É–µ—Ç hardcoded hidden_dim=64, dropout=0.1")
    
    print("\n2. –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥–æ–º (—Ö–æ—Ä–æ—à–æ):")
    new_model = NewModelWithConfig(100)
    print(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç config: hidden_dim={config.model.hidden_dim}, "
          f"dropout={config.architecture.cnf_dropout_rate}")
    
    print("\n3. –ú–æ–¥–µ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –º–∏–≥—Ä–∞—Ü–∏–∏ (—Å strict_no_hardcoded):")
    transition_model = TransitionModel(100)
    print(f"   üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–º–µ–Ω—è–µ—Ç hardcoded –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞")
    
    print("\n4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö hardcoded –∑–Ω–∞—á–µ–Ω–∏–π:")
    import torch
    data = torch.rand(10)
    result = process_data_with_checks(data)
    print(f"   ‚úÖ Threshold –≤–∑—è—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {config.embedding_mapping.surface_coverage}")
    
    print("\n" + "=" * 60)
    print("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞!")


if __name__ == "__main__":
    demonstrate_migration()