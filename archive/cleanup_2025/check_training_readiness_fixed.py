#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –∫ —Ä–µ–∞–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é
================================================================

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –ù–æ–≤—ã–π unified dataset loader –≤ new_rebuild —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π neighbor_count
"""

import torch
import sys
from pathlib import Path
import json
import logging
from typing import Dict, List, Any

from new_rebuild.config import SimpleProjectConfig

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_gpu_availability() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU"""
    result = {
        "cuda_available": torch.cuda.is_available(),
        "gpu_count": 0,
        "gpu_names": [],
        "memory_info": {}
    }
    
    if torch.cuda.is_available():
        result["gpu_count"] = torch.cuda.device_count()
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            result["gpu_names"].append(gpu_name)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3   # GB
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
            
            result["memory_info"][f"gpu_{i}"] = {
                "name": gpu_name,
                "total_memory_gb": round(memory_total, 2),
                "allocated_gb": round(memory_allocated, 2),
                "reserved_gb": round(memory_reserved, 2),
                "free_gb": round(memory_total - memory_reserved, 2)
            }
    
    return result


def check_dataset_availability() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    result = {
        "dialogue_cache": {"available": False, "count": 0},
        "prepared_embeddings": {"available": False, "count": 0},
        "cache_embeddings": {"available": False, "count": 0}
    }
    
    # Dialogue cache
    dialogue_dir = Path("cache/dialogue_dataset")
    if dialogue_dir.exists():
        dialogue_files = list(dialogue_dir.glob("*.pt"))
        result["dialogue_cache"] = {
            "available": len(dialogue_files) > 0,
            "count": len(dialogue_files)
        }
    
    # Prepared embeddings
    embeddings_dir = Path("data/embeddings")
    if embeddings_dir.exists():
        embedding_files = list(embeddings_dir.glob("*.pt"))
        result["prepared_embeddings"] = {
            "available": len(embedding_files) > 0,
            "count": len(embedding_files)
        }
    
    # Cache embeddings (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é)
    cache_files = list(Path("cache").glob("llm_*.pt"))
    valid_cache_files = 0
    
    for file in cache_files[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
        try:
            data = torch.load(file, map_location='cpu')
            if isinstance(data, torch.Tensor) and data.shape[-1] == 768:
                valid_cache_files += 1
        except:
            continue
    
    result["cache_embeddings"] = {
        "available": valid_cache_files > 0,
        "count": valid_cache_files,
        "total_files": len(cache_files)
    }
    
    return result


def check_central_config() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    result = {"success": False, "error": None, "config_values": {}}
    
    try:
        from new_rebuild.config import SimpleProjectConfig
        
        config = SimpleProjectConfig()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        result["config_values"] = {
            "lattice_dimensions": config.lattice.dimensions,
            "test_mode": config.training_embedding.test_mode,
            "num_epochs": config.training_embedding.num_epochs,
            "target_embedding_dim": config.training_embedding.target_embedding_dim,
            "batch_size": config.training_embedding.embedding_batch_size,
            "neighbor_count": config.model.neighbor_count,
            "state_size": config.model.state_size,
            "hidden_dim": config.model.hidden_dim
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω—Ñ–∏–≥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        checks = []
        if config.training_embedding.test_mode == False:
            checks.append("‚úÖ Real training mode enabled")
        else:
            checks.append("‚ö†Ô∏è Test mode still enabled")
            
        if config.model.neighbor_count == -1:
            checks.append("‚úÖ Dynamic neighbor count enabled")
        else:
            checks.append(f"‚ÑπÔ∏è Static neighbor count: {config.model.neighbor_count}")
            
        if config.lattice.dimensions == (8, 8, 8):
            checks.append("‚úÖ Correct lattice size for first training")
        else:
            checks.append(f"‚ÑπÔ∏è Lattice size: {config.lattice.dimensions}")
        
        result["config_checks"] = checks
        result["success"] = True
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def test_new_dataset_loader() -> Dict[str, Any]:
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π unified dataset loader"""
    result = {"success": False, "error": None, "sample_count": 0}
    
    try:
        from new_rebuild.config import SimpleProjectConfig
        from new_rebuild.core.training.utils import create_training_dataloader
        
        config = SimpleProjectConfig()
        
        # –°–æ–∑–¥–∞–µ–º DataLoader —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
        dataloader, stats = create_training_dataloader(
            config=config,
            max_samples_per_source=10,  # –¢–æ–ª—å–∫–æ 10 –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
            shuffle=True
        )
        
        result["sample_count"] = stats.total_samples
        result["embedding_dim"] = stats.embedding_dim
        result["source_distribution"] = stats.source_distribution
        result["success"] = stats.total_samples > 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω –±–∞—Ç—á
        if stats.total_samples > 0:
            for batch in dataloader:
                embeddings = batch['embedding']
                if embeddings.shape[1] == config.embedding.input_dim:
                    result["embedding_dim_ok"] = True
                    result["batch_shape"] = list(embeddings.shape)
                else:
                    result["embedding_dim_ok"] = False
                    result["expected_dim"] = config.embedding.input_dim
                    result["actual_dim"] = embeddings.shape[1]
                break
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def test_embedding_trainer_creation() -> Dict[str, Any]:
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ EmbeddingTrainer —Å –Ω–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    result = {"success": False, "error": None}
    
    try:
        from new_rebuild.config import SimpleProjectConfig
        from new_rebuild.core.training import EmbeddingTrainer
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = SimpleProjectConfig()
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ç–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        config.training_embedding.test_mode = True
        # config.lattice.dimensions = (4, 4, 4)  # –ù–ï –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
        
        trainer = EmbeddingTrainer(config)
        result["success"] = True
        result["total_parameters"] = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def check_dynamic_neighbors_quick(config: SimpleProjectConfig) -> Dict[str, Any]:
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–æ—Å–µ–¥–µ–π"""
    result = {"success": False, "error": None}
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        neighbor_count = config.model.neighbor_count
        dynamic_enabled = getattr(config.neighbors, 'dynamic_count', False) if hasattr(config, 'neighbors') and config.neighbors else False
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–µ—à–µ—Ç–∫–∏
        dimensions = config.lattice.dimensions
        max_radius = config.lattice.max_radius
        local_threshold = config.lattice.local_distance_threshold
        functional_threshold = config.lattice.functional_distance_threshold
        
        result.update({
            "success": True,
            "neighbor_count_setting": neighbor_count,
            "dynamic_enabled": dynamic_enabled,
            "lattice_dimensions": dimensions,
            "max_radius": round(max_radius, 2),
            "thresholds": {
                "local": round(local_threshold, 2),
                "functional": round(functional_threshold, 2)
            },
            "is_dynamic": neighbor_count == -1,
            "legacy_detected": neighbor_count in [6, 26]
        })
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def check_dependencies() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python"""
    required_modules = [
        "torch",
        "numpy", 
        "pathlib",
        "json",
        "logging"
    ]
    
    result = {"missing_modules": [], "available_modules": []}
    
    for module in required_modules:
        try:
            __import__(module)
            result["available_modules"].append(module)
        except ImportError:
            result["missing_modules"].append(module)
    
    return result


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏"""
    print("üîç CHECKING TRAINING READINESS (FIXED VERSION)")
    print("=" * 55)
    
    all_checks = {}
    
    # 1. GPU –ø—Ä–æ–≤–µ—Ä–∫–∞
    print("\nüñ•Ô∏è Checking GPU availability...")
    gpu_check = check_gpu_availability()
    all_checks["gpu"] = gpu_check
    
    if gpu_check["cuda_available"]:
        print(f"‚úÖ CUDA available: {gpu_check['gpu_count']} GPUs")
        for gpu_name in gpu_check["gpu_names"]:
            print(f"   - {gpu_name}")
            
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å RTX 5090
        for gpu_id, info in gpu_check["memory_info"].items():
            if "RTX 5090" in info["name"]:
                print(f"   üíæ {info['name']}: {info['free_gb']:.1f}GB free")
    else:
        print("‚ùå CUDA not available - will use CPU")
    
    # 2. –î–∞—Ç–∞—Å–µ—Ç—ã
    print("\nüìÇ Checking dataset availability...")
    dataset_check = check_dataset_availability()
    all_checks["datasets"] = dataset_check
    
    total_samples = 0
    for dataset_type, info in dataset_check.items():
        status = "‚úÖ" if info["available"] else "‚ùå"
        count = info.get("count", 0)
        if dataset_type == "cache_embeddings":
            total_files = info.get("total_files", 0)
            print(f"   {status} {dataset_type}: {count} valid (of {total_files} total)")
            total_samples += count * 50  # –ü—Ä–∏–º–µ—Ä–Ω–æ
        else:
            total_samples += count * 4  # –ü—Ä–∏–º–µ—Ä–Ω–æ –ø–æ 4 —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –Ω–∞ —Ñ–∞–π–ª
            print(f"   {status} {dataset_type}: {count} files")
    
    print(f"üìä Estimated total samples: ~{total_samples}")
    
    # 3. –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    print("\nüì¶ Checking Python dependencies...")
    deps_check = check_dependencies()
    all_checks["dependencies"] = deps_check
    
    if deps_check["missing_modules"]:
        print("‚ùå Missing modules:")
        for module in deps_check["missing_modules"]:
            print(f"   - {module}")
    else:
        print("‚úÖ All required modules available")
    
    # 4. –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    print("\n‚öôÔ∏è Checking central configuration...")
    config_check = check_central_config()
    all_checks["central_config"] = config_check
    
    if config_check["success"]:
        print("‚úÖ Central config loads successfully")
        for check in config_check["config_checks"]:
            print(f"   {check}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        values = config_check["config_values"]
        print(f"   üìè Lattice: {values['lattice_dimensions']}")
        print(f"   üß† State size: {values['state_size']}, Hidden: {values['hidden_dim']}")
        print(f"   üìä Batch size: {values['batch_size']}, Epochs: {values['num_epochs']}")
        print(f"   üéØ Target dim: {values['target_embedding_dim']}")
    else:
        print(f"‚ùå Central config failed: {config_check.get('error')}")
    
    # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–æ—Å–µ–¥–∏
    print("\nüéØ Checking dynamic neighbors...")
    if config_check["success"]:
        config = SimpleProjectConfig()
        neighbors_check = check_dynamic_neighbors_quick(config)
        all_checks["dynamic_neighbors"] = neighbors_check
        
        if neighbors_check["success"]:
            print("‚úÖ Dynamic neighbors analysis completed")
            print(f"   üìè Lattice: {neighbors_check['lattice_dimensions']}")
            print(f"   üéØ Max radius: {neighbors_check['max_radius']}")
            print(f"   üîµ Local threshold: {neighbors_check['thresholds']['local']}")
            print(f"   üü° Functional threshold: {neighbors_check['thresholds']['functional']}")
            
            if neighbors_check["is_dynamic"]:
                print("   ‚úÖ Dynamic neighbor count enabled (neighbor_count = -1)")
            elif neighbors_check["legacy_detected"]:
                print(f"   ‚ùå Legacy neighbor count detected: {neighbors_check['neighbor_count_setting']}")
            else:
                print(f"   ‚ÑπÔ∏è Static neighbor count: {neighbors_check['neighbor_count_setting']}")
                
            if neighbors_check["dynamic_enabled"]:
                print("   ‚úÖ Dynamic count enabled in NeighborSettings")
        else:
            print(f"‚ùå Dynamic neighbors check failed: {neighbors_check.get('error')}")
    else:
        print("‚è≠Ô∏è Skipping (config failed)")
    
    # 6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π dataset loader
    print("\nüîÑ Testing new unified dataset loader...")
    loader_test = test_new_dataset_loader()
    all_checks["dataset_loader"] = loader_test
    
    if loader_test["success"]:
        print(f"‚úÖ Dataset loader works: {loader_test['sample_count']} samples loaded")
        print(f"   üìä Source distribution: {loader_test['source_distribution']}")
        if loader_test.get("embedding_dim_ok"):
            print(f"‚úÖ Embedding dimensions correct ({loader_test['embedding_dim']}D)")
        else:
            print(f"‚ùå Wrong embedding dimension: expected {loader_test.get('expected_dim')}, got {loader_test.get('actual_dim')}")
    else:
        print(f"‚ùå Dataset loader failed: {loader_test.get('error')}")
    
    # 7. –¢–µ—Å—Ç–∏—Ä—É–µ–º EmbeddingTrainer
    print("\nüß† Testing EmbeddingTrainer creation...")
    trainer_test = test_embedding_trainer_creation()
    all_checks["trainer"] = trainer_test
    
    if trainer_test["success"]:
        params = trainer_test.get("total_parameters", 0)
        print(f"‚úÖ EmbeddingTrainer creates successfully ({params:,} parameters)")
    else:
        print(f"‚ùå EmbeddingTrainer failed: {trainer_test.get('error')}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_file = Path("training_readiness_check_fixed.json")
    with open(results_file, 'w') as f:
        json.dump(all_checks, f, indent=2)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print(f"\nüìä READINESS SUMMARY")
    print("=" * 30)
    
    critical_checks = [
        (gpu_check["cuda_available"], "GPU (CUDA)"),
        (dataset_check["dialogue_cache"]["available"] or 
         dataset_check["prepared_embeddings"]["available"] or 
         dataset_check["cache_embeddings"]["available"], "Datasets"),
        (len(deps_check["missing_modules"]) == 0, "Dependencies"),
        (config_check["success"], "Central Config"),
        (all_checks.get("dynamic_neighbors", {}).get("success", False) and 
         not all_checks.get("dynamic_neighbors", {}).get("legacy_detected", False), "Dynamic Neighbors"),
        (loader_test["success"], "Dataset Loader"),
        (trainer_test["success"], "EmbeddingTrainer")
    ]
    
    all_ready = True
    for is_ready, check_name in critical_checks:
        status = "‚úÖ" if is_ready else "‚ùå"
        print(f"{status} {check_name}")
        if not is_ready:
            all_ready = False
    
    print(f"\n{'üöÄ SYSTEM READY FOR TRAINING!' if all_ready else '‚ö†Ô∏è FIX ISSUES BEFORE TRAINING'}")
    
    if all_ready:
        print("\nNext steps:")
        print("1. Run: python real_training_simple.py")
        print("2. Monitor: experiments/")
        print("3. Check logs for progress")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        if config_check["success"] and config_check["config_values"]["test_mode"]:
            print("\n‚ö†Ô∏è NOTE: test_mode=True in config")
            print("For real training, edit new_rebuild/config/config_components.py:")
            print("  Change: test_mode: bool = False")
    else:
        print("\nFix the issues marked with ‚ùå before proceeding")
    
    print(f"\nüìÑ Detailed results saved to: {results_file}")


if __name__ == "__main__":
    main()