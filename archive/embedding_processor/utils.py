"""
EmbeddingProcessor Utils - –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
=============================================================

–ù–∞–±–æ—Ä —É—Ç–∏–ª–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ EmbeddingProcessor.
"""

import torch
import numpy as np
import json
import time
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import logging

from .config import EmbeddingConfig, ProcessingMode
from .metrics import calculate_processing_quality, evaluate_semantic_preservation

logger = logging.getLogger(__name__)


def create_test_embedding_batch(batch_size: int = 4, 
                               embedding_dim: int = 768,
                               embedding_type: str = "random") -> torch.Tensor:
    """
    –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ EmbeddingProcessor
    
    Args:
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        embedding_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        embedding_type: –¢–∏–ø —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ ("random", "semantic", "normalized")
        
    Returns:
        torch.Tensor: –¢–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á [batch_size, embedding_dim]
    """
    if embedding_type == "random":
        # –°–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
        embeddings = torch.randn(batch_size, embedding_dim)
        
    elif embedding_type == "semantic":
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ (–∏–º–∏—Ç–∏—Ä—É—é—Ç —Ä–µ–∞–ª—å–Ω—ã–µ)
        embeddings = []
        for i in range(batch_size):
            # –°–æ–∑–¥–∞–µ–º "—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π" –ø–∞—Ç—Ç–µ—Ä–Ω
            base = torch.randn(embedding_dim) * 0.5
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            semantic_component = torch.sin(torch.arange(embedding_dim, dtype=torch.float) * (i + 1) * 0.01)
            context_component = torch.cos(torch.arange(embedding_dim, dtype=torch.float) * (i + 1) * 0.005)
            
            embedding = base + semantic_component * 0.3 + context_component * 0.2
            embeddings.append(embedding)
        
        embeddings = torch.stack(embeddings)
        
    elif embedding_type == "normalized":
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ (–µ–¥–∏–Ω–∏—á–Ω–∞—è –¥–ª–∏–Ω–∞)
        embeddings = torch.randn(batch_size, embedding_dim)
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {embedding_type}")
    
    logger.debug(f"üß™ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á: {embeddings.shape} ({embedding_type})")
    return embeddings


def validate_processor_output(input_embeddings: torch.Tensor,
                             output_embeddings: torch.Tensor,
                             config: EmbeddingConfig) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥ EmbeddingProcessor
    
    Args:
        input_embeddings: –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
        output_embeddings: –í—ã—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    validation_results = {
        "shape_valid": False,
        "dtype_valid": False,
        "similarity_valid": False,
        "quality_metrics": {},
        "errors": []
    }
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã
        if input_embeddings.shape == output_embeddings.shape:
            validation_results["shape_valid"] = True
        else:
            validation_results["errors"].append(
                f"Shape mismatch: {input_embeddings.shape} != {output_embeddings.shape}"
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
        if input_embeddings.dtype == output_embeddings.dtype:
            validation_results["dtype_valid"] = True
        else:
            validation_results["errors"].append(
                f"Dtype mismatch: {input_embeddings.dtype} != {output_embeddings.dtype}"
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_metrics = calculate_processing_quality(input_embeddings, output_embeddings)
        validation_results["quality_metrics"] = quality_metrics
        
        mean_similarity = quality_metrics["mean_cosine_similarity"]
        if mean_similarity >= config.target_similarity:
            validation_results["similarity_valid"] = True
        else:
            validation_results["errors"].append(
                f"Similarity too low: {mean_similarity:.3f} < {config.target_similarity:.3f}"
            )
        
        # –û–±—â–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        validation_results["all_valid"] = (
            validation_results["shape_valid"] and 
            validation_results["dtype_valid"] and 
            validation_results["similarity_valid"]
        )
        
    except Exception as e:
        validation_results["errors"].append(f"Validation error: {str(e)}")
        validation_results["all_valid"] = False
    
    return validation_results


def benchmark_processing_speed(processor, 
                              test_sizes: List[Tuple[int, int]] = None,
                              num_runs: int = 5) -> Dict[str, Any]:
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ EmbeddingProcessor
    
    Args:
        processor: EmbeddingProcessor instance
        test_sizes: –°–ø–∏—Å–æ–∫ (batch_size, embedding_dim) –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        num_runs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞
    """
    if test_sizes is None:
        test_sizes = [(1, 768), (4, 768), (8, 768), (16, 768)]
    
    benchmark_results = {
        "test_configurations": [],
        "summary": {}
    }
    
    all_throughputs = []
    
    for batch_size, embedding_dim in test_sizes:
        logger.info(f"üî¨ –ë–µ–Ω—á–º–∞—Ä–∫: batch_size={batch_size}, embedding_dim={embedding_dim}")
        
        times = []
        throughputs = []
        
        for run in range(num_runs):
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_batch = create_test_embedding_batch(batch_size, embedding_dim, "normalized")
            
            # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
            start_time = time.time()
            
            with torch.no_grad():
                output = processor.forward(test_batch)
            
            processing_time = time.time() - start_time
            throughput = batch_size / processing_time
            
            times.append(processing_time)
            throughputs.append(throughput)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_results = {
            "batch_size": batch_size,
            "embedding_dim": embedding_dim,
            "mean_time": np.mean(times),
            "std_time": np.std(times),
            "min_time": np.min(times),
            "max_time": np.max(times),
            "mean_throughput": np.mean(throughputs),
            "std_throughput": np.std(throughputs),
            "runs": num_runs
        }
        
        benchmark_results["test_configurations"].append(config_results)
        all_throughputs.extend(throughputs)
        
        logger.info(f"[FAST] –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {config_results['mean_throughput']:.1f} ¬± {config_results['std_throughput']:.1f} —ç–º–±/—Å–µ–∫")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    benchmark_results["summary"] = {
        "overall_mean_throughput": np.mean(all_throughputs),
        "overall_std_throughput": np.std(all_throughputs),
        "best_throughput": np.max(all_throughputs),
        "total_tests": len(test_sizes) * num_runs
    }
    
    return benchmark_results


def run_comprehensive_test(processor, 
                          test_modes: List[ProcessingMode] = None,
                          batch_size: int = 8) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç EmbeddingProcessor
    
    Args:
        processor: EmbeddingProcessor instance
        test_modes: –†–µ–∂–∏–º—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        batch_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –±–∞—Ç—á–∞
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
    """
    if test_modes is None:
        test_modes = [ProcessingMode.AUTOENCODER, ProcessingMode.GENERATOR, ProcessingMode.DIALOGUE]
    
    test_results = {
        "mode_tests": {},
        "overall_summary": {}
    }
    
    all_similarities = []
    all_times = []
    
    for mode in test_modes:
        logger.info(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∂–∏–º–∞: {mode.value}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º
        original_mode = processor.config.processing_mode
        processor.set_mode(mode)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_input = create_test_embedding_batch(batch_size, 768, "semantic")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞
        start_time = time.time()
        test_output = processor.forward(test_input)
        processing_time = time.time() - start_time
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        validation = validate_processor_output(test_input, test_output, processor.config)
        
        # –ö–∞—á–µ—Å—Ç–≤–æ
        quality = calculate_processing_quality(test_input, test_output)
        semantic_eval = evaluate_semantic_preservation(test_input, test_output)
        
        mode_results = {
            "mode": mode.value,
            "processing_time": processing_time,
            "validation": validation,
            "quality_metrics": quality,
            "semantic_preservation": semantic_eval,
            "throughput": batch_size / processing_time
        }
        
        test_results["mode_tests"][mode.value] = mode_results
        
        all_similarities.append(quality["mean_cosine_similarity"])
        all_times.append(processing_time)
        
        logger.info(f"[OK] {mode.value}: similarity={quality['mean_cosine_similarity']:.3f}, time={processing_time:.3f}s")
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º
    processor.set_mode(original_mode)
    
    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
    test_results["overall_summary"] = {
        "all_modes_tested": len(test_modes),
        "mean_similarity": np.mean(all_similarities),
        "min_similarity": np.min(all_similarities),
        "max_similarity": np.max(all_similarities),
        "mean_processing_time": np.mean(all_times),
        "total_test_time": np.sum(all_times),
        "all_passed": all(
            result["validation"]["all_valid"] 
            for result in test_results["mode_tests"].values()
        )
    }
    
    return test_results


def export_processing_results(results: Dict[str, Any], 
                             output_path: str = "outputs/embedding_processor_results.json"):
    """
    –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Ñ–∞–π–ª
    
    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
    json_results = convert_to_json_serializable(results)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    json_results["export_metadata"] = {
        "export_time": time.time(),
        "export_path": str(output_path),
        "phase": "2.5",
        "module": "embedding_processor"
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"[FOLDER] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤: {output_path}")


def convert_to_json_serializable(obj: Any) -> Any:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
    
    if isinstance(obj, dict):
        return {key: convert_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return list(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.detach().cpu().numpy().tolist()
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif isinstance(obj, ProcessingMode):
        return obj.value
    elif hasattr(obj, '__dict__'):
        return convert_to_json_serializable(obj.__dict__)
    else:
        return obj


def create_quality_report(processor,
                         num_samples: int = 100,
                         report_path: str = "outputs/phase_2_5_quality_report.json") -> Dict[str, Any]:
    """
    –°–æ–∑–¥–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–∞–±–æ—Ç—ã EmbeddingProcessor
    
    Args:
        processor: EmbeddingProcessor instance
        num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        report_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
        
    Returns:
        Dict —Å –ø–æ–ª–Ω—ã–º –æ—Ç—á–µ—Ç–æ–º –æ –∫–∞—á–µ—Å—Ç–≤–µ
    """
    logger.info(f"[DATA] –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ (samples={num_samples})")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö
    test_types = ["random", "semantic", "normalized"]
    
    quality_report = {
        "test_summary": {},
        "detailed_results": {},
        "phase_2_5_assessment": {}
    }
    
    all_similarities = []
    
    for test_type in test_types:
        logger.info(f"üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {test_type} —ç–º–±–µ–¥–∏–Ω–≥–∞—Ö")
        
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
        similarities = []
        processing_times = []
        
        for i in range(num_samples // 10):  # –ë–∞—Ç—á–∏ –ø–æ 10
            test_batch = create_test_embedding_batch(10, 768, test_type)
            
            start_time = time.time()
            output_batch = processor.forward(test_batch)
            proc_time = time.time() - start_time
            
            quality = calculate_processing_quality(test_batch, output_batch)
            
            similarities.append(quality["mean_cosine_similarity"])
            processing_times.append(proc_time)
            all_similarities.append(quality["mean_cosine_similarity"])
        
        type_results = {
            "test_type": test_type,
            "samples_tested": len(similarities) * 10,
            "mean_similarity": np.mean(similarities),
            "std_similarity": np.std(similarities),
            "min_similarity": np.min(similarities),
            "max_similarity": np.max(similarities),
            "mean_processing_time": np.mean(processing_times),
            "target_achievement_rate": np.mean(np.array(similarities) >= processor.config.target_similarity)
        }
        
        quality_report["detailed_results"][test_type] = type_results
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
    overall_similarity = np.mean(all_similarities)
    target_achievement = np.mean(np.array(all_similarities) >= processor.config.target_similarity)
    
    # Phase 2.5 assessment
    phase_2_5_ready = overall_similarity >= processor.config.target_similarity
    
    quality_report["test_summary"] = {
        "total_samples": num_samples,
        "overall_mean_similarity": overall_similarity,
        "overall_target_achievement": target_achievement,
        "test_types_count": len(test_types)
    }
    
    quality_report["phase_2_5_assessment"] = {
        "target_similarity": processor.config.target_similarity,
        "achieved_similarity": overall_similarity,
        "phase_2_5_ready": phase_2_5_ready,
        "quality_level": "excellent" if overall_similarity >= 0.95 else 
                        "good" if overall_similarity >= 0.90 else
                        "acceptable" if overall_similarity >= 0.80 else "poor",
        "recommendation": "Ready for Phase 3" if phase_2_5_ready else "Needs improvement"
    }
    
    # –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞
    export_processing_results(quality_report, report_path)
    
    logger.info(f"[OK] –û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤: similarity={overall_similarity:.3f}, Phase 2.5 ready: {phase_2_5_ready}")
    
    return quality_report 