# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: Embedding Loader Module

**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 5 –∏—é–Ω—è 2025  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é  
**–í–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è:** 1.0.0

---

## üöÄ –ë–ê–ó–û–í–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Word2Vec

```python
from data.embedding_loader import EmbeddingLoader

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
loader = EmbeddingLoader()

# –ó–∞–≥—Ä—É–∑–∫–∞ Word2Vec —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
embeddings = loader.load_embeddings(
    path="./data/embeddings/word2vec.txt",
    format_type="word2vec",
    preprocess=True
)

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
print(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {embeddings.dtype}")
print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {embeddings.device}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç–º–±–µ–¥–∏–Ω–≥–∞—Ö
info = loader.get_embedding_info(embeddings)
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {info}")
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**

```
–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: torch.Size([400000, 300])
–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: torch.float32
–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu
–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {
    'shape': torch.Size([400000, 300]),
    'memory_mb': 457.76,
    'min_value': -1.2345,
    'max_value': 1.8765,
    'mean_value': 0.0123,
    'std_value': 0.9876
}
```

### –ü—Ä–∏–º–µ—Ä 2: –ó–∞–≥—Ä—É–∑–∫–∞ GloVe —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π

```python
from data.embedding_loader import EmbeddingLoader, EmbeddingPreprocessor

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
loader = EmbeddingLoader(cache_dir="./my_cache/")
preprocessor = EmbeddingPreprocessor()

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
raw_embeddings = loader.load_embeddings(
    path="./data/embeddings/glove.6B.300d.txt",
    format_type="glove",
    preprocess=False  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
)

print(f"–°—ã—Ä—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏: {raw_embeddings.shape}")

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
processed_embeddings = preprocessor.preprocess(
    raw_embeddings,
    normalize=True,      # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    center=True,         # –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
    clip_outliers=True,  # –û–±—Ä–µ–∑–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
    outlier_std=2.5      # –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤
)

print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏: {processed_embeddings.shape}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
stats = preprocessor.get_statistics()
print(f"–î–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {stats['original_mean']:.4f}")
print(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {stats['processed_mean']:.4f}")
```

### –ü—Ä–∏–º–µ—Ä 3: –†–∞–±–æ—Ç–∞ —Å BERT —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏

```python
from data.embedding_loader import EmbeddingLoader
import torch

loader = EmbeddingLoader()

# –ó–∞–≥—Ä—É–∑–∫–∞ BERT —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∏–∑ PyTorch —Ñ–∞–π–ª–∞
bert_embeddings = loader.load_embeddings(
    path="./data/embeddings/bert_embeddings.pt",
    format_type="bert",
    preprocess=True
)

print(f"BERT —ç–º–±–µ–¥–∏–Ω–≥–∏: {bert_embeddings.shape}")
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {bert_embeddings.shape[1]} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 768 –¥–ª—è BERT-base)")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–∏—è
has_nan = torch.isnan(bert_embeddings).any()
print(f"–°–æ–¥–µ—Ä–∂–∏—Ç NaN: {has_nan}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
loader.cache_embeddings(bert_embeddings, "bert_base_processed")

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑
cached = loader.load_from_cache("bert_base_processed")
if cached is not None:
    print("–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞!")
```

---

## üîß –ü–†–û–î–í–ò–ù–£–¢–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Lattice3D

```python
from data.embedding_loader import EmbeddingLoader
from core.lattice_3d import Lattice3D
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
loader = EmbeddingLoader()
embeddings = loader.load_embeddings(
    path="./data/embeddings/word2vec.bin",  # –ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª
    format_type="word2vec",
    preprocess=True
)

# –°–æ–∑–¥–∞–Ω–∏–µ 3D —Ä–µ—à–µ—Ç–∫–∏
lattice = Lattice3D(width=10, height=10, depth=10)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ—Ç–∫–∏
# –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100 –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–π –≥—Ä–∞–Ω–∏
input_data = embeddings[:100]

# –ü–æ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—Ö–æ–¥–Ω—É—é –≥—Ä–∞–Ω—å —Ä–µ—à–µ—Ç–∫–∏
lattice.set_input_face(input_data)
print(f"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–∞–Ω—ã –Ω–∞ —Ä–µ—à–µ—Ç–∫—É: {input_data.shape}")

# –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
output = lattice.propagate()
print(f"–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—à–µ—Ç–∫–∏: {output.shape}")

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
output_info = loader.get_embedding_info(output)
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—ã—Ö–æ–¥–∞: {output_info}")
```

### –ü—Ä–∏–º–µ—Ä 5: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤

```python
from data.embedding_loader import EmbeddingLoader
import torch
from pathlib import Path

def process_large_embeddings(file_path: str, batch_size: int = 10000):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –ø–æ —á–∞—Å—Ç—è–º.
    """
    loader = EmbeddingLoader()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
    embeddings = loader.load_embeddings(file_path, "glove", preprocess=True)

    total_size = embeddings.shape[0]
    print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size} –≤–µ–∫—Ç–æ—Ä–æ–≤")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –±–∞—Ç—á–∞–º
    processed_batches = []

    for i in range(0, total_size, batch_size):
        end_idx = min(i + batch_size, total_size)
        batch = embeddings[i:end_idx]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        batch_processed = loader.preprocessor.standardize_embeddings(batch)
        processed_batches.append(batch_processed)

        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –±–∞—Ç—á {i//batch_size + 1}: {batch.shape}")

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_result = torch.cat(processed_batches, dim=0)
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {final_result.shape}")

    return final_result

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = process_large_embeddings("./data/embeddings/glove.840B.300d.txt")
print("Converted embeddings shape:", result.shape)
print("Used preprocessing:", loader.stats['preprocessing_used'])
```

### –ü—Ä–∏–º–µ—Ä 6: –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π

```python
from data.embedding_loader import EmbeddingLoader
import yaml

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
custom_config = {
    'cache': {
        'cache_dir': './custom_cache/',
        'max_cache_size': '4GB'
    },
    'preprocessing': {
        'default': {
            'normalize': False,
            'center': True,
            'clip_outliers': True,
            'outlier_std': 2.0
        }
    }
}

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('./temp_config.yaml', 'w') as f:
    yaml.dump(custom_config, f)

# –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
loader = EmbeddingLoader(
    cache_dir=custom_config['cache']['cache_dir'],
    max_cache_size=custom_config['cache']['max_cache_size']
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
embeddings = loader.load_embeddings(
    path="./data/embeddings/test.txt",
    format_type="glove",
    preprocess=False  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é
)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
processed = loader.preprocess_embeddings(
    embeddings,
    normalize=custom_config['preprocessing']['default']['normalize'],
    center=custom_config['preprocessing']['default']['center']
)

print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {processed.shape}")
```

---

## üß™ –ü–†–ò–ú–ï–†–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø

### –ü—Ä–∏–º–µ—Ä 7: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
import time
from data.embedding_loader import EmbeddingLoader
import torch

def benchmark_loading(file_path: str, format_type: str, num_runs: int = 3):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤.
    """
    loader = EmbeddingLoader()

    times = []

    for run in range(num_runs):
        # –û—á–∏—â–∞–µ–º –∫—ç—à –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
        loader.clear_cache()

        start_time = time.time()
        embeddings = loader.load_embeddings(file_path, format_type, preprocess=True)
        end_time = time.time()

        load_time = end_time - start_time
        times.append(load_time)

        print(f"–ó–∞–ø—É—Å–∫ {run + 1}: {load_time:.2f} —Å–µ–∫, "
              f"–†–∞–∑–º–µ—Ä: {embeddings.shape}, "
              f"–°–∫–æ—Ä–æ—Å—Ç—å: {embeddings.shape[0] / load_time:.0f} vectors/sec")

    avg_time = sum(times) / len(times)
    print(f"\n–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏: {avg_time:.2f} —Å–µ–∫")

    return avg_time

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
benchmark_loading("./data/embeddings/glove.6B.100d.txt", "glove")
```

### –ü—Ä–∏–º–µ—Ä 8: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

```python
from data.embedding_loader import EmbeddingLoader
import time

loader = EmbeddingLoader()

# –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (—Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç)
print("=== –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–±–µ–∑ –∫—ç—à–∞) ===")
start = time.time()
embeddings1 = loader.load_embeddings("./data/embeddings/test.txt", "glove")
first_load_time = time.time() - start
print(f"–í—Ä–µ–º—è –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {first_load_time:.2f} —Å–µ–∫")

# –í—Ç–æ—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–∏–∑ –∫—ç—à–∞)
print("\n=== –í—Ç–æ—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–∏–∑ –∫—ç—à–∞) ===")
start = time.time()
embeddings2 = loader.load_embeddings("./data/embeddings/test.txt", "glove")
second_load_time = time.time() - start
print(f"–í—Ä–µ–º—è –≤—Ç–æ—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {second_load_time:.2f} —Å–µ–∫")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏
are_identical = torch.equal(embeddings1, embeddings2)
print(f"–î–∞–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {are_identical}")

# –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –∫—ç—à–∞
speedup = first_load_time / second_load_time
print(f"–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –∫—ç—à–∞: {speedup:.1f}x")
```

---

## üîç –ü–†–ò–ú–ï–†–´ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò

### –ü—Ä–∏–º–µ—Ä 9: –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤

```python
from data.embedding_loader import EmbeddingLoader, EmbeddingPreprocessor
import torch
import numpy as np

def analyze_embedding_quality(embeddings: torch.Tensor):
    """
    –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤.
    """
    print("=== –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –≠–ú–ë–ï–î–ò–ù–ì–û–í ===")

    # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"–†–∞–∑–º–µ—Ä: {embeddings.shape}")
    print(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {embeddings.dtype}")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {embeddings.device}")
    print(f"–ü–∞–º—è—Ç—å: {embeddings.element_size() * embeddings.nelement() / 1024**2:.1f} MB")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–π
    print(f"\n–ú–∏–Ω –∑–Ω–∞—á–µ–Ω–∏–µ: {embeddings.min():.4f}")
    print(f"–ú–∞–∫—Å –∑–Ω–∞—á–µ–Ω–∏–µ: {embeddings.max():.4f}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ: {embeddings.mean():.4f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {embeddings.std():.4f}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã
    nan_count = torch.isnan(embeddings).sum().item()
    inf_count = torch.isinf(embeddings).sum().item()
    zero_vectors = (embeddings.norm(dim=1) == 0).sum().item()

    print(f"\n–ü—Ä–æ–±–ª–µ–º—ã:")
    print(f"NaN –∑–Ω–∞—á–µ–Ω–∏–π: {nan_count}")
    print(f"Inf –∑–Ω–∞—á–µ–Ω–∏–π: {inf_count}")
    print(f"–ù—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {zero_vectors}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–æ—Ä–º
    norms = torch.norm(embeddings, dim=1)
    print(f"\n–ù–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–æ–≤:")
    print(f"–°—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞: {norms.mean():.4f}")
    print(f"–ú–∏–Ω –Ω–æ—Ä–º–∞: {norms.min():.4f}")
    print(f"–ú–∞–∫—Å –Ω–æ—Ä–º–∞: {norms.max():.4f}")

    return {
        'shape': embeddings.shape,
        'has_issues': nan_count > 0 or inf_count > 0 or zero_vectors > 0,
        'mean_norm': float(norms.mean()),
        'std_norm': float(norms.std())
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
loader = EmbeddingLoader()
embeddings = loader.load_embeddings("./data/embeddings/test.txt", "glove")
quality_report = analyze_embedding_quality(embeddings)

if quality_report['has_issues']:
    print("\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏!")
else:
    print("\n‚úÖ –≠–º–±–µ–¥–∏–Ω–≥–∏ –≤—ã–≥–ª—è–¥—è—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏")
```

### –ü—Ä–∏–º–µ—Ä 10: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤

```python
from data.embedding_loader import EmbeddingLoader
import time

def compare_formats():
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤.
    """
    loader = EmbeddingLoader()

    files_to_test = [
        ("./data/embeddings/word2vec.txt", "word2vec"),
        ("./data/embeddings/word2vec.bin", "word2vec"),
        ("./data/embeddings/glove.txt", "glove"),
        ("./data/embeddings/bert.pt", "bert")
    ]

    results = []

    for file_path, format_type in files_to_test:
        try:
            print(f"\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {format_type}: {file_path} ===")

            start_time = time.time()
            embeddings = loader.load_embeddings(file_path, format_type)
            load_time = time.time() - start_time

            info = loader.get_embedding_info(embeddings)

            result = {
                'format': format_type,
                'file': file_path,
                'load_time': load_time,
                'shape': embeddings.shape,
                'memory_mb': info['memory_mb'],
                'speed_vectors_per_sec': embeddings.shape[0] / load_time
            }

            results.append(result)

            print(f"–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.2f} —Å–µ–∫")
            print(f"–†–∞–∑–º–µ—Ä: {embeddings.shape}")
            print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {result['speed_vectors_per_sec']:.0f} vectors/sec")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print("\n" + "="*80)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê")
    print("="*80)
    print(f"{'–§–æ—Ä–º–∞—Ç':<12} {'–í—Ä–µ–º—è (—Å–µ–∫)':<12} {'–†–∞–∑–º–µ—Ä':<20} {'–°–∫–æ—Ä–æ—Å—Ç—å (v/s)':<15}")
    print("-" * 80)

    for result in results:
        print(f"{result['format']:<12} {result['load_time']:<12.2f} "
              f"{str(result['shape']):<20} {result['speed_vectors_per_sec']:<15.0f}")

# –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
compare_formats()
```

---

## üìù –ó–ê–ú–ï–¢–ö–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ

### –í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:

1. **–ü–∞–º—è—Ç—å**: –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏
2. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
3. **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞**: –í—Å–µ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è–π—Ç–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
4. **–§–æ—Ä–º–∞—Ç—ã**: –ë–∏–Ω–∞—Ä–Ω—ã–µ Word2Vec —Ñ–∞–π–ª—ã —Ç—Ä–µ–±—É—é—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π gensim
5. **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ batch –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π workflow:

```python
# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –∫—ç—à–µ–º
loader = EmbeddingLoader(cache_dir="./cache/")

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
embeddings = loader.load_embeddings(path, format_type, preprocess=True)

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
info = loader.get_embedding_info(embeddings)
print(f"–ö–∞—á–µ—Å—Ç–≤–æ: {info}")

# 4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ—à–µ—Ç–∫–æ–π
lattice.set_input_face(embeddings[:batch_size])

# 5. –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
loader.clear_cache()
```

---

## üöÄ –ù–û–í–´–ï –ü–†–ò–ú–ï–†–´: LLM & Knowledge Distillation

### –ü—Ä–∏–º–µ—Ä 11: –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM

```python
from data.embedding_loader import EmbeddingLoader

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
loader = EmbeddingLoader()

# –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
texts = [
    "Artificial intelligence is transforming the world",
    "Machine learning models are becoming more sophisticated",
    "Neural networks can learn complex patterns"
]

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ LLM
embeddings = loader.load_from_llm(
    texts=texts,
    model_key="distilbert",  # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    pooling_strategy="mean"
)

print(f"Generated embeddings: {embeddings.shape}")
print(f"Model used: distilbert")
```

### –ü—Ä–∏–º–µ—Ä 12: Knowledge Distillation —Å LLaMA

```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è production
texts = [
    "The future of AI is bright",
    "Deep learning revolutionizes computing",
    "Natural language processing advances rapidly"
]

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å LLaMA 2 (—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
try:
    embeddings = loader.load_from_llm(
        texts=texts,
        model_key="llama2-7b",  # –ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å
        pooling_strategy="mean",
        use_cache=True  # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    )
    print(f"LLaMA embeddings: {embeddings.shape}")
except Exception as e:
    print(f"LLaMA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
    # Fallback –Ω–∞ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å
    embeddings = loader.load_from_llm(texts=texts, model_key="distilbert")
```

### –ü—Ä–∏–º–µ—Ä 13: –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞

```python
# –ë–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
training_texts = [
    "Quantum computing will change cryptography",
    "Blockchain technology ensures data integrity",
    "Cloud computing provides scalable infrastructure",
    "Internet of Things connects everyday devices",
    "Augmented reality enhances user experience",
    # ... –µ—â–µ —Ç—ã—Å—è—á–∏ —Ç–µ–∫—Å—Ç–æ–≤
]

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è Knowledge Distillation
dataset = loader.create_knowledge_distillation_dataset(
    texts=training_texts,
    teacher_model="llama3-8b",  # Teacher: –º–æ—â–Ω–∞—è LLM
    save_path="./datasets/kd_dataset_llama3.pt"
)

print(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {dataset['num_samples']} –æ–±—Ä–∞–∑—Ü–æ–≤")
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {dataset['embedding_dim']}")
print(f"Teacher –º–æ–¥–µ–ª—å: {dataset['teacher_model']}")

# –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D CNN –≤ Phase 3
```

### –ü—Ä–∏–º–µ—Ä 14: –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö

```python
# –ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–æ–≤
large_text_corpus = ["Text sample " + str(i) for i in range(10000)]

# –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
embeddings = loader.batch_load_from_llm(
    texts=large_text_corpus,
    model_key="roberta",
    batch_size=32  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 32 —Ç–µ–∫—Å—Ç–∞
)

print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(large_text_corpus)} —Ç–µ–∫—Å—Ç–æ–≤")
print(f"–†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–∏: {embeddings.shape}")
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {loader.stats}")
```

### –ü—Ä–∏–º–µ—Ä 15: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π

```python
# –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
test_text = ["The quick brown fox jumps over the lazy dog"]

# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
models_to_test = ["distilbert", "roberta", "gpt2"]

results = {}
for model in models_to_test:
    try:
        embeddings = loader.load_from_llm(
            texts=test_text,
            model_key=model,
            pooling_strategy="mean"
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        model_info = loader.get_llm_info(model)

        results[model] = {
            'embeddings': embeddings,
            'shape': embeddings.shape,
            'hidden_size': model_info['hidden_size'],
            'memory_req': "Unknown"  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        }

        print(f"‚úÖ {model}: {embeddings.shape}")

    except Exception as e:
        print(f"‚ùå {model}: {e}")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
for model, data in results.items():
    print(f"{model}: {data['shape']}, hidden_size: {data['hidden_size']}")
```

### –ü—Ä–∏–º–µ—Ä 16: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å 3D CNN (–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ Phase 3)

```python
from core.lattice_3d import Lattice3D

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D CNN
texts = ["AI will transform education", "Machine learning improves healthcare"]

# Teacher: LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
teacher_embeddings = loader.load_from_llm(
    texts=texts,
    model_key="mistral-7b",
    pooling_strategy="mean"
)

# Student: –Ω–∞—à–∞ 3D CNN (–±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ Phase 3)
lattice = Lattice3D(size=(10, 10, 10))

# –ü–æ–¥–∞—á–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –Ω–∞ –≤—Ö–æ–¥–Ω—É—é –≥—Ä–∞–Ω—å —Ä–µ—à–µ—Ç–∫–∏
input_face = teacher_embeddings[:, :lattice.get_face_size()]
lattice.set_input_face(input_face)

print("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å 3D CNN –≥–æ—Ç–æ–≤–∞ –¥–ª—è Phase 3!")
print(f"Teacher embeddings: {teacher_embeddings.shape}")
print(f"Lattice input: {input_face.shape}")
```

### –ü—Ä–∏–º–µ—Ä 17: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ pooling

```python
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
text = ["Natural language processing is a branch of artificial intelligence"]

pooling_strategies = ["mean", "cls", "max"]
results = {}

for strategy in pooling_strategies:
    embeddings = loader.load_from_llm(
        texts=text,
        model_key="distilbert",
        pooling_strategy=strategy
    )
    results[strategy] = embeddings
    print(f"{strategy} pooling: {embeddings.shape}")

# –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
mean_emb = results["mean"]
cls_emb = results["cls"]
max_emb = results["max"]

print(f"Mean vs CLS similarity: {torch.cosine_similarity(mean_emb, cls_emb).item():.4f}")
print(f"Mean vs Max similarity: {torch.cosine_similarity(mean_emb, max_emb).item():.4f}")
```

### –ü—Ä–∏–º–µ—Ä 18: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

```python
import time

# –¢–µ—Å—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
texts = ["This is a test sentence for caching performance"]

# –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å (–±–µ–∑ –∫—ç—à–∞)
start_time = time.time()
embeddings1 = loader.load_from_llm(texts=texts, model_key="gpt2", use_cache=True)
first_call_time = time.time() - start_time

# –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å (–∏–∑ –∫—ç—à–∞)
start_time = time.time()
embeddings2 = loader.load_from_llm(texts=texts, model_key="gpt2", use_cache=True)
cached_call_time = time.time() - start_time

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
identical = torch.allclose(embeddings1, embeddings2)

print(f"–ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤: {first_call_time:.2f}s")
print(f"–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–∑–æ–≤: {cached_call_time:.2f}s")
print(f"–£—Å–∫–æ—Ä–µ–Ω–∏–µ: {first_call_time/cached_call_time:.1f}x")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {identical}")
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞: hits={loader.stats['cache_hits']}, misses={loader.stats['cache_misses']}")
```

### –ü—Ä–∏–º–µ—Ä 19: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

```python
# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª—è—Ö
supported_models = loader.list_supported_llm_models()
print("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ LLM –º–æ–¥–µ–ª–∏:")
for model in supported_models:
    print(f"  - {model}")

# –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
model_info = loader.get_llm_info("distilbert")
print(f"\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
for key, value in model_info.items():
    print(f"  {key}: {value}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ embedding_loader:")
for key, value in loader.stats.items():
    print(f"  {key}: {value}")
```

### –ü—Ä–∏–º–µ—Ä 20: Production Pipeline (–≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ Phase 3)

```python
# –ü–æ–ª–Ω—ã–π production pipeline –¥–ª—è Knowledge Distillation
class ProductionKnowledgeDistillation:
    def __init__(self):
        self.loader = EmbeddingLoader()
        self.teacher_model = "llama3-8b"  # –ú–æ—â–Ω–∞—è teacher –º–æ–¥–µ–ª—å

    def prepare_training_data(self, text_corpus, save_dir="./kd_datasets/"):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D CNN"""

        # –°–æ–∑–¥–∞–Ω–∏–µ train/validation –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        train_texts = text_corpus[:int(0.8 * len(text_corpus))]
        val_texts = text_corpus[int(0.8 * len(text_corpus)):]

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è train –¥–∞—Ç–∞—Å–µ—Ç–∞
        train_dataset = self.loader.create_knowledge_distillation_dataset(
            texts=train_texts,
            teacher_model=self.teacher_model,
            save_path=f"{save_dir}/train_kd_dataset.pt"
        )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è validation –¥–∞—Ç–∞—Å–µ—Ç–∞
        val_dataset = self.loader.create_knowledge_distillation_dataset(
            texts=val_texts,
            teacher_model=self.teacher_model,
            save_path=f"{save_dir}/val_kd_dataset.pt"
        )

        return train_dataset, val_dataset

    def get_teacher_embeddings(self, texts):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ teacher —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è inference"""
        return self.loader.load_from_llm(
            texts=texts,
            model_key=self.teacher_model,
            pooling_strategy="mean",
            use_cache=True
        )

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
pipeline = ProductionKnowledgeDistillation()

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Phase 3
sample_corpus = [f"Training sample {i}" for i in range(1000)]
train_data, val_data = pipeline.prepare_training_data(sample_corpus)

print("üöÄ Production pipeline –≥–æ—Ç–æ–≤!")
print(f"Train dataset: {train_data['num_samples']} –æ–±—Ä–∞–∑—Ü–æ–≤")
print(f"Validation dataset: {val_data['num_samples']} –æ–±—Ä–∞–∑—Ü–æ–≤")
print("‚úÖ –ì–æ—Ç–æ–≤–æ –¥–ª—è Phase 3: Training Infrastructure")
```

---

## üìã –†–µ–∑—é–º–µ –Ω–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

### üéØ –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∏—á–∏

1. **LLM Integration**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 8+ –º–æ–¥–µ–ª–µ–π (LLaMA, Mistral, GPT –∏ –¥—Ä.)
2. **Knowledge Distillation**: –ü–æ–ª–Ω—ã–π pipeline —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
3. **Real-time Generation**: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
4. **Smart Caching**: –ò–Ω—Ç–µ–ª–ª–∏–≥–µ–Ω—Ç–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
5. **Batch Processing**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
6. **Multi-device Support**: CPU –∏ GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞
7. **Production Ready**: –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ Phase 3 Training Infrastructure

### üöÄ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ Phase 3

- ‚úÖ **Teacher-Student Architecture**: –ì–æ—Ç–æ–≤–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è KD
- ‚úÖ **Dataset Generation**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ **Integration Points**: API –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ training loop
- ‚úÖ **Quality Metrics**: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
- ‚úÖ **Scalability**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ production –Ω–∞–≥—Ä—É–∑–æ–∫

**üéØ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥**: –ü–µ—Ä–µ—Ö–æ–¥ –∫ Phase 3 –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ training infrastructure —Å –ø–æ–ª–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π knowledge distillation!
