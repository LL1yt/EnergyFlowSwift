"""
ProcessingMetrics - –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
=======================================================

–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Phase 2.5:
- Cosine similarity (–≥–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - —Ü–µ–ª—å >90%)
- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ 
- –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
- –ö–∞—á–µ—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
"""

import time
import numpy as np
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import torch
import logging

logger = logging.getLogger(__name__)


@dataclass 
class ProcessingMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤"""
    
    # === –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò ===
    similarities: List[float] = field(default_factory=list)      # Cosine similarities
    processing_times: List[float] = field(default_factory=list)  # –í—Ä–µ–º–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫)
    batch_sizes: List[int] = field(default_factory=list)         # –†–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π
    
    # === –°–¢–ê–¢–ò–°–¢–ò–ö–ò ===
    total_processed: int = 0                                     # –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    start_time: float = field(default_factory=time.time)        # –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–π
    
    # === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
    target_similarity: float = 0.90                             # –¶–µ–ª–µ–≤–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å Phase 2.5
    max_history_size: int = 1000                                 # –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å–µ–π –≤ –∏—Å—Ç–æ—Ä–∏–∏
    
    def update(self, similarity: float, processing_time: float, batch_size: int = 1):
        """
        –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        
        Args:
            similarity: Cosine similarity –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º –∏ –≤—ã—Ö–æ–¥–æ–º
            processing_time: –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            batch_size: –†–∞–∑–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∞
        """
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        self.similarities.append(similarity)
        self.processing_times.append(processing_time)
        self.batch_sizes.append(batch_size)
        self.total_processed += batch_size
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        if len(self.similarities) > self.max_history_size:
            self.similarities = self.similarities[-self.max_history_size:]
            self.processing_times = self.processing_times[-self.max_history_size:]
            self.batch_sizes = self.batch_sizes[-self.max_history_size:]
    
    def get_similarity_stats(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ cosine similarity"""
        if not self.similarities:
            return {"mean": 0.0, "min": 0.0, "max": 0.0, "std": 0.0}
        
        similarities = np.array(self.similarities)
        
        return {
            "mean": float(np.mean(similarities)),
            "min": float(np.min(similarities)),
            "max": float(np.max(similarities)),
            "std": float(np.std(similarities)),
            "median": float(np.median(similarities)),
            "target_achievement": float(np.mean(similarities >= self.target_similarity))
        }
    
    def get_performance_stats(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.processing_times:
            return {"mean_time": 0.0, "throughput": 0.0}
        
        times = np.array(self.processing_times)
        batches = np.array(self.batch_sizes)
        
        total_time = time.time() - self.start_time
        
        return {
            "mean_processing_time": float(np.mean(times)),
            "min_processing_time": float(np.min(times)),
            "max_processing_time": float(np.max(times)),
            "total_time": float(total_time),
            "throughput_embeddings_per_sec": float(self.total_processed / max(total_time, 0.001)),
            "mean_batch_size": float(np.mean(batches))
        }
    
    def get_quality_assessment(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        similarity_stats = self.get_similarity_stats()
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = similarity_stats["mean"]
        target_achievement = similarity_stats["target_achievement"]
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        if quality_score >= 0.95:
            quality_level = "excellent"
        elif quality_score >= 0.90:
            quality_level = "good"
        elif quality_score >= 0.80:
            quality_level = "acceptable"
        else:
            quality_level = "poor"
        
        return {
            "quality_score": quality_score,
            "quality_level": quality_level,
            "target_achievement_rate": target_achievement,
            "phase_2_5_ready": quality_score >= self.target_similarity,
            "consistency": 1.0 - similarity_stats["std"]  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π std = –±–æ–ª—å—à–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        }
    
    def get_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å–≤–æ–¥–∫—É –º–µ—Ç—Ä–∏–∫"""
        return {
            "similarity": self.get_similarity_stats(),
            "performance": self.get_performance_stats(),
            "quality": self.get_quality_assessment(),
            "total_processed": self.total_processed,
            "measurements_count": len(self.similarities)
        }
    
    def reset(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏"""
        self.similarities.clear()
        self.processing_times.clear()
        self.batch_sizes.clear()
        self.total_processed = 0
        self.start_time = time.time()
        
        logger.info("[DATA] –ú–µ—Ç—Ä–∏–∫–∏ —Å–±—Ä–æ—à–µ–Ω—ã")
    
    def log_current_stats(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        similarity_stats = self.get_similarity_stats()
        quality = self.get_quality_assessment()
        performance = self.get_performance_stats()
        
        logger.info("=== –¢–ï–ö–£–©–ò–ï –ú–ï–¢–†–ò–ö–ò EMBEDDINGPROCESSOR ===")
        logger.info(f"[DATA] –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity_stats['mean']:.3f} (—Ü–µ–ª—å: {self.target_similarity:.3f})")
        logger.info(f"[TARGET] –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–∏: {quality['target_achievement_rate']:.1%}")
        logger.info(f"[STAR] –£—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞: {quality['quality_level']}")
        logger.info(f"[FAST] –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {performance['throughput_embeddings_per_sec']:.1f} —ç–º–±/—Å–µ–∫")
        logger.info(f"üî¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—Å–µ–≥–æ: {self.total_processed} —ç–º–±–µ–¥–∏–Ω–≥–æ–≤")
        logger.info(f"[OK] Phase 2.5 –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {'–î–ê' if quality['phase_2_5_ready'] else '–ù–ï–¢'}")


def calculate_processing_quality(input_embeddings: torch.Tensor, 
                                output_embeddings: torch.Tensor) -> Dict[str, float]:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –±–∞—Ç—á–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    
    Args:
        input_embeddings: –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ [batch_size, embedding_dim]
        output_embeddings: –í—ã—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ [batch_size, embedding_dim]
        
    Returns:
        Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    batch_size = input_embeddings.shape[0]
    
    # Cosine similarities –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
    similarities = []
    for i in range(batch_size):
        similarity = torch.nn.functional.cosine_similarity(
            input_embeddings[i], output_embeddings[i], dim=0
        ).item()
        similarities.append(similarity)
    
    similarities = np.array(similarities)
    
    # L2 distances
    l2_distances = []
    for i in range(batch_size):
        l2_dist = torch.norm(input_embeddings[i] - output_embeddings[i], p=2).item()
        l2_distances.append(l2_dist)
    
    l2_distances = np.array(l2_distances)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    return {
        "mean_cosine_similarity": float(np.mean(similarities)),
        "min_cosine_similarity": float(np.min(similarities)),
        "max_cosine_similarity": float(np.max(similarities)),
        "std_cosine_similarity": float(np.std(similarities)),
        "mean_l2_distance": float(np.mean(l2_distances)),
        "median_cosine_similarity": float(np.median(similarities)),
        "batch_size": batch_size
    }


def evaluate_semantic_preservation(input_embeddings: torch.Tensor,
                                  output_embeddings: torch.Tensor,
                                  threshold: float = 0.90) -> Dict[str, Any]:
    """
    –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    
    Args:
        input_embeddings: –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
        output_embeddings: –í—ã—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏  
        threshold: –ü–æ—Ä–æ–≥ –ø—Ä–∏–µ–º–ª–µ–º–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
    """
    quality_metrics = calculate_processing_quality(input_embeddings, output_embeddings)
    
    mean_similarity = quality_metrics["mean_cosine_similarity"]
    preservation_rate = float(np.mean([
        torch.nn.functional.cosine_similarity(
            input_embeddings[i], output_embeddings[i], dim=0
        ).item() >= threshold
        for i in range(input_embeddings.shape[0])
    ]))
    
    return {
        "semantic_preservation_score": mean_similarity,
        "preservation_rate": preservation_rate,
        "threshold": threshold,
        "quality_passed": mean_similarity >= threshold,
        "batch_preservation_rate": preservation_rate,
        "detailed_metrics": quality_metrics
    }


class QualityMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"""
    
    def __init__(self, target_similarity: float = 0.90):
        self.target_similarity = target_similarity
        self.metrics = ProcessingMetrics(target_similarity=target_similarity)
        self.quality_history = []
        
    def monitor_batch(self, input_batch: torch.Tensor, output_batch: torch.Tensor, 
                     processing_time: float):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –±–∞—Ç—á–∞"""
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality = calculate_processing_quality(input_batch, output_batch)
        self.quality_history.append(quality)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        self.metrics.update(
            similarity=quality["mean_cosine_similarity"],
            processing_time=processing_time,
            batch_size=input_batch.shape[0]
        )
    
    def get_trend_analysis(self, window_size: int = 10) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞"""
        if len(self.quality_history) < window_size:
            return {"trend": "insufficient_data"}
        
        recent_scores = [q["mean_cosine_similarity"] for q in self.quality_history[-window_size:]]
        earlier_scores = [q["mean_cosine_similarity"] for q in self.quality_history[-2*window_size:-window_size]]
        
        if not earlier_scores:
            return {"trend": "insufficient_data"}
        
        recent_mean = np.mean(recent_scores)
        earlier_mean = np.mean(earlier_scores)
        
        trend_direction = "improving" if recent_mean > earlier_mean else "declining"
        trend_magnitude = abs(recent_mean - earlier_mean)
        
        return {
            "trend": trend_direction,
            "trend_magnitude": trend_magnitude,
            "recent_mean": recent_mean,
            "earlier_mean": earlier_mean,
            "stability": 1.0 - np.std(recent_scores)
        } 