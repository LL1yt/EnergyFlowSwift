#!/usr/bin/env python3
"""
[MAGNIFY] –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–ò–ô –°–ö–†–ò–ü–¢: –ü—Ä–æ–±–ª–µ–º–∞ —Å –Ω—É–ª–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏

–¶–µ–ª—å: –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–æ–±–ª–µ–º—ã —Å Loss: 0.0000 –∏ Similarity: 0.0000

–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. Forward pass –∏ tensor flow
2. Loss function computation
3. Gradient flow —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É
4. Target/input data integrity
5. Dimension matching
"""

import sys
import os
sys.path.insert(0, os.path.abspath('.'))

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, Any, List, Tuple

# Setup logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
    EmergentCubeTrainer, EmergentTrainingConfig
)
from training.embedding_trainer.dialogue_dataset import create_dialogue_dataset
from data.embedding_loader.format_handlers import create_llm_handler

class ZeroLossDiagnostics:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω—É–ª–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = {}
        
        print(f"[MAGNIFY] [–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ device: {self.device}")
        
    def run_full_diagnostics(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        
        print("\n" + "="*50)
        print("[TARGET] –ù–ê–ß–ê–õ–û –°–ò–°–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–û–ô –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò")
        print("="*50)
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        trainer = self._test_model_creation()
        if trainer is None:
            return
            
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        dataset = self._test_data_creation()
        if not dataset:
            return
            
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ forward pass
        outputs = self._test_forward_pass(trainer, dataset)
        if outputs is None:
            return
            
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ loss computation
        loss_results = self._test_loss_computation(trainer, outputs, dataset)
        if loss_results is None:
            return
            
        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ gradient flow
        self._test_gradient_flow(trainer, outputs, dataset)
        
        # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        self._analyze_results()
        
    def _test_model_creation(self) -> 'EmergentCubeTrainer':
        """–¢–µ—Å—Ç 1: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("\n[CONFIG] [–¢–ï–°–¢ 1] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        
        try:
            config = EmergentTrainingConfig()
            config.teacher_model = "distilbert-base-uncased"
            config.cube_dimensions = (15, 15, 11)
            config.mixed_precision = False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            
            trainer = EmergentCubeTrainer(config, device=str(self.device))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            total_params = sum(p.numel() for p in trainer.parameters())
            trainable_params = sum(p.numel() for p in trainer.parameters() if p.requires_grad)
            
            print(f"[OK] [–¢–ï–°–¢ 1] –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            print(f"   - –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
            print(f"   - –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
            print(f"   - Device: {trainer.device}")
            
            self.results['model_creation'] = {
                'status': 'success',
                'total_params': total_params,
                'trainable_params': trainable_params
            }
            
            return trainer
            
        except Exception as e:
            print(f"[ERROR] [–¢–ï–°–¢ 1] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            self.results['model_creation'] = {'status': 'failed', 'error': str(e)}
            return None

    def _test_data_creation(self) -> List:
        """–¢–µ—Å—Ç 2: –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        print("\n[DATA] [–¢–ï–°–¢ 2] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π dataset
            dialogue_pairs = [
                {"question": "What is AI?", "answer": "AI is artificial intelligence."},
                {"question": "How do neural networks work?", "answer": "Neural networks process information."}
            ]
            
            dataset = create_dialogue_dataset(
                dialogue_pairs,
                teacher_model="distilbert-base-uncased",
                cache_embeddings=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                validation_split=0.0,
                normalize_embeddings=True
            )
            
            print(f"[OK] [–¢–ï–°–¢ 2] Dataset —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
            if dataset:
                sample = dataset[0]
                print(f"   - –ö–ª—é—á–∏ –≤ –ø—Ä–∏–º–µ—Ä–µ: {list(sample.keys())}")
                for key, value in sample.items():
                    if torch.is_tensor(value):
                        print(f"   - {key}: shape={value.shape}, dtype={value.dtype}")
                    else:
                        print(f"   - {key}: type={type(value)}")
            
            self.results['data_creation'] = {
                'status': 'success',
                'dataset_size': len(dataset),
                'sample_keys': list(dataset[0].keys()) if dataset else []
            }
            
            return dataset
            
        except Exception as e:
            print(f"[ERROR] [–¢–ï–°–¢ 2] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.results['data_creation'] = {'status': 'failed', 'error': str(e)}
            return []

    def _test_forward_pass(self, trainer: 'EmergentCubeTrainer', dataset: List) -> Dict:
        """–¢–µ—Å—Ç 3: Forward pass"""
        print("\n[FAST] [–¢–ï–°–¢ 3] –ü—Ä–æ–≤–µ—Ä–∫–∞ forward pass...")
        
        try:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä
            sample = dataset[0]
            question_embedding = sample['question_embedding'].unsqueeze(0).to(self.device)
            answer_embedding = sample['answer_embedding'].unsqueeze(0).to(self.device)
            
            print(f"   - Input shapes:")
            print(f"     Question: {question_embedding.shape}")
            print(f"     Answer: {question_embedding.shape}")
            
            # Forward pass
            with torch.no_grad():
                outputs = trainer.forward(question_embedding)
            
            print(f"[OK] [–¢–ï–°–¢ 3] Forward pass –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            print(f"   - Output keys: {list(outputs.keys())}")
            
            for key, tensor in outputs.items():
                if torch.is_tensor(tensor):
                    print(f"   - {key}: shape={tensor.shape}, requires_grad={tensor.requires_grad}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω—É–ª–∏
                    zero_count = (tensor == 0).sum().item()
                    total_elements = tensor.numel()
                    zero_percentage = (zero_count / total_elements) * 100
                    print(f"     –ù—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {zero_count}/{total_elements} ({zero_percentage:.1f}%)")
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    if tensor.numel() > 0:
                        print(f"     Min: {tensor.min().item():.6f}, Max: {tensor.max().item():.6f}")
                        print(f"     Mean: {tensor.mean().item():.6f}, Std: {tensor.std().item():.6f}")
                        
            self.results['forward_pass'] = {
                'status': 'success',
                'output_keys': list(outputs.keys()),
                'output_shapes': {k: list(v.shape) if torch.is_tensor(v) else None 
                                 for k, v in outputs.items()}
            }
            
            return outputs
            
        except Exception as e:
            print(f"[ERROR] [–¢–ï–°–¢ 3] –û—à–∏–±–∫–∞ forward pass: {e}")
            self.results['forward_pass'] = {'status': 'failed', 'error': str(e)}
            return None

    def _test_loss_computation(self, trainer: 'EmergentCubeTrainer', outputs: Dict, dataset: List) -> Dict:
        """–¢–µ—Å—Ç 4: Loss computation"""
        print("\nüí∞ [–¢–ï–°–¢ 4] –ü—Ä–æ–≤–µ—Ä–∫–∞ loss computation...")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º targets
            sample = dataset[0]
            answer_embedding = sample['answer_embedding'].unsqueeze(0).to(self.device)
            
            targets = {
                'target_embedding': answer_embedding,
                'target_surface': outputs['input_surface']
            }
            
            print(f"   - Target shapes:")
            for key, tensor in targets.items():
                print(f"     {key}: {tensor.shape}")
            
            # Compute loss
            losses = trainer.compute_loss(outputs, targets)
            
            print(f"[OK] [–¢–ï–°–¢ 4] Loss computation –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            print(f"   - Loss components: {list(losses.keys())}")
            
            for key, loss_tensor in losses.items():
                if torch.is_tensor(loss_tensor):
                    loss_value = loss_tensor.item()
                    print(f"   - {key}: {loss_value:.6f}")
                    print(f"     requires_grad: {loss_tensor.requires_grad}")
                    print(f"     grad_fn: {loss_tensor.grad_fn}")
                    
                    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ—á–µ–º—É loss = 0?
                    if loss_value == 0.0:
                        print(f"     [ALERT] –ö–†–ò–¢–ò–ß–ù–û: {key} = 0.0!")
                        self._debug_zero_loss_component(key, outputs, targets, trainer)
                        
            self.results['loss_computation'] = {
                'status': 'success',
                'loss_values': {k: v.item() if torch.is_tensor(v) else v 
                               for k, v in losses.items()}
            }
            
            return losses
            
        except Exception as e:
            print(f"[ERROR] [–¢–ï–°–¢ 4] –û—à–∏–±–∫–∞ loss computation: {e}")
            self.results['loss_computation'] = {'status': 'failed', 'error': str(e)}
            return None

    def _debug_zero_loss_component(self, component: str, outputs: Dict, targets: Dict, trainer):
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω—É–ª–µ–≤–æ–≥–æ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞"""
        print(f"\n[MAGNIFY] [–î–ï–¢–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê] {component}")
        
        if component == 'surface_reconstruction_loss':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º surface reconstruction
            if 'output_surface' in outputs and 'input_surface' in outputs:
                output_surface = outputs['output_surface']
                input_surface = outputs['input_surface']
                
                print(f"   - output_surface: shape={output_surface.shape}")
                print(f"   - input_surface: shape={input_surface.shape}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º projection
                if hasattr(trainer.loss_function, 'embedding_to_surface'):
                    projected_input = trainer.loss_function.embedding_to_surface(input_surface)
                    print(f"   - projected_input: shape={projected_input.shape}")
                    
                    # –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç MSE
                    mse_manual = torch.mean((output_surface - projected_input) ** 2)
                    print(f"   - –†—É—á–Ω–æ–π MSE: {mse_manual.item():.6f}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å
                    if torch.allclose(output_surface, projected_input):
                        print("   - [ALERT] output_surface –∏–¥–µ–Ω—Ç–∏—á–µ–Ω projected_input!")
                    else:
                        print(f"   - –†–∞–∑–ª–∏—á–∏–µ –Ω–∞–π–¥–µ–Ω–æ: max_diff={torch.max(torch.abs(output_surface - projected_input)).item():.6f}")
                        
        elif component == 'dialogue_similarity_loss':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º dialogue similarity
            if 'final_output' in outputs and 'target_embedding' in targets:
                final_output = outputs['final_output']
                target_embedding = targets['target_embedding']
                
                print(f"   - final_output: shape={final_output.shape}")
                print(f"   - target_embedding: shape={target_embedding.shape}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º projection
                if hasattr(trainer.loss_function, 'embedding_to_surface'):
                    projected_target = trainer.loss_function.embedding_to_surface(target_embedding)
                    print(f"   - projected_target: shape={projected_target.shape}")
                    
                    # –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç cosine similarity
                    cos_sim = torch.nn.functional.cosine_similarity(final_output, projected_target, dim=-1)
                    dialogue_loss_manual = 1.0 - torch.mean(cos_sim)
                    print(f"   - –†—É—á–Ω–æ–π cosine similarity: {torch.mean(cos_sim).item():.6f}")
                    print(f"   - –†—É—á–Ω–æ–π dialogue loss: {dialogue_loss_manual.item():.6f}")

    def _test_gradient_flow(self, trainer: 'EmergentCubeTrainer', outputs: Dict, dataset: List):
        """–¢–µ—Å—Ç 5: Gradient flow"""
        print("\nüåä [–¢–ï–°–¢ 5] –ü—Ä–æ–≤–µ—Ä–∫–∞ gradient flow...")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å requires_grad
            sample = dataset[0]
            question_embedding = sample['question_embedding'].unsqueeze(0).to(self.device).requires_grad_(True)
            answer_embedding = sample['answer_embedding'].unsqueeze(0).to(self.device)
            
            # Forward pass —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
            trainer.train()
            outputs = trainer.forward(question_embedding)
            
            targets = {
                'target_embedding': answer_embedding,
                'target_surface': outputs['input_surface']
            }
            
            losses = trainer.compute_loss(outputs, targets)
            total_loss = losses['total_loss']
            
            print(f"   - Total loss –¥–ª—è backward: {total_loss.item():.6f}")
            print(f"   - requires_grad: {total_loss.requires_grad}")
            print(f"   - grad_fn: {total_loss.grad_fn}")
            
            # Backward pass
            total_loss.backward()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            grad_stats = {}
            for name, param in trainer.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    grad_stats[name] = grad_norm
                    if grad_norm == 0:
                        print(f"   - [ALERT] Zero gradient: {name}")
                    elif grad_norm > 0:
                        print(f"   - [OK] Non-zero gradient: {name} = {grad_norm:.6f}")
                else:
                    print(f"   - [ERROR] No gradient: {name}")
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            non_zero_grads = sum(1 for g in grad_stats.values() if g > 0)
            zero_grads = sum(1 for g in grad_stats.values() if g == 0)
            no_grads = len([p for p in trainer.parameters() if p.grad is None])
            
            print(f"[OK] [–¢–ï–°–¢ 5] Gradient analysis –∑–∞–≤–µ—Ä—à–µ–Ω")
            print(f"   - Non-zero gradients: {non_zero_grads}")
            print(f"   - Zero gradients: {zero_grads}")
            print(f"   - No gradients: {no_grads}")
            
            self.results['gradient_flow'] = {
                'status': 'success',
                'non_zero_grads': non_zero_grads,
                'zero_grads': zero_grads,
                'no_grads': no_grads,
                'total_params': len(list(trainer.parameters()))
            }
            
        except Exception as e:
            print(f"[ERROR] [–¢–ï–°–¢ 5] –û—à–∏–±–∫–∞ gradient flow: {e}")
            self.results['gradient_flow'] = {'status': 'failed', 'error': str(e)}

    def _analyze_results(self):
        """–§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        print("\n" + "="*50)
        print("[DATA] –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò")
        print("="*50)
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–∞
        for test_name, result in self.results.items():
            status = result.get('status', 'unknown')
            print(f"\n[MAGNIFY] {test_name.upper()}:")
            print(f"   Status: {'[OK] –£–°–ü–ï–•' if status == 'success' else '[ERROR] –û–®–ò–ë–ö–ê'}")
            
            if status == 'failed':
                print(f"   Error: {result.get('error', 'Unknown')}")
            else:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                for key, value in result.items():
                    if key != 'status':
                        print(f"   {key}: {value}")
        
        # –í—ã—è–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        print("\n[TARGET] –í–ï–†–û–Ø–¢–ù–´–ï –ü–†–ò–ß–ò–ù–´ –ü–†–û–ë–õ–ï–ú–´:")
        self._identify_root_causes()
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
        print("\n[IDEA] –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:")
        self._provide_recommendations()

    def _identify_root_causes(self):
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω –ø—Ä–æ–±–ª–µ–º—ã"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º loss computation
        if 'loss_computation' in self.results and self.results['loss_computation']['status'] == 'success':
            loss_values = self.results['loss_computation']['loss_values']
            
            if loss_values.get('total_loss', 0) == 0.0:
                print("   [ALERT] –ö–†–ò–¢–ò–ß–ù–û: total_loss = 0.0")
                
                if loss_values.get('surface_reconstruction_loss', 0) == 0.0:
                    print("     - Surface reconstruction loss = 0 (–≤–æ–∑–º–æ–∂–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ input/output)")
                    
                if loss_values.get('dialogue_similarity_loss', 0) == 0.0:
                    print("     - Dialogue similarity loss = 0 (–≤–æ–∑–º–æ–∂–Ω–æ perfect similarity –∏–ª–∏ dimension error)")
                    
                if loss_values.get('internal_consistency_loss', 0) == 0.0:
                    print("     - Internal consistency loss = 0 (–≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ—Ç internal states)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        if 'gradient_flow' in self.results and self.results['gradient_flow']['status'] == 'success':
            grad_info = self.results['gradient_flow']
            if grad_info['non_zero_grads'] == 0:
                print("   [ALERT] –ö–†–ò–¢–ò–ß–ù–û: –ù–µ—Ç –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
                print("     - Backward pass –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–ª–∏ loss –Ω–µ —Å–≤—è–∑–∞–Ω —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")

    def _provide_recommendations(self):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é"""
        
        print("   1. [CONFIG] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å loss function implementation:")
        print("      - –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ loss components –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã")
        print("      - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å dimension matching –≤ cosine similarity")
        print("      - –î–æ–±–∞–≤–∏—Ç—å epsilon –¥–ª—è numerical stability")
        
        print("   2. [MAGNIFY] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å forward pass:")
        print("      - –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ model –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ outputs")
        print("      - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ parameters –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è")
        
        print("   3. üåä –ü—Ä–æ–≤–µ—Ä–∏—Ç—å gradient flow:")
        print("      - –î–æ–±–∞–≤–∏—Ç—å gradient debugging –≤ train_step")
        print("      - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å retain_graph usage")
        print("      - –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ optimizer.step() –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è")

def main():
    """–ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    diagnostics = ZeroLossDiagnostics()
    diagnostics.run_full_diagnostics()
    
    print("\n[TARGET] –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—à–µ.")
    print("–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ.")

if __name__ == "__main__":
    main()