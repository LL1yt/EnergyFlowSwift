#!/usr/bin/env python3
"""
üß™ Test Model Inference - Task 1.1

–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π 3D Cellular Neural Network –Ω–∞ –Ω–æ–≤—ã—Ö —Ñ—Ä–∞–∑–∞—Ö.
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ —Å baseline DistilBERT.

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
- –°—Ä–∞–≤–Ω–∏—Ç—å –≤—ã—Ö–æ–¥—ã —Å baseline (DistilBERT –Ω–∞–ø—Ä—è–º—É—é)
- –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ semantic transformations

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–î–∞—Ç–∞: 2025-01-09
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
sys.path.insert(0, os.path.abspath('.'))

import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import time
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import json

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã
from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
    EmergentCubeTrainer, 
    EmergentTrainingConfig,
    create_emergent_trainer
)
from data.embedding_loader import EmbeddingLoader
from inference.lightweight_decoder.generative_decoder import (
    GenerativeDecoder,
    create_generative_decoder
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/inference_testing.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class InferenceTestConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è inference"""
    model_checkpoint: str = "checkpoints/versioned/milestone_overnight_fixed_final_1290/trainer_milestone_overnight_fixed_final_1290.pt"
    baseline_model: str = "distilbert-base-uncased"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Test settings
    similarity_threshold: float = 0.7  # –ü–æ—Ä–æ–≥ –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ "—Ö–æ—Ä–æ—à–∏–º"
    top_k_similar: int = 3            # –°–∫–æ–ª—å–∫–æ —Ç–æ–ø-–ø–æ—Ö–æ–∂–∏—Ö –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å
    
    # Output settings
    save_results: bool = True
    results_file: str = "results/inference_test_results.json"
    detailed_analysis: bool = True


class ModelInferenceTester:
    """–¢–µ—Å—Ç–µ—Ä –¥–ª—è inference –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, config: InferenceTestConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ baseline
        self.model = None
        self.baseline_model = None
        self.baseline_tokenizer = None
        self.embedding_loader = None
        self.decoder = None
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
        self.test_results = []
        
        logger.info(f"[INIT] Initializing InferenceTester on {self.device}")
        
    def load_trained_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π 3D –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"[LOAD] Loading trained model from {self.config.model_checkpoint}")
            
            # –°–æ–∑–¥–∞–µ–º trainer —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            config = EmergentTrainingConfig(
                teacher_model="distilbert-base-uncased",
                cube_dimensions=(15, 15, 11),
                enable_full_cube_gradient=True,
                spatial_propagation_depth=11,
                emergent_specialization=True
            )
            
            self.model = EmergentCubeTrainer(config, device=str(self.device))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            checkpoint = torch.load(self.config.model_checkpoint, map_location=self.device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É checkpoint'–∞
            if 'model_state_dict' in checkpoint:
                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏
                model_state_dict = checkpoint['model_state_dict']
                logger.info(f"[LOAD] Loading from new checkpoint format")
            else:
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - –≤–µ—Å—å checkpoint —ç—Ç–æ state_dict
                model_state_dict = checkpoint
                logger.info(f"[LOAD] Loading from legacy checkpoint format")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —Å strict=False, —á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ –∫–ª—é—á–∏
            missing_keys, unexpected_keys =             self.model.load_state_dict(model_state_dict, strict=False)
            
            if missing_keys:
                logger.warning(f"[LOAD] Missing keys in model: {len(missing_keys)} keys")
                logger.debug(f"Missing: {missing_keys[:5]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            
            if unexpected_keys:
                logger.warning(f"[LOAD] Unexpected keys in checkpoint: {len(unexpected_keys)} keys")
                logger.debug(f"Unexpected: {unexpected_keys[:5]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("[SUCCESS] Trained model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to load trained model: {e}")
            return False
    
    def load_baseline_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ baseline DistilBERT –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"[LOAD] Loading baseline model: {self.config.baseline_model}")
            
            self.baseline_tokenizer = AutoTokenizer.from_pretrained(self.config.baseline_model)
            self.baseline_model = AutoModel.from_pretrained(self.config.baseline_model)
            self.baseline_model.to(self.device)
            self.baseline_model.eval()
            
            logger.info("[SUCCESS] Baseline model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to load baseline model: {e}")
            return False
    
    def setup_embedding_loader(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ embedding loader –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —ç–º–±–µ–¥–∏–Ω–≥–∏"""
        try:
            logger.info("[SETUP] Setting up embedding loader")
            
            self.embedding_loader = EmbeddingLoader(
                cache_dir="cache/embeddings"
            )
            
            logger.info("[SUCCESS] Embedding loader setup complete")
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to setup embedding loader: {e}")
            return False
    
    def setup_decoder(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ generative decoder –¥–ª—è human-readable output"""
        try:
            logger.info("[SETUP] Setting up generative decoder")
            
            self.decoder = create_generative_decoder(
                architecture="resource_efficient_v21",
                embedding_dim=768,
                verbose_logging=False,
                fallback_enabled=True
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º decoder –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.decoder.to(self.device)
            
            logger.info("[SUCCESS] Generative decoder setup complete")
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to setup decoder: {e}")
            return False
    
    def prepare_test_questions(self) -> List[Dict[str, str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        
        test_questions = [
            # AI/ML —Ç–µ–º—ã (—Å—Ö–æ–∂–∏–µ —Å –æ–±—É—á–∞—é—â–∏–º–∏)
            {
                "category": "AI/ML",
                "question": "What is the difference between supervised and unsupervised learning?",
                "expected_topic": "machine learning fundamentals"
            },
            {
                "category": "AI/ML", 
                "question": "How does gradient descent work in neural networks?",
                "expected_topic": "neural network optimization"
            },
            {
                "category": "AI/ML",
                "question": "What are the advantages of transformer architecture?",
                "expected_topic": "deep learning architectures"
            },
            {
                "category": "AI/ML",
                "question": "Explain the concept of overfitting in machine learning",
                "expected_topic": "model generalization"
            },
            
            # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã (out-of-domain)
            {
                "category": "General",
                "question": "What is the capital of France?",
                "expected_topic": "geography"
            },
            {
                "category": "General",
                "question": "How do plants perform photosynthesis?",
                "expected_topic": "biology"
            },
            {
                "category": "General",
                "question": "What is the meaning of life?",
                "expected_topic": "philosophy"
            },
            
            # –°–ª–æ–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            {
                "category": "Technical",
                "question": "How does quantum computing differ from classical computing?",
                "expected_topic": "quantum physics and computing"
            },
            {
                "category": "Technical",
                "question": "What are the principles of distributed systems consensus?",
                "expected_topic": "computer systems"
            },
            
            # –ü—Ä–æ—Å—Ç—ã–µ –∂–∏—Ç–µ–π—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            {
                "category": "Simple",
                "question": "How to make a good cup of coffee?",
                "expected_topic": "cooking and beverages"
            },
            {
                "category": "Simple",
                "question": "What should I wear today?",
                "expected_topic": "daily life"
            }
        ]
        
        logger.info(f"[PREP] Prepared {len(test_questions)} test questions across {len(set(q['category'] for q in test_questions))} categories")
        return test_questions
    
    def get_model_response(self, question: str) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –≤ —ç–º–±–µ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ LLM loader
            question_embedding = self.embedding_loader.load_from_llm([question], model_key="distilbert")
            question_embedding = question_embedding[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π (–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π) —ç–º–±–µ–¥–∏–Ω–≥
            
            if question_embedding is None:
                logger.error(f"Failed to get embedding for question: {question}")
                return None
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –¥–æ–±–∞–≤–ª—è–µ–º batch dimension
            question_tensor = torch.tensor(question_embedding, dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
            with torch.no_grad():
                outputs = self.model(question_tensor)
                # –ë–µ—Ä–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –∫–∞–∫ –æ—Ç–≤–µ—Ç
                response_surface = outputs['output_surface']  # [batch, 225]
                
                # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å DistilBERT (768)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º linear —Å–ª–æ–π –∏–∑ loss function –º–æ–¥–µ–ª–∏
                response_embedding = self.model.loss_function.surface_to_embedding(response_surface)  # [batch, 768]
                
            return response_embedding.squeeze(0)  # –£–±–∏—Ä–∞–µ–º batch dimension
            
        except Exception as e:
            logger.error(f"Error getting model response: {e}")
            return None
    
    def get_baseline_response(self, question: str) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ baseline –æ—Ç–≤–µ—Ç–∞ –æ—Ç DistilBERT"""
        try:
            # Tokenize –∏ encode
            inputs = self.baseline_tokenizer(question, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–∏–Ω–≥–∏
            with torch.no_grad():
                outputs = self.baseline_model(**inputs)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º [CLS] token embedding
                baseline_embedding = outputs.last_hidden_state[:, 0, :]  # [batch, hidden_size]
                
            return baseline_embedding.squeeze(0)  # –£–±–∏—Ä–∞–µ–º batch dimension
            
        except Exception as e:
            logger.error(f"Error getting baseline response: {e}")
            return None
    
    def calculate_similarity(self, embedding1: torch.Tensor, embedding2: torch.Tensor) -> float:
        """–†–∞—Å—á–µ—Ç cosine similarity –º–µ–∂–¥—É —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏"""
        try:
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ numpy –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            emb1 = embedding1.cpu().numpy().reshape(1, -1)  
            emb2 = embedding2.cpu().numpy().reshape(1, -1)
            
            # Cosine similarity
            similarity = cosine_similarity(emb1, emb2)[0, 0]
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Error calculating similarity: {e}")
            return 0.0
    
    def run_single_test(self, question_data: Dict[str, str]) -> Dict:
        """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞"""
        question = question_data["question"]
        category = question_data["category"]
        expected_topic = question_data["expected_topic"]
        
        logger.info(f"[TEST] Testing [{category}]: {question}")
        
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç—ã –æ—Ç –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        model_response = self.get_model_response(question)
        baseline_response = self.get_baseline_response(question)
        
        inference_time = time.time() - start_time
        
        if model_response is None or baseline_response is None:
            logger.error(f"Failed to get responses for question: {question}")
            return None
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.debug(f"Model response shape: {model_response.shape}")
        logger.debug(f"Baseline response shape: {baseline_response.shape}")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º similarity
        similarity = self.calculate_similarity(model_response, baseline_response)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç
        decoded_text = "N/A"
        decode_time = 0
        if self.decoder:
            try:
                decode_start = time.time()
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ tensor –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º device
                model_response_device = model_response.to(self.device)
                decode_result = self.decoder.generate(model_response_device)
                decode_time = (time.time() - decode_start) * 1000
                decoded_text = decode_result['text']
                logger.info(f"   [DECODED]: '{decoded_text}'")
            except Exception as e:
                logger.warning(f"   [DECODE_ERROR]: {e}")
                decoded_text = f"Decode error: {str(e)}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "question": question,
            "category": category,
            "expected_topic": expected_topic,
            "similarity_to_baseline": similarity,
            "inference_time_ms": inference_time * 1000,
            "decode_time_ms": decode_time,
            "decoded_answer": decoded_text,
            "model_embedding_shape": list(model_response.shape),
            "baseline_embedding_shape": list(baseline_response.shape),
            "is_good_response": similarity >= self.config.similarity_threshold
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        status = "GOOD" if result["is_good_response"] else "POOR"
        logger.info(f"   [{status}] | Similarity: {similarity:.3f} | Time: {inference_time*1000:.1f}ms | Decode: {decode_time:.1f}ms")
        
        return result
    
    def run_comprehensive_test(self) -> Dict:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤"""
        logger.info("[START] Starting comprehensive inference testing")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        test_questions = self.prepare_test_questions()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
        results = []
        successful_tests = 0
        
        for question_data in test_questions:
            result = self.run_single_test(question_data)
            if result:
                results.append(result)
                if result["is_good_response"]:
                    successful_tests += 1
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total_tests = len(results)
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        avg_similarity = np.mean([r["similarity_to_baseline"] for r in results]) if results else 0
        avg_inference_time = np.mean([r["inference_time_ms"] for r in results]) if results else 0
        avg_decode_time = np.mean([r["decode_time_ms"] for r in results]) if results else 0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_stats = {}
        for result in results:
            category = result["category"]
            if category not in category_stats:
                category_stats[category] = {"total": 0, "good": 0, "similarities": []}
            
            category_stats[category]["total"] += 1
            category_stats[category]["similarities"].append(result["similarity_to_baseline"])
            if result["is_good_response"]:
                category_stats[category]["good"] += 1
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        summary = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "average_similarity": avg_similarity,
            "average_inference_time_ms": avg_inference_time,
            "average_decode_time_ms": avg_decode_time,
            "similarity_threshold": self.config.similarity_threshold,
            "category_statistics": {},
            "detailed_results": results,
            "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, stats in category_stats.items():
            summary["category_statistics"][category] = {
                "total": stats["total"],
                "good": stats["good"],
                "success_rate": (stats["good"] / stats["total"]) * 100,
                "avg_similarity": np.mean(stats["similarities"])
            }
        
        self.test_results = summary
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–∏
        logger.info("=" * 60)
        logger.info("[COMPLETE] INFERENCE TESTING COMPLETE")
        logger.info("=" * 60)
        logger.info(f"[STATS] Total Tests: {total_tests}")
        logger.info(f"[STATS] Successful: {successful_tests} ({success_rate:.1f}%)")
        logger.info(f"[STATS] Average Similarity: {avg_similarity:.3f}")
        logger.info(f"[STATS] Average Inference: {avg_inference_time:.1f}ms")
        logger.info(f"[STATS] Average Decode: {avg_decode_time:.1f}ms")
        logger.info("")
        
        logger.info("[BREAKDOWN] Category Breakdown:")
        for category, stats in summary["category_statistics"].items():
            logger.info(f"   {category}: {stats['good']}/{stats['total']} ({stats['success_rate']:.1f}%) | Avg: {stats['avg_similarity']:.3f}")
        
        return summary
    
    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not self.config.save_results or not self.test_results:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            results_path = Path(self.config.results_file)
            results_path.parent.mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            with open(self.config.results_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"[SAVE] Results saved to {self.config.results_file}")
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to save results: {e}")
    
    def run_full_test_suite(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
            if not self.load_trained_model():
                return False
                
            if not self.load_baseline_model():
                return False
                
            if not self.setup_embedding_loader():
                return False
                
            if not self.setup_decoder():
                return False
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
            self.run_comprehensive_test()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.save_results()
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Test suite failed: {e}")
            return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger.info("[MAIN] Starting Model Inference Testing")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = InferenceTestConfig()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if not Path(config.model_checkpoint).exists():
        logger.error(f"[ERROR] Model checkpoint not found: {config.model_checkpoint}")
        return False
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    Path("logs").mkdir(exist_ok=True)
    Path("results").mkdir(exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    tester = ModelInferenceTester(config)
    success = tester.run_full_test_suite()
    
    if success:
        logger.info("[SUCCESS] Model inference testing completed successfully!")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π summary
        if tester.test_results:
            results = tester.test_results
            print("\n" + "="*60)
            print("[FINAL] RESULTS SUMMARY")
            print("="*60)
            print(f"Success Rate: {results['success_rate']:.1f}% ({results['successful_tests']}/{results['total_tests']})")
            print(f"Average Similarity: {results['average_similarity']:.3f}")
            print(f"Average Inference Time: {results['average_inference_time_ms']:.1f}ms")
            print(f"Average Decode Time: {results['average_decode_time_ms']:.1f}ms")
            print("")
            print("Category Performance:")
            for category, stats in results['category_statistics'].items():
                print(f"  {category}: {stats['success_rate']:.1f}% (similarity: {stats['avg_similarity']:.3f})")
            print("")
            print("Sample Decoded Answers:")
            for i, result in enumerate(results['detailed_results'][:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"  Q{i+1}: {result['question'][:50]}...")
                print(f"      A: {result['decoded_answer']}")
            print("="*60)
        
        return True
    else:
        logger.error("[ERROR] Model inference testing failed!")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 