#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –¥–µ–∫–æ–¥–µ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç
===============================

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –±–∞–∑–æ–≤–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.
"""

import logging
import torch
from pathlib import Path

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from new_rebuild.config.simple_config import get_project_config
from new_rebuild.core.inference.text_decoder import (
    SimpleTextDecoder, 
    JointTextDecoder,
    EmbeddingTextCache,
    create_text_decoder
)
from new_rebuild.utils.logging import get_logger

logger = get_logger(__name__)


def test_embedding_cache():
    """–¢–µ—Å—Ç –∫—ç—à–∞ —ç–º–±–µ–¥–∏–Ω–≥-—Ç–µ–∫—Å—Ç"""
    logger.info("=== –¢–µ—Å—Ç EmbeddingTextCache ===")
    
    cache = EmbeddingTextCache(max_size=5)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    embeddings = [
        torch.randn(768),
        torch.randn(768),
        torch.randn(768)
    ]
    
    texts = [
        "Hello world!",
        "This is a test sentence.",
        "Another example text."
    ]
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    for emb, text in zip(embeddings, texts):
        cache.put(emb, text)
        logger.info(f"  Saved: '{text}'")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ
    logger.info("\n–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞:")
    for i, emb in enumerate(embeddings):
        retrieved = cache.get(emb)
        logger.info(f"  Retrieved {i}: '{retrieved}'")
        assert retrieved == texts[i], f"Cache mismatch: {retrieved} != {texts[i]}"
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
    logger.info("\n–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö:")
    similar_emb = embeddings[0] + torch.randn(768) * 0.01  # –û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–π
    retrieved_similar = cache.get(similar_emb)
    logger.info(f"  Similar embedding retrieved: '{retrieved_similar}'")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—ç—à–∞
    logger.info("\n–¢–µ—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –∫—ç—à–∞:")
    for i in range(10):
        cache.put(torch.randn(768), f"Overflow text {i}")
    
    stats = cache.get_stats()
    logger.info(f"  Cache stats: {stats}")
    
    logger.info("‚úÖ EmbeddingTextCache test passed!")
    return cache


def test_simple_decoder():
    """–¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞"""
    logger.info("\n=== –¢–µ—Å—Ç SimpleTextDecoder ===")
    
    config = get_project_config()
    config.embedding.decoder_cache_enabled = True
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞
    cache_dir = Path("temp_cache")
    cache_dir.mkdir(exist_ok=True)
    config.embedding.cache_dir = str(cache_dir)
    
    decoder = SimpleTextDecoder(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏
    test_embeddings = torch.randn(4, 768)
    
    logger.info("–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤:")
    decoded_texts = decoder.decode_embeddings(test_embeddings)
    
    for i, text in enumerate(decoded_texts):
        logger.info(f"  Embedding {i}: '{text}'")
    
    # –¢–µ—Å—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à)
    logger.info("\n–ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–µ—Å—Ç –∫—ç—à–∞):")
    decoded_texts_2 = decoder.decode_embeddings(test_embeddings)
    
    for i, text in enumerate(decoded_texts_2):
        logger.info(f"  Cached {i}: '{text}'")
        assert text == decoded_texts[i], "Cache consistency error"
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞
    cache_stats = decoder.get_cache_stats()
    logger.info(f"\nCache stats: {cache_stats}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞
    decoder.save_cache()
    logger.info("Cache saved to disk")
    
    logger.info("‚úÖ SimpleTextDecoder test passed!")
    return decoder


def test_joint_decoder():
    """–¢–µ—Å—Ç joint –¥–µ–∫–æ–¥–µ—Ä–∞"""
    logger.info("\n=== –¢–µ—Å—Ç JointTextDecoder ===")
    
    config = get_project_config()
    joint_decoder = JointTextDecoder(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    embeddings = torch.randn(3, 768)
    target_texts = ["Hello world", "Test sentence", "Another example"]
    
    # –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è
    joint_decoder.train()
    joint_decoder.set_training_mode(True)
    
    results = joint_decoder(embeddings, target_texts)
    logger.info(f"Training mode results:")
    logger.info(f"  Logits shape: {results['logits'].shape if results['logits'] is not None else None}")
    logger.info(f"  Loss: {results['loss']}")
    
    for i, text in enumerate(results['decoded_texts']):
        logger.info(f"  Decoded {i}: '{text}'")
    
    # –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ inference
    joint_decoder.eval()
    joint_decoder.set_training_mode(False)
    
    results_inf = joint_decoder(embeddings)
    logger.info(f"\nInference mode results:")
    for i, text in enumerate(results_inf['decoded_texts']):
        logger.info(f"  Decoded {i}: '{text}'")
    
    logger.info("‚úÖ JointTextDecoder test passed!")
    return joint_decoder


def test_decoder_factory():
    """–¢–µ—Å—Ç —Ñ–∞–±—Ä–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
    logger.info("\n=== –¢–µ—Å—Ç —Ñ–∞–±—Ä–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ ===")
    
    config = get_project_config()
    
    # –ü—Ä–æ—Å—Ç–æ–π –¥–µ–∫–æ–¥–µ—Ä
    simple_decoder = create_text_decoder(config, joint_training=False)
    logger.info(f"Simple decoder type: {type(simple_decoder).__name__}")
    
    # Joint –¥–µ–∫–æ–¥–µ—Ä
    joint_decoder = create_text_decoder(config, joint_training=True)
    logger.info(f"Joint decoder type: {type(joint_decoder).__name__}")
    
    logger.info("‚úÖ Factory function test passed!")


def test_cache_persistence():
    """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞"""
    logger.info("\n=== –¢–µ—Å—Ç –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫—ç—à–∞ ===")
    
    cache_path = "temp_cache/test_cache.json"
    
    # –°–æ–∑–¥–∞–µ–º –∫—ç—à –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏
    cache1 = EmbeddingTextCache(max_size=10)
    
    test_data = [
        (torch.randn(768), "First test sentence"),
        (torch.randn(768), "Second test sentence"),
        (torch.randn(768), "Third test sentence")
    ]
    
    for emb, text in test_data:
        cache1.put(emb, text)
    
    logger.info(f"Created cache with {len(test_data)} items")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    cache1.save(cache_path)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫—ç—à –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
    cache2 = EmbeddingTextCache(max_size=10)
    cache2.load(cache_path)
    
    logger.info(f"Loaded cache with {cache2.get_stats()['size']} items")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å
    for emb, original_text in test_data:
        retrieved_text = cache2.get(emb)
        logger.info(f"  Original: '{original_text}' -> Retrieved: '{retrieved_text}'")
        assert retrieved_text == original_text, "Cache persistence error"
    
    # –û—á–∏—Å—Ç–∫–∞
    Path(cache_path).unlink(missing_ok=True)
    
    logger.info("‚úÖ Cache persistence test passed!")


def test_batch_processing():
    """–¢–µ—Å—Ç –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    logger.info("\n=== –¢–µ—Å—Ç –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===")
    
    config = get_project_config()
    decoder = SimpleTextDecoder(config)
    
    # –¢–µ—Å—Ç —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π
    batch_sizes = [1, 4, 8, 16]
    
    for batch_size in batch_sizes:
        logger.info(f"\n–ë–∞—Ç—á —Ä–∞–∑–º–µ—Ä: {batch_size}")
        
        embeddings = torch.randn(batch_size, 768)
        decoded_texts = decoder.decode_embeddings(embeddings)
        
        assert len(decoded_texts) == batch_size, f"Batch size mismatch: {len(decoded_texts)} != {batch_size}"
        logger.info(f"  ‚úì Decoded {len(decoded_texts)} texts")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ
        for i, text in enumerate(decoded_texts[:3]):
            logger.info(f"    {i}: '{text[:50]}...'")
    
    logger.info("‚úÖ Batch processing test passed!")


def test_gpu_performance():
    """–¢–µ—Å—Ç GPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è RTX 5090"""
    logger.info("\n=== –¢–µ—Å—Ç GPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (RTX 5090) ===")
    
    config = get_project_config()
    decoder = SimpleTextDecoder(config)
    
    if not decoder.use_gpu_acceleration:
        logger.info("‚ö†Ô∏è  GPU acceleration not available, skipping GPU tests")
        return
    
    logger.info(f"üöÄ GPU acceleration enabled (batch size: {decoder.gpu_batch_size})")
    
    # –¢–µ—Å—Ç –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
    large_batch_sizes = [32, 64, 128, 256, 512]
    
    import time
    
    for batch_size in large_batch_sizes:
        logger.info(f"\n–¢–µ—Å—Ç –±–æ–ª—å—à–æ–≥–æ –±–∞—Ç—á–∞: {batch_size}")
        
        embeddings = torch.randn(batch_size, 768)
        
        # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
        start_time = time.time()
        decoded_texts = decoder.decode_embeddings(embeddings)
        elapsed = time.time() - start_time
        
        assert len(decoded_texts) == batch_size
        throughput = batch_size / elapsed
        
        logger.info(f"  ‚úì Decoded {batch_size} embeddings in {elapsed:.3f}s ({throughput:.1f} emb/s)")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã GPU-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        for i, text in enumerate(decoded_texts[:2]):
            logger.info(f"    GPU[{i}]: '{text}'")
    
    # –¢–µ—Å—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–∫—ç—à)
    logger.info(f"\n–¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è GPU:")
    large_embeddings = torch.randn(128, 768)
    
    # –ü–µ—Ä–≤—ã–π —Ä–∞–∑
    start_time = time.time()
    first_decode = decoder.decode_embeddings(large_embeddings)
    first_time = time.time() - start_time
    
    # –í—Ç–æ—Ä–æ–π —Ä–∞–∑ (–∫—ç—à)
    start_time = time.time()
    second_decode = decoder.decode_embeddings(large_embeddings)
    second_time = time.time() - start_time
    
    speedup = first_time / second_time if second_time > 0 else float('inf')
    logger.info(f"  First decode: {first_time:.3f}s")
    logger.info(f"  Cached decode: {second_time:.3f}s") 
    logger.info(f"  Cache speedup: {speedup:.1f}x")
    
    cache_stats = decoder.get_cache_stats()
    logger.info(f"  Cache stats: {cache_stats}")
    
    logger.info("‚úÖ GPU performance test passed!")


if __name__ == "__main__":
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ TextDecoder")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    test_embedding_cache()
    test_simple_decoder()
    test_joint_decoder()
    test_decoder_factory()
    test_cache_persistence()
    test_batch_processing()
    test_gpu_performance()  # –ù–æ–≤—ã–π —Ç–µ—Å—Ç –¥–ª—è RTX 5090
    
    logger.info("\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã TextDecoder –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    
    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    import shutil
    shutil.rmtree("temp_cache", ignore_errors=True)
    
    logger.info("üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã")