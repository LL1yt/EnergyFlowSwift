# 🧠 Stage 3.1.4.1: Emergent Training Infrastructure Configuration
# ===============================================================
#
# Emergent processing концепция:
# TRAINING MODE: 4096D LLaMA → 225D Surface → FULL CUBE INFLUENCE → 225D Surface → Learning
# INFERENCE MODE: Question → 225D Front → [EMERGENT PROCESSING] → 225D Back → Answer

# === PROJECT METADATA ===
project:
  name: "3D Cellular Neural Network - Stage 3.1.4.1"
  version: "3.1.4.1-emergent"
  description: "Emergent training infrastructure с full cube gradient flow"
  stage: "3.1.4.1"
  architecture_type: "emergent_processing"

# === EMERGENT TRAINING CONFIGURATION ===
emergent_training:
  # Base configuration
  teacher_model: "Meta-Llama-3-8B"
  cube_dimensions: [15, 15, 11] # 2,475 cells total

  # Emergent processing settings
  enable_full_cube_gradient: true
  spatial_propagation_depth: 11 # All layers
  emergent_specialization: true

  # gMLP cell configuration OPTIMIZED для точного 25K params target
  gmlp_config:
    state_size: 32 # OPTIMIZED from parameter analysis
    neighbor_count: 6 # Standard 6-connectivity
    hidden_dim: 32 # OPTIMIZED from 128 → 32
    external_input_size: 12 # OPTIMIZED input dimension
    memory_dim: 16 # OPTIMIZED from 32 → 16
    use_memory: true
    activation: "gelu"
    dropout: 0.1
    spatial_connections: true # EMERGENT FEATURE - spatial connectivity

    # Spatial gating configuration
    spatial_gating:
      sequence_length: 7 # 6 neighbors + own state
      init_eps: 0.001
      gating_activation: "sigmoid"

  # Multi-objective loss configuration
  loss_weights:
    surface_reconstruction: 0.3 # Surface input → output consistency
    internal_consistency: 0.3 # Internal layer coherence
    dialogue_similarity: 0.4 # Final Q→A similarity

  # Training settings
  learning_rate: 0.001
  batch_size: 8
  epochs: 15
  warmup_epochs: 3

  # Optimization settings
  gradient_balancing: true
  adaptive_loss_weighting: true
  gradient_clip_norm: 1.0
  weight_decay: 0.01

# === ADAPTER INTEGRATION ===
adapter_integration:
  # Use optimal settings from Stage 3.1.3
  adapter_strategy: "hierarchical" # Best from LLaMA testing
  adapter_learning_rate: 0.001
  joint_training: true

  # Compression settings
  teacher_embedding_dim: 4096 # LLaMA-3-8B
  surface_embedding_dim: 225 # 15×15 surface
  compression_ratio: 18.2 # 4096/225 ≈ 18.2x

  # Quality targets
  reconstruction_loss_target: 0.051 # From Stage 3.1.3 success
  compression_quality_target: 0.587 # From hierarchical strategy

# === SPATIAL PROPAGATION SYSTEM ===
spatial_propagation:
  # Cross-layer influence settings
  enable_cross_layer: true
  layer_connections: 10 # 11 layers → 10 connections

  # Propagation mechanisms (UPDATED for optimized state_size)
  propagation_gate:
    input_dim: 64 # 2 * state_size (2 * 32)
    output_dim: 32 # state_size (optimized)
    activation: "sigmoid"

  # Connection initialization
  connection_weights:
    init_method: "xavier_normal"
    init_scale: 0.01
    requires_grad: true

# === TRAINING OPTIMIZATION ===
training_optimization:
  # Optimizer settings
  optimizer: "AdamW"
  scheduler: "ReduceLROnPlateau"
  scheduler_settings:
    mode: "min"
    factor: 0.5
    patience: 3

  # Memory optimization
  gradient_checkpointing: false # Not needed for 15×15×11
  mixed_precision: false # Disabled for stability

  # GPU settings
  device: "cpu" # Auto-detect available
  pin_memory: true
  num_workers: 0 # Single thread for stability

# === MONITORING & EVALUATION ===
monitoring:
  # Metrics tracking
  primary_metrics:
    - "total_loss"
    - "cosine_similarity"
    - "dialogue_similarity_loss"

  secondary_metrics:
    - "surface_reconstruction_loss"
    - "internal_consistency_loss"
    - "learning_rate"

  # Logging settings
  log_interval: 10 # Log every 10 steps
  save_interval: 50 # Save checkpoint every 50 steps

  # Target thresholds
  convergence_threshold: 0.001
  max_patience: 5

  # Emergent behavior tracking
  emergent_analysis:
    enable_layer_analysis: true
    activation_variance_tracking: true
    spatial_specialization_detection: true

# === SYSTEM RESOURCES ===
system_resources:
  # Parameter estimation
  total_cells: 2475 # 15×15×11
  target_params_per_cell: 25000
  estimated_total_params: 61875000 # 2,475 × 25K

  # Memory estimation (CONFIRMED from optimization)
  estimated_memory_gb: 0.2 # ACTUAL result from parameter optimization
  max_memory_gb: 4.0

  # Performance targets
  target_forward_time_ms: 150 # Per batch
  target_training_step_s: 3.0 # Per step

  # Feasibility flags (CONFIRMED)
  cpu_feasible: true # CONFIRMED: Only 0.2GB memory needed
  gpu_recommended: false # CPU sufficient for development
  production_ready: true # READY для emergent training!

# === OUTPUT CONFIGURATION ===
output:
  # Results saving
  save_directory: "results/emergent_training_3_1_4_1"
  save_checkpoints: true
  save_metrics: true
  save_system_info: true

  # File formats
  checkpoint_format: "pytorch"
  metrics_format: "json"
  system_info_format: "yaml"

  # Backup settings
  auto_backup: true
  backup_interval: 100 # Every 100 steps

# === EXPERIMENTAL SETTINGS ===
experimental:
  # Advanced features (experimental)
  enable_attention_visualization: false
  enable_gradient_analysis: true
  enable_emergent_pattern_detection: true

  # Research features
  analyze_layer_specialization: true
  track_information_flow: true
  measure_compression_efficiency: true

  # Debug settings
  debug_mode: false
  verbose_logging: true
  save_intermediate_states: false # Memory intensive

# === SUCCESS CRITERIA ===
success_criteria:
  # Primary goals
  primary:
    full_cube_gradient_flow: ">80%" # Percentage of cells with gradients
    system_stability: "no_nan_inf" # No NaN/Inf values
    training_convergence: "<0.05" # Final loss threshold

  # Secondary goals
  secondary:
    parameter_efficiency: "20K-30K" # Parameters per cell range
    memory_usage: "<4GB" # Total memory consumption
    training_speed: "<5min_per_epoch" # Training performance

  # Research goals
  research:
    emergent_specialization: "detectable" # Layer differentiation
    information_preservation: ">85%" # Input→Output similarity
    spatial_organization: "measurable" # Cross-layer patterns

# === INTEGRATION COMPATIBILITY ===
integration:
  # Backward compatibility
  compatible_with_stage_3_1_3: true
  uses_adapter_cube_trainer: true
  extends_universal_adapter: true

  # Forward compatibility
  prepares_stage_3_1_4_2: true # Surface-only inference
  emergent_architecture_ready: true
  production_pipeline_ready: false # Requires 3.1.4.2

# === CONFIGURATION VALIDATION ===
validation:
  # Required components
  required_modules:
    - "training.embedding_trainer.emergent_training_stage_3_1_4_1"
    - "training.embedding_trainer.adapter_integration"
    - "core.lattice_3d"
    - "core.cell_prototype.architectures.gmlp_cell"

  # Configuration checks
  parameter_ranges:
    learning_rate: [0.0001, 0.01]
    batch_size: [1, 16]
    hidden_dim: [64, 256]
    memory_dim: [16, 64]

  # System requirements
  minimum_requirements:
    ram_gb: 4
    cpu_cores: 2
    python_version: "3.8+"
    pytorch_version: "1.12+"

# Enhanced GPU Optimization Settings
gpu_optimization:
  enable_gpu: true
  device: "cuda" # "cuda", "cpu", or "auto"
  mixed_precision: true
  memory_efficient: true
  batch_size_gpu_factor: 2.0 # Scale batch size on GPU
  parallel_cell_processing: true

# Memory Management
memory_management:
  gradient_checkpointing: true
  tensor_sharing: true
  dynamic_memory_cleanup: true
  max_gpu_memory_gb: 4.0

# Performance Optimization
performance:
  compile_model: true # PyTorch 2.0 compile
  cudnn_benchmark: true
  pin_memory: true
  non_blocking_transfer: true
