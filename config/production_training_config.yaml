# ðŸŽ¯ Production Training Configuration
# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ð¾Ð³Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ

# Training stages configuration
stages:
  validation:
    description: "System validation - Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð²ÑÐµÑ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²"
    epochs: 3
    target_loss: 1.0
    target_similarity: 0.15
    batch_size: 2
    learning_rate: 0.001
    early_stopping_patience: 5
    save_checkpoints: true

  convergence:
    description: "Convergence testing - Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° ÑÑ€ÐµÐ´Ð½Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"
    epochs: 10
    target_loss: 0.5
    target_similarity: 0.25
    batch_size: 4
    learning_rate: 0.0005
    early_stopping_patience: 8
    save_checkpoints: true

  production:
    description: "Production training - Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ Ñ†ÐµÐ»ÐµÐ¹"
    epochs: 50
    target_loss: 0.3
    target_similarity: 0.40 # Realistic based on current 38.5%
    batch_size: 8
    learning_rate: 0.0003
    early_stopping_patience: 15
    save_checkpoints: true

  optimization:
    description: "Final optimization - fine-tuning Ð´Ð»Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸"
    epochs: 25
    target_loss: 0.2
    target_similarity: 0.45
    batch_size: 6
    learning_rate: 0.0001
    early_stopping_patience: 10
    save_checkpoints: true

# Model configuration
model:
  cube_dimensions: [15, 15, 11] # Full 3D lattice
  teacher_model: "llama3-8b-local"
  adapter_strategy: "hierarchical"
  enable_nca: true
  mixed_precision: true # Enable if CUDA available

# Training behavior
training:
  checkpoint_frequency: 5 # Save checkpoint every N epochs
  validation_frequency: 2 # Run validation every N epochs
  log_frequency: 1 # Log metrics every N batches

  # Early stopping
  early_stopping:
    monitor: "loss" # or "similarity"
    mode: "min" # or "max" for similarity
    patience: 10
    min_delta: 0.001

  # Failure handling
  failure_handling:
    validation_abort_threshold: 2.0 # Abort if validation loss > this
    convergence_retry_attempts: 2
    continue_on_failure: true # Continue to next stage Ð´Ð°Ð¶Ðµ Ð¿Ñ€Ð¸ failure

# Dataset configuration
dataset:
  cache_embeddings: true
  normalize_embeddings: true
  validation_split: 0.1 # 10% for validation
  augmentation:
    noise_level: 0.01
    rotation_probability: 0.1

# Resource management
resources:
  max_memory_gb: 8 # Maximum GPU memory to use
  num_workers: 4 # DataLoader workers
  pin_memory: true
  persistent_workers: true

# Monitoring and logging
monitoring:
  log_level: "INFO"
  save_plots: true
  save_metrics: true
  tensorboard: false # Enable TensorBoard logging

  # Metrics to track
  track_metrics:
    - "loss"
    - "similarity"
    - "gradient_norm"
    - "learning_rate"
    - "epoch_time"
    - "memory_usage"

# Output configuration
output:
  checkpoint_dir: "checkpoints"
  results_dir: "results"
  log_dir: "logs"
  save_model_every_stage: true
  compress_checkpoints: false

# Advanced features
advanced:
  gradient_clipping: 1.0
  weight_decay: 0.01
  warmup_epochs: 3
  cosine_lr_schedule: false

  # NCA specific
  nca:
    stochastic_update_rate: 0.7
    pattern_preservation_weight: 0.1
    spatial_coherence_weight: 0.05

  # GPU optimization
  gpu:
    channels_last: true
    compile_model: false # PyTorch 2.0 compilation
    gradient_accumulation_steps: 1
