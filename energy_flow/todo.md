# Energy Flow Training Pipeline - Analysis & Optimization Plan

## üîç Detailed Pipeline Analysis

### **1. Data Loading & Preparation**

#### `load_experiment_dataset()` - CRITICAL ISSUES

```python
dataset = torch.load(dataset_path, map_location='cuda', weights_only=False)
```

**Problems:**

- ‚ùå **Blocking load**: Entire dataset (31MB) loaded to GPU memory at once
- ‚ùå **Memory underutilization**: With RTX 5090 32GB can load 100+ datasets in parallel
- ‚ùå **No streaming**: Cannot work with datasets > GPU memory

**RTX 5090 Optimizations:**

- ‚úÖ Streaming batch loading: disk ‚Üí CPU ‚Üí GPU pipeline
- ‚úÖ Prefetch multiple datasets to GPU memory
- ‚úÖ `torch.utils.data.DataLoader` with `pin_memory=True`, `prefetch_factor=4`

**Logging:**

```python
logger.log(DEBUG_MEMORY, f"Dataset loaded: {size_mb:.1f}MB, GPU mem: {torch.cuda.memory_allocated()/1e9:.1f}GB")
```

#### `ExperimentDatasetWrapper` - BOTTLENECKS

```python
def __getitem__(self, idx):
    input_text, target_text = self.dataset['text_pairs'][idx]
    return {
        'input_embedding': self.dataset['input_embeddings'][idx],  # Tensor copying!
        'target_embedding': self.dataset['target_embeddings'][idx],
        'input_text': input_text,
        'target_text': target_text
    }
```

**Problems:**

- ‚ùå **Tensor copying**: `__getitem__` copies tensors each time ‚Üí slow
- ‚ùå **No batch access**: Single element access only
- ‚ùå **Suboptimal collate_fn**: `torch.stack()` creates new tensors

**Optimizations:**

- ‚úÖ Pre-batching: `torch.split()` dataset into batches
- ‚úÖ Memory-mapped files for large datasets
- ‚úÖ Async loading of next batches

---

### **2. Experiment Configuration**

#### `create_experiment_config()` - RTX 5090 UNDERUTILIZATION

```python
return EnergyConfig(
    lattice_width=50,      # 2500 cells on surface
    lattice_height=50,     # Total: 50K cells
    lattice_depth=20,
    max_active_flows=5000, # Potential bottleneck
    batch_size=16,         # TOO SMALL for 32GB!
    carrier_hidden_size=512,  # Can increase
    carrier_num_layers=2,
)
```

**Performance Issues:**

- ‚ùå **Small batch_size=16**: RTX 5090 can handle 64-128
- ‚ùå **Conservative lattice sizes**: 50x50x20 = 50K cells, can do 200K+
- ‚ùå **Underutilized parallelism**: `max_active_flows=5000` may be bottleneck

**RTX 5090 Optimized Configuration:**

```python
def create_rtx5090_config() -> EnergyConfig:
    """Maximum optimized config for RTX 5090 32GB"""
    return EnergyConfig(
        lattice_width=80,           # 6400 cells on surface
        lattice_height=80,          # Total: ~400K cells
        lattice_depth=60,
        max_active_flows=20000,     # Increase parallelism
        batch_size=64,              # Maximum memory utilization
        carrier_hidden_size=1024,   # More parameters = better quality
        carrier_num_layers=3,

        # Memory optimizations
        gradient_accumulation_steps=4,  # Effective batch_size = 256
        use_mixed_precision=True,       # bfloat16 for memory savings
        dataloader_num_workers=12,      # Parallel data loading
    )
```

---

### **3. Main Training Loop**

#### `EnergyTrainer.train_step()` - MAJOR BOTTLENECKS

**Step 1: Cube Forward Pass**

```python
cube_output_surface = self.flow_processor.forward(teacher_input_embeddings, max_steps=50)
```

**Detailed Performance Breakdown:**

1. **EmbeddingMapper** (768‚Üí2500): ~0.1ms, parallel
2. **FlowProcessor.forward()** (main time consumer):
   ```python
   for step in range(max_steps):  # 50 iterations
       # For each active flow:
       for flow in active_flows:  # Up to 5000 flows
           neuron_output = simple_neuron(cell_state)      # ~0.01ms √ó 5000 = 50ms
           energy_output = energy_carrier(neuron_output)  # ~0.1ms √ó 5000 = 500ms
           # Update positions and energies: ~0.01ms √ó 5000 = 50ms
   ```
   **Total: 50 √ó (50 + 500 + 50) = 30,000ms = 30 seconds per batch!**

**CRITICAL Issues:**

- ‚ùå **Fixed max_steps=50**: No adaptive stopping
- ‚ùå **Sequential processing**: Each step waits for previous completion
- ‚ùå **Suboptimal batching**: Flows processed individually
- ‚ùå **Computation duplication**: Repeated forward passes in text_bridge

**Optimizations:**

```python
# Adaptive stopping
convergence_threshold = 0.01
for step in range(max_steps):
    if self.check_convergence(active_flows, convergence_threshold):
        logger.log(DEBUG_CONVERGENCE, f"Converged at step {step}")
        break

# Vectorized flow processing
flow_outputs = self.process_flows_vectorized(active_flows)  # All at once

# Mixed precision
with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
    cube_output_surface = self.flow_processor.forward(...)
```

**Step 2: Text Bridge - CATASTROPHIC Performance**

```python
# Current code:
for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    surface_input = teacher_input_embeddings[i:i+1]
    surface_output = self.flow_processor.forward(surface_input, max_steps=50)  # REPEATED forward!
    pred_texts_batch = self.text_decoder.decode_surface(surface_output)
```

**Problems:**

- ‚ùå **O(batch_size¬≤) complexity**: Repeated forward pass for each text
- ‚ùå **Duplication**: `flow_processor.forward()` called batch_size+1 times
- ‚ùå **Sequential processing**: No batch text processing

**Optimization:**

```python
# Batch processing
encoder_outputs = self.text_encoder.encode_text(input_texts)  # All at once
# Use already computed cube_output_surface!
predicted_texts = self.text_decoder.decode_surface_batch(cube_output_surface)
```

**Potential speedup: 16x (batch size)**

---

### **4. Logging & Metrics System**

#### Current Issues

```python
# Too frequent logging in production:
logger.log(DEBUG_TRAINING, f"üîÑ Starting train_step: batch_size={batch_size}")
logger.log(DEBUG_TRAINING, f"üìä Teacher embeddings: {teacher_input_embeddings.shape}")
```

**Problems:**

- ‚ùå **Production overhead**: Each train_step writes 20+ log entries
- ‚ùå **No conditional metrics**: Metrics computed always, even when not needed
- ‚ùå **Missing performance metrics**: No throughput, GPU utilization

**Optimized System:**

```python
# Conditional metrics through logging levels
if logger.isEnabledFor(DEBUG_TRAINING):
    # Expensive computations only for debug
    gpu_util = torch.cuda.utilization()
    memory_used = torch.cuda.memory_allocated() / 1e9
    logger.log(DEBUG_TRAINING, f"GPU: {gpu_util}%, Memory: {memory_used:.1f}GB")

# Throughput metrics
if logger.isEnabledFor(DEBUG_PERFORMANCE):
    tokens_per_second = batch_size * seq_len / step_time
    logger.log(DEBUG_PERFORMANCE, f"Throughput: {tokens_per_second:.0f} tokens/s")
```

**New Logging Levels:**

- `DEBUG_PERFORMANCE = 21`: Throughput, GPU utilization, memory usage
- `DEBUG_PROFILING = 22`: Detailed component execution times

---

### **5. RTX 5090 Memory Usage Assessment**

#### Current Usage (experiment config):

```
Model:
- FlowProcessor: ~10M params √ó 4 bytes = 40MB
- EnergyCarrier (GRU): ~3M params √ó 4 bytes = 12MB
- TextBridge: ~5M params √ó 4 bytes = 20MB
Total model: ~72MB

Data (batch_size=16):
- Input embeddings: 16 √ó 768 √ó 4 = 48KB
- Activations: 16 √ó 50K cells √ó 4 ‚âà 3.2MB
- Gradients: ~72MB (same as model)
Total: ~150MB

TOTAL USAGE: ~222MB out of 32GB = 0.7%!
```

#### Optimized Usage (RTX 5090 config):

```
Model (increased):
- FlowProcessor: ~50M params = 200MB
- EnergyCarrier: ~15M params = 60MB
- TextBridge: ~20M params = 80MB
Total model: ~340MB

Data (batch_size=64):
- Input embeddings: 64 √ó 768 √ó 4 = 192KB
- Activations: 64 √ó 400K cells √ó 4 ‚âà 100MB
- Gradients: ~340MB
- Dataset cache: ~500MB (multiple datasets)
Total: ~1.28GB

TOTAL USAGE: ~1.28GB out of 32GB = 4%
POTENTIAL for increase: 8x current size!
```

---

## üìã Optimization Plan (Priority Order)

### **üî• CRITICAL PRIORITY**

#### 1. **Text Bridge Refactoring** (potential: 16x speedup) ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω

- [ ] Batch processing for `encode_text()` and `decode_surface()`
- [ ] Eliminate duplicated forward passes
- [ ] Cache intermediate results

#### 2. **Adaptive max_steps** (potential: 2-3x speedup) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù (8.33x —É—Å–∫–æ—Ä–µ–Ω–∏–µ!)**

- [x] Convergence checking for energy flows - convergence_threshold = 0.95
- [x] Early stopping at threshold values - patience = 3 steps
- [x] Dynamic step count adjustment - —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ 44/50 —à–∞–≥–æ–≤
- [x] **–†–µ–∑—É–ª—å—Ç–∞—Ç: 8.33x speedup** (99% –ø–æ—Ç–æ–∫–æ–≤ –∑–∞ 6 —à–∞–≥–æ–≤ –≤–º–µ—Å—Ç–æ 50)

#### 3. **FlowProcessor Vectorization** (potential: 5x speedup) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù (1.43x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)**

- [x] Parallel processing of all flows - –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Å–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
- [x] Loop optimization via vectorized operations - –∑–∞–º–µ–Ω–µ–Ω —Ü–∏–∫–ª for –Ω–∞ batch –æ–ø–µ—Ä–∞—Ü–∏–∏
- [x] Eliminate sequential dependencies - O(1) spawn lookup, batch updates
- [x] **–†–µ–∑—É–ª—å—Ç–∞—Ç: 1.43x speedup** (107s ‚Üí 74.59s –Ω–∞ –±–∞—Ç—á, 100% completion rate)

### **üöÄ HIGH PRIORITY**

#### 4. **RTX 5090 Configuration** (potential: 4x throughput) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù**

- [x] Increase batch_size to 64 (was 16) - 4x –±–æ–ª—å—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
- [x] Increase lattice depth to 60 (was 20) - 3x –≥–ª—É–±–∂–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ 50x50 surface
- [x] Gradient accumulation for effective batch_size = 256 - –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ accumulation_steps=4
- [x] Adaptive convergence –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≥–ª—É–±–æ–∫–æ–π —Ä–µ—à–µ—Ç–∫–∏ (min_steps=10, patience=5)
- [x] **–†–µ–∑—É–ª—å—Ç–∞—Ç: 4x batch throughput** + –±–æ–ª—å—à–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏

#### 5. **GPU Utilization Fix** (potential: 8% ‚Üí 75% GPU load) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù**

- [x] –£—Å—Ç—Ä–∞–Ω–∏—Ç—å `.item()` CPU-GPU —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≤ flow_processor.py:366,370,374,382
- [x] –ó–∞–º–µ–Ω–∏—Ç—å —Ü–∏–∫–ª—ã `for idx in dead_indices/alive_indices` –Ω–∞ –ø–æ–ª–Ω—É—é –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
- [x] Batch –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è deactivate_flow/update_flow –≤–º–µ—Å—Ç–æ –ø–æ–æ—á–µ—Ä–µ–¥–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
- [x] –£–±—Ä–∞—Ç—å –±–ª–æ–∫–∏—Ä—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–∑ hot path –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤

#### 6. **Memory Management** (potential: —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù**

- [x] –î–æ–±–∞–≤–∏—Ç—å `torch.cuda.empty_cache()` –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ train_step()
- [ ] Memory monitoring —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–æ–π –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–æ–≤
- [ ] –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –≤ FlowProcessor
- [ ] –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ memory leaks –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏

#### 7. **DataLoader Optimization** (potential: 2x I/O speedup)

- [ ] `num_workers=12`, `pin_memory=True`, `persistent_workers=True`
- [ ] Prefetch next batches
- [ ] Async data loading

#### 8. **Mixed Precision Training** (potential: 1.5x speedup, 50% memory) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù**

- [x] `torch.autocast` for forward pass - –ø—Ä–∏–º–µ–Ω–µ–Ω–æ –∫ FlowProcessor.forward() –∏ loss computation
- [x] bfloat16 –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π, float32 –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ - –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤ EnergyConfig
- [x] Gradient scaling –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ - –ø–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å GradScaler
- [x] **–†–µ–∑—É–ª—å—Ç–∞—Ç: 1.5x speedup + 50% memory savings** —á–µ—Ä–µ–∑ autocast –∏ gradient scaling

### **üìä MEDIUM PRIORITY**

#### 9. **Metrics & Profiling System**

- [ ] Conditional metrics through logging levels
- [ ] Built-in profiler with `torch.profiler`
- [ ] Real-time GPU utilization monitoring

#### 10. **Checkpoint System**

- [ ] Save performance metrics
- [ ] Automatic cleanup of old checkpoints
- [ ] Checkpoint compression for space saving

### **üîß LOW PRIORITY**

#### 11. **torch.compile() Optimization**

- [ ] Compile core components
- [ ] Fuse operations into kernels
- [ ] Optimize memory layout

#### 12. **Advanced Techniques**

- [ ] Gradient checkpointing for large models
- [ ] Flash Attention for text bridge
- [ ] Custom CUDA kernels for specific operations

---

## üéØ Expected Results

### **Before Optimization (current state):**

- Batch size: 16
- Time per batch: ~30 seconds
- GPU utilization: ~10-20%
- Memory usage: ~0.7% (222MB of 32GB)
- Throughput: ~0.5 samples/second

### **After GPU Utilization Fix (current target):**

- Batch size: 64+ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é)
- Time per batch: ~1-2 seconds (15-30x speedup)
- GPU utilization: ~75-90% (–±—ã–ª–æ 8%!)
- Memory usage: —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–æ–π
- Throughput: ~32-64 samples/second (64-128x increase)

**–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –ü–†–ò–†–û–°–¢: GPU load 8% ‚Üí 75%, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å, 64-128x throughput**

---

## üî¨ Metrics to Track

### **Performance Metrics:**

```python
# Add to train_step():
step_metrics = {
    'throughput_samples_per_sec': batch_size / step_time,
    'gpu_utilization_percent': torch.cuda.utilization(),
    'memory_used_gb': torch.cuda.memory_allocated() / 1e9,
    'memory_utilization_percent': torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100,
    'flow_convergence_steps': actual_steps_used,
    'text_bridge_time_ms': text_bridge_time * 1000,
    'energy_computation_time_ms': energy_time * 1000,
}
```

### **Logging Level System for Metrics:**

- **Production** (INFO): Only basic metrics (loss, epoch time)
- **DEBUG_TRAINING**: Add throughput, GPU utilization
- **DEBUG_PERFORMANCE**: Add detailed component timing
- **DEBUG_PROFILING**: Add memory layout, kernel timing

---

This plan will ensure maximum RTX 5090 32GB utilization and achieve 40x throughput speedup while maintaining training quality.
