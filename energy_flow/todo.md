# Energy Flow Training Pipeline - Analysis & Optimization Plan

## üîç Detailed Pipeline Analysis

### **1. Data Loading & Preparation**

#### `load_experiment_dataset()` - CRITICAL ISSUES

```python
dataset = torch.load(dataset_path, map_location='cuda', weights_only=False)
```

**Problems:**

- ‚ùå **Blocking load**: Entire dataset (31MB) loaded to GPU memory at once
- ‚ùå **Memory underutilization**: With RTX 5090 32GB can load 100+ datasets in parallel
- ‚ùå **No streaming**: Cannot work with datasets > GPU memory

**RTX 5090 Optimizations:**

- ‚úÖ Streaming batch loading: disk ‚Üí CPU ‚Üí GPU pipeline
- ‚úÖ Prefetch multiple datasets to GPU memory
- ‚úÖ `torch.utils.data.DataLoader` with `pin_memory=True`, `prefetch_factor=4`

**Logging:**

```python
logger.log(DEBUG_MEMORY, f"Dataset loaded: {size_mb:.1f}MB, GPU mem: {torch.cuda.memory_allocated()/1e9:.1f}GB")
```

#### `ExperimentDatasetWrapper` - BOTTLENECKS

```python
def __getitem__(self, idx):
    input_text, target_text = self.dataset['text_pairs'][idx]
    return {
        'input_embedding': self.dataset['input_embeddings'][idx],  # Tensor copying!
        'target_embedding': self.dataset['target_embeddings'][idx],
        'input_text': input_text,
        'target_text': target_text
    }
```

**Problems:**

- ‚ùå **Tensor copying**: `__getitem__` copies tensors each time ‚Üí slow
- ‚ùå **No batch access**: Single element access only
- ‚ùå **Suboptimal collate_fn**: `torch.stack()` creates new tensors

**Optimizations:**

- ‚úÖ Pre-batching: `torch.split()` dataset into batches
- ‚úÖ Memory-mapped files for large datasets
- ‚úÖ Async loading of next batches

---

### **2. Experiment Configuration**

#### `create_experiment_config()` - RTX 5090 UNDERUTILIZATION

```python
return EnergyConfig(
    lattice_width=50,      # 2500 cells on surface
    lattice_height=50,     # Total: 50K cells
    lattice_depth=20,
    max_active_flows=5000, # Potential bottleneck
    batch_size=16,         # TOO SMALL for 32GB!
    carrier_hidden_size=512,  # Can increase
    carrier_num_layers=2,
)
```

**Performance Issues:**

- ‚ùå **Small batch_size=16**: RTX 5090 can handle 64-128
- ‚ùå **Conservative lattice sizes**: 50x50x20 = 50K cells, can do 200K+
- ‚ùå **Underutilized parallelism**: `max_active_flows=5000` may be bottleneck

**RTX 5090 Optimized Configuration:**

```python
def create_rtx5090_config() -> EnergyConfig:
    """Maximum optimized config for RTX 5090 32GB"""
    return EnergyConfig(
        lattice_width=80,           # 6400 cells on surface
        lattice_height=80,          # Total: ~400K cells
        lattice_depth=60,
        max_active_flows=20000,     # Increase parallelism
        batch_size=64,              # Maximum memory utilization
        carrier_hidden_size=1024,   # More parameters = better quality
        carrier_num_layers=3,

        # Memory optimizations
        gradient_accumulation_steps=4,  # Effective batch_size = 256
        use_mixed_precision=True,       # bfloat16 for memory savings
        dataloader_num_workers=12,      # Parallel data loading
    )
```

---

### **3. Main Training Loop**

#### `EnergyTrainer.train_step()` - MAJOR BOTTLENECKS

**Step 1: Cube Forward Pass**

```python
cube_output_surface = self.flow_processor.forward(teacher_input_embeddings, max_steps=50)
```

**Detailed Performance Breakdown:**

1. **EmbeddingMapper** (768‚Üí2500): ~0.1ms, parallel
2. **FlowProcessor.forward()** (main time consumer):
   ```python
   for step in range(max_steps):  # 50 iterations
       # For each active flow:
       for flow in active_flows:  # Up to 5000 flows
           neuron_output = simple_neuron(cell_state)      # ~0.01ms √ó 5000 = 50ms
           energy_output = energy_carrier(neuron_output)  # ~0.1ms √ó 5000 = 500ms
           # Update positions and energies: ~0.01ms √ó 5000 = 50ms
   ```
   **Total: 50 √ó (50 + 500 + 50) = 30,000ms = 30 seconds per batch!**

**CRITICAL Issues:**

- ‚ùå **Fixed max_steps=50**: No adaptive stopping
- ‚ùå **Sequential processing**: Each step waits for previous completion
- ‚ùå **Suboptimal batching**: Flows processed individually
- ‚ùå **Computation duplication**: Repeated forward passes in text_bridge

**Optimizations:**

```python
# Adaptive stopping
convergence_threshold = 0.01
for step in range(max_steps):
    if self.check_convergence(active_flows, convergence_threshold):
        logger.log(DEBUG_CONVERGENCE, f"Converged at step {step}")
        break

# Vectorized flow processing
flow_outputs = self.process_flows_vectorized(active_flows)  # All at once

# Mixed precision
with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
    cube_output_surface = self.flow_processor.forward(...)
```

**Step 2: Text Bridge - CATASTROPHIC Performance**

```python
# Current code:
for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    surface_input = teacher_input_embeddings[i:i+1]
    surface_output = self.flow_processor.forward(surface_input, max_steps=50)  # REPEATED forward!
    pred_texts_batch = self.text_decoder.decode_surface(surface_output)
```

**Problems:**

- ‚ùå **O(batch_size¬≤) complexity**: Repeated forward pass for each text
- ‚ùå **Duplication**: `flow_processor.forward()` called batch_size+1 times
- ‚ùå **Sequential processing**: No batch text processing

**Optimization:**

```python
# Batch processing
encoder_outputs = self.text_encoder.encode_text(input_texts)  # All at once
# Use already computed cube_output_surface!
predicted_texts = self.text_decoder.decode_surface_batch(cube_output_surface)
```

**Potential speedup: 16x (batch size)**

---

### **4. Logging & Metrics System**

#### Current Issues

```python
# Too frequent logging in production:
logger.log(DEBUG_TRAINING, f"üîÑ Starting train_step: batch_size={batch_size}")
logger.log(DEBUG_TRAINING, f"üìä Teacher embeddings: {teacher_input_embeddings.shape}")
```

**Problems:**

- ‚ùå **Production overhead**: Each train_step writes 20+ log entries
- ‚ùå **No conditional metrics**: Metrics computed always, even when not needed
- ‚ùå **Missing performance metrics**: No throughput, GPU utilization

**Optimized System:**

```python
# Conditional metrics through logging levels
if logger.isEnabledFor(DEBUG_TRAINING):
    # Expensive computations only for debug
    gpu_util = torch.cuda.utilization()
    memory_used = torch.cuda.memory_allocated() / 1e9
    logger.log(DEBUG_TRAINING, f"GPU: {gpu_util}%, Memory: {memory_used:.1f}GB")

# Throughput metrics
if logger.isEnabledFor(DEBUG_PERFORMANCE):
    tokens_per_second = batch_size * seq_len / step_time
    logger.log(DEBUG_PERFORMANCE, f"Throughput: {tokens_per_second:.0f} tokens/s")
```

**New Logging Levels:**

- `DEBUG_PERFORMANCE = 21`: Throughput, GPU utilization, memory usage
- `DEBUG_PROFILING = 22`: Detailed component execution times

---

### **5. RTX 5090 Memory Usage Assessment**

#### Current Usage (experiment config):

```
Model:
- FlowProcessor: ~10M params √ó 4 bytes = 40MB
- EnergyCarrier (GRU): ~3M params √ó 4 bytes = 12MB
- TextBridge: ~5M params √ó 4 bytes = 20MB
Total model: ~72MB

Data (batch_size=16):
- Input embeddings: 16 √ó 768 √ó 4 = 48KB
- Activations: 16 √ó 50K cells √ó 4 ‚âà 3.2MB
- Gradients: ~72MB (same as model)
Total: ~150MB

TOTAL USAGE: ~222MB out of 32GB = 0.7%!
```

#### Optimized Usage (RTX 5090 config):

```
Model (increased):
- FlowProcessor: ~50M params = 200MB
- EnergyCarrier: ~15M params = 60MB
- TextBridge: ~20M params = 80MB
Total model: ~340MB

Data (batch_size=64):
- Input embeddings: 64 √ó 768 √ó 4 = 192KB
- Activations: 64 √ó 400K cells √ó 4 ‚âà 100MB
- Gradients: ~340MB
- Dataset cache: ~500MB (multiple datasets)
Total: ~1.28GB

TOTAL USAGE: ~1.28GB out of 32GB = 4%
POTENTIAL for increase: 8x current size!
```

---

## üìã Optimization Plan (Priority Order)

### **üî• CRITICAL PRIORITY**

#### 1. **Text Bridge Refactoring** (potential: 16x speedup) ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω

- [ ] Batch processing for `encode_text()` and `decode_surface()`
- [ ] Eliminate duplicated forward passes
- [ ] Cache intermediate results

#### 2. **Adaptive max_steps** (potential: 2-3x speedup) ‚úÖ **–ó–ê–í–ï–†–®–ï–ù (8.33x —É—Å–∫–æ—Ä–µ–Ω–∏–µ!)**

- [x] Convergence checking for energy flows - convergence_threshold = 0.95
- [x] Early stopping at threshold values - patience = 3 steps
- [x] Dynamic step count adjustment - —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ 44/50 —à–∞–≥–æ–≤
- [x] **–†–µ–∑—É–ª—å—Ç–∞—Ç: 8.33x speedup** (99% –ø–æ—Ç–æ–∫–æ–≤ –∑–∞ 6 —à–∞–≥–æ–≤ –≤–º–µ—Å—Ç–æ 50)

#### 3. **FlowProcessor Vectorization** (potential: 5x speedup)

- [ ] Parallel processing of all flows
- [ ] Loop optimization via `torch.vmap()`
- [ ] Eliminate sequential dependencies

### **üöÄ HIGH PRIORITY**

#### 4. **RTX 5090 Configuration** (potential: 4x throughput)

- [ ] Increase batch_size to 64-128
- [ ] Increase lattice sizes to 80x80x60
- [ ] Gradient accumulation for effective batch_size = 256

#### 5. **DataLoader Optimization** (potential: 2x I/O speedup)

- [ ] `num_workers=12`, `pin_memory=True`, `persistent_workers=True`
- [ ] Prefetch next batches
- [ ] Async data loading

#### 6. **Mixed Precision Training** (potential: 1.5x speedup, 50% memory)

- [ ] `torch.autocast` for forward pass
- [ ] bfloat16 for activations, float32 for gradients
- [ ] Gradient scaling for stability

### **üìä MEDIUM PRIORITY**

#### 7. **Metrics & Profiling System**

- [ ] Conditional metrics through logging levels
- [ ] Built-in profiler with `torch.profiler`
- [ ] Real-time GPU utilization monitoring

#### 8. **Checkpoint System**

- [ ] Save performance metrics
- [ ] Automatic cleanup of old checkpoints
- [ ] Checkpoint compression for space saving

### **üîß LOW PRIORITY**

#### 9. **torch.compile() Optimization**

- [ ] Compile core components
- [ ] Fuse operations into kernels
- [ ] Optimize memory layout

#### 10. **Advanced Techniques**

- [ ] Gradient checkpointing for large models
- [ ] Flash Attention for text bridge
- [ ] Custom CUDA kernels for specific operations

---

## üéØ Expected Results

### **Before Optimization (current state):**

- Batch size: 16
- Time per batch: ~30 seconds
- GPU utilization: ~10-20%
- Memory usage: ~0.7% (222MB of 32GB)
- Throughput: ~0.5 samples/second

### **After Optimization (RTX 5090):**

- Batch size: 64 (4x increase)
- Time per batch: ~3 seconds (10x speedup)
- GPU utilization: ~80-90%
- Memory usage: ~4% (1.3GB of 32GB)
- Throughput: ~20 samples/second (40x increase)

**TOTAL SPEEDUP: 40x throughput with 4x better RTX 5090 utilization**

---

## üî¨ Metrics to Track

### **Performance Metrics:**

```python
# Add to train_step():
step_metrics = {
    'throughput_samples_per_sec': batch_size / step_time,
    'gpu_utilization_percent': torch.cuda.utilization(),
    'memory_used_gb': torch.cuda.memory_allocated() / 1e9,
    'memory_utilization_percent': torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100,
    'flow_convergence_steps': actual_steps_used,
    'text_bridge_time_ms': text_bridge_time * 1000,
    'energy_computation_time_ms': energy_time * 1000,
}
```

### **Logging Level System for Metrics:**

- **Production** (INFO): Only basic metrics (loss, epoch time)
- **DEBUG_TRAINING**: Add throughput, GPU utilization
- **DEBUG_PERFORMANCE**: Add detailed component timing
- **DEBUG_PROFILING**: Add memory layout, kernel timing

---

This plan will ensure maximum RTX 5090 32GB utilization and achieve 40x throughput speedup while maintaining training quality.
