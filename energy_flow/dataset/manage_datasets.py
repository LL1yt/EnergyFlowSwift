#!/usr/bin/env python3
"""
Dataset Management Utilities
=============================

–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ energy_flow:
- –ê–Ω–∞–ª–∏–∑ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
- –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–µ–π –∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–æ–º
- –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import sys
import argparse
import torch
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import hashlib

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path  
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from energy_flow.dataset.generator import create_dataset_generator
from energy_flow.utils.logging import get_logger

logger = get_logger(__name__)


class DatasetManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ energy_flow"""
    
    def __init__(self, active_dir: str = "data/energy_flow/active", 
                 archive_dir: str = "data/energy_flow/archive"):
        self.active_dir = Path(active_dir)
        self.archive_dir = Path(archive_dir)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        self.active_dir.mkdir(parents=True, exist_ok=True)
        self.archive_dir.mkdir(parents=True, exist_ok=True)
    
    def list_datasets(self, include_archive: bool = False, detailed: bool = False) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        datasets = []
        
        # –ê–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        for file_path in self.active_dir.glob("*.pt"):
            info = self._analyze_dataset(file_path, "active", detailed)
            if info:
                datasets.append(info)
        
        # –ê—Ä—Ö–∏–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        if include_archive:
            for file_path in self.archive_dir.rglob("*.pt"):
                info = self._analyze_dataset(file_path, "archive", detailed)
                if info:
                    datasets.append(info)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
        datasets.sort(key=lambda x: x.get('creation_time', 0), reverse=True)
        
        return datasets
    
    def _analyze_dataset(self, file_path: Path, category: str, detailed: bool = False) -> Optional[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
            stat = file_path.stat()
            info = {
                'filename': file_path.name,
                'filepath': str(file_path),
                'category': category,
                'file_size_mb': stat.st_size / 1024 / 1024,
                'creation_time': stat.st_mtime,
                'creation_date': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
            }
            
            if not detailed:
                return info
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            try:
                data = torch.load(file_path, map_location='cpu')
                
                if isinstance(data, dict):
                    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–º–µ—Ä–µ –¥–∞–Ω–Ω—ã—Ö  
                    if 'input_embeddings' in data and 'target_embeddings' in data:
                        input_emb = data['input_embeddings']
                        target_emb = data['target_embeddings']
                        
                        info.update({
                            'sample_count': input_emb.shape[0],
                            'embedding_dimension': input_emb.shape[1],
                            'input_shape': list(input_emb.shape),
                            'target_shape': list(target_emb.shape),
                        })
                        
                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                        info.update({
                            'input_norm_mean': input_emb.norm(dim=1).mean().item(),
                            'input_norm_std': input_emb.norm(dim=1).std().item(),
                            'target_norm_mean': target_emb.norm(dim=1).mean().item(),
                            'target_norm_std': target_emb.norm(dim=1).std().item(),
                        })
                    
                    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    if 'generation_info' in data:
                        gen_info = data['generation_info']
                        info.update({
                            'mode': gen_info.get('mode', 'unknown'),
                            'sources': gen_info.get('sources', []),
                            'generation_time': gen_info.get('generation_time', 0),
                            'target_pairs': gen_info.get('target_pairs', 0),
                            'generation_timestamp': gen_info.get('generation_timestamp', 'unknown')
                        })
                    
                    # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    if 'system_info' in data:
                        sys_info = data['system_info']
                        info.update({
                            'torch_version': sys_info.get('torch_version', 'unknown'),
                            'device_used': sys_info.get('device', 'unknown'),
                            'cuda_available': sys_info.get('cuda_available', False)
                        })
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞  
                    if 'dataset_stats' in data:
                        ds_stats = data['dataset_stats']
                        info['dataset_stats'] = ds_stats
                    
                    # Hash –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–µ–π
                    info['content_hash'] = self._calculate_content_hash(data)
                    
                else:
                    info['data_type'] = type(data).__name__
                    info['valid_format'] = False
                    
            except Exception as e:
                info['load_error'] = str(e)
                info['valid_format'] = False
            
            return info
            
        except Exception as e:
            logger.warning(f"Failed to analyze {file_path.name}: {e}")
            return None
    
    def _calculate_content_hash(self, data: Dict) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö—ç—à–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–µ–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–æ–≤ –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ö—ç—à–∞
            hash_data = {
                'input_shape': data['input_embeddings'].shape if 'input_embeddings' in data else None,
                'target_shape': data['target_embeddings'].shape if 'target_embeddings' in data else None,
                'sample_values': data['input_embeddings'][:5].sum().item() if 'input_embeddings' in data else 0
            }
            
            hash_str = json.dumps(hash_data, sort_keys=True)
            return hashlib.md5(hash_str.encode()).hexdigest()[:8]
            
        except Exception:
            return "unknown"
    
    def get_dataset_info(self, filename: str, detailed: bool = True) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        # –ò—â–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã—Ö
        active_path = self.active_dir / filename
        if active_path.exists():
            return self._analyze_dataset(active_path, "active", detailed)
        
        # –ò—â–µ–º –≤ –∞—Ä—Ö–∏–≤–µ
        for file_path in self.archive_dir.rglob(filename):
            if file_path.name == filename:
                return self._analyze_dataset(file_path, "archive", detailed)
        
        return None
    
    def find_duplicates(self) -> List[List[Dict[str, Any]]]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        logger.info("üîç Searching for duplicate datasets...")
        
        datasets = self.list_datasets(include_archive=True, detailed=True)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ö—ç—à—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        hash_groups = {}
        for dataset in datasets:
            content_hash = dataset.get('content_hash', 'unknown')
            if content_hash != 'unknown':
                if content_hash not in hash_groups:
                    hash_groups[content_hash] = []
                hash_groups[content_hash].append(dataset)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥—Ä—É–ø–ø—ã —Å –¥—É–±–ª—è–º–∏
        duplicates = [group for group in hash_groups.values() if len(group) > 1]
        
        logger.info(f"Found {len(duplicates)} groups of duplicates")
        return duplicates
    
    def cleanup_duplicates(self, keep_newest: bool = True, dry_run: bool = True) -> Dict[str, Any]:
        """–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ñ–∞–π–ª–æ–≤"""
        duplicates = self.find_duplicates()
        
        if not duplicates:
            return {'removed_count': 0, 'saved_space_mb': 0, 'errors': []}
        
        removed_files = []
        errors = []
        saved_space_mb = 0
        
        for group in duplicates:
            try:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
                sorted_group = sorted(group, key=lambda x: x['creation_time'], reverse=keep_newest)
                
                # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π (—Å–∞–º—ã–π –Ω–æ–≤—ã–π/—Å—Ç–∞—Ä—ã–π), —É–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
                keep_file = sorted_group[0]
                remove_files = sorted_group[1:]
                
                logger.info(f"Keeping: {keep_file['filename']}")
                
                for file_info in remove_files:
                    file_path = Path(file_info['filepath'])
                    
                    if dry_run:
                        logger.info(f"Would remove: {file_info['filename']} ({file_info['file_size_mb']:.1f} MB)")
                        saved_space_mb += file_info['file_size_mb']
                    else:
                        try:
                            file_path.unlink()
                            removed_files.append(file_info['filename'])
                            saved_space_mb += file_info['file_size_mb']
                            logger.info(f"Removed: {file_info['filename']}")
                        except Exception as e:
                            error_msg = f"Failed to remove {file_info['filename']}: {e}"
                            errors.append(error_msg)
                            logger.error(error_msg)
                            
            except Exception as e:
                error_msg = f"Error processing duplicate group: {e}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        return {
            'removed_count': len(removed_files),
            'removed_files': removed_files,
            'saved_space_mb': saved_space_mb,
            'errors': errors,
            'dry_run': dry_run
        }
    
    def validate_datasets(self) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å"""
        logger.info("üîç Validating all datasets...")
        
        datasets = self.list_datasets(include_archive=True, detailed=True)
        
        valid_datasets = []
        invalid_datasets = []
        
        for dataset in datasets:
            if dataset.get('valid_format', True) and dataset.get('sample_count', 0) > 0:
                valid_datasets.append(dataset)
            else:
                invalid_datasets.append(dataset)
        
        return {
            'total_datasets': len(datasets),
            'valid_count': len(valid_datasets),
            'invalid_count': len(invalid_datasets),
            'invalid_datasets': invalid_datasets
        }
    
    def archive_by_pattern(self, pattern: str, max_age_days: Optional[int] = None) -> Dict[str, Any]:
        """–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É –∏–º–µ–Ω–∏ –∏–ª–∏ –≤–æ–∑—Ä–∞—Å—Ç—É"""
        logger.info(f"üìÅ Archiving datasets matching pattern: {pattern}")
        
        archived_files = []
        errors = []
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –∞—Ä—Ö–∏–≤–∞ –ø–æ –¥–∞—Ç–µ
        archive_subdir = self.archive_dir / datetime.now().strftime("%Y-%m")
        archive_subdir.mkdir(parents=True, exist_ok=True)
        
        current_time = datetime.now().timestamp()
        cutoff_time = current_time - (max_age_days * 24 * 60 * 60) if max_age_days else 0
        
        for file_path in self.active_dir.glob(pattern):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                if max_age_days and file_path.stat().st_mtime > cutoff_time:
                    continue
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∞—Ä—Ö–∏–≤
                archive_path = archive_subdir / file_path.name
                file_path.rename(archive_path)
                
                archived_files.append({
                    'filename': file_path.name,
                    'archive_path': str(archive_path)
                })
                
                logger.info(f"Archived: {file_path.name}")
                
            except Exception as e:
                error_msg = f"Failed to archive {file_path.name}: {e}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        return {
            'archived_count': len(archived_files),
            'archived_files': archived_files,
            'archive_directory': str(archive_subdir),
            'errors': errors
        }
    
    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        datasets = self.list_datasets(include_archive=True, detailed=True)
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        active_datasets = [d for d in datasets if d['category'] == 'active']
        archive_datasets = [d for d in datasets if d['category'] == 'archive']
        
        total_size_mb = sum(d['file_size_mb'] for d in datasets)
        active_size_mb = sum(d['file_size_mb'] for d in active_datasets)
        archive_size_mb = sum(d['file_size_mb'] for d in archive_datasets)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∂–∏–º–∞–º
        mode_stats = {}
        for dataset in datasets:
            mode = dataset.get('mode', 'unknown')
            if mode not in mode_stats:
                mode_stats[mode] = {'count': 0, 'size_mb': 0}
            mode_stats[mode]['count'] += 1
            mode_stats[mode]['size_mb'] += dataset['file_size_mb']
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        source_stats = {}
        for dataset in datasets:
            sources = dataset.get('sources', [])
            sources_key = '+'.join(sorted(sources)) if sources else 'unknown'
            if sources_key not in source_stats:
                source_stats[sources_key] = {'count': 0, 'size_mb': 0}
            source_stats[sources_key]['count'] += 1
            source_stats[sources_key]['size_mb'] += dataset['file_size_mb']
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report_lines = [
            "="*60,
            "ENERGY FLOW DATASETS MANAGEMENT REPORT",
            "="*60,
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "üìä OVERVIEW:",
            f"  Total datasets: {len(datasets)}",
            f"  Active: {len(active_datasets)} ({active_size_mb:.1f} MB)",
            f"  Archive: {len(archive_datasets)} ({archive_size_mb:.1f} MB)",
            f"  Total size: {total_size_mb:.1f} MB",
            "",
            "üè∑Ô∏è BY MODE:",
        ]
        
        for mode, stats in sorted(mode_stats.items()):
            report_lines.append(f"  {mode}: {stats['count']} files ({stats['size_mb']:.1f} MB)")
        
        report_lines.extend([
            "",
            "üìã BY SOURCES:",
        ])
        
        for sources, stats in sorted(source_stats.items()):
            report_lines.append(f"  {sources}: {stats['count']} files ({stats['size_mb']:.1f} MB)")
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        if active_datasets:
            report_lines.extend([
                "",
                "üïí RECENT ACTIVE DATASETS:",
            ])
            
            for dataset in active_datasets[:5]:
                report_lines.append(
                    f"  {dataset['filename']} "
                    f"({dataset.get('sample_count', 'N/A'):,} samples, "
                    f"{dataset['file_size_mb']:.1f} MB, "
                    f"{dataset['creation_date']})"
                )
        
        report_lines.extend([
            "",
            "="*60
        ])
        
        return "\n".join(report_lines)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = argparse.ArgumentParser(
        description="Energy Flow Dataset Management Utilities",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("--list", action="store_true", help="List all datasets")
    parser.add_argument("--list-archive", action="store_true", help="Include archive in listing")
    parser.add_argument("--info", metavar="FILENAME", help="Show detailed info about specific dataset")
    parser.add_argument("--duplicates", action="store_true", help="Find duplicate datasets")
    parser.add_argument("--cleanup", action="store_true", help="Remove duplicate datasets")
    parser.add_argument("--validate", action="store_true", help="Validate all datasets")
    parser.add_argument("--report", action="store_true", help="Generate comprehensive report")
    parser.add_argument("--archive-pattern", metavar="PATTERN", help="Archive datasets matching pattern")
    parser.add_argument("--archive-older", type=int, metavar="DAYS", help="Archive datasets older than N days")
    parser.add_argument("--active-dir", default="data/energy_flow/active", help="Active datasets directory")
    parser.add_argument("--archive-dir", default="data/energy_flow/archive", help="Archive directory")
    parser.add_argument("--dry-run", action="store_true", help="Dry run mode (don't actually delete files)")
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
    manager = DatasetManager(args.active_dir, args.archive_dir)
    
    try:
        if args.list or args.list_archive:
            datasets = manager.list_datasets(include_archive=args.list_archive, detailed=True)
            
            if not datasets:
                print("üì≠ No datasets found")
                return
            
            print(f"üìã Found {len(datasets)} datasets:")
            print("-" * 50)
            
            for dataset in datasets:
                status = "üü¢" if dataset['category'] == 'active' else "üì¶"
                print(f"{status} {dataset['filename']}")
                print(f"   üìä {dataset.get('sample_count', 'N/A'):,} samples, "
                      f"{dataset['file_size_mb']:.1f} MB")
                print(f"   üè∑Ô∏è {dataset.get('mode', 'unknown')} / "
                      f"{', '.join(dataset.get('sources', []))}")
                print(f"   üïí {dataset['creation_date']}")
                print()
        
        elif args.info:
            info = manager.get_dataset_info(args.info, detailed=True)
            if info:
                print(f"üìã Dataset Information: {args.info}")
                print("-" * 50)
                print(json.dumps(info, indent=2, default=str))
            else:
                print(f"‚ùå Dataset not found: {args.info}")
        
        elif args.duplicates:
            duplicates = manager.find_duplicates()
            if duplicates:
                print(f"üîç Found {len(duplicates)} groups of duplicates:")
                for i, group in enumerate(duplicates, 1):
                    print(f"\nGroup {i}:")
                    for dataset in group:
                        print(f"  - {dataset['filename']} ({dataset['category']}, "
                              f"{dataset['file_size_mb']:.1f} MB)")
            else:
                print("‚úÖ No duplicates found")
        
        elif args.cleanup:
            result = manager.cleanup_duplicates(dry_run=args.dry_run)
            
            if args.dry_run:
                print(f"üîç Dry run - would remove {result['removed_count']} files")
                print(f"üíæ Would save {result['saved_space_mb']:.1f} MB")
            else:
                print(f"üóëÔ∏è Removed {result['removed_count']} duplicate files")
                print(f"üíæ Saved {result['saved_space_mb']:.1f} MB")
            
            if result['errors']:
                print(f"\n‚ö†Ô∏è Errors ({len(result['errors'])}):")
                for error in result['errors']:
                    print(f"  ‚ùå {error}")
        
        elif args.validate:
            result = manager.validate_datasets()
            print(f"üîç Dataset Validation Results:")
            print(f"  Total: {result['total_datasets']}")
            print(f"  Valid: {result['valid_count']} ‚úÖ")
            print(f"  Invalid: {result['invalid_count']} ‚ùå")
            
            if result['invalid_datasets']:
                print(f"\n‚ùå Invalid datasets:")
                for dataset in result['invalid_datasets']:
                    reason = dataset.get('load_error', 'Unknown error')
                    print(f"  - {dataset['filename']}: {reason}")
        
        elif args.report:
            report = manager.generate_report()
            print(report)
        
        elif args.archive_pattern:
            result = manager.archive_by_pattern(
                args.archive_pattern, 
                args.archive_older
            )
            
            print(f"üìÅ Archived {result['archived_count']} files")
            if result['archived_files']:
                for file_info in result['archived_files']:
                    print(f"  üìÅ {file_info['filename']}")
            
            if result['errors']:
                print(f"\n‚ö†Ô∏è Errors:")
                for error in result['errors']:
                    print(f"  ‚ùå {error}")
        
        else:
            parser.print_help()
    
    except Exception as e:
        logger.error(f"Error in dataset management: {e}")
        print(f"‚ùå Error: {e}")


if __name__ == "__main__":
    main()