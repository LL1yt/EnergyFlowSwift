"""
Dataset Generator –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
===========================================================

–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏:
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–æ–º
"""

import torch
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
import time
from datetime import datetime
import json

from .config import DatasetConfig, create_dataset_config_from_energy
from .manager import DatasetManager, create_dataset_manager
from ..config import EnergyConfig
from ..utils.logging import get_logger

logger = get_logger(__name__)


@dataclass
class GeneratorConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    mode: str = "experiment"  # debug, experiment, production, custom
    target_pairs: int = 5000
    sources: List[str] = field(default_factory=lambda: ["precomputed", "snli"])
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã SNLI
    snli_fraction: float = 0.2
    snli_min_text_length: int = 10
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    normalize_embeddings: bool = True
    shuffle_data: bool = True
    validate_data: bool = True
    
    # –ü—É—Ç–∏ –∏ –∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
    output_dir: str = "data/energy_flow/active"
    archive_dir: str = "data/energy_flow/archive"
    name_template: str = "{mode}_{sources}_{count}pairs_{timestamp}"
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
    save_text_pairs: bool = True  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    save_metadata: bool = True
    max_file_size_mb: int = 1024  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    
    def __post_init__(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        assert self.mode in ["debug", "experiment", "production", "custom"], \
            f"Invalid mode: {self.mode}"
        assert self.target_pairs > 0, "target_pairs must be > 0"
        assert 0 < self.snli_fraction <= 1.0, "snli_fraction must be in (0, 1]"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.archive_dir).mkdir(parents=True, exist_ok=True)
    
    def get_sources_string(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        if set(self.sources) == {"precomputed", "snli"}:
            return "mixed"
        elif self.sources == ["snli"]:
            return "snli"
        elif self.sources == ["precomputed"]:
            return "precomputed"
        else:
            return "custom"
    
    def generate_filename(self, actual_count: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        sources_str = self.get_sources_string()
        
        filename = self.name_template.format(
            mode=self.mode,
            sources=sources_str,
            count=actual_count,
            timestamp=timestamp
        )
        
        return f"{filename}.pt"


# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
def create_debug_generator_config() -> GeneratorConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    return GeneratorConfig(
        mode="debug",
        target_pairs=500,
        sources=["precomputed"],  # –¢–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        snli_fraction=0.05,
        max_file_size_mb=64
    )


def create_experiment_generator_config() -> GeneratorConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    return GeneratorConfig(
        mode="experiment", 
        target_pairs=5000,
        sources=["precomputed", "snli"],
        snli_fraction=0.2,
        max_file_size_mb=256
    )


def create_production_generator_config() -> GeneratorConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω –æ–±—É—á–µ–Ω–∏—è"""
    return GeneratorConfig(
        mode="production",
        target_pairs=50000,
        sources=["precomputed", "snli"],
        snli_fraction=0.3,
        max_file_size_mb=1024
    )


class DatasetGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è energy_flow
    
    –°–æ–∑–¥–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –Ω–∞–ø—Ä—è–º—É—é –∑–∞–≥—Ä—É–∂–∞—Ç—å
    –≤ training —Å–∫—Ä–∏–ø—Ç–∞—Ö –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    """
    
    def __init__(self, config: GeneratorConfig, energy_config: Optional[EnergyConfig] = None):
        """
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
            energy_config: –û—Å–Ω–æ–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è energy_flow
        """
        self.config = config
        self.energy_config = energy_config
        
        # –°–æ–∑–¥–∞–µ–º DatasetManager –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        dataset_config = self._create_dataset_config()
        self.dataset_manager = create_dataset_manager(dataset_config, energy_config)
        
        logger.info(f"üîß DatasetGenerator initialized: mode={config.mode}, "
                   f"target={config.target_pairs} pairs")
    
    def _create_dataset_config(self) -> DatasetConfig:
        """–°–æ–∑–¥–∞–Ω–∏–µ DatasetConfig –Ω–∞ –æ—Å–Ω–æ–≤–µ GeneratorConfig"""
        dataset_config_params = {
            'dataset_sources': self.config.sources,
            'max_samples_per_source': self.config.target_pairs // len(self.config.sources) if len(self.config.sources) > 1 else self.config.target_pairs,
            'snli_fraction': self.config.snli_fraction,
            'snli_min_text_length': self.config.snli_min_text_length,
            'normalize_embeddings': self.config.normalize_embeddings,
            'shuffle_data': self.config.shuffle_data,
            'validate_embeddings': self.config.validate_data
        }
        
        if self.energy_config:
            return create_dataset_config_from_energy(self.energy_config, **dataset_config_params)
        else:
            return DatasetConfig(**dataset_config_params)
    
    def generate_dataset(self, custom_name: Optional[str] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        
        Args:
            custom_name: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
        """
        logger.info(f"üöÄ Starting dataset generation: {self.config.mode} mode")
        start_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã
        validation = self.dataset_manager.validate_setup()
        if not validation['overall_status']:
            raise RuntimeError(f"Dataset system not ready: {validation['errors']}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        logger.info("üìä Preparing dataset...")
        dataset = self.dataset_manager.prepare_dataset()
        if not dataset:
            raise RuntimeError("Failed to prepare dataset")
        
        actual_count = len(dataset)
        logger.info(f"‚úÖ Dataset prepared: {actual_count} samples")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
        estimated_size_mb = self._estimate_file_size(dataset)
        if estimated_size_mb > self.config.max_file_size_mb:
            logger.warning(f"‚ö†Ô∏è Estimated file size {estimated_size_mb:.1f}MB exceeds limit {self.config.max_file_size_mb}MB")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        if custom_name:
            filename = f"{custom_name}.pt"
        else:
            filename = self.config.generate_filename(actual_count)
        
        output_path = Path(self.config.output_dir) / filename
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_data = self._prepare_save_data(dataset, actual_count, start_time)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        logger.info(f"üíæ Saving dataset: {filename}")
        torch.save(save_data, output_path)
        
        generation_time = time.time() - start_time
        actual_size_mb = output_path.stat().st_size / 1024 / 1024
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        result = {
            'filename': filename,
            'filepath': str(output_path),
            'sample_count': actual_count,
            'file_size_mb': actual_size_mb,
            'generation_time': generation_time,
            'mode': self.config.mode,
            'sources': self.config.sources,
            'config': self.config.__dict__.copy()
        }
        
        logger.info(f"üéâ Dataset generated successfully:")
        logger.info(f"   File: {filename}")
        logger.info(f"   Samples: {actual_count:,}")
        logger.info(f"   Size: {actual_size_mb:.1f} MB")
        logger.info(f"   Time: {generation_time:.1f}s")
        
        return result
    
    def _prepare_save_data(self, dataset, actual_count: int, start_time: float) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        save_data = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            'input_embeddings': dataset.input_embeddings,
            'target_embeddings': dataset.target_embeddings,
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            'generation_info': {
                'mode': self.config.mode,
                'sources': self.config.sources,
                'target_pairs': self.config.target_pairs,
                'actual_pairs': actual_count,
                'generation_timestamp': datetime.now().isoformat(),
                'generation_time': time.time() - start_time,
                'config': self.config.__dict__.copy()
            },
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'dataset_stats': self.dataset_manager.get_statistics()
        }
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã
        if self.config.save_text_pairs:
            save_data['text_pairs'] = dataset.text_pairs
            save_data['metadata'] = dataset.metadata
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        if self.config.save_metadata:
            save_data['system_info'] = {
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'device': str(dataset.input_embeddings.device),
                'embedding_dimension': dataset.input_embeddings.shape[1]
            }
        
        return save_data
    
    def _estimate_file_size(self, dataset) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –≤ MB"""
        # –†–∞–∑–º–µ—Ä —Ç–µ–Ω–∑–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings_size = dataset.input_embeddings.numel() + dataset.target_embeddings.numel()
        embeddings_mb = embeddings_size * 4 / 1024 / 1024  # float32 = 4 bytes
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø—Ä–∏–º–µ—Ä–Ω–æ 20% –æ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        total_mb = embeddings_mb * 1.2
        
        return total_mb
    
    def list_available_datasets(self, include_archive: bool = False) -> List[Dict[str, Any]]:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        datasets = []
        
        # –ê–∫—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        active_dir = Path(self.config.output_dir)
        if active_dir.exists():
            for file_path in active_dir.glob("*.pt"):
                info = self._get_dataset_info(file_path, "active")
                if info:
                    datasets.append(info)
        
        # –ê—Ä—Ö–∏–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        if include_archive:
            archive_dir = Path(self.config.archive_dir)
            if archive_dir.exists():
                for file_path in archive_dir.rglob("*.pt"):  # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
                    info = self._get_dataset_info(file_path, "archive")
                    if info:
                        datasets.append(info)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        datasets.sort(key=lambda x: x.get('creation_time', 0), reverse=True)
        
        return datasets
    
    def _get_dataset_info(self, file_path: Path, category: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            data = torch.load(file_path, map_location='cpu')
            
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
            info = {
                'filename': file_path.name,
                'filepath': str(file_path),
                'category': category,
                'file_size_mb': file_path.stat().st_size / 1024 / 1024,
                'creation_time': file_path.stat().st_mtime
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
            if isinstance(data, dict):
                # –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
                if 'input_embeddings' in data:
                    info['sample_count'] = data['input_embeddings'].shape[0]
                    info['embedding_dimension'] = data['input_embeddings'].shape[1]
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                if 'generation_info' in data:
                    gen_info = data['generation_info']
                    info.update({
                        'mode': gen_info.get('mode', 'unknown'),
                        'sources': gen_info.get('sources', []),
                        'generation_time': gen_info.get('generation_time', 0),
                        'target_pairs': gen_info.get('target_pairs', 0)
                    })
            
            return info
            
        except Exception as e:
            logger.warning(f"‚ùå Failed to read dataset info from {file_path.name}: {e}")
            return None
    
    def archive_old_datasets(self, days_old: int = 7) -> Dict[str, Any]:
        """–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        logger.info(f"üóÇÔ∏è Archiving datasets older than {days_old} days...")
        
        current_time = time.time()
        cutoff_time = current_time - (days_old * 24 * 60 * 60)
        
        active_dir = Path(self.config.output_dir)
        archive_dir = Path(self.config.archive_dir)
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∞—Ä—Ö–∏–≤–∞ –ø–æ –¥–∞—Ç–µ
        archive_subdir = archive_dir / datetime.now().strftime("%Y-%m")
        archive_subdir.mkdir(parents=True, exist_ok=True)
        
        archived_files = []
        errors = []
        
        if active_dir.exists():
            for file_path in active_dir.glob("*.pt"):
                try:
                    if file_path.stat().st_mtime < cutoff_time:
                        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∞—Ä—Ö–∏–≤
                        archive_path = archive_subdir / file_path.name
                        file_path.rename(archive_path)
                        
                        archived_files.append({
                            'filename': file_path.name,
                            'archive_path': str(archive_path)
                        })
                        
                        logger.info(f"üìÅ Archived: {file_path.name}")
                        
                except Exception as e:
                    errors.append(f"Failed to archive {file_path.name}: {e}")
                    logger.error(f"‚ùå {errors[-1]}")
        
        result = {
            'archived_count': len(archived_files),
            'archived_files': archived_files,
            'errors': errors,
            'archive_directory': str(archive_subdir)
        }
        
        logger.info(f"‚úÖ Archiving completed: {len(archived_files)} files moved")
        
        return result


def create_dataset_generator(mode: str = "experiment", 
                           energy_config: Optional[EnergyConfig] = None) -> DatasetGenerator:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DatasetGenerator
    
    Args:
        mode: –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (debug, experiment, production, custom)
        energy_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è energy_flow
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π DatasetGenerator
    """
    if mode == "debug":
        config = create_debug_generator_config()
    elif mode == "experiment":
        config = create_experiment_generator_config()
    elif mode == "production":
        config = create_production_generator_config()
    else:
        config = GeneratorConfig(mode="custom")
    
    return DatasetGenerator(config, energy_config)