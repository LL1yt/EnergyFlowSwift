"""
PrecomputedProvider - –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
======================================================

–ê–¥–∞–ø—Ç–∞—Ü–∏—è legacy precomputed_embedding_loader.py –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É:
- –ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö .pt —Ñ–∞–π–ª–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
"""

import torch
from typing import List, Tuple, Optional, Dict, Any
from pathlib import Path
import json

from .base_provider import BaseDataProvider
from ...utils.logging import get_logger, DEBUG_INIT

logger = get_logger(__name__)


class PrecomputedProvider(BaseDataProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ .pt —Ñ–∞–π–ª–æ–≤
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
    - –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω–Ω—ã–µ generate_snli_embedding_dataset.py
    - Unified dataset loader —Ñ–∞–π–ª—ã
    - –ü—Ä–æ—Å—Ç—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    
    def __init__(self, config):
        super().__init__("Precomputed", config)
        
        self.embeddings_dir = config.get_absolute_embeddings_dir()
        self.cache_dir = config.get_absolute_cache_dir()
        
        # –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        self._loaded_files = {}
        self._file_metadata = {}
        
        logger.log(DEBUG_INIT, f"PrecomputedProvider: embeddings_dir={self.embeddings_dir}")
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≥–æ—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            # –ò—â–µ–º .pt —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            embedding_files = list(self.embeddings_dir.glob("*.pt"))
            
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            cache_files = []
            if self.cache_dir.exists():
                cache_files = list(self.cache_dir.glob("*.pt"))
            
            total_files = len(embedding_files) + len(cache_files)
            
            if total_files > 0:
                logger.info(f"‚úÖ Found {total_files} precomputed files: "
                           f"{len(embedding_files)} in embeddings, {len(cache_files)} in cache")
                return True
            else:
                logger.info("üìÅ No precomputed embedding files found")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Precomputed availability check failed: {e}")
            return False
    
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        logger.log(DEBUG_INIT, "Initializing precomputed provider...")
        
        try:
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self._scan_available_files()
            
            if self._file_metadata:
                logger.info(f"‚úÖ Precomputed provider initialized: {len(self._file_metadata)} files available")
                return True
            else:
                logger.warning("‚ö†Ô∏è No valid precomputed files found")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Precomputed provider initialization failed: {e}")
            return False
    
    def _scan_available_files(self):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        self._file_metadata.clear()
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if self.embeddings_dir.exists():
            self._scan_directory(self.embeddings_dir, "embeddings")
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –∫—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if self.cache_dir.exists():
            self._scan_directory(self.cache_dir, "cache")
        
        logger.info(f"üìä Scanned files: {len(self._file_metadata)} valid precomputed files")
    
    def _scan_directory(self, directory: Path, source_type: str):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        for file_path in directory.glob("*.pt"):
            try:
                # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
                metadata = self._extract_file_metadata(file_path, source_type)
                if metadata:
                    self._file_metadata[str(file_path)] = metadata
                    
            except Exception as e:
                logger.warning(f"‚ùå Failed to scan {file_path.name}: {e}")
    
    def _extract_file_metadata(self, file_path: Path, source_type: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            data = torch.load(file_path, map_location='cpu')
            
            metadata = {
                'file_path': str(file_path),
                'filename': file_path.name,
                'source_type': source_type,
                'file_size_mb': file_path.stat().st_size / 1024 / 1024,
                'format': 'unknown'
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
            if isinstance(data, dict):
                # –§–æ—Ä–º–∞—Ç generate_snli_embedding_dataset.py
                if 'question_embeddings' in data and 'answer_embeddings' in data:
                    metadata.update({
                        'format': 'snli_dataset',
                        'size': data.get('size', 'unknown'),
                        'teacher_model': data.get('teacher_model', 'unknown'),
                        'embedding_dim': data['question_embeddings'].shape[1] if data['question_embeddings'].dim() == 2 else None,
                        'sample_count': len(data['question_embeddings'])
                    })
                
                # –§–æ—Ä–º–∞—Ç unified_dataset_loader.py  
                elif 'input_embeddings' in data and 'target_embeddings' in data:
                    metadata.update({
                        'format': 'unified_dataset',
                        'embedding_dim': data['input_embeddings'].shape[1] if data['input_embeddings'].dim() == 2 else None,
                        'sample_count': len(data['input_embeddings'])
                    })
                
                # –î—Ä—É–≥–∏–µ dict —Ñ–æ—Ä–º–∞—Ç—ã
                else:
                    metadata['format'] = 'dict_format'
                    metadata['keys'] = list(data.keys())
            
            elif isinstance(data, torch.Tensor):
                # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–Ω–∑–æ—Ä
                metadata.update({
                    'format': 'tensor',
                    'tensor_shape': list(data.shape),
                    'embedding_dim': data.shape[1] if data.dim() == 2 else None,
                    'sample_count': data.shape[0] if data.dim() >= 1 else 1
                })
            
            return metadata
            
        except Exception as e:
            logger.warning(f"‚ùå Metadata extraction failed for {file_path.name}: {e}")
            return None
    
    def _load_file_data(self, file_path: str) -> Optional[Dict[str, torch.Tensor]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if file_path in self._loaded_files:
            return self._loaded_files[file_path]
        
        try:
            logger.info(f"üì• Loading precomputed file: {Path(file_path).name}")
            
            data = torch.load(file_path, map_location=self.device)
            metadata = self._file_metadata.get(file_path, {})
            format_type = metadata.get('format', 'unknown')
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –µ–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            converted_data = None
            
            if format_type == 'snli_dataset':
                # –§–æ—Ä–º–∞—Ç generate_snli_embedding_dataset.py
                converted_data = {
                    'input_embeddings': data['question_embeddings'],
                    'target_embeddings': data['answer_embeddings'],
                    'metadata': {
                        'teacher_model': data.get('teacher_model'),
                        'dataset_info': data.get('dataset_info', {}),
                        'sample_pairs': data.get('sample_pairs', [])
                    }
                }
                
            elif format_type == 'unified_dataset':
                # –§–æ—Ä–º–∞—Ç unified_dataset_loader.py
                converted_data = {
                    'input_embeddings': data['input_embeddings'],
                    'target_embeddings': data['target_embeddings'],
                    'metadata': data.get('metadata', {})
                }
                
            elif format_type == 'tensor':
                # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–Ω–∑–æ—Ä - —Å–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –¥–ª—è input –∏ target
                converted_data = {
                    'input_embeddings': data,
                    'target_embeddings': data.clone(),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ø–∏—é
                    'metadata': {'source': 'tensor_file'}
                }
                
            elif isinstance(data, dict):
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ dict
                input_emb = self._extract_embeddings_from_dict(data, 'input')
                target_emb = self._extract_embeddings_from_dict(data, 'target')
                
                if input_emb is not None and target_emb is not None:
                    converted_data = {
                        'input_embeddings': input_emb,
                        'target_embeddings': target_emb,
                        'metadata': data.get('metadata', {})
                    }
            
            if converted_data:
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                if self._validate_loaded_data(converted_data):
                    self._loaded_files[file_path] = converted_data
                    logger.info(f"‚úÖ Loaded {Path(file_path).name}: "
                               f"{converted_data['input_embeddings'].shape[0]} samples")
                    return converted_data
                else:
                    logger.warning(f"‚ùå Data validation failed for {Path(file_path).name}")
            else:
                logger.warning(f"‚ùå Unknown format for {Path(file_path).name}")
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load {Path(file_path).name}: {e}")
            return None
    
    def _extract_embeddings_from_dict(self, data: Dict, emb_type: str) -> Optional[torch.Tensor]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ dict —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        # –í–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è input —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        input_keys = ['input_embeddings', 'question_embeddings', 'premise_embeddings', 'embeddings']
        target_keys = ['target_embeddings', 'answer_embeddings', 'hypothesis_embeddings', 'labels']
        
        keys_to_check = input_keys if emb_type == 'input' else target_keys
        
        for key in keys_to_check:
            if key in data and isinstance(data[key], torch.Tensor):
                tensor = data[key]
                if tensor.dim() == 2:  # [samples, embedding_dim]
                    return tensor
        
        return None
    
    def _validate_loaded_data(self, data: Dict[str, torch.Tensor]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            input_emb = data['input_embeddings']
            target_emb = data['target_embeddings']
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            if input_emb.dim() != 2 or target_emb.dim() != 2:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
            if input_emb.shape[0] != target_emb.shape[0]:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if input_emb.shape[1] != target_emb.shape[1]:
                return False
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å
            return (self.validate_embeddings(input_emb, "precomputed_input") and 
                   self.validate_embeddings(target_emb, "precomputed_target"))
            
        except Exception as e:
            logger.error(f"‚ùå Data validation error: {e}")
            return False
    
    def get_text_pairs(self, max_samples: Optional[int] = None) -> List[Tuple[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)"""
        if not self.ensure_initialized():
            return []
        
        text_pairs = []
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        for file_path, metadata in self._file_metadata.items():
            if metadata.get('format') == 'snli_dataset':
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è sample_pairs
                data = self._load_file_data(file_path)
                if data and 'metadata' in data:
                    sample_pairs = data['metadata'].get('sample_pairs', [])
                    for pair in sample_pairs:
                        if isinstance(pair, dict) and 'question' in pair and 'answer' in pair:
                            text_pairs.append((pair['question'], pair['answer']))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        if max_samples:
            text_pairs = text_pairs[:max_samples]
        
        if self.config.max_samples_per_source:
            text_pairs = text_pairs[:self.config.max_samples_per_source]
        
        logger.debug(f"üìù Precomputed text pairs: {len(text_pairs)} samples")
        return text_pairs
    
    def get_embeddings(self, max_samples: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        if not self.ensure_initialized():
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        all_input_embeddings = []
        all_target_embeddings = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        for file_path, metadata in self._file_metadata.items():
            data = self._load_file_data(file_path)
            if data:
                input_emb = data['input_embeddings']
                target_emb = data['target_embeddings']
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
                input_emb = self.normalize_embeddings(input_emb)
                target_emb = self.normalize_embeddings(target_emb)
                
                all_input_embeddings.append(input_emb)
                all_target_embeddings.append(target_emb)
        
        if not all_input_embeddings:
            logger.warning("‚ùå No valid embeddings found in precomputed files")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        input_embeddings = torch.cat(all_input_embeddings, dim=0)
        target_embeddings = torch.cat(all_target_embeddings, dim=0)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        if max_samples:
            input_embeddings = input_embeddings[:max_samples]
            target_embeddings = target_embeddings[:max_samples]
        
        if self.config.max_samples_per_source:
            limit = self.config.max_samples_per_source
            input_embeddings = input_embeddings[:limit]
            target_embeddings = target_embeddings[:limit]
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.config.shuffle_data and len(input_embeddings) > 1:
            indices = torch.randperm(len(input_embeddings))
            input_embeddings = input_embeddings[indices]
            target_embeddings = target_embeddings[indices]
        
        logger.info(f"‚úÖ Precomputed embeddings loaded: {input_embeddings.shape}")
        return input_embeddings, target_embeddings
    
    def get_file_list(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        if not self.ensure_initialized():
            return []
        
        return list(self._file_metadata.values())
    
    def get_statistics(self) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è precomputed provider"""
        base_stats = super().get_statistics()
        
        if self._is_initialized:
            total_samples = 0
            file_formats = {}
            
            for metadata in self._file_metadata.values():
                samples = metadata.get('sample_count', 0)
                total_samples += samples
                
                format_type = metadata.get('format', 'unknown')
                file_formats[format_type] = file_formats.get(format_type, 0) + 1
            
            base_stats.update({
                'total_files': len(self._file_metadata),
                'total_samples': total_samples,
                'file_formats': file_formats,
                'loaded_files_count': len(self._loaded_files),
                'embeddings_dir': str(self.embeddings_dir),
                'cache_dir': str(self.cache_dir)
            })
        
        return base_stats


def create_precomputed_provider(config) -> PrecomputedProvider:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è PrecomputedProvider"""
    return PrecomputedProvider(config)