"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
===================================

–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ dataset –º–æ–¥—É–ª–µ
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Tuple, Optional, Any, Iterator
import torch
from pathlib import Path

from ...utils.logging import get_logger

logger = get_logger(__name__)


class BaseDataProvider(ABC):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–∞—Ä –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    
    def __init__(self, name: str, config):
        """
        Args:
            name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            config: DatasetConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        """
        self.name = name
        self.config = config
        self.device = torch.device(config.device or 'cuda')
        self._is_initialized = False
        self._cached_data = None
        
        logger.info(f"üîß Initializing {self.name} provider on {self.device}")
    
    @abstractmethod
    def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            True –µ—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        """
        pass
    
    @abstractmethod
    def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä (–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π, –¥–∞–Ω–Ω—ã—Ö –∏ —Ç.–¥.)
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞
        """
        pass
    
    @abstractmethod
    def get_text_pairs(self, max_samples: Optional[int] = None) -> List[Tuple[str, str]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ (input, target)
        
        Args:
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä (None = –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (input_text, target_text)
        """
        pass
    
    @abstractmethod
    def get_embeddings(self, max_samples: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–∞—Ä —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä
            
        Returns:
            Tuple (input_embeddings, target_embeddings) –∫–∞–∂–¥—ã–π —Ä–∞–∑–º–µ—Ä–æ–º [N, embed_dim]
        """
        pass
    
    def get_mixed_data(self, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏ —Ç–µ–∫—Å—Ç—ã –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        
        Args:
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏: 'text_pairs', 'input_embeddings', 'target_embeddings'
        """
        text_pairs = self.get_text_pairs(max_samples)
        input_embeddings, target_embeddings = self.get_embeddings(max_samples)
        
        return {
            'text_pairs': text_pairs,
            'input_embeddings': input_embeddings,
            'target_embeddings': target_embeddings,
            'source': self.name,
            'count': len(text_pairs)
        }
    
    def validate_embeddings(self, embeddings: torch.Tensor, name: str = "embeddings") -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
        
        Args:
            embeddings: –¢–µ–Ω–∑–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–∞–ª–∏–¥–Ω—ã
        """
        if not self.config.validate_embeddings:
            return True
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã
            if embeddings.dim() != 2:
                logger.warning(f"‚ùå {name}: –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {embeddings.shape}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
            if self.config.check_nan_inf:
                if torch.isnan(embeddings).any():
                    logger.warning(f"‚ùå {name}: —Å–æ–¥–µ—Ä–∂–∏—Ç NaN –∑–Ω–∞—á–µ–Ω–∏—è")
                    return False
                if torch.isinf(embeddings).any():
                    logger.warning(f"‚ùå {name}: —Å–æ–¥–µ—Ä–∂–∏—Ç Inf –∑–Ω–∞—á–µ–Ω–∏—è")
                    return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º
            norms = torch.norm(embeddings, dim=1)
            min_norm, max_norm = norms.min().item(), norms.max().item()
            
            if min_norm < self.config.min_embedding_norm:
                logger.warning(f"‚ùå {name}: —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞—è –Ω–æ—Ä–º–∞ {min_norm:.6f}")
                return False
            if max_norm > self.config.max_embedding_norm:
                logger.warning(f"‚ùå {name}: —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –Ω–æ—Ä–º–∞ {max_norm:.6f}")
                return False
            
            logger.debug(f"‚úÖ {name}: –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞, norm range [{min_norm:.4f}, {max_norm:.4f}]")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå {name}: –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - {e}")
            return False
    
    def normalize_embeddings(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            embeddings: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        """
        if not self.config.normalize_embeddings:
            return embeddings
        
        normalized = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        logger.debug(f"üìê Normalized embeddings: {embeddings.shape}")
        return normalized
    
    def ensure_initialized(self) -> bool:
        """
        –£–±–µ–∂–¥–∞–µ—Ç—Å—è —á—Ç–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
        
        Returns:
            True –µ—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ
        """
        if not self._is_initialized:
            if not self.is_available():
                logger.error(f"‚ùå {self.name}: –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
            
            if not self.initialize():
                logger.error(f"‚ùå {self.name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä")
                return False
                
            self._is_initialized = True
            logger.info(f"‚úÖ {self.name}: –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        return True
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        if not self.ensure_initialized():
            return {'error': 'Provider not initialized'}
        
        try:
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            text_pairs = self.get_text_pairs(max_samples=100)  # –í—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            
            stats = {
                'name': self.name,
                'is_available': self.is_available(),
                'is_initialized': self._is_initialized,
                'device': str(self.device),
                'sample_count': len(text_pairs),
            }
            
            if text_pairs:
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–∞–º —Ç–µ–∫—Å—Ç–æ–≤
                input_lengths = [len(pair[0].split()) for pair in text_pairs]
                target_lengths = [len(pair[1].split()) for pair in text_pairs]
                
                stats.update({
                    'avg_input_length': sum(input_lengths) / len(input_lengths),
                    'avg_target_length': sum(target_lengths) / len(target_lengths),
                    'max_input_length': max(input_lengths),
                    'max_target_length': max(target_lengths)
                })
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå {self.name}: –æ—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ - {e}")
            return {'error': str(e)}
    
    def __str__(self) -> str:
        return f"{self.name}Provider(initialized={self._is_initialized}, device={self.device})"
    
    def __repr__(self) -> str:
        return self.__str__()