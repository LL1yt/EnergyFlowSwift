"""
SNLIProvider - –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞
==========================================

–ê–¥–∞–ø—Ç–∞—Ü–∏—è legacy generate_snli_embedding_dataset.py –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É:
- –ó–∞–≥—Ä—É–∑–∫–∞ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ HuggingFace
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ TeacherModelProvider
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import torch
from typing import List, Tuple, Optional, Dict, Any
import random
from datasets import load_dataset

from .base_provider import BaseDataProvider
from .teacher_model import TeacherModelProvider
from ...utils.logging import get_logger, DEBUG_INIT

logger = get_logger(__name__)


class SNLIProvider(BaseDataProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Stanford Natural Language Inference (SNLI) –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã premise-hypothesis –≤ —Ñ–æ—Ä–º–∞—Ç–µ question-answer
    –¥–ª—è –æ–±—É—á–µ–Ω–∏—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    """
    
    def __init__(self, config, teacher_provider: Optional[TeacherModelProvider] = None):
        super().__init__("SNLI", config)
        
        self.teacher_provider = teacher_provider
        self.snli_fraction = config.snli_fraction
        self.min_text_length = getattr(config, 'snli_min_text_length', 10)
        
        # –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self._snli_pairs = None
        self._embeddings_cache = None
        
        logger.log(DEBUG_INIT, f"SNLIProvider: fraction={self.snli_fraction}, "
                              f"min_length={self.min_text_length}")
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ HuggingFace datasets
            from datasets import load_dataset
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            logger.info("üîç Checking SNLI dataset availability...")
            dataset = load_dataset("snli", split="train[:1%]")  # –ó–∞–≥—Ä—É–∂–∞–µ–º 1% –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
            if len(dataset) > 0:
                logger.info(f"‚úÖ SNLI dataset available: {len(dataset)} samples in test load")
                return True
            else:
                logger.warning("‚ùå SNLI dataset appears empty")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå SNLI dataset not available: {e}")
            return False
    
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö)"""
        logger.log(DEBUG_INIT, "Initializing SNLI provider...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º teacher provider –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
            if self.teacher_provider and not self.teacher_provider.ensure_initialized():
                logger.warning("‚ùå Teacher provider not available, SNLI will work without embeddings")
            
            logger.info(f"‚úÖ SNLI provider initialized (fraction={self.snli_fraction})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå SNLI provider initialization failed: {e}")
            return False
    
    def _load_snli_data(self, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è SNLI –¥–∞–Ω–Ω—ã—Ö"""
        if self._snli_pairs is not None and max_samples is None:
            return self._snli_pairs  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –µ—Å–ª–∏ –Ω–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        
        logger.info(f"üì• Loading SNLI dataset (fraction={self.snli_fraction})")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º SNLI –¥–∞—Ç–∞—Å–µ—Ç
            dataset = load_dataset("snli")
            train_data = dataset["train"]
            total_size = len(train_data)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏
            target_size = int(total_size * self.snli_fraction)
            if max_samples:
                target_size = min(target_size, max_samples)
            
            logger.info(f"üìä SNLI total size: {total_size:,}, will use: {target_size:,}")
            
            # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            indices = random.sample(range(total_size), target_size)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            pairs = []
            valid_labels = {0, 1, 2}  # entailment, neutral, contradiction
            
            for idx in indices:
                example = train_data[idx]
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö
                if (
                    example["label"] in valid_labels  # –í–∞–ª–∏–¥–Ω—ã–π label
                    and example["premise"]  # –ù–µ –ø—É—Å—Ç–æ–π premise
                    and example["hypothesis"]  # –ù–µ –ø—É—Å—Ç–æ–π hypothesis
                    and len(example["premise"].strip()) >= self.min_text_length
                    and len(example["hypothesis"].strip()) >= self.min_text_length
                ):
                    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ question-answer
                    pair = {
                        "input_text": example["premise"],  # premise –∫–∞–∫ input
                        "target_text": example["hypothesis"],  # hypothesis –∫–∞–∫ target
                        "label": example["label"],
                        "snli_id": idx,
                        "source": "snli"
                    }
                    pairs.append(pair)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ labels
            label_counts = {}
            for pair in pairs:
                label = pair["label"]
                label_counts[label] = label_counts.get(label, 0) + 1
            
            logger.info(f"‚úÖ SNLI data loaded: {len(pairs):,} valid pairs")
            label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
            for label_id, count in label_counts.items():
                label_name = label_names.get(label_id, "unknown")
                logger.info(f"   {label_name}: {count:,} ({count/len(pairs)*100:.1f}%)")
            
            # –ö—ç—à–∏—Ä—É–µ–º –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            if max_samples is None:
                self._snli_pairs = pairs
            
            return pairs
            
        except Exception as e:
            logger.error(f"‚ùå SNLI data loading failed: {e}")
            return []
    
    def get_text_pairs(self, max_samples: Optional[int] = None) -> List[Tuple[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ SNLI"""
        if not self.ensure_initialized():
            return []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        effective_max = max_samples
        if self.config.max_samples_per_source:
            if effective_max:
                effective_max = min(effective_max, self.config.max_samples_per_source)
            else:
                effective_max = self.config.max_samples_per_source
        
        snli_data = self._load_snli_data(effective_max)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç (input, target)
        text_pairs = [(pair["input_text"], pair["target_text"]) for pair in snli_data]
        
        logger.debug(f"üìù SNLI text pairs: {len(text_pairs)} samples")
        return text_pairs
    
    def get_embeddings(self, max_samples: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è SNLI –ø–∞—Ä"""
        if not self.ensure_initialized():
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        if not self.teacher_provider:
            logger.warning("‚ùå No teacher provider available for embedding generation")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã
        text_pairs = self.get_text_pairs(max_samples)
        if not text_pairs:
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ input –∏ target —Ç–µ–∫—Å—Ç—ã
        input_texts = [pair[0] for pair in text_pairs]
        target_texts = [pair[1] for pair in text_pairs]
        
        logger.info(f"üîÑ Generating SNLI embeddings for {len(text_pairs)} pairs...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ teacher model
            input_embeddings = self.teacher_provider.encode_texts(input_texts)
            target_embeddings = self.teacher_provider.encode_texts(target_texts)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if not self.validate_embeddings(input_embeddings, "SNLI_input"):
                logger.warning("‚ùå SNLI input embeddings validation failed")
            if not self.validate_embeddings(target_embeddings, "SNLI_target"):
                logger.warning("‚ùå SNLI target embeddings validation failed")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            input_embeddings = self.normalize_embeddings(input_embeddings)
            target_embeddings = self.normalize_embeddings(target_embeddings)
            
            logger.info(f"‚úÖ SNLI embeddings generated: {input_embeddings.shape}")
            return input_embeddings, target_embeddings
            
        except Exception as e:
            logger.error(f"‚ùå SNLI embedding generation failed: {e}")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
    
    def get_mixed_data(self, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å SNLI –¥–∞–Ω–Ω—ã–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        base_data = super().get_mixed_data(max_samples)
        
        # –î–æ–±–∞–≤–ª—è–µ–º SNLI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if base_data['text_pairs']:
            snli_data = self._load_snli_data(max_samples)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            base_data.update({
                'snli_labels': [pair['label'] for pair in snli_data],
                'snli_ids': [pair['snli_id'] for pair in snli_data],
                'dataset_fraction': self.snli_fraction,
                'label_distribution': self._get_label_distribution(snli_data)
            })
        
        return base_data
    
    def _get_label_distribution(self, snli_data: List[Dict]) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ labels –≤ –¥–∞–Ω–Ω—ã—Ö"""
        label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
        distribution = {}
        
        for pair in snli_data:
            label_name = label_names.get(pair["label"], "unknown")
            distribution[label_name] = distribution.get(label_name, 0) + 1
        
        return distribution
    
    def get_statistics(self) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è SNLI"""
        base_stats = super().get_statistics()
        
        if self._is_initialized:
            # –ë—ã—Å—Ç—Ä–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            sample_data = self._load_snli_data(max_samples=100)
            
            if sample_data:
                base_stats.update({
                    'dataset_fraction': self.snli_fraction,
                    'min_text_length': self.min_text_length,
                    'label_distribution': self._get_label_distribution(sample_data),
                    'estimated_total_pairs': int(len(sample_data) / min(100 / len(sample_data), 1)),
                    'teacher_provider_available': self.teacher_provider is not None
                })
        
        return base_stats


def create_snli_provider(config, teacher_provider: Optional[TeacherModelProvider] = None) -> SNLIProvider:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è SNLIProvider"""
    return SNLIProvider(config, teacher_provider)