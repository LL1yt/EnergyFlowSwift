"""
SNLIProvider - –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞
==========================================

–ê–¥–∞–ø—Ç–∞—Ü–∏—è legacy generate_snli_embedding_dataset.py –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É:
- –ó–∞–≥—Ä—É–∑–∫–∞ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ HuggingFace
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ TeacherModelProvider
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import torch
from typing import List, Tuple, Optional, Dict, Any
import random
from datasets import load_dataset
from pathlib import Path
import json

from .base_provider import BaseDataProvider
from .teacher_model import TeacherModelProvider
from ...utils.logging import get_logger, DEBUG_INIT

logger = get_logger(__name__)


class SNLIProvider(BaseDataProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Stanford Natural Language Inference (SNLI) –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã premise-hypothesis –≤ —Ñ–æ—Ä–º–∞—Ç–µ question-answer
    –¥–ª—è –æ–±—É—á–µ–Ω–∏—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    """
    
    def __init__(self, config, teacher_provider: Optional[TeacherModelProvider] = None):
        super().__init__("SNLI", config)
        
        self.teacher_provider = teacher_provider
        self.snli_fraction = config.snli_fraction
        self.min_text_length = getattr(config, 'snli_min_text_length', 10)
        # –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–≤ –ø–∞–º—è—Ç–∏)
        self._snli_pairs = None
        self._embeddings_cache = None

        # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à
        self._cache_dir = Path(getattr(config, 'snli_cache_dir', 'cache/snli'))
        self._cache_enabled = getattr(config, 'snli_cache_enabled', True)
        self._cache_file = self._cache_dir / self._build_cache_filename()

        logger.log(DEBUG_INIT, f"SNLIProvider: fraction={self.snli_fraction}, "
                               f"min_length={self.min_text_length}")
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ HuggingFace datasets
            from datasets import load_dataset
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            logger.info("üîç Checking SNLI dataset availability...")
            dataset = load_dataset("snli", split="train[:1%]")  # –ó–∞–≥—Ä—É–∂–∞–µ–º 1% –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
            if len(dataset) > 0:
                logger.info(f"‚úÖ SNLI dataset available: {len(dataset)} samples in test load")
                return True
            else:
                logger.warning("‚ùå SNLI dataset appears empty")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå SNLI dataset not available: {e}")
            return False
    
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö)"""
        logger.log(DEBUG_INIT, "Initializing SNLI provider...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º teacher provider –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
            if self.teacher_provider and not self.teacher_provider.ensure_initialized():
                logger.warning("‚ùå Teacher provider not available, SNLI will work without embeddings")
            
            logger.info(f"‚úÖ SNLI provider initialized (fraction={self.snli_fraction})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå SNLI provider initialization failed: {e}")
            return False
    
    def _load_snli_data(self, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è SNLI –¥–∞–Ω–Ω—ã—Ö"""
        if self._snli_pairs is not None and max_samples is None:
            return self._snli_pairs  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –µ—Å–ª–∏ –Ω–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        
        logger.info(f"üì• Loading SNLI dataset (fraction={self.snli_fraction})")
        # –ü–æ–ø—ã—Ç–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ (—Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω–µ—Ç max_samples –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
        if self._cache_enabled and max_samples is None and self._cache_file.exists():
            try:
                logger.info(f"üóÑÔ∏è  Loading SNLI pairs from local cache: {self._cache_file}")
                with self._cache_file.open('r', encoding='utf-8') as f:
                    pairs = [json.loads(line) for line in f]
                self._snli_pairs = pairs
                logger.info(f"‚úÖ Loaded {len(pairs):,} SNLI pairs from cache")
                return pairs
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load SNLI cache ({e}), rebuilding ...")

        try:
            dataset = load_dataset("snli")
            train_data = dataset["train"]
            total_size = len(train_data)
            target_size = int(total_size * self.snli_fraction)
            if max_samples:
                target_size = min(target_size, max_samples)
            logger.info(f"üìä SNLI total size: {total_size:,}, will use: {target_size:,}")

            indices = random.sample(range(total_size), target_size)
            pairs = []
            valid_labels = {0, 1, 2}
            for idx in indices:
                example = train_data[idx]
                if (
                    example["label"] in valid_labels and
                    example["premise"] and example["hypothesis"] and
                    len(example["premise"].strip()) >= self.min_text_length and
                    len(example["hypothesis"].strip()) >= self.min_text_length
                ):
                    pairs.append({
                        "input_text": example["premise"],
                        "target_text": example["hypothesis"],
                        "label": example["label"],
                        "snli_id": idx,
                        "source": "snli"
                    })

            # –õ–æ–≥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            label_counts = {}
            for pair in pairs:
                label_counts[pair['label']] = label_counts.get(pair['label'], 0) + 1
            logger.info(f"‚úÖ SNLI data loaded: {len(pairs):,} valid pairs")
            label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
            for lid, cnt in label_counts.items():
                logger.info(f"   {label_names.get(lid,'unknown')}: {cnt:,} ({cnt/len(pairs)*100:.1f}%)")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à (—Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–∑ max_samples)
            if self._cache_enabled and max_samples is None:
                try:
                    self._cache_dir.mkdir(parents=True, exist_ok=True)
                    with self._cache_file.open('w', encoding='utf-8') as f:
                        for rec in pairs:
                            f.write(json.dumps(rec, ensure_ascii=False) + '\n')
                    logger.info(f"üíæ Cached SNLI pairs to {self._cache_file}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to write SNLI cache: {e}")
                self._snli_pairs = pairs
            return pairs
        except Exception as e:
            logger.error(f"‚ùå SNLI data loading failed: {e}")
            return []

    def _build_cache_filename(self) -> str:
        """–ò–º—è —Ñ–∞–π–ª–∞ –∫—ç—à–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        # –ü—Ä–∏–º–µ—Ä: snli_frac0.2_min10.jsonl
        frac = f"{self.snli_fraction}".replace('.', 'p')
        return f"snli_frac{frac}_min{self.min_text_length}.jsonl"
    
    def get_text_pairs(self, max_samples: Optional[int] = None) -> List[Tuple[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ SNLI"""
        if not self.ensure_initialized():
            return []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        effective_max = max_samples
        if self.config.max_samples_per_source:
            if effective_max:
                effective_max = min(effective_max, self.config.max_samples_per_source)
            else:
                effective_max = self.config.max_samples_per_source
        
        snli_data = self._load_snli_data(effective_max)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç (input, target)
        text_pairs = [(pair["input_text"], pair["target_text"]) for pair in snli_data]
        
        logger.debug(f"üìù SNLI text pairs: {len(text_pairs)} samples")
        return text_pairs
    
    def get_embeddings(self, max_samples: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è SNLI –ø–∞—Ä"""
        if not self.ensure_initialized():
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        if not self.teacher_provider:
            logger.warning("‚ùå No teacher provider available for embedding generation")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã
        text_pairs = self.get_text_pairs(max_samples)
        if not text_pairs:
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ input –∏ target —Ç–µ–∫—Å—Ç—ã
        input_texts = [pair[0] for pair in text_pairs]
        target_texts = [pair[1] for pair in text_pairs]
        
        logger.info(f"üîÑ Generating SNLI embeddings for {len(text_pairs)} pairs...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ teacher model
            input_embeddings = self.teacher_provider.encode_texts(input_texts)
            target_embeddings = self.teacher_provider.encode_texts(target_texts)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if not self.validate_embeddings(input_embeddings, "SNLI_input"):
                logger.warning("‚ùå SNLI input embeddings validation failed")
            if not self.validate_embeddings(target_embeddings, "SNLI_target"):
                logger.warning("‚ùå SNLI target embeddings validation failed")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            input_embeddings = self.normalize_embeddings(input_embeddings)
            target_embeddings = self.normalize_embeddings(target_embeddings)
            
            logger.info(f"‚úÖ SNLI embeddings generated: {input_embeddings.shape}")
            return input_embeddings, target_embeddings
            
        except Exception as e:
            logger.error(f"‚ùå SNLI embedding generation failed: {e}")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
    
    def get_mixed_data(self, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å SNLI –¥–∞–Ω–Ω—ã–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        base_data = super().get_mixed_data(max_samples)
        
        # –î–æ–±–∞–≤–ª—è–µ–º SNLI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if base_data['text_pairs']:
            snli_data = self._load_snli_data(max_samples)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            base_data.update({
                'snli_labels': [pair['label'] for pair in snli_data],
                'snli_ids': [pair['snli_id'] for pair in snli_data],
                'dataset_fraction': self.snli_fraction,
                'label_distribution': self._get_label_distribution(snli_data)
            })
        
        return base_data
    
    def _get_label_distribution(self, snli_data: List[Dict]) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ labels –≤ –¥–∞–Ω–Ω—ã—Ö"""
        label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
        distribution = {}
        
        for pair in snli_data:
            label_name = label_names.get(pair["label"], "unknown")
            distribution[label_name] = distribution.get(label_name, 0) + 1
        
        return distribution
    
    def get_statistics(self) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è SNLI"""
        base_stats = super().get_statistics()
        
        if self._is_initialized:
            # –ë—ã—Å—Ç—Ä–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            sample_data = self._load_snli_data(max_samples=100)
            
            if sample_data:
                base_stats.update({
                    'dataset_fraction': self.snli_fraction,
                    'min_text_length': self.min_text_length,
                    'label_distribution': self._get_label_distribution(sample_data),
                    'estimated_total_pairs': int(len(sample_data) / min(100 / len(sample_data), 1)),
                    'teacher_provider_available': self.teacher_provider is not None
                })
        
        return base_stats


def create_snli_provider(config, teacher_provider: Optional[TeacherModelProvider] = None) -> SNLIProvider:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è SNLIProvider"""
    return SNLIProvider(config, teacher_provider)