"""
SNLIProvider - –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞
==========================================

–ê–¥–∞–ø—Ç–∞—Ü–∏—è legacy generate_snli_embedding_dataset.py –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É:
- –ó–∞–≥—Ä—É–∑–∫–∞ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ HuggingFace
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ TeacherModelProvider
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import torch
from typing import List, Tuple, Optional, Dict, Any
import random
from datasets import load_dataset
from pathlib import Path
import json

from .base_provider import BaseDataProvider
from .teacher_model import TeacherModelProvider
from ...utils.logging import get_logger, DEBUG_INIT

logger = get_logger(__name__)


class SNLIProvider(BaseDataProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Stanford Natural Language Inference (SNLI) –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã premise-hypothesis –≤ —Ñ–æ—Ä–º–∞—Ç–µ question-answer
    –¥–ª—è –æ–±—É—á–µ–Ω–∏—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

        –°–µ–º–∞–Ω—Ç–∏–∫–∞ snli_fraction / max_samples (--snli-limit):
        - snli_fraction –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –†–ê–ó–ú–ï–† –ë–ê–ó–û–í–û–ì–û –ü–û–î–ú–ù–û–ñ–ï–°–¢–í–ê (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–∏ snli_seed)
            –∫–æ—Ç–æ—Ä–æ–µ —Å—Ç—Ä–æ–∏—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –∏ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≤ JSONL (cache/snli/...)
        - max_samples (–∏–ª–∏ --snli-limit –≤ CLI generate_text_embedding_jsonl.py) –¢–û–õ–¨–ö–û
            –¥–µ–ª–∞–µ—Ç —Å—Ä–µ–∑ —Å–≤–µ—Ä—Ö—É —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ/–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞, –Ω–µ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π.
        - –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏ –¥–∞–∂–µ —Å limit –∫—ç—à –≤—Å—ë —Ä–∞–≤–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ fraction,
            –∑–∞—Ç–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ–∂–µ—Ç—Å—è. –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –≤—ã–∑–æ–≤—ã –±—ã—Å—Ç—Ä—ã–µ.
        - –ò–∑–º–µ–Ω–µ–Ω–∏–µ fraction —Ç—Ä–µ–±—É–µ—Ç –æ—á–∏—Å—Ç–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ cache-—Ñ–∞–π–ª–∞ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è snli_seed.
        - –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –¥–µ–ª–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–π.
    """
    
    def __init__(self, config, teacher_provider: Optional[TeacherModelProvider] = None):
        super().__init__("SNLI", config)
        
        self.teacher_provider = teacher_provider
        self.snli_fraction = config.snli_fraction
        self.min_text_length = getattr(config, 'snli_min_text_length', 10)
        self.snli_seed = getattr(config, 'snli_seed', None)
        # –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–≤ –ø–∞–º—è—Ç–∏)
        self._snli_pairs = None
        self._embeddings_cache = None

        # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à
        self._cache_dir = Path(getattr(config, 'snli_cache_dir', 'cache/snli'))
        self._cache_enabled = getattr(config, 'snli_cache_enabled', True)
        self._cache_file = self._cache_dir / self._build_cache_filename()

        logger.log(DEBUG_INIT, f"SNLIProvider: fraction={self.snli_fraction}, "
                               f"min_length={self.min_text_length}")
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ HuggingFace datasets
            from datasets import load_dataset
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            logger.info("üîç Checking SNLI dataset availability...")
            dataset = load_dataset("snli", split="train[:1%]")  # –ó–∞–≥—Ä—É–∂–∞–µ–º 1% –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
            if len(dataset) > 0:
                logger.info(f"‚úÖ SNLI dataset available: {len(dataset)} samples in test load")
                return True
            else:
                logger.warning("‚ùå SNLI dataset appears empty")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå SNLI dataset not available: {e}")
            return False
    
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö)"""
        logger.log(DEBUG_INIT, "Initializing SNLI provider...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º teacher provider –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
            if self.teacher_provider and not self.teacher_provider.ensure_initialized():
                logger.warning("‚ùå Teacher provider not available, SNLI will work without embeddings")
            
            logger.info(f"‚úÖ SNLI provider initialized (fraction={self.snli_fraction})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå SNLI provider initialization failed: {e}")
            return False
    
    def _load_snli_data(self, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è SNLI –¥–∞–Ω–Ω—ã—Ö.

        –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê:
        1. –í—Å–µ–≥–¥–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏ –∫—ç—à–∏—Ä—É–µ–º "–±–∞–∑–æ–≤—ã–π" –Ω–∞–±–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ int(total*snli_fraction)
           (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏ snli_seed) –æ–¥–∏–Ω —Ä–∞–∑.
        2. –ê—Ä–≥—É–º–µ–Ω—Ç max_samples (–∏–ª–∏ --snli-limit) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¢–û–õ–¨–ö–û –∫–∞–∫ –æ–±—Ä–µ–∑–∫–∞ —Å–≤–µ—Ä—Ö—É
           —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.
        3. –ï—Å–ª–∏ –∫—ç—à —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äì —á–∏—Ç–∞–µ–º –ø–æ–ª–Ω—ã–π –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏ –ª–∏—à—å –∑–∞—Ç–µ–º —Ä–µ–∂–µ–º.
        4. –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è snli_fraction –∏ –ª–∏–º–∏—Ç–∞.
        """

        # –ï—Å–ª–∏ –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä —É–∂–µ –≤ –ø–∞–º—è—Ç–∏ ‚Äì –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ —É—Å–µ—á–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        if self._snli_pairs is not None:
            if max_samples is not None:
                return self._snli_pairs[:max_samples]
            return self._snli_pairs

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–π –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏–∑ –∫—ç—à–∞
        if self._cache_enabled and self._cache_file.exists():
            try:
                logger.info(f"üóÑÔ∏è  Loading SNLI base subset from cache: {self._cache_file}")
                with self._cache_file.open('r', encoding='utf-8') as f:
                    cached = [json.loads(line) for line in f]
                self._snli_pairs = cached
                logger.info(f"‚úÖ Loaded cached SNLI base subset: {len(cached):,} pairs")
                if max_samples is not None:
                    return cached[:max_samples]
                return cached
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to read SNLI cache ({e}), rebuilding ...")

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        logger.info(f"üì• Building SNLI base subset (fraction={self.snli_fraction}, seed={self.snli_seed})")
        try:
            dataset = load_dataset("snli")
            train_data = dataset["train"]
            total_size = len(train_data)
            base_size = int(total_size * self.snli_fraction)
            logger.info(f"üìä SNLI total size: {total_size:,}; base subset size: {base_size:,}")

            rng = random.Random(self.snli_seed) if self.snli_seed is not None else random
            indices = rng.sample(range(total_size), base_size)

            pairs: List[Dict[str, Any]] = []
            valid_labels = {0, 1, 2}
            append = pairs.append
            for idx in indices:
                example = train_data[idx]
                premise = example.get("premise")
                hypothesis = example.get("hypothesis")
                if (
                    example.get("label") in valid_labels and
                    premise and hypothesis and
                    len(premise.strip()) >= self.min_text_length and
                    len(hypothesis.strip()) >= self.min_text_length
                ):
                    append({
                        "input_text": premise,
                        "target_text": hypothesis,
                        "label": example.get("label"),
                        "snli_id": idx,
                        "source": "snli"
                    })

            # –õ–æ–≥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            total_pairs = len(pairs)
            if total_pairs == 0:
                logger.error("‚ùå SNLI produced zero valid pairs after filtering")
                return []
            label_counts: Dict[int, int] = {}
            for pair in pairs:
                lbl = pair['label']
                label_counts[lbl] = label_counts.get(lbl, 0) + 1
            label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
            logger.info(f"‚úÖ Built SNLI base subset: {total_pairs:,} pairs")
            for lid, cnt in label_counts.items():
                logger.info(f"   {label_names.get(lid,'unknown')}: {cnt:,} ({cnt/total_pairs*100:.1f}%)")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä (–≤—Å–µ–≥–¥–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª –∑–∞–ø—Ä–æ—à–µ–Ω –ª–∏–º–∏—Ç)
            if self._cache_enabled:
                try:
                    self._cache_dir.mkdir(parents=True, exist_ok=True)
                    with self._cache_file.open('w', encoding='utf-8') as f:
                        for rec in pairs:
                            f.write(json.dumps(rec, ensure_ascii=False) + '\n')
                    logger.info(f"üíæ Cached SNLI base subset to {self._cache_file}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to write SNLI cache: {e}")

            self._snli_pairs = pairs
            if max_samples is not None:
                return pairs[:max_samples]
            return pairs
        except Exception as e:
            logger.error(f"‚ùå SNLI data loading failed: {e}")
            return []

    def _build_cache_filename(self) -> str:
        """–ò–º—è —Ñ–∞–π–ª–∞ –∫—ç—à–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        # –ü—Ä–∏–º–µ—Ä: snli_frac0.2_min10.jsonl
        frac = f"{self.snli_fraction}".replace('.', 'p')
        return f"snli_frac{frac}_min{self.min_text_length}.jsonl"
    
    def get_text_pairs(self, max_samples: Optional[int] = None) -> List[Tuple[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ SNLI"""
        if not self.ensure_initialized():
            return []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        effective_max = max_samples
        if self.config.max_samples_per_source:
            if effective_max is not None:
                effective_max = min(effective_max, self.config.max_samples_per_source)
            else:
                effective_max = self.config.max_samples_per_source
        
        snli_data = self._load_snli_data(effective_max)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç (input, target)
        text_pairs = [(pair["input_text"], pair["target_text"]) for pair in snli_data]
        
        logger.debug(f"üìù SNLI text pairs: {len(text_pairs)} samples")
        return text_pairs
    
    def get_embeddings(self, max_samples: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è SNLI –ø–∞—Ä"""
        if not self.ensure_initialized():
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        if not self.teacher_provider:
            logger.warning("‚ùå No teacher provider available for embedding generation")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã
        text_pairs = self.get_text_pairs(max_samples)
        if not text_pairs:
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ input –∏ target —Ç–µ–∫—Å—Ç—ã
        input_texts = [pair[0] for pair in text_pairs]
        target_texts = [pair[1] for pair in text_pairs]
        
        logger.info(f"üîÑ Generating SNLI embeddings for {len(text_pairs)} pairs...")
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ teacher model
            input_embeddings = self.teacher_provider.encode_texts(input_texts)
            target_embeddings = self.teacher_provider.encode_texts(target_texts)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if not self.validate_embeddings(input_embeddings, "SNLI_input"):
                logger.warning("‚ùå SNLI input embeddings validation failed")
            if not self.validate_embeddings(target_embeddings, "SNLI_target"):
                logger.warning("‚ùå SNLI target embeddings validation failed")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            input_embeddings = self.normalize_embeddings(input_embeddings)
            target_embeddings = self.normalize_embeddings(target_embeddings)
            
            logger.info(f"‚úÖ SNLI embeddings generated: {input_embeddings.shape}")
            return input_embeddings, target_embeddings
            
        except Exception as e:
            logger.error(f"‚ùå SNLI embedding generation failed: {e}")
            empty_tensor = torch.empty(0, 768)
            return empty_tensor, empty_tensor
    
    def get_mixed_data(self, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å SNLI –¥–∞–Ω–Ω—ã–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        base_data = super().get_mixed_data(max_samples)
        
        # –î–æ–±–∞–≤–ª—è–µ–º SNLI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if base_data['text_pairs']:
            snli_data = self._load_snli_data(max_samples)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            base_data.update({
                'snli_labels': [pair['label'] for pair in snli_data],
                'snli_ids': [pair['snli_id'] for pair in snli_data],
                'dataset_fraction': self.snli_fraction,
                'label_distribution': self._get_label_distribution(snli_data)
            })
        
        return base_data
    
    def _get_label_distribution(self, snli_data: List[Dict]) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ labels –≤ –¥–∞–Ω–Ω—ã—Ö"""
        label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
        distribution = {}
        
        for pair in snli_data:
            label_name = label_names.get(pair["label"], "unknown")
            distribution[label_name] = distribution.get(label_name, 0) + 1
        
        return distribution
    
    def get_statistics(self) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è SNLI"""
        base_stats = super().get_statistics()
        
        if self._is_initialized:
            # –ë—ã—Å—Ç—Ä–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            sample_data = self._load_snli_data(max_samples=100)
            
            if sample_data:
                base_stats.update({
                    'dataset_fraction': self.snli_fraction,
                    'min_text_length': self.min_text_length,
                    'label_distribution': self._get_label_distribution(sample_data),
                    'estimated_total_pairs': int(len(sample_data) / min(100 / len(sample_data), 1)),
                    'teacher_provider_available': self.teacher_provider is not None
                })
        
        return base_stats


def create_snli_provider(config, teacher_provider: Optional[TeacherModelProvider] = None) -> SNLIProvider:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è SNLIProvider"""
    return SNLIProvider(config, teacher_provider)