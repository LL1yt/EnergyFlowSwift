"""
TeacherModelProvider - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—å—é-—É—á–∏—Ç–µ–ª–µ–º (DistilBERT)
==============================================================

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—å—é-—É—á–∏—Ç–µ–ª–µ–º:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ HuggingFace –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏  
- GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å legacy download_distilbert.py
"""

import torch
from transformers import AutoTokenizer, AutoModel
from typing import List, Tuple, Optional, Dict, Any
from pathlib import Path
import os
import time

from .base_provider import BaseDataProvider
from ...utils.logging import get_logger, DEBUG_INIT, DEBUG_MEMORY

logger = get_logger(__name__)


class TeacherModelProvider(BaseDataProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—å—é-—É—á–∏—Ç–µ–ª–µ–º (DistilBERT)
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ü—Ä–æ–≤–µ—Ä–∫—É –Ω–∞–ª–∏—á–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, config):
        super().__init__("TeacherModel", config)
        
        self.model_name = config.teacher_model
        self.local_model_path = config.get_absolute_local_model_path()
        self.use_local_model = config.use_local_model
        
        self.tokenizer = None
        self.model = None
        self.embedding_cache = {}  # –ü—Ä–æ—Å—Ç–æ–π cache –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤
        
        logger.log(DEBUG_INIT, f"TeacherModelProvider: model={self.model_name}, "
                              f"local_path={self.local_model_path}, use_local={self.use_local_model}")
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if self.use_local_model and self._check_local_model():
                logger.info(f"‚úÖ Local model available: {self.local_model_path}")
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤ HuggingFace
            logger.info(f"üåê Checking HuggingFace availability for {self.model_name}")
            return self._check_huggingface_model()
            
        except Exception as e:
            logger.error(f"‚ùå Model availability check failed: {e}")
            return False
    
    def _check_local_model(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.local_model_path.exists():
            logger.info(f"üìÅ Local model path not found: {self.local_model_path}")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        required_files = ["config.json", "tokenizer.json"]
        model_files = ["pytorch_model.bin", "model.safetensors"]  # –û–¥–∏–Ω –∏–∑ —ç—Ç–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å
        missing_files = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        for file in required_files:
            file_path = self.local_model_path / file
            if not file_path.exists():
                missing_files.append(file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏
        model_file_exists = any((self.local_model_path / model_file).exists() for model_file in model_files)
        if not model_file_exists:
            missing_files.append("model file (pytorch_model.bin or model.safetensors)")
        
        if missing_files:
            logger.warning(f"üìÅ Local model incomplete, missing: {missing_files}")
            return False
        
        logger.info(f"‚úÖ Local model complete: {self.local_model_path}")
        return True
    
    def _check_huggingface_model(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ HuggingFace"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(self.model_name)
            return config is not None
        except Exception as e:
            logger.warning(f"üåê HuggingFace model check failed: {e}")
            return False
    
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        logger.log(DEBUG_INIT, f"Initializing teacher model: {self.model_name}")
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏
            model_source = str(self.local_model_path) if (self.use_local_model and self._check_local_model()) else self.model_name
            
            logger.info(f"üîÑ Loading model from: {model_source}")
            start_time = time.time()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.tokenizer = AutoTokenizer.from_pretrained(model_source)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            self.model = AutoModel.from_pretrained(model_source)
            self.model.to(self.device)
            self.model.eval()
            
            load_time = time.time() - start_time
            
            # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            logger.info(f"‚úÖ Teacher model loaded successfully:")
            logger.info(f"   Source: {model_source}")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Load time: {load_time:.2f}s")
            logger.info(f"   Parameters: {total_params:,} total, {trainable_params:,} trainable")
            logger.log(DEBUG_MEMORY, f"Model memory: ~{total_params * 4 / 1024 / 1024:.1f} MB")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize teacher model: {e}")
            return False
    
    def download_model_if_needed(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        
        –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å legacy download_distilbert.py –ª–æ–≥–∏–∫–æ–π
        """
        if not self.use_local_model:
            return True  # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        
        if self._check_local_model():
            return True  # –ú–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å
        
        logger.info(f"üì• Downloading model to local cache: {self.local_model_path}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            self.local_model_path.mkdir(parents=True, exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            logger.info("üì• Downloading tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            tokenizer.save_pretrained(self.local_model_path)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            logger.info("üì• Downloading model...")
            model = AutoModel.from_pretrained(self.model_name)
            model.save_pretrained(self.local_model_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª–æ—Å—å
            if self._check_local_model():
                file_size_mb = sum(f.stat().st_size for f in self.local_model_path.rglob('*') if f.is_file()) / 1024 / 1024
                logger.info(f"‚úÖ Model downloaded successfully: {file_size_mb:.1f} MB")
                
                # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –∏–º–µ–Ω–Ω–æ –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
                saved_files = [f.name for f in self.local_model_path.iterdir() if f.is_file()]
                logger.info(f"   Files saved: {', '.join(saved_files)}")
                return True
            else:
                logger.error("‚ùå Model download verification failed")
                # –ü–æ–∫–∞–∂–µ–º —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if self.local_model_path.exists():
                    actual_files = [f.name for f in self.local_model_path.iterdir() if f.is_file()]
                    logger.error(f"   Found files: {actual_files}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Model download failed: {e}")
            return False
    
    def encode_texts(self, texts: List[str], use_cache: bool = True) -> torch.Tensor:
        """
        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –¢–µ–Ω–∑–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ [len(texts), embedding_dim]
        """
        if not self.ensure_initialized():
            raise RuntimeError("Teacher model not initialized")
        
        if not texts:
            raise ValueError("Empty text list")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        embeddings = []
        texts_to_process = []
        cache_hits = 0
        
        for text in texts:
            if use_cache and self.config.embedding_cache_enabled and text in self.embedding_cache:
                embeddings.append(self.embedding_cache[text])
                cache_hits += 1
            else:
                embeddings.append(None)
                texts_to_process.append((len(embeddings) - 1, text))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        if texts_to_process:
            logger.log(DEBUG_MEMORY, f"Processing {len(texts_to_process)} texts, cache hits: {cache_hits}")
            
            # –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            batch_size = self.config.cache_batch_size
            for i in range(0, len(texts_to_process), batch_size):
                batch_items = texts_to_process[i:i + batch_size]
                batch_texts = [item[1] for item in batch_items]
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞
                batch_embeddings = self._generate_embeddings_batch(batch_texts)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –∫—ç—à
                for j, (orig_idx, text) in enumerate(batch_items):
                    embedding = batch_embeddings[j]
                    embeddings[orig_idx] = embedding
                    
                    # –ö—ç—à–∏—Ä—É–µ–º –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                    if use_cache and self.config.embedding_cache_enabled:
                        self.embedding_cache[text] = embedding
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
        result = torch.stack(embeddings)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if not self.validate_embeddings(result, "teacher_embeddings"):
            logger.warning("‚ùå Generated embeddings failed validation")
        
        result = self.normalize_embeddings(result)
        
        logger.debug(f"‚úÖ Generated embeddings: {result.shape}, cache size: {len(self.embedding_cache)}")
        return result
    
    def _generate_embeddings_batch(self, texts: List[str]) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        with torch.no_grad():
            outputs = self.model(**inputs)
            hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden]
            
            # Mean pooling —Å —É—á–µ—Ç–æ–º attention mask
            attention_mask = inputs["attention_mask"].unsqueeze(-1)
            embeddings = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)
        
        return embeddings.cpu()
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        cache_size = len(self.embedding_cache)
        self.embedding_cache.clear()
        logger.info(f"üßπ Embedding cache cleared: {cache_size} entries removed")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if not self.embedding_cache:
            return {'size': 0, 'memory_mb': 0}
        
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏
        sample_embedding = next(iter(self.embedding_cache.values()))
        embedding_size_bytes = sample_embedding.numel() * 4  # float32
        total_memory_mb = len(self.embedding_cache) * embedding_size_bytes / 1024 / 1024
        
        return {
            'size': len(self.embedding_cache),
            'memory_mb': total_memory_mb,
            'embedding_dim': sample_embedding.shape[0] if sample_embedding.dim() == 1 else sample_embedding.shape[1]
        }
    
    # –ú–µ—Ç–æ–¥—ã BaseDataProvider (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è TeacherModel)
    def get_text_pairs(self, max_samples: Optional[int] = None) -> List[Tuple[str, str]]:
        """TeacherModel –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã"""
        return []
    
    def get_embeddings(self, max_samples: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """TeacherModel –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        empty_tensor = torch.empty(0, 768)  # DistilBERT dimension
        return empty_tensor, empty_tensor
    
    def get_statistics(self) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è TeacherModel"""
        base_stats = super().get_statistics()
        
        if self._is_initialized:
            cache_info = self.get_cache_info()
            base_stats.update({
                'model_name': self.model_name,
                'local_model_available': self._check_local_model(),
                'cache_info': cache_info,
                'embedding_dimension': 768 if self.model else None
            })
        
        return base_stats


def create_teacher_model_provider(config) -> TeacherModelProvider:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è TeacherModelProvider"""
    return TeacherModelProvider(config)