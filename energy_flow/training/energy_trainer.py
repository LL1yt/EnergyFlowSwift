"""
EnergyTrainer - –æ—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
=========================================================================

–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π text_bridge –º–æ–¥—É–ª—è:
- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ energy flow + text decoders
- GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å CUDA –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é  
- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏
- –ß–µ–∫–ø–æ–∏–Ω—Ç–∏–Ω–≥ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:
input_text ‚Üí TextToCubeEncoder ‚Üí surface_embedding ‚Üí FlowProcessor ‚Üí 
output_surface_embedding ‚Üí CubeToTextDecoder ‚Üí predicted_text

Loss = energy_loss + text_loss_weight √ó text_loss
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
from pathlib import Path
import time
from datetime import datetime
import json

from ..utils.logging import get_logger, DEBUG_TRAINING, DEBUG_ENERGY, DEBUG_CONVERGENCE
from ..utils.device_manager import get_device_manager
from ..config import EnergyConfig, get_energy_config, create_debug_config, set_energy_config
from ..core import FlowProcessor, EnergyLattice, SimpleNeuron, EnergyCarrier
from ..text_bridge import (
    TextToCubeEncoder, CubeToTextDecoder, TextCache,
    create_text_to_cube_encoder, create_cube_to_text_decoder, create_text_cache,
    CachedTextToCubeEncoder, CachedCubeToTextDecoder
)

logger = get_logger(__name__)


class EnergyTrainer:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è energy_flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π text_bridge:
    - –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ç–æ–∫–∏ —á–µ—Ä–µ–∑ 3D —Ä–µ—à–µ—Ç–∫—É
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ text decoders
    - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π loss (energy + text)
    - GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, config: Optional[EnergyConfig] = None):
        """
        Args:
            config: EnergyConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        if config is None:
            config = create_debug_config()
            set_energy_config(config)
        self.config = config
        
        # Device management  
        self.device_manager = get_device_manager() 
        self.device = self.device_manager.device
        
        logger.log(DEBUG_TRAINING, f"üöÄ EnergyTrainer initialization on {self.device}")
        logger.log(DEBUG_TRAINING, f"Config: {config.lattice_width}x{config.lattice_height}x{config.lattice_depth}, "
                                  f"text_bridge={config.text_bridge_enabled}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._init_core_components()
        self._init_text_bridge()
        self._init_optimizer()
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        self.training_history = {
            "total_losses": [],
            "energy_losses": [],
            "text_losses": [],
            "learning_rates": [],
            "flow_statistics": [],
            "performance_metrics": []
        }
        
        # –°—á–µ—Ç—á–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float('inf')
        
        logger.log(DEBUG_TRAINING, "‚úÖ EnergyTrainer successfully initialized")
    
    def _init_core_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ energy_flow"""
        logger.log(DEBUG_TRAINING, "Initializing core energy_flow components...")
        
        # FlowProcessor –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ core –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.flow_processor = FlowProcessor(self.config).to(self.device)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.energy_lattice = self.flow_processor.energy_lattice
        self.simple_neuron = self.flow_processor.simple_neuron
        self.energy_carrier = self.flow_processor.energy_carrier
        
        logger.log(DEBUG_TRAINING, f"Core components initialized: "
                                  f"FlowProcessor, EnergyLattice({self.config.lattice_width}x{self.config.lattice_height}x{self.config.lattice_depth})")
    
    def _init_text_bridge(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è text_bridge –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        if not self.config.text_bridge_enabled:
            logger.log(DEBUG_TRAINING, "Text bridge disabled, skipping initialization")
            self.text_encoder = None
            self.text_decoder = None
            self.text_cache = None
            return
            
        logger.log(DEBUG_TRAINING, "Initializing text_bridge components...")
        
        # Text cache (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if self.config.text_cache_enabled:
            self.text_cache = create_text_cache(
                max_size=self.config.text_cache_size,
                cache_file=self.config.text_cache_file
            )
            logger.log(DEBUG_TRAINING, f"TextCache initialized with size {self.config.text_cache_size}")
        else:
            self.text_cache = None
        
        # Text encoder (text ‚Üí surface embeddings)
        base_encoder = create_text_to_cube_encoder(self.config.surface_dimension).to(self.device)
        if self.text_cache:
            self.text_encoder = CachedTextToCubeEncoder(base_encoder, self.text_cache)
        else:
            self.text_encoder = base_encoder
            
        # Text decoder (surface embeddings ‚Üí text)
        base_decoder = create_cube_to_text_decoder(self.config.surface_dimension).to(self.device)
        if self.text_cache:
            self.text_decoder = CachedCubeToTextDecoder(base_decoder, self.text_cache)
        else:
            self.text_decoder = base_decoder
        
        logger.log(DEBUG_TRAINING, f"Text bridge initialized: encoder({base_encoder.count_parameters()} params), "
                                  f"decoder({base_decoder.count_parameters()} params)")
    
    def _init_optimizer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = list(self.flow_processor.parameters())
        
        if self.config.text_bridge_enabled:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã text_bridge –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            if hasattr(self.text_encoder, 'model'):  # Cached version
                params.extend(self.text_encoder.model.parameters())
            else:  # Direct model
                params.extend(self.text_encoder.parameters())
                
            if hasattr(self.text_decoder, 'model'):  # Cached version  
                params.extend(self.text_decoder.model.parameters())
            else:  # Direct model
                params.extend(self.text_decoder.parameters())
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.AdamW(
            params,
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=10,
            verbose=True
        )
        
        total_params = sum(p.numel() for p in params if p.requires_grad)
        logger.log(DEBUG_TRAINING, f"Optimizer initialized: AdamW, lr={self.config.learning_rate}, "
                                  f"total_params={total_params:,}")
    
    def train_step(self, input_texts: List[str], target_texts: List[str]) -> Dict[str, float]:
        """
        –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            input_texts: –°–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            target_texts: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —à–∞–≥–∞
        """
        self.optimizer.zero_grad()
        
        batch_size = len(input_texts)
        step_start_time = time.time()
        
        try:
            # 1. –≠–Ω–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤ surface embeddings
            if self.config.text_bridge_enabled:
                with torch.no_grad():  # Encoder –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ
                    surface_inputs = []
                    for text in input_texts:
                        surface_emb = self.text_encoder.encode_text(text)
                        surface_inputs.append(surface_emb)
                    surface_inputs = torch.stack(surface_inputs, dim=0)  # [batch_size, surface_dim]
            else:
                # Fallback: —Å–ª—É—á–∞–π–Ω—ã–µ surface embeddings –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                surface_inputs = torch.randn(batch_size, self.config.surface_dimension, 
                                           device=self.device, dtype=self.config.dtype)
            
            # 2. –ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ energy flow –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
            flow_start_time = time.time()
            surface_outputs = self.flow_processor.forward(surface_inputs, max_steps=50)
            flow_time = time.time() - flow_start_time
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Ç–æ–∫–æ–≤
            flow_stats = self.flow_processor.get_flow_statistics()
            
            # 3. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ energy loss (MSE –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º –∏ –≤—ã—Ö–æ–¥–æ–º)
            energy_loss = nn.functional.mse_loss(surface_outputs, surface_inputs)
            
            # 4. Text loss (–µ—Å–ª–∏ text_bridge –≤–∫–ª—é—á–µ–Ω)
            text_loss = torch.tensor(0.0, device=self.device)
            if self.config.text_bridge_enabled and self.config.text_loss_weight > 0:
                try:
                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ surface embeddings –≤ —Ç–µ–∫—Å—Ç
                    predicted_texts = []
                    for i in range(batch_size):
                        pred_text = self.text_decoder.decode_surface(surface_outputs[i])
                        predicted_texts.append(pred_text)
                    
                    # –≠–Ω–∫–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    target_surface_embeddings = []
                    for text in target_texts:
                        target_emb = self.text_encoder.encode_text(text)
                        target_surface_embeddings.append(target_emb)
                    target_surface_embeddings = torch.stack(target_surface_embeddings, dim=0)
                    
                    # Text loss –∫–∞–∫ MSE –º–µ–∂–¥—É surface embeddings
                    text_loss = nn.functional.mse_loss(surface_outputs, target_surface_embeddings)
                    
                except Exception as e:
                    logger.warning(f"Text loss computation failed: {e}")
                    text_loss = torch.tensor(0.0, device=self.device)
            
            # 5. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π loss
            total_loss = energy_loss + self.config.text_loss_weight * text_loss
            
            # 6. –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            total_loss.backward()
            
            # Gradient clipping
            if self.config.gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.optimizer.param_groups[0]['params'], 
                    self.config.gradient_clip
                )
            
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —à–∞–≥–∞
            step_time = time.time() - step_start_time
            
            step_metrics = {
                'total_loss': total_loss.item(),
                'energy_loss': energy_loss.item(), 
                'text_loss': text_loss.item(),
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'step_time': step_time,
                'flow_time': flow_time,
                'active_flows': flow_stats.get('active_flows', 0),
                'spawned_flows': flow_stats.get('spawned_flows', 0),
                'flows_reached_output': flow_stats.get('flows_reached_output', 0),
                'batch_size': batch_size
            }
            
            self.global_step += 1
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if self.global_step % self.config.log_interval == 0:
                logger.log(DEBUG_TRAINING, 
                          f"Step {self.global_step}: total_loss={total_loss.item():.4f}, "
                          f"energy_loss={energy_loss.item():.4f}, text_loss={text_loss.item():.4f}")
                logger.log(DEBUG_ENERGY,
                          f"Flow stats: active={flow_stats.get('active_flows', 0)}, "
                          f"spawned={flow_stats.get('spawned_flows', 0)}, "
                          f"reached_output={flow_stats.get('flows_reached_output', 0)}")
            
            return step_metrics
            
        except Exception as e:
            logger.error(f"Training step failed: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º dummy –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            return {
                'total_loss': float('inf'),
                'energy_loss': float('inf'),
                'text_loss': 0.0,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'step_time': time.time() - step_start_time,
                'flow_time': 0.0,
                'active_flows': 0,
                'spawned_flows': 0, 
                'flows_reached_output': 0,
                'batch_size': batch_size,
                'error': str(e)
            }
    
    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ
        
        Args:
            dataloader: DataLoader —Å –ø–∞—Ä–∞–º–∏ (input_texts, target_texts)
            
        Returns:
            –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç–ø–æ—Ö–µ
        """
        self.flow_processor.train()
        if self.config.text_bridge_enabled:
            self.text_encoder.train() if hasattr(self.text_encoder, 'train') else None
            self.text_decoder.train()
        
        epoch_metrics = {
            'total_loss': 0.0,
            'energy_loss': 0.0,
            'text_loss': 0.0,
            'step_time': 0.0,
            'flow_time': 0.0,
            'active_flows': 0.0,
            'spawned_flows': 0.0,
            'flows_reached_output': 0.0
        }
        
        total_batches = 0
        epoch_start_time = time.time()
        
        for batch_idx, batch_data in enumerate(dataloader):
            # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ñ–æ—Ä–º–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç DataLoader)
            if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 2:
                input_texts, target_texts = batch_data[0], batch_data[1]
            else:
                logger.warning(f"Unexpected batch format: {type(batch_data)}")
                continue
            
            # –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            step_metrics = self.train_step(input_texts, target_texts)
            
            # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
            for key in epoch_metrics:
                if key in step_metrics:
                    epoch_metrics[key] += step_metrics[key]
            
            total_batches += 1
            
            # Periodic logging –≤–Ω—É—Ç—Ä–∏ —ç–ø–æ—Ö–∏
            if batch_idx % (self.config.log_interval * 5) == 0:
                logger.log(DEBUG_TRAINING,
                          f"Epoch {self.epoch}, Batch {batch_idx}/{len(dataloader)}: "
                          f"loss={step_metrics.get('total_loss', 0):.4f}")
        
        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ —ç–ø–æ—Ö–µ
        if total_batches > 0:
            for key in epoch_metrics:
                epoch_metrics[key] /= total_batches
        
        epoch_time = time.time() - epoch_start_time
        epoch_metrics['epoch_time'] = epoch_time
        epoch_metrics['total_batches'] = total_batches
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
        self.scheduler.step(epoch_metrics['total_loss'])
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–æ—Ö–∏
        logger.log(DEBUG_TRAINING,
                  f"‚úÖ Epoch {self.epoch} completed: "
                  f"avg_loss={epoch_metrics['total_loss']:.4f}, "
                  f"time={epoch_time:.1f}s, batches={total_batches}")
        logger.log(DEBUG_CONVERGENCE,
                  f"Convergence stats: flows_reached_output={epoch_metrics['flows_reached_output']:.1f}, "
                  f"active_flows={epoch_metrics['active_flows']:.1f}")
        
        self.epoch += 1
        return epoch_metrics
    
    def train(self, dataloader: DataLoader, num_epochs: int = 10) -> Dict[str, List]:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            dataloader: DataLoader —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            
        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üöÄ Starting training: {num_epochs} epochs, batch_size={self.config.batch_size}")
        
        training_start_time = time.time()
        
        for epoch in range(num_epochs):
            epoch_metrics = self.train_epoch(dataloader)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            for key in epoch_metrics:
                if key in self.training_history:
                    self.training_history[key].append(epoch_metrics[key])
            
            # –ß–µ–∫–ø–æ–∏–Ω—Ç–∏–Ω–≥ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if epoch_metrics['total_loss'] < self.best_loss:
                self.best_loss = epoch_metrics['total_loss']
                self.save_checkpoint(f"best_model_epoch_{epoch}.pt")
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            if epoch % self.config.checkpoint_interval == 0:
                self.save_checkpoint(f"checkpoint_epoch_{epoch}.pt")
        
        training_time = time.time() - training_start_time
        
        logger.info(f"‚úÖ Training completed: {num_epochs} epochs, "
                   f"total_time={training_time:.1f}s, "
                   f"best_loss={self.best_loss:.4f}")
        
        return self.training_history
    
    def validate(self, input_texts: List[str], target_texts: List[str]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
        Args:
            input_texts: –í—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            target_texts: –¶–µ–ª–µ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        self.flow_processor.eval()
        if self.config.text_bridge_enabled:
            if hasattr(self.text_encoder, 'eval'):
                self.text_encoder.eval()
            self.text_decoder.eval()
        
        with torch.no_grad():
            val_metrics = self.train_step(input_texts, target_texts)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            examples = []
            if self.config.text_bridge_enabled:
                num_examples = min(3, len(input_texts))
                for i in range(num_examples):
                    try:
                        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π pipeline
                        surface_input = self.text_encoder.encode_text(input_texts[i])
                        surface_output = self.flow_processor.forward(surface_input.unsqueeze(0), max_steps=50)
                        predicted_text = self.text_decoder.decode_surface(surface_output[0])
                        
                        examples.append({
                            'input': input_texts[i],
                            'target': target_texts[i],
                            'predicted': predicted_text
                        })
                    except Exception as e:
                        logger.warning(f"Example generation failed for sample {i}: {e}")
        
        val_metrics['examples'] = examples
        
        logger.log(DEBUG_TRAINING, f"Validation: loss={val_metrics.get('total_loss', 0):.4f}")
        if examples:
            logger.log(DEBUG_TRAINING, f"Example - Input: '{examples[0]['input'][:50]}...'")
            logger.log(DEBUG_TRAINING, f"Example - Predicted: '{examples[0]['predicted'][:50]}...'")
        
        return val_metrics
    
    def save_checkpoint(self, filepath: Union[str, Path]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –º–æ–¥–µ–ª–∏"""
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'config': self.config.to_dict(),
            'model_state_dict': self.flow_processor.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_loss': self.best_loss,
            'training_history': self.training_history
        }
        
        if self.config.text_bridge_enabled:
            if hasattr(self.text_encoder, 'state_dict'):
                checkpoint['text_encoder_state_dict'] = self.text_encoder.state_dict()
            checkpoint['text_decoder_state_dict'] = self.text_decoder.state_dict()
        
        torch.save(checkpoint, filepath)
        logger.info(f"üíæ Checkpoint saved: {filepath}")
    
    def load_checkpoint(self, filepath: Union[str, Path]) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –º–æ–¥–µ–ª–∏"""
        filepath = Path(filepath)
        if not filepath.exists():
            raise FileNotFoundError(f"Checkpoint not found: {filepath}")
        
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.best_loss = checkpoint['best_loss']
        self.training_history = checkpoint['training_history']
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
        self.flow_processor.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.config.text_bridge_enabled:
            if 'text_encoder_state_dict' in checkpoint and hasattr(self.text_encoder, 'load_state_dict'):
                self.text_encoder.load_state_dict(checkpoint['text_encoder_state_dict'])
            if 'text_decoder_state_dict' in checkpoint:
                self.text_decoder.load_state_dict(checkpoint['text_decoder_state_dict'])
        
        logger.info(f"üìÅ Checkpoint loaded: {filepath}, epoch={self.epoch}, step={self.global_step}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –∏ –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö"""
        info = {
            'config': self.config.to_dict(),
            'epoch': self.epoch,
            'global_step': self.global_step,
            'best_loss': self.best_loss,
            'device': str(self.device)
        }
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        flow_params = sum(p.numel() for p in self.flow_processor.parameters() if p.requires_grad)
        info['flow_processor_parameters'] = flow_params
        
        if self.config.text_bridge_enabled:
            if hasattr(self.text_encoder, 'count_parameters'):
                info['text_encoder_parameters'] = self.text_encoder.count_parameters()
            if hasattr(self.text_decoder, 'count_parameters'):
                info['text_decoder_parameters'] = self.text_decoder.count_parameters()
        
        return info


def create_energy_trainer(config: Optional[EnergyConfig] = None) -> EnergyTrainer:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è EnergyTrainer"""
    return EnergyTrainer(config)