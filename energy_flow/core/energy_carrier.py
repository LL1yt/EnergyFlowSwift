"""
Energy Carrier - GRU-based —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ç–æ–∫–∏
================================================

GRU –º–æ–¥–µ–ª—å —Å ~10M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏.
–û–±—â–∏–µ –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö GRU –ø–æ—Ç–æ–∫–æ–≤ –≤ —Ä–µ—à–µ—Ç–∫–µ.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

from ..utils.logging import get_logger
from ..config import create_debug_config, set_energy_config

logger = get_logger(__name__)


@dataclass
class SpawnInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ batch —ç–ª–µ–º–µ–Ω—Ç–∞"""
    energies: List[torch.Tensor]    # –≠–Ω–µ—Ä–≥–∏–∏ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    parent_batch_idx: int          # –ò–Ω–¥–µ–∫—Å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞


@dataclass
class EnergyOutput:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ EnergyCarrier"""
    energy_value: torch.Tensor      # –¢–µ–∫—É—â–∞—è —ç–Ω–µ—Ä–≥–∏—è/—ç–º–±–µ–¥–¥–∏–Ω–≥
    next_position: torch.Tensor     # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–ª–µ–¥—É—é—â–µ–π –∫–ª–µ—Ç–∫–∏
    spawn_info: List[SpawnInfo]     # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ spawn'–∞—Ö
    
    # –§–ª–∞–≥–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ (–¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ FlowProcessor)
    is_terminated: torch.Tensor     # [batch] - –º–∞—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    termination_reason: List[str]   # –ü—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞


class EnergyCarrier(nn.Module):
    """
    GRU-based –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
    - –í—ã—Ö–æ–¥ SimpleNeuron
    - –ß–∞—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    - –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
    
    –í—ã–¥–∞–µ—Ç:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —Å —ç–Ω–µ—Ä–≥–∏–µ–π, –ø–æ–∑–∏—Ü–∏–µ–π –∏ –Ω–æ–≤—ã–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏
    """
    
    def __init__(self, config=None):
        """
        Args:
            config: EnergyConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏. –ï—Å–ª–∏ None - –±–µ—Ä–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        """
        super().__init__()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config is None:
            config = create_debug_config()
            set_energy_config(config)
        self.config = config
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        self.hidden_size = config.carrier_hidden_size
        self.num_layers = config.carrier_num_layers
        # –£–î–ê–õ–ï–ù–û: dropout —Å–ª–æ–∏ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Ç–æ–∫–æ–≤ —Ç–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –¥–ª–∏–Ω–µ —Å–º–µ—â–µ–Ω–∏—è, –∞ –Ω–µ –Ω–∞ dropout
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.neuron_output_dim = config.neuron_output_dim  # –í—ã—Ö–æ–¥ SimpleNeuron (64)
        self.energy_dim = 1                                # –°–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –æ—Ç mapper'–∞
        self.input_dim = self.neuron_output_dim + self.energy_dim  # 64 + 1 = 65
        
        # GRU —Å–ª–æ–∏
        self.gru = nn.GRU(
            input_size=self.input_dim,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=0.0,  # Dropout –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
            batch_first=True
        )
        
        # Projection heads –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        # 1. –°–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è (–≤—ã—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∫–∞–ª—è—Ä–æ–º –¥–ª—è consistency)
        self.energy_projection = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.GELU(),
            # Dropout —Å–ª–æ–π —É–¥–∞–ª–µ–Ω
            nn.Linear(self.hidden_size // 2, self.energy_dim),  # –í—ã—Ö–æ–¥: 1 —Å–∫–∞–ª—è—Ä
            nn.Tanh()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [-1, 1]
        )
        
        # 2. –°–º–µ—â–µ–Ω–∏—è - –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è (Œîx, Œîy, Œîz)
        self.displacement_projection = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.GELU(),
            # Dropout —Å–ª–æ–π —É–¥–∞–ª–µ–Ω
            nn.Linear(64, 3)  # Œîx, Œîy, Œîz —Å–º–µ—â–µ–Ω–∏—è (–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
        )
        self.displacement_activation = self.config.normalization_manager.get_displacement_activation()  # Tanh –¥–ª—è [-1, 1]
        
        # 3. –£–î–ê–õ–ï–ù–û: spawn_gate –∏ spawn_energy_projection
        # –í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç spawn –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è 
        # —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–º–µ—â–µ–Ω–∏—è –≤ FlowProcessor
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"EnergyCarrier initialized with {total_params:,} parameters")
        logger.debug(f"GRU: input={self.input_dim}, hidden={self.hidden_size}, layers={self.num_layers}")
        
    
    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å smart initialization –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –≤–ø–µ—Ä–µ–¥"""
        # GRU —É–∂–µ –∏–º–µ–µ—Ç —Ö–æ—Ä–æ—à—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º projection heads (spawn –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É–¥–∞–ª–µ–Ω—ã)
        for module in [self.energy_projection, self.displacement_projection]:
            if isinstance(module, nn.Sequential):
                for layer in module:
                    if isinstance(layer, nn.Linear):
                        nn.init.xavier_normal_(layer.weight)
                        if layer.bias is not None:
                            nn.init.zeros_(layer.bias)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
        # –î–ª—è –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç:
        # - –ù–µ—Ç smart initialization (—Å–º–µ—â–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ 0)
        # - –ù–µ—Ç bias –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –≤–ø–µ—Ä–µ–¥ (–º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–∞–º–∞)
        logger.debug_init("üèóÔ∏è Relative coordinates architecture: no smart initialization, model learns naturally")
    
    def forward(self, 
                neuron_output: torch.Tensor,
                embedding_part: torch.Tensor,
                hidden_state: Optional[torch.Tensor] = None,
                current_position: Optional[torch.Tensor] = None,
                flow_age: Optional[torch.Tensor] = None,
                global_training_step: Optional[int] = None) -> Tuple[EnergyOutput, torch.Tensor]:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ EnergyCarrier
        
        Args:
            neuron_output: [batch, neuron_output_dim] - –≤—ã—Ö–æ–¥ SimpleNeuron
            embedding_part: [batch, embedding_dim] - —á–∞—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            hidden_state: [num_layers, batch, hidden_size] - —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
            current_position: [batch, 3] - —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–π)
            flow_age: [batch] - –≤–æ–∑—Ä–∞—Å—Ç –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è progressive bias
            global_training_step: –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è curriculum learning
            
        Returns:
            output: EnergyOutput - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            new_hidden: [num_layers, batch, hidden_size] - –Ω–æ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        """
        batch_size = neuron_output.shape[0]
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ö–æ–¥–∞
        if global_training_step is not None:
            logger.debug_energy(f"üîÑ EnergyCarrier forward: batch={batch_size}, global_step={global_training_step}")
            if current_position is not None:
                current_z = current_position[:, 2]
                logger.debug_energy(f"üìç Current Z positions: min={current_z.min():.3f}, "
                           f"max={current_z.max():.3f}, mean={current_z.mean():.3f}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Ö–æ–¥—ã
        combined_input = torch.cat([neuron_output, embedding_part], dim=-1)
        combined_input = combined_input.unsqueeze(1)  # [batch, 1, input_dim] –¥–ª—è GRU
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ GRU
        gru_output, new_hidden = self.gru(combined_input, hidden_state)
        gru_output = gru_output.squeeze(1)  # [batch, hidden_size]
        
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —ç–Ω–µ—Ä–≥–∏—é
        energy_value = self.energy_projection(gru_output)  # [batch, embedding_dim]
        
        # 2. –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏—è (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º GRU –≤—ã—Ö–æ–¥ –ø–µ—Ä–µ–¥ displacement_projection
        if global_training_step is not None and global_training_step == 0:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —à–∞–≥
            logger.debug_forward(f"üß† GRU output stats: min={gru_output.min():.3f}, max={gru_output.max():.3f}, "
                       f"mean={gru_output.mean():.3f}, std={gru_output.std():.3f}")
            
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º bias'—ã –≤ displacement_projection
            for i, module in enumerate(self.displacement_projection):
                if isinstance(module, nn.Linear) and module.bias is not None:
                    bias_stats = module.bias.data
                    logger.debug_forward(f"üìä displacement_projection[{i}] bias: "
                                       f"min={bias_stats.min():.4f}, max={bias_stats.max():.4f}, "
                                       f"mean={bias_stats.mean():.4f}, std={bias_stats.std():.4f}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥ —Å–º–µ—â–µ–Ω–∏–π (–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
        displacement_raw = self.displacement_projection(gru_output)  # [batch, 3] –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (–î–û Clamp)
        if global_training_step is not None and global_training_step == 0:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —à–∞–≥
            raw_delta_z = displacement_raw[:, 2]
            logger.debug_forward(f"üî• RAW displacement output (before Clamp): ŒîZ min={raw_delta_z.min():.3f}, "
                       f"max={raw_delta_z.max():.3f}, mean={raw_delta_z.mean():.3f}, std={raw_delta_z.std():.3f}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (Clamp –≤–º–µ—Å—Ç–æ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ Tanh)
        displacement_normalized = torch.clamp(displacement_raw, -1.0, 1.0)  # [batch, 3] –≤ [-1, 1]
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è (–ü–û–°–õ–ï Clamp)
        norm_delta_z = displacement_normalized[:, 2]
        logger.debug_energy(f"üìä Normalized displacement (after Clamp): ŒîZ min={norm_delta_z.min():.3f}, "
                       f"max={norm_delta_z.max():.3f}, mean={norm_delta_z.mean():.3f}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è –∫ —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏ (–≤—Å–µ –≤ [-1, 1] –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ)
        if current_position is not None:
            next_position = current_position + displacement_normalized
        else:
            # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—â–µ–Ω–∏—è –∫–∞–∫ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            logger.warning("‚ö†Ô∏è Current position is None, using displacement as absolute position")
            next_position = displacement_normalized
        
        # Exploration noise –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—É—Ç–µ–π (–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ)
        if self.config.use_exploration_noise:
            # Exploration noise —Ç–æ–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
            noise = torch.randn_like(displacement_normalized) * self.config.exploration_noise
            next_position += noise
            logger.debug(f"üé≤ Added normalized exploration noise: std={self.config.exploration_noise}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –Ω–æ–≤–æ–π —Ç—Ä–µ—Ö–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        next_position, is_terminated, termination_reasons = self._compute_next_position_relative(next_position)
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if logger.isEnabledFor(10):  # DEBUG level
            terminated_count = is_terminated.sum().item()
            logger.debug(f"üõ°Ô∏è Termination: {terminated_count}/{batch_size} flows terminated")
            if terminated_count > 0:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã
                reason_counts = {}
                for reason in termination_reasons:
                    reason_counts[reason] = reason_counts.get(reason, 0) + 1
                logger.debug(f"üìà Termination reasons: {reason_counts}")
        
        # 3. Spawn –ø–æ—Ç–æ–∫–æ–≤ —Ç–µ–ø–µ—Ä—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ movement_based_spawn –≤ FlowProcessor
        # –£—Å—Ç–∞—Ä–µ–≤—à–∞—è –ª–æ–≥–∏–∫–∞ spawn –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É–¥–∞–ª–µ–Ω–∞
        spawn_info = []  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, spawn –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è –≤ FlowProcessor
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        output = EnergyOutput(
            energy_value=energy_value,
            next_position=next_position,
            spawn_info=spawn_info,
            is_terminated=is_terminated,
            termination_reason=termination_reasons
        )
        
        return output, new_hidden
    
    def _compute_next_position_relative(self, 
                                   next_position: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–ª–µ–¥—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é –¥–ª—è —Ç—Ä–µ—Ö–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        
        –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ —Ç—Ä–µ—Ö–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
        1. –í—Ö–æ–¥–Ω–∞—è –ø–ª–æ—Å–∫–æ—Å—Ç—å: Z = depth/2 (—Ü–µ–Ω—Ç—Ä –∫—É–±–∞)
        2. –í—ã—Ö–æ–¥–Ω—ã–µ –ø–ª–æ—Å–∫–æ—Å—Ç–∏: Z = 0 –∏ Z = depth (–∫—Ä–∞—è –∫—É–±–∞)
        3. X/Y –≥—Ä–∞–Ω–∏—Ü—ã: –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ FlowProcessor
        4. –ü–æ—Ç–æ–∫–∏ –∑–∞–≤–µ—Ä—à–∞—é—Ç—Å—è –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ Z ‚â§ 0 –∏–ª–∏ Z ‚â• depth
        
        Args:
            next_position: [batch, 3] - –ø–æ–∑–∏—Ü–∏—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è
            
        Returns:
            next_position: [batch, 3] - —Å–ª–µ–¥—É—é—â–∞—è –ø–æ–∑–∏—Ü–∏—è (—Ü–µ–ª—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
            is_terminated: [batch] - –º–∞—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤  
            termination_reasons: List[str] - –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        """
        batch_size = next_position.shape[0]
        is_terminated = torch.zeros(batch_size, dtype=torch.bool, device=next_position.device)
        termination_reasons = []
        
        depth = self.config.lattice_depth
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–æ Z –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ –≤ —Ç—Ä–µ—Ö–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
        # Z ‚â§ 0: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –ª–µ–≤–æ–π –≤—ã—Ö–æ–¥–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏
        # Z ‚â• depth: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–π –≤—ã—Ö–æ–¥–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏
        reached_z0_plane = next_position[:, 2] <= 0
        reached_zdepth_plane = next_position[:, 2] >= depth
        reached_output_plane = reached_z0_plane | reached_zdepth_plane
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –ø–æ X –∏ Y (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ–º –≤ FlowProcessor)
        out_of_bounds_x = (next_position[:, 0] < 0) | (next_position[:, 0] >= self.config.lattice_width)
        out_of_bounds_y = (next_position[:, 1] < 0) | (next_position[:, 1] >= self.config.lattice_height)
        out_of_bounds_xy = out_of_bounds_x | out_of_bounds_y
        
        # –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ X/Y –≥—Ä–∞–Ω–∏—Ü—ã –ù–ï –∑–∞–≤–µ—Ä—à–∞—é—Ç –ø–æ—Ç–æ–∫ (–æ—Ç—Ä–∞–∂–µ–Ω–∏–µ)
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –ø–æ Z
        is_terminated = reached_output_plane
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        for i in range(batch_size):
            if reached_z0_plane[i]:
                termination_reasons.append("reached_z0_plane")  # –õ–µ–≤–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è –ø–ª–æ—Å–∫–æ—Å—Ç—å
            elif reached_zdepth_plane[i]:
                termination_reasons.append("reached_zdepth_plane")  # –ü—Ä–∞–≤–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è –ø–ª–æ—Å–∫–æ—Å—Ç—å
            elif out_of_bounds_xy[i]:
                termination_reasons.append("xy_reflection_needed")  # –¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ (–Ω–æ –ø–æ—Ç–æ–∫ –∞–∫—Ç–∏–≤–µ–Ω)
            else:
                termination_reasons.append("active")  # –ü–æ—Ç–æ–∫ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ
        
        # –î–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤—ã—Ö–æ–¥–Ω—É—é –ø–ª–æ—Å–∫–æ—Å—Ç—å
        final_position = next_position.clone()
        
        # –ü—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Z=0 –ø–ª–æ—Å–∫–æ—Å—Ç—å
        if reached_z0_plane.any():
            final_position[reached_z0_plane, 2] = 0
        
        # –ü—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Z=depth –ø–ª–æ—Å–∫–æ—Å—Ç—å
        if reached_zdepth_plane.any():
            final_position[reached_zdepth_plane, 2] = depth
        
        # –û–∫—Ä—É–≥–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π —Ä–µ—à–µ—Ç–∫–∏
        final_position = torch.round(final_position)
        
        return final_position, is_terminated, termination_reasons
    
    def init_hidden(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è GRU"""
        return torch.zeros(
            self.num_layers, batch_size, self.hidden_size,
            device=device, dtype=torch.float32
        )
    
    # –£–î–ê–õ–ï–ù: check_energy_level() - –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç 
    # –ø–æ—Ç–æ–∫–∏ –Ω–µ —É–º–∏—Ä–∞—é—Ç –æ—Ç "–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —ç–Ω–µ—Ä–≥–∏–∏". –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ - —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ, –∞ –Ω–µ —ç–Ω–µ—Ä–≥–∏—è.


def create_energy_carrier(config=None) -> EnergyCarrier:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è EnergyCarrier"""
    return EnergyCarrier(config)