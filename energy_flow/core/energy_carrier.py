"""
Energy Carrier - GRU-based —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ç–æ–∫–∏
================================================

GRU –º–æ–¥–µ–ª—å —Å ~10M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏.
–û–±—â–∏–µ –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö GRU –ø–æ—Ç–æ–∫–æ–≤ –≤ —Ä–µ—à–µ—Ç–∫–µ.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

from ..utils.logging import get_logger
from ..config import create_debug_config, set_energy_config

logger = get_logger(__name__)


@dataclass
class SpawnInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ batch —ç–ª–µ–º–µ–Ω—Ç–∞"""
    energies: List[torch.Tensor]    # –≠–Ω–µ—Ä–≥–∏–∏ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    parent_batch_idx: int          # –ò–Ω–¥–µ–∫—Å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞


@dataclass
class EnergyOutput:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ EnergyCarrier"""
    energy_value: torch.Tensor      # –¢–µ–∫—É—â–∞—è —ç–Ω–µ—Ä–≥–∏—è/—ç–º–±–µ–¥–¥–∏–Ω–≥
    next_position: torch.Tensor     # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–ª–µ–¥—É—é—â–µ–π –∫–ª–µ—Ç–∫–∏
    spawn_info: List[SpawnInfo]     # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ spawn'–∞—Ö
    
    # –§–ª–∞–≥–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ (–¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ FlowProcessor)
    is_terminated: torch.Tensor     # [batch] - –º–∞—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    termination_reason: List[str]   # –ü—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞


class EnergyCarrier(nn.Module):
    """
    GRU-based –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
    - –í—ã—Ö–æ–¥ SimpleNeuron
    - –ß–∞—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    - –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
    
    –í—ã–¥–∞–µ—Ç:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —Å —ç–Ω–µ—Ä–≥–∏–µ–π, –ø–æ–∑–∏—Ü–∏–µ–π –∏ –Ω–æ–≤—ã–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏
    """
    
    def __init__(self, config=None):
        """
        Args:
            config: EnergyConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏. –ï—Å–ª–∏ None - –±–µ—Ä–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        """
        super().__init__()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config is None:
            config = create_debug_config()
            set_energy_config(config)
        self.config = config
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        self.hidden_size = config.carrier_hidden_size
        self.num_layers = config.carrier_num_layers
        self.dropout = config.carrier_dropout
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.neuron_output_dim = config.neuron_output_dim  # –í—ã—Ö–æ–¥ SimpleNeuron (64)
        self.energy_dim = 1                                # –°–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –æ—Ç mapper'–∞
        self.input_dim = self.neuron_output_dim + self.energy_dim  # 64 + 1 = 65
        
        # GRU —Å–ª–æ–∏
        self.gru = nn.GRU(
            input_size=self.input_dim,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=self.dropout if self.num_layers > 1 else 0,
            batch_first=True
        )
        
        # Projection heads –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        # 1. –°–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è (–≤—ã—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∫–∞–ª—è—Ä–æ–º –¥–ª—è consistency)
        self.energy_projection = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.hidden_size // 2, self.energy_dim),  # –í—ã—Ö–æ–¥: 1 —Å–∫–∞–ª—è—Ä
            nn.Tanh()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [-1, 1]
        )
        
        # 2. –°–ª–µ–¥—É—é—â–∞—è –ø–æ–∑–∏—Ü–∏—è - –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        self.position_projection_base = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(64, 3)  # x, y, z –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
        )
        self.position_activation = self.config.normalization_manager.get_coordinate_activation()  # Tanh –¥–ª—è [-1, 1]
        
        # 3. –ü–æ—Ä–æ–∂–¥–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        self.spawn_gate = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.GELU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–∂–¥–µ–Ω–∏—è
        )
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è —ç–Ω–µ—Ä–≥–∏–∏ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (—Ç–∞–∫–∂–µ —Å–∫–∞–ª—è—Ä–Ω–∞—è)
        self.spawn_energy_projection = nn.Sequential(
            nn.Linear(self.hidden_size, self.energy_dim),
            nn.Tanh()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [-1, 1]
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"EnergyCarrier initialized with {total_params:,} parameters")
        logger.debug(f"GRU: input={self.input_dim}, hidden={self.hidden_size}, layers={self.num_layers}")
        logger.info(f"üéì Curriculum settings: initial_z_bias={self.config.initial_z_bias}, "
                   f"use_forward_bias={self.config.use_forward_movement_bias}, "
                   f"decay_steps={getattr(self.config, 'bias_decay_steps', 'N/A')}")
    
    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å smart initialization –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –≤–ø–µ—Ä–µ–¥"""
        # GRU —É–∂–µ –∏–º–µ–µ—Ç —Ö–æ—Ä–æ—à—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º projection heads
        for module in [self.energy_projection, self.position_projection_base, 
                      self.spawn_gate, self.spawn_energy_projection]:
            if isinstance(module, nn.Sequential):
                for layer in module:
                    if isinstance(layer, nn.Linear):
                        nn.init.xavier_normal_(layer.weight)
                        if layer.bias is not None:
                            nn.init.zeros_(layer.bias)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
        # SMART INITIALIZATION: –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π bias –¥–ª—è Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        # –ü–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞—É—á–∏—Ç—å—Å—è –¥–≤–∏–∂–µ–Ω–∏—é –≤–ø–µ—Ä–µ–¥ –±–µ–∑ –∂–µ—Å—Ç–∫–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        with torch.no_grad():
            # position_projection_base[-1] —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π Linear —Å–ª–æ–π –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π
            # –ò–Ω–¥–µ–∫—Å 2 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ –≤ –≤—ã—Ö–æ–¥–µ [x, y, z]
            if hasattr(self.position_projection_base, '__getitem__') and len(self.position_projection_base) >= 2:
                final_linear = None
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π Linear —Å–ª–æ–π
                for i in range(len(self.position_projection_base) - 1, -1, -1):
                    if isinstance(self.position_projection_base[i], nn.Linear):
                        final_linear = self.position_projection_base[i]
                        break
                
                if final_linear is not None and final_linear.bias is not None:
                    # SMART INITIALIZATION: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑ config (—Ç–µ–ø–µ—Ä—å 0.0 –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏)
                    smart_init_bias = self.config.smart_init_bias
                    final_linear.bias[2] = smart_init_bias
                    logger.debug_init(f"üé© SMART INITIALIZATION: Z-coordinate bias set to {smart_init_bias:.2f} (DISABLED for diagnostics)")
                    logger.debug_init(f"Full position_projection bias: {final_linear.bias.data}")
                else:
                    logger.warning("‚ö†Ô∏è Smart initialization FAILED: could not find final linear layer with bias!")
    
    def forward(self, 
                neuron_output: torch.Tensor,
                embedding_part: torch.Tensor,
                hidden_state: Optional[torch.Tensor] = None,
                current_position: Optional[torch.Tensor] = None,
                flow_age: Optional[torch.Tensor] = None,
                global_training_step: Optional[int] = None) -> Tuple[EnergyOutput, torch.Tensor]:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ EnergyCarrier
        
        Args:
            neuron_output: [batch, neuron_output_dim] - –≤—ã—Ö–æ–¥ SimpleNeuron
            embedding_part: [batch, embedding_dim] - —á–∞—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            hidden_state: [num_layers, batch, hidden_size] - —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
            current_position: [batch, 3] - —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–π)
            flow_age: [batch] - –≤–æ–∑—Ä–∞—Å—Ç –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è progressive bias
            global_training_step: –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è curriculum learning
            
        Returns:
            output: EnergyOutput - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            new_hidden: [num_layers, batch, hidden_size] - –Ω–æ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        """
        batch_size = neuron_output.shape[0]
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ö–æ–¥–∞
        if global_training_step is not None:
            logger.debug_energy(f"üîÑ EnergyCarrier forward: batch={batch_size}, global_step={global_training_step}")
            if current_position is not None:
                current_z = current_position[:, 2]
                logger.debug_energy(f"üìç Current Z positions: min={current_z.min():.3f}, "
                           f"max={current_z.max():.3f}, mean={current_z.mean():.3f}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Ö–æ–¥—ã
        combined_input = torch.cat([neuron_output, embedding_part], dim=-1)
        combined_input = combined_input.unsqueeze(1)  # [batch, 1, input_dim] –¥–ª—è GRU
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ GRU
        gru_output, new_hidden = self.gru(combined_input, hidden_state)
        gru_output = gru_output.squeeze(1)  # [batch, hidden_size]
        
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —ç–Ω–µ—Ä–≥–∏—é
        energy_value = self.energy_projection(gru_output)  # [batch, embedding_dim]
        
        # 2. –í—ã—á–∏—Å–ª—è–µ–º —Å–ª–µ–¥—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é)
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º GRU –≤—ã—Ö–æ–¥ –ø–µ—Ä–µ–¥ position_projection
        if global_training_step is not None and global_training_step == 0:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —à–∞–≥
            logger.debug_forward(f"üß† GRU output stats: min={gru_output.min():.3f}, max={gru_output.max():.3f}, "
                       f"mean={gru_output.mean():.3f}, std={gru_output.std():.3f}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥ (–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
        predicted_position_raw = self.position_projection_base(gru_output)  # [batch, 3] –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (–î–û Tanh)
        if global_training_step is not None and global_training_step == 0:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —à–∞–≥
            raw_z = predicted_position_raw[:, 2]
            logger.debug_forward(f"üî• RAW model output (before Tanh): Z min={raw_z.min():.3f}, "
                       f"max={raw_z.max():.3f}, mean={raw_z.mean():.3f}, std={raw_z.std():.3f}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏—é (Tanh)
        predicted_position_normalized = self.position_activation(predicted_position_raw)  # [batch, 3] –≤ [-1, 1]
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–ü–û–°–õ–ï Tanh)
        norm_z = predicted_position_normalized[:, 2]
        logger.debug_energy(f"üìä Normalized Z (after Tanh): min={norm_z.min():.3f}, "
                       f"max={norm_z.max():.3f}, mean={norm_z.mean():.3f}")
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º Z-–¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        z_range = self.config.normalization_manager.ranges.z_range
        logger.debug_energy(f"üîß Z normalization range: {z_range} (depth={self.config.lattice_depth}, zones=[0,{self.config.lattice_depth-1}]|[{self.config.lattice_depth},{self.config.lattice_depth*2-1}]|{self.config.lattice_depth*2}+)")
        
        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è bias'–æ–≤ –∏ —à—É–º–∞
        predicted_position = self.config.normalization_manager.denormalize_coordinates(
            predicted_position_normalized
        )
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        denorm_z = predicted_position[:, 2]
        logger.debug_energy(f"üìä Denormalized Z (before bias): min={denorm_z.min():.3f}, "
                       f"max={denorm_z.max():.3f}, mean={denorm_z.mean():.3f}")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        expected_max_z = self.config.lattice_depth * 2 - 1  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è —Ç—Ä–µ—Ö–∑–æ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–∏
        if denorm_z.max() > expected_max_z * 1.2:  # –î–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ
            logger.error(f"üö´ DENORMALIZATION ERROR: Z > expected max ({expected_max_z})! "
                       f"Check normalization range: {self.config.normalization_manager.ranges.z_range}")

        # CURRICULUM LEARNING: –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ bias'–∞ –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –≤–ø–µ—Ä–µ–¥
        if self.config.use_forward_movement_bias and self.config.initial_z_bias > 0:
            if global_training_step is not None:
                # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ bias'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è
                bias_decay_factor = max(0.0, 1.0 - (global_training_step / self.config.bias_decay_steps))
                current_bias = self.config.initial_z_bias * bias_decay_factor
                
                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º curriculum learning –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                logger.debug_energy(f"üìì Curriculum step {global_training_step}: "
                           f"decay_factor={bias_decay_factor:.4f}, current_bias={current_bias:.4f}")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π progressive bias –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –ø–æ—Ç–æ–∫–∞
                if flow_age is not None:
                    age_bonus = flow_age * self.config.progressive_z_multiplier * bias_decay_factor
                    total_bias = current_bias + age_bonus  # –ú–æ–∂–µ—Ç –±—ã—Ç—å —Ç–µ–Ω–∑–æ—Ä–æ–º [batch]
                else:
                    total_bias = current_bias  # –°–∫–∞–ª—è—Ä
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º bias —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –≤—Å–µ –µ—â–µ –∑–Ω–∞—á–∏–º—ã–π
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ total_bias –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–µ–Ω–∑–æ—Ä–æ–º
                
                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –î–û –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è bias'–∞
                if logger.isEnabledFor(10):  # DEBUG level
                    z_before = predicted_position[:, 2]
                    logger.debug(f"üìä Z-coords BEFORE bias: min={z_before.min():.3f}, "
                               f"max={z_before.max():.3f}, mean={z_before.mean():.3f}")
                
                if isinstance(total_bias, torch.Tensor):
                    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ bias'–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
                    valid_bias_mask = total_bias > 0.01
                    flows_with_bias = valid_bias_mask.sum().item()
                    if valid_bias_mask.any():
                        predicted_position[valid_bias_mask, 2] += total_bias[valid_bias_mask]
                        logger.debug(f"‚úÖ Applied tensor bias to {flows_with_bias}/{batch_size} flows")
                else:
                    # total_bias - —Å–∫–∞–ª—è—Ä
                    if total_bias > 0.01:
                        predicted_position[:, 2] += total_bias
                        logger.debug(f"‚úÖ Applied scalar bias {total_bias:.4f} to all {batch_size} flows")
                
                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ü–û–°–õ–ï –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è bias'–∞
                if logger.isEnabledFor(10):  # DEBUG level
                    z_after = predicted_position[:, 2]
                    logger.debug(f"üìä Z-coords AFTER bias: min={z_after.min():.3f}, "
                               f"max={z_after.max():.3f}, mean={z_after.mean():.3f}")
            
            elif flow_age is not None:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–æ–∑—Ä–∞—Å—Ç –ø–æ—Ç–æ–∫–∞ –±–µ–∑ curriculum learning
                dynamic_z_bias = self.config.initial_z_bias + (flow_age * self.config.progressive_z_multiplier)
                predicted_position[:, 2] += dynamic_z_bias
                logger.debug(f"‚ö†Ô∏è Fallback: applied age-based bias without global_training_step")
        
        else:
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º, –∫–æ–≥–¥–∞ bias –ù–ï –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
            if logger.isEnabledFor(10):  # DEBUG level
                reason = "disabled" if not self.config.use_forward_movement_bias else "zero_initial_bias"
                logger.debug(f"‚ùå NO BIAS applied: reason={reason}")
        
        # Exploration noise –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—É—Ç–µ–π
        if self.config.use_exploration_noise:
            noise = torch.randn_like(predicted_position) * self.config.exploration_noise
            predicted_position += noise
            logger.debug(f"üé≤ Added exploration noise: std={self.config.exploration_noise}")
        
        # –ü–†–û–í–ï–†–ö–ê –ì–†–ê–ù–ò–¶: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
        if logger.isEnabledFor(10):  # DEBUG level
            z_coords = predicted_position[:, 2]
            max_expected_z = self.config.lattice_depth + 10  # –î–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã
            if torch.any(z_coords > max_expected_z):
                out_of_bounds_count = (z_coords > max_expected_z).sum().item()
                logger.error(f"üö´ Z-COORDINATE BOUNDS ERROR: {out_of_bounds_count}/{predicted_position.shape[0]} "
                           f"flows have Z > {max_expected_z} (max={z_coords.max():.2f})")
                logger.error(f"üîç This indicates coordinate system malfunction!")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤
        next_position, is_terminated, termination_reasons = self._compute_next_position(predicted_position)
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if logger.isEnabledFor(10):  # DEBUG level
            terminated_count = is_terminated.sum().item()
            logger.debug(f"üõ°Ô∏è Termination: {terminated_count}/{batch_size} flows terminated")
            if terminated_count > 0:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã
                reason_counts = {}
                for reason in termination_reasons:
                    reason_counts[reason] = reason_counts.get(reason, 0) + 1
                logger.debug(f"üìà Termination reasons: {reason_counts}")
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–∂–¥–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        spawn_prob = self.spawn_gate(gru_output).squeeze(-1)  # [batch]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–æ—Ä–æ–≥–æ–≤
        spawn_decisions = spawn_prob > self.config.spawn_threshold
        spawn_info = []
        
        for i in range(batch_size):
            if spawn_decisions[i] and spawn_prob[i].item() > self.config.spawn_threshold:
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏–ª—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                num_spawns = min(
                    int((spawn_prob[i].item() - self.config.spawn_threshold) / 
                        (1 - self.config.spawn_threshold) * self.config.max_spawn_per_step) + 1,
                    self.config.max_spawn_per_step
                )
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
                spawn_energy = self.spawn_energy_projection(gru_output[i])
                energies = []
                
                # –î–µ–ª–∏–º —ç–Ω–µ—Ä–≥–∏—é –º–µ–∂–¥—É –ø–æ—Ç–æ–∫–∞–º–∏
                for j in range(num_spawns):
                    energy_fraction = spawn_energy / (num_spawns + 1)  # +1 –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª—è
                    energies.append(energy_fraction.to(gru_output.device))
                
                # –°–æ–∑–¥–∞–µ–º SpawnInfo –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ batch —ç–ª–µ–º–µ–Ω—Ç–∞
                spawn_info.append(SpawnInfo(
                    energies=energies,
                    parent_batch_idx=i
                ))
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        output = EnergyOutput(
            energy_value=energy_value,
            next_position=next_position,
            spawn_info=spawn_info,
            is_terminated=is_terminated,
            termination_reason=termination_reasons
        )
        
        return output, new_hidden
    
    def _compute_next_position(self, 
                              predicted_position: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–ª–µ–¥—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
        
        –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ (–±–µ–∑ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è):
        1. –ü–æ—Ç–æ–∫ –¥–≤–∏–∂–µ—Ç—Å—è —Ç—É–¥–∞, –∫—É–¥–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç predicted_position
        2. –ï—Å–ª–∏ –ø–æ—Ç–æ–∫ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –ø–æ X,Y - –æ–Ω –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è (–Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–æ–ª–∂–Ω–∞ –æ–±—É—á–∏—Ç—å—Å—è –Ω–µ –¥–µ–ª–∞—Ç—å —ç—Ç–æ–≥–æ)
        3. –ï—Å–ª–∏ –ø–æ—Ç–æ–∫ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –ø–æ Z (depth*2-1) - –æ–Ω –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏
        4. FlowProcessor –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ —ç–Ω–µ—Ä–≥–∏–∏
        
        Args:
            predicted_position: [batch, 3] - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (—Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
            
        Returns:
            next_position: [batch, 3] - —Å–ª–µ–¥—É—é—â–∞—è –ø–æ–∑–∏—Ü–∏—è (—Ü–µ–ª—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
            is_terminated: [batch] - –º–∞—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            termination_reasons: List[str] - –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞–ø—Ä—è–º—É—é (—É–∂–µ –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        next_position = predicted_position
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –≤–º–µ—Å—Ç–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        batch_size = predicted_position.shape[0]
        is_terminated = torch.zeros(batch_size, dtype=torch.bool, device=predicted_position.device)
        termination_reasons = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –ø–æ X –∏ Y –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
        out_of_bounds_x = (predicted_position[:, 0] < 0) | (predicted_position[:, 0] >= self.config.lattice_width)
        out_of_bounds_y = (predicted_position[:, 1] < 0) | (predicted_position[:, 1] >= self.config.lattice_height)
        out_of_bounds_xy = out_of_bounds_x | out_of_bounds_y
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è Z –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã:
        # Z ‚àà [0, depth-1] - –∞–∫—Ç–∏–≤–Ω–∞—è –∑–æ–Ω–∞
        # Z ‚àà [depth, depth*2-1] - –∑–æ–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ)
        # Z ‚â• depth*2 - –≤—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã (–æ—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏)
        
        depth = self.config.lattice_depth
        max_valid_z = depth * 2 - 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ Z –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ
        reached_output_zone = (predicted_position[:, 2] >= depth) & (predicted_position[:, 2] <= max_valid_z)
        out_of_bounds_z = predicted_position[:, 2] > max_valid_z
        
        # –û—Ç–º–µ—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
        is_terminated = out_of_bounds_xy | reached_output_zone | out_of_bounds_z
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        for i in range(batch_size):
            if out_of_bounds_xy[i]:
                termination_reasons.append("out_of_bounds_xy")
            elif out_of_bounds_z[i]:
                termination_reasons.append("out_of_bounds_z")  # –û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            elif reached_output_zone[i]:
                termination_reasons.append("reached_output_surface")  # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            else:
                termination_reasons.append("active")  # –ü–æ—Ç–æ–∫ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ
        
        # –î–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –æ–∫—Ä—É–≥–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–æ —Ü–µ–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        next_position = torch.round(predicted_position.clone())
        
        # –î–ª—è –ø–æ—Ç–æ–∫–æ–≤ –≤ –∑–æ–Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è [depth, depth*2-1] - —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å –≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å—é
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ X,Y –Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Z = depth –¥–ª—è –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏
        output_surface_mask = reached_output_zone
        if output_surface_mask.any():
            next_position[output_surface_mask, 2] = depth  # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å –≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å—é Z=depth
        
        return next_position, is_terminated, termination_reasons
    
    def init_hidden(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è GRU"""
        return torch.zeros(
            self.num_layers, batch_size, self.hidden_size,
            device=device, dtype=torch.float32
        )
    
    def check_energy_level(self, energy: torch.Tensor) -> torch.Tensor:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —ç–Ω–µ—Ä–≥–∏–∏
        
        Args:
            energy: [batch, 1] - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1] (Tanh)
        
        Returns:
            is_alive: [batch] - –º–∞—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —ç–Ω–µ—Ä–≥–∏–∏
        return self.config.normalization_manager.check_energy_threshold(
            energy, self.config.energy_threshold
        )


def create_energy_carrier(config=None) -> EnergyCarrier:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è EnergyCarrier"""
    return EnergyCarrier(config)