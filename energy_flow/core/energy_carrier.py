"""
Energy Carrier - GRU-based —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ç–æ–∫–∏
================================================

GRU –º–æ–¥–µ–ª—å —Å ~10M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏.
–û–±—â–∏–µ –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö GRU –ø–æ—Ç–æ–∫–æ–≤ –≤ —Ä–µ—à–µ—Ç–∫–µ.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

from ..utils.logging import get_logger, gated_log, summarize_step, format_first_n
from ..config import create_debug_config, set_energy_config

logger = get_logger(__name__)


@dataclass
class SpawnInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ batch —ç–ª–µ–º–µ–Ω—Ç–∞"""
    energies: List[torch.Tensor]    # –≠–Ω–µ—Ä–≥–∏–∏ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    parent_batch_idx: int          # –ò–Ω–¥–µ–∫—Å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞


@dataclass
class EnergyOutput:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ EnergyCarrier"""
    energy_value: torch.Tensor      # –¢–µ–∫—É—â–∞—è —ç–Ω–µ—Ä–≥–∏—è/—ç–º–±–µ–¥–¥–∏–Ω–≥
    next_position: torch.Tensor     # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–ª–µ–¥—É—é—â–µ–π –∫–ª–µ—Ç–∫–∏
    raw_next_position: Optional[torch.Tensor]  # –ù–µ–∫–ª–∞–º–ø–ª–µ–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è/—à—É–º–æ–≤ (–¥–ª—è X/Y –æ—Ç—Ä–∞–∂–µ–Ω–∏–π)
    spawn_info: List[SpawnInfo]     # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ spawn'–∞—Ö
    
    # –§–ª–∞–≥–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ (–¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ FlowProcessor)
    is_terminated: torch.Tensor     # [batch] - –º–∞—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    termination_reason: List[str]   # –ü—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞


class EnergyCarrier(nn.Module):
    """
    GRU-based –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
    - –í—ã—Ö–æ–¥ SimpleNeuron
    - –ß–∞—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    - –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
    
    –í—ã–¥–∞–µ—Ç:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —Å —ç–Ω–µ—Ä–≥–∏–µ–π, –ø–æ–∑–∏—Ü–∏–µ–π –∏ –Ω–æ–≤—ã–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏
    """
    
    def __init__(self, config=None):
        """
        Args:
            config: EnergyConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏. –ï—Å–ª–∏ None - –±–µ—Ä–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        """
        super().__init__()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config is None:
            config = create_debug_config()
            set_energy_config(config)
        self.config = config
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        self.hidden_size = config.carrier_hidden_size
        self.num_layers = config.carrier_num_layers
        # –£–î–ê–õ–ï–ù–û: dropout —Å–ª–æ–∏ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Ç–æ–∫–æ–≤ —Ç–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –¥–ª–∏–Ω–µ —Å–º–µ—â–µ–Ω–∏—è, –∞ –Ω–µ –Ω–∞ dropout
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.neuron_output_dim = config.neuron_output_dim  # –í—ã—Ö–æ–¥ SimpleNeuron (64)
        self.energy_dim = 1                                # –°–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –æ—Ç mapper'–∞
        self.input_dim = self.neuron_output_dim + self.energy_dim  # 64 + 1 = 65
        
        # GRU —Å–ª–æ–∏
        self.gru = nn.GRU(
            input_size=self.input_dim,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            dropout=0.0,  # Dropout –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
            batch_first=True
        )
        
        # –ü–∞–º—è—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–∑–∏—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.position_memory_size = 5  # –•—Ä–∞–Ω–∏–º 5 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–∑–∏—Ü–∏–π
        self.position_memory = nn.Linear(
            3 * self.position_memory_size,  # 5 –ø–æ–∑–∏—Ü–∏–π * 3 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã = 15
            self.hidden_size // 4
        )
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–∑–∏—Ü–∏–π —Å GRU –≤—ã—Ö–æ–¥–æ–º
        self.history_fusion = nn.Sequential(
            nn.Linear(self.hidden_size + self.hidden_size // 4, self.hidden_size),
            nn.GELU(),
            nn.Linear(self.hidden_size, self.hidden_size)
        )
        
        # Projection heads –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        # 1. –°–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è (–≤—ã—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∫–∞–ª—è—Ä–æ–º –¥–ª—è consistency)
        self.energy_projection = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.GELU(),
            # Dropout —Å–ª–æ–π —É–¥–∞–ª–µ–Ω
            nn.Linear(self.hidden_size // 2, self.energy_dim),  # –í—ã—Ö–æ–¥: 1 —Å–∫–∞–ª—è—Ä
            nn.Tanh()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [-1, 1]
        )
        
        # 2. –°–º–µ—â–µ–Ω–∏—è - –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è (Œîx, Œîy, Œîz)
        self.displacement_projection = nn.Sequential(
            nn.Linear(self.hidden_size, 64),
            nn.GELU(),
            # Dropout —Å–ª–æ–π —É–¥–∞–ª–µ–Ω
            nn.Linear(64, 3)  # Œîx, Œîy, Œîz —Å–º–µ—â–µ–Ω–∏—è (–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
        )
        self.displacement_activation = self.config.normalization_manager.get_displacement_activation()  # Tanh –¥–ª—è [-1, 1]
        
        # 3. –£–î–ê–õ–ï–ù–û: spawn_gate –∏ spawn_energy_projection
        # –í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç spawn –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è 
        # —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–º–µ—â–µ–Ω–∏—è –≤ FlowProcessor
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"EnergyCarrier initialized with {total_params:,} parameters")
        logger.debug(f"GRU: input={self.input_dim}, hidden={self.hidden_size}, layers={self.num_layers}")
        
    
    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é –¥–ª—è GRU –∏ –ø—Ä–æ–µ–∫—Ü–∏–π"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GRU —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É –¥–ª—è –ª—É—á—à–µ–π —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        init_method = getattr(self.config, 'gru_initialization_method', 'orthogonal')
        for name, param in self.gru.named_parameters():
            if 'weight_ih' in name:
                if init_method == 'xavier':
                    nn.init.xavier_uniform_(param)
                else:
                    nn.init.xavier_uniform_(param)  # input‚Üíhidden –æ–±—ã—á–Ω–æ Xavier
            elif 'weight_hh' in name:
                # hidden‚Üíhidden ‚Äî –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞—è –¥–ª—è RNN-—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                nn.init.orthogonal_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
                # –î–ª—è GRU –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å bias_hh (reset/update)
                try:
                    seg = param.data.size(0) // 3
                    param.data[seg:2*seg].fill_(1.0)
                except Exception:
                    pass
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º projection heads
        for module in [self.energy_projection, self.displacement_projection]:
            if isinstance(module, nn.Sequential):
                for layer in module:
                    if isinstance(layer, nn.Linear):
                        nn.init.xavier_normal_(layer.weight)
                        if layer.bias is not None:
                            nn.init.zeros_(layer.bias)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
        logger.debug_init("üèóÔ∏è Weights initialized: GRU[orthogonal hh, xavier ih], heads[xavier]")
    
    def forward(self, 
                neuron_output: torch.Tensor,
                embedding_part: torch.Tensor,
                hidden_state: Optional[torch.Tensor] = None,
                current_position: Optional[torch.Tensor] = None,
                flow_age: Optional[torch.Tensor] = None,
                global_training_step: Optional[int] = None,
                position_history: Optional[torch.Tensor] = None) -> Tuple[EnergyOutput, torch.Tensor]:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ EnergyCarrier
        
        Args:
            neuron_output: [batch, neuron_output_dim] - –≤—ã—Ö–æ–¥ SimpleNeuron
            embedding_part: [batch, embedding_dim] - —á–∞—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            hidden_state: [num_layers, batch, hidden_size] - —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
            current_position: [batch, 3] - —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–π)
            flow_age: [batch] - –≤–æ–∑—Ä–∞—Å—Ç –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è progressive bias
            global_training_step: –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è curriculum learning
            position_history: [batch, memory_size, 3] - –∏—Å—Ç–æ—Ä–∏—è –ø–æ–∑–∏—Ü–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            output: EnergyOutput - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            new_hidden: [num_layers, batch, hidden_size] - –Ω–æ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        """
        batch_size = neuron_output.shape[0]
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê (—Å —á–∞—Å—Ç–æ—Ç–Ω—ã–º –≥–µ–π—Ç–æ–º –Ω–∞ –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏)
        if global_training_step is not None:
            gated_log(
                logger,
                'DEBUG_ENERGY',
                step=global_training_step,
                key='carrier_forward_intro',
                msg_or_factory=lambda: (
                    f"üîÑ EnergyCarrier forward: batch={batch_size}, global_step={global_training_step}"
                    + (
                        (lambda cz: f"; Z(min={cz.min():.3f}, max={cz.max():.3f}, mean={cz.mean():.3f})")(
                            current_position[:, 2]
                        ) if current_position is not None else ""
                    )
                ),
                first_n_steps=3,
                every=0,
            )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Ö–æ–¥—ã
        combined_input = torch.cat([neuron_output, embedding_part], dim=-1)
        combined_input = combined_input.unsqueeze(1)  # [batch, 1, input_dim] –¥–ª—è GRU
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ GRU —Å —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–µ–π –≤—Ö–æ–¥–æ–≤/–≤—ã—Ö–æ–¥–æ–≤
        if getattr(self.config, 'enable_gru_nan_protection', True):
            # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–æ–≤
            combined_input = torch.nan_to_num(
                combined_input,
                nan=0.0,
                posinf=self.config.gru_input_clip_value,
                neginf=-self.config.gru_input_clip_value,
            )
            if hidden_state is not None:
                hidden_state = torch.nan_to_num(
                    hidden_state,
                    nan=0.0,
                    posinf=self.config.gru_input_clip_value,
                    neginf=-self.config.gru_input_clip_value,
                )
            # –ö–ª–∏–ø –ø–æ –º–æ–¥—É–ª—é
            clip_v = float(getattr(self.config, 'gru_input_clip_value', 10.0))
            if clip_v > 0:
                combined_input = combined_input.clamp(-clip_v, clip_v)
                if hidden_state is not None:
                    hidden_state = hidden_state.clamp(-clip_v, clip_v)
        
        gru_output, new_hidden = self.gru(combined_input, hidden_state)
        
        if getattr(self.config, 'enable_gru_nan_protection', True):
            # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤
            gru_output = torch.nan_to_num(
                gru_output, nan=0.0,
                posinf=self.config.gru_output_clip_value,
                neginf=-self.config.gru_output_clip_value,
            )
            new_hidden = torch.nan_to_num(
                new_hidden, nan=0.0,
                posinf=self.config.gru_output_clip_value,
                neginf=-self.config.gru_output_clip_value,
            )
            clip_o = float(getattr(self.config, 'gru_output_clip_value', 10.0))
            if clip_o > 0:
                gru_output = gru_output.clamp(-clip_o, clip_o)
                new_hidden = new_hidden.clamp(-clip_o, clip_o)
        
        gru_output = gru_output.squeeze(1)  # [batch, hidden_size]
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
        if position_history is not None and position_history.shape[1] > 0:
            # Flatten history: [batch, memory_size * 3]
            history_flat = position_history.view(batch_size, -1)
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –∏—Å—Ç–æ—Ä–∏—è –∫–æ—Ä–æ—á–µ memory_size
            if history_flat.shape[1] < 3 * self.position_memory_size:
                padding_size = 3 * self.position_memory_size - history_flat.shape[1]
                padding = torch.zeros(batch_size, padding_size, device=history_flat.device)
                history_flat = torch.cat([history_flat, padding], dim=1)
            
            # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ features
            history_features = self.position_memory(history_flat)  # [batch, hidden_size // 4]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å GRU –≤—ã—Ö–æ–¥–æ–º
            combined_features = torch.cat([gru_output, history_features], dim=-1)
            gru_output = self.history_fusion(combined_features)  # [batch, hidden_size]
            
            if global_training_step is not None and global_training_step <= 3:
                logger.debug_forward(f"üìú Position history integrated: shape={position_history.shape}, "
                                   f"history_features norm={history_features.norm(dim=-1).mean():.3f}")
        
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —ç–Ω–µ—Ä–≥–∏—é
        energy_value = self.energy_projection(gru_output)  # [batch, embedding_dim]
        
        # 2. –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏—è (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º GRU –≤—ã—Ö–æ–¥ –∏ bias'—ã (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏, –ª–µ–Ω–∏–≤–æ)
        if global_training_step is not None and global_training_step <= 3:
            gated_log(
                logger,
                'DEBUG_FORWARD',
                step=global_training_step,
                key='gru_output_stats',
                msg_or_factory=lambda: (
                    f"üß† GRU output stats: min={gru_output.min():.3f}, max={gru_output.max():.3f}, "
                    f"mean={gru_output.mean():.3f}, std={gru_output.std():.3f}"
                ),
                first_n_steps=3,
                every=0,
            )
            def _bias_msg():
                parts = []
                for i, module in enumerate(self.displacement_projection):
                    if isinstance(module, nn.Linear) and module.bias is not None:
                        b = module.bias.data
                        parts.append(
                            f"[{i}] min={b.min():.4f}, max={b.max():.4f}, mean={b.mean():.4f}, std={b.std():.4f}"
                        )
                return "üìä displacement_projection bias: " + "; ".join(parts) if parts else "üìä displacement_projection bias: none"
            gated_log(
                logger,
                'DEBUG_FORWARD',
                step=global_training_step,
                key='disp_bias_stats',
                msg_or_factory=_bias_msg,
                first_n_steps=3,
                every=0,
            )
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥ —Å–º–µ—â–µ–Ω–∏–π (–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
        displacement_raw = self.displacement_projection(gru_output)  # [batch, 3] –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (–î–û Clamp) ‚Äî –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏
        if global_training_step is not None and global_training_step <= 3:
            gated_log(
                logger,
                'DEBUG_FORWARD',
                step=global_training_step,
                key='raw_displacement_stats',
                msg_or_factory=lambda: (
                    lambda d: f"üî• RAW displacement ŒîZ: min={d.min():.3f}, max={d.max():.3f}, mean={d.mean():.3f}, std={d.std():.3f}"
                )(displacement_raw[:, 2]),
                first_n_steps=3,
                every=0,
            )
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        current_scale = self._calculate_displacement_scale(global_training_step)
        displacement_scaled = displacement_raw * current_scale
        
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô clamp —Å–º–µ—â–µ–Ω–∏–π –î–û –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫ –ø–æ–∑–∏—Ü–∏–∏
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–º–µ—â–µ–Ω–∏—è –Ω–µ –≤—ã–≤–µ–¥—É—Ç –ø–æ–∑–∏—Ü–∏—é –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã [-1, 1]
        displacement_normalized = torch.clamp(displacement_scaled, -0.5, 0.5)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–º–µ—â–µ–Ω–∏—è
        
        if global_training_step is not None and global_training_step % self.config.displacement_scale_update_interval == 0:
            gated_log(
                logger,
                'DEBUG_FORWARD',
                step=global_training_step,
                key='displacement_scaling',
                msg_or_factory=lambda: f"üîß DISPLACEMENT SCALING: step={global_training_step}, scale={current_scale:.3f}",
                first_n_steps=1,
                every=self.config.displacement_scale_update_interval,
            )
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º —Å–º–µ—â–µ–Ω–∏—è –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è (–î–û —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ clamp)
        norm_delta_z = displacement_normalized[:, 2]
        gated_log(
            logger,
            'DEBUG_ENERGY',
            step=global_training_step or 0,
            key='scaled_displacement_stats',
            msg_or_factory=lambda: (
                f"üìä Scaled displacement ŒîZ: min={norm_delta_z.min():.3f}, max={norm_delta_z.max():.3f}, mean={norm_delta_z.mean():.3f}"
            ),
            first_n_steps=3,
            every=0,
        )
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê —Å–º–µ—â–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤—ã—Ö —à–∞–≥–∞—Ö)
        if global_training_step is not None and global_training_step <= 3:  # –ü–µ—Ä–≤—ã–µ 3 —à–∞–≥–∞
            depth = self.config.lattice_depth
            real_displacement_z = norm_delta_z * (depth / 2)
            gated_log(
                logger,
                'DEBUG_FORWARD',
                step=global_training_step,
                key='real_world_disp_z',
                msg_or_factory=lambda: (
                    f"üîç Real Z displacement: min={real_displacement_z.min():.3f}, max={real_displacement_z.max():.3f}, "
                    f"mean={real_displacement_z.mean():.3f} (depth={depth})"
                ),
                first_n_steps=3,
                every=0,
            )
        
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è –∫ —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏ (–≤—Å–µ –≤ [-1, 1] –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ)
        if current_position is not None:
            # –°–Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª–∏–º "—Å—ã—Ä—É—é" —Å–ª–µ–¥—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é –ë–ï–ó clamp ‚Äî –Ω—É–∂–Ω–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã X/Y
            raw_next_position = current_position + displacement_normalized
            # –ó–∞—Ç–µ–º —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é —Å clamp –∫–∞–∫ –±–∞–∑—É –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            next_position = torch.clamp(raw_next_position, -1.0, 1.0)
            
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê Z-–¥–≤–∏–∂–µ–Ω–∏—è: –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏
            if global_training_step is not None and global_training_step <= 3:
                z_current = current_position[:, 2]
                z_next = next_position[:, 2]
                z_delta = z_next - z_current
                depth = self.config.lattice_depth
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: clamp –≤ [-1,1] –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ –∂—ë—Å—Ç–∫–æ–≥–æ assert
                nm = self.config.normalization_manager
                safe_curr = torch.clamp(current_position, -1.0, 1.0)
                safe_next = torch.clamp(next_position, -1.0, 1.0)
                current_real = nm.denormalize_coordinates(safe_curr)[:, 2]
                next_real = nm.denormalize_coordinates(safe_next)[:, 2]
                def _z_analysis_msg():
                    positive_z_count = (z_delta < 0).sum().item()
                    negative_z_count = (z_delta > 0).sum().item()
                    neutral_count = (z_delta == 0).sum().item()
                    return (
                        "üéØ Z-POSITION ANALYSIS: "
                        f"curr_norm=[{z_current.min():.3f},{z_current.max():.3f}] mean={z_current.mean():.3f}; "
                        f"curr_real=[{current_real.min():.1f},{current_real.max():.1f}] mean={current_real.mean():.1f} (depth={depth}); "
                        f"delta=[{z_delta.min():.3f},{z_delta.max():.3f}] mean={z_delta.mean():.3f}; "
                        f"next_norm=[{z_next.min():.3f},{z_next.max():.3f}] mean={z_next.mean():.3f}; "
                        f"next_real=[{next_real.min():.1f},{next_real.max():.1f}] mean={next_real.mean():.1f}; "
                        f"dirs: +={negative_z_count}, -={positive_z_count}, 0={neutral_count} (both directions valid)"
                    )
                gated_log(
                    logger,
                    'DEBUG_FORWARD',
                    step=global_training_step,
                    key='z_position_analysis',
                    msg_or_factory=_z_analysis_msg,
                    first_n_steps=3,
                    every=0,
                )
        else:
            # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—â–µ–Ω–∏—è –∫–∞–∫ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            logger.warning("‚ö†Ô∏è Current position is None, using displacement as absolute position")
            raw_next_position = displacement_normalized
            next_position = torch.clamp(raw_next_position, -1.0, 1.0)
        
        # Exploration noise –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—É—Ç–µ–π (–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ)
        if self.config.use_exploration_noise:
            # Exploration noise —Ç–æ–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
            noise = torch.randn_like(next_position) * self.config.exploration_noise
            if not getattr(self.config, 'exploration_noise_apply_to_z', False):
                # –û–±–Ω—É–ª—è–µ–º —à—É–º –ø–æ Z, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –ø–ª–æ—Å–∫–æ—Å—Ç—è–º
                noise[:, 2] = 0.0
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—É–º —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º clamp –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ –≥—Ä–∞–Ω–∏—Ü
            raw_next_position = raw_next_position + noise
            next_position = torch.clamp(next_position + noise, -1.0, 1.0)
            logger.debug(f"üé≤ Added normalized exploration noise: std={self.config.exploration_noise}")

        # –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à—É–º–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º Z-–∞–Ω–∞–ª–∏–∑, —á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏—è curr/next —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É
        if current_position is not None and global_training_step is not None and global_training_step <= 3:
            z_current = current_position[:, 2]
            z_next = next_position[:, 2]
            z_delta = z_next - z_current
            depth = self.config.lattice_depth
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: clamp –≤ [-1,1] –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ –∂—ë—Å—Ç–∫–æ–≥–æ assert
            nm = self.config.normalization_manager
            safe_curr = torch.clamp(current_position, -1.0, 1.0)
            safe_next = torch.clamp(next_position, -1.0, 1.0)
            current_real = nm.denormalize_coordinates(safe_curr)[:, 2]
            next_real = nm.denormalize_coordinates(safe_next)[:, 2]
            def _z_analysis_post_noise():
                positive_z_count = (z_delta < 0).sum().item()
                negative_z_count = (z_delta > 0).sum().item()
                neutral_count = (z_delta == 0).sum().item()
                return (
                    "üéØ Z-POSITION ANALYSIS (post-noise): "
                    f"curr_norm=[{z_current.min():.3f},{z_current.max():.3f}] mean={z_current.mean():.3f}; "
                    f"curr_real=[{current_real.min():.1f},{current_real.max():.1f}] mean={current_real.mean():.1f} (depth={depth}); "
                    f"delta=[{z_delta.min():.3f},{z_delta.max():.3f}] mean={z_delta.mean():.3f}; "
                    f"next_norm=[{z_next.min():.3f},{z_next.max():.3f}] mean={z_next.mean():.3f}; "
                    f"next_real=[{next_real.min():.1f},{next_real.max():.1f}] mean={next_real.mean():.1f}; "
                    f"dirs: +={negative_z_count}, -={positive_z_count}, 0={neutral_count} (both directions valid)"
                )
            gated_log(
                logger,
                'DEBUG_FORWARD',
                step=global_training_step,
                key='z_position_analysis_post_noise',
                msg_or_factory=_z_analysis_post_noise,
                first_n_steps=3,
                every=0,
            )
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏–∫—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –Ω–æ–≤–æ–π —Ç—Ä–µ—Ö–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        next_position, is_terminated, termination_reasons = self._compute_next_position_relative(next_position, global_training_step, raw_next_position=raw_next_position)
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è —Å–≤–æ–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–π (gated)
        terminated_count = is_terminated.sum().item()
        def _term_summary():
            reason_counts = {}
            for reason in termination_reasons:
                reason_counts[reason] = reason_counts.get(reason, 0) + 1
            return summarize_step({
                'terminated': terminated_count,
                'active': batch_size - terminated_count,
                **{f"r:{k}": v for k, v in reason_counts.items()}
            }, prefix='TERM')
        gated_log(
            logger,
            'DEBUG',
            step=global_training_step or 0,
            key='termination_summary',
            msg_or_factory=_term_summary,
            first_n_steps=5,
            every=0,
        )
        
        # 3. Spawn –ø–æ—Ç–æ–∫–æ–≤ —Ç–µ–ø–µ—Ä—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ movement_based_spawn –≤ FlowProcessor
        # –£—Å—Ç–∞—Ä–µ–≤—à–∞—è –ª–æ–≥–∏–∫–∞ spawn –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É–¥–∞–ª–µ–Ω–∞
        spawn_info = []  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, spawn –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è –≤ FlowProcessor
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        output = EnergyOutput(
            energy_value=energy_value,
            next_position=next_position,
            raw_next_position=raw_next_position,
            spawn_info=spawn_info,
            is_terminated=is_terminated,
            termination_reason=termination_reasons
        )
        
        return output, new_hidden

    def validate_forward_outputs(self, gru_output: torch.Tensor, new_hidden: torch.Tensor) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—Ö–æ–¥–æ–≤ GRU (NaN/Inf/—ç–∫—Å—Ç—Ä–µ–º—É–º—ã)."""
        issues = []
        try:
            if torch.isnan(gru_output).any():
                issues.append("NaN in gru_output")
            if torch.isnan(new_hidden).any():
                issues.append("NaN in new_hidden")
            if torch.isinf(gru_output).any():
                issues.append("Inf in gru_output")
            if torch.isinf(new_hidden).any():
                issues.append("Inf in new_hidden")
            max_thr = 1000.0
            if gru_output.abs().max() > max_thr:
                issues.append(f"Extreme gru_output (max={gru_output.abs().max().item():.1f})")
            if new_hidden.abs().max() > max_thr:
                issues.append(f"Extreme new_hidden (max={new_hidden.abs().max().item():.1f})")
        except Exception:
            pass
        if issues:
            logger.error(f"GRU output validation failed: {', '.join(issues)}")
            return False
        return True
    
    def _compute_next_position_relative(self, 
                                   next_position: torch.Tensor,
                                   global_training_step: Optional[int] = None,
                                   raw_next_position: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–ª–µ–¥—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é –¥–ª—è —Ç—Ä–µ—Ö–ø–ª–æ—Å–∫–æ—Å—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        
        DUAL OUTPUT PLANES –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
        1. –í—Ö–æ–¥–Ω–∞—è –ø–ª–æ—Å–∫–æ—Å—Ç—å: Z = depth/2 (—Ü–µ–Ω—Ç—Ä –∫—É–±–∞) - normalized Z = 0.0
        2. –í—ã—Ö–æ–¥–Ω—ã–µ –ø–ª–æ—Å–∫–æ—Å—Ç–∏: Z = 0 (normalized Z = -1.0) –ò Z = depth (normalized Z = +1.0)
        3. X/Y –≥—Ä–∞–Ω–∏—Ü—ã: –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ FlowProcessor
        4. –ü–æ—Ç–æ–∫–∏ –∑–∞–≤–µ—Ä—à–∞—é—Ç—Å—è –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª—é–±–æ–π –∏–∑ –¥–≤—É—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        
        –í–ê–ñ–ù–û: –î–≤–∏–∂–µ–Ω–∏–µ –≤ –ª—é–±–æ–º Z-–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –≤–∞–ª–∏–¥–Ω–æ! –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é
        –≤—ã—Ö–æ–¥–Ω—É—é –ø–ª–æ—Å–∫–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            next_position: [batch, 3] - –ø–æ–∑–∏—Ü–∏—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è
            global_training_step: –®–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            
        Returns:
            next_position: [batch, 3] - —Å–ª–µ–¥—É—é—â–∞—è –ø–æ–∑–∏—Ü–∏—è (—Ü–µ–ª—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
            is_terminated: [batch] - –º–∞—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤  
            termination_reasons: List[str] - –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        """
        batch_size = next_position.shape[0]
        is_terminated = torch.zeros(batch_size, dtype=torch.bool, device=next_position.device)
        termination_reasons = []
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–æ Z –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ –≤ –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–û–ú –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
        # Z ‚â§ -1.0: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –ª–µ–≤–æ–π –≤—ã—Ö–æ–¥–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏ (raw Z=0)
        # Z ‚â• +1.0: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–π –≤—ã—Ö–æ–¥–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏ (raw Z=depth)
        reached_z0_plane = next_position[:, 2] <= -1.0
        reached_zdepth_plane = next_position[:, 2] >= 1.0
        reached_output_plane = reached_z0_plane | reached_zdepth_plane
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê Z: –ª–æ–≥–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        if reached_output_plane.any():
            num_z0 = reached_z0_plane.sum().item()
            num_zdepth = reached_zdepth_plane.sum().item()
            logger.debug_forward(f"üîç Z TERMINATION: z0_plane={num_z0}, zdepth_plane={num_zdepth}, total={reached_output_plane.sum().item()}")
        
        # –ü–†–û–ë–õ–ï–ú–ê –ù–ê–ô–î–ï–ù–ê: –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        # –ù–ï –≤ raw –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö —Ä–µ—à–µ—Ç–∫–∏!
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        if batch_size <= 10000:  # –ò–∑–±–µ–≥–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π
            x_min, x_max = next_position[:, 0].min().item(), next_position[:, 0].max().item()
            y_min, y_max = next_position[:, 1].min().item(), next_position[:, 1].max().item()
            z_min, z_max = next_position[:, 2].min().item(), next_position[:, 2].max().item()
            logger.debug_forward(f"üîç BOUNDS CHECK: positions range X[{x_min:.3f}, {x_max:.3f}], "
                               f"Y[{y_min:.3f}, {y_max:.3f}], Z[{z_min:.3f}, {z_max:.3f}]")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã—Ö–æ–¥ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã X/Y –ø–æ –°–´–†–´–ú –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º –¥–æ clamp, –µ—Å–ª–∏ –æ–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
        pos_for_bounds = raw_next_position if raw_next_position is not None else next_position
        out_of_bounds_x = (pos_for_bounds[:, 0] < -1.0) | (pos_for_bounds[:, 0] > 1.0)
        out_of_bounds_y = (pos_for_bounds[:, 1] < -1.0) | (pos_for_bounds[:, 1] > 1.0)
        out_of_bounds_xy = out_of_bounds_x | out_of_bounds_y
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç—Ä–∞–∂–µ–Ω–∏—è (–ø–æ "—Å—ã—Ä—ã–º" –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º –¥–æ clamp)
        if out_of_bounds_xy.any():
            num_x_left = (pos_for_bounds[:, 0] < -1.0).sum().item()
            num_x_right = (pos_for_bounds[:, 0] > 1.0).sum().item()
            num_y_left = (pos_for_bounds[:, 1] < -1.0).sum().item()
            num_y_right = (pos_for_bounds[:, 1] > 1.0).sum().item()
            logger.debug_forward(f"üîç OUT OF BOUNDS: X_left={num_x_left}, X_right={num_x_right}, "
                               f"Y_left={num_y_left}, Y_right={num_y_right}, total={out_of_bounds_xy.sum().item()}")
        
        # –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ X/Y –≥—Ä–∞–Ω–∏—Ü—ã –ù–ï –∑–∞–≤–µ—Ä—à–∞—é—Ç –ø–æ—Ç–æ–∫ (–æ—Ç—Ä–∞–∂–µ–Ω–∏–µ)
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –ø–æ Z
        is_terminated = reached_output_plane
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        for i in range(batch_size):
            if reached_z0_plane[i]:
                termination_reasons.append("reached_z0_plane")  # –õ–µ–≤–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è –ø–ª–æ—Å–∫–æ—Å—Ç—å
            elif reached_zdepth_plane[i]:
                termination_reasons.append("reached_zdepth_plane")  # –ü—Ä–∞–≤–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è –ø–ª–æ—Å–∫–æ—Å—Ç—å
            elif out_of_bounds_xy[i]:
                termination_reasons.append("xy_reflection_needed")  # –¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ (–Ω–æ –ø–æ—Ç–æ–∫ –∞–∫—Ç–∏–≤–µ–Ω)
            else:
                termination_reasons.append("active")  # –ü–æ—Ç–æ–∫ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ
        
        # –î–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤—ã—Ö–æ–¥–Ω—É—é –ø–ª–æ—Å–∫–æ—Å—Ç—å
        final_position = next_position.clone()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –ø–ª–æ—Å–∫–æ—Å—Ç–∏
        # –ü—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Z=0 –ø–ª–æ—Å–∫–æ—Å—Ç—å (norm Z = -1.0)
        if reached_z0_plane.any():
            final_position[reached_z0_plane, 2] = -1.0
        
        # –ü—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Z=depth –ø–ª–æ—Å–∫–æ—Å—Ç—å (norm Z = +1.0)
        if reached_zdepth_plane.any():
            final_position[reached_zdepth_plane, 2] = 1.0
        
        # –í–ê–ñ–ù–û: –ù–ï –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π/–∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        
        return final_position, is_terminated, termination_reasons
    
    def _calculate_displacement_scale(self, global_training_step: Optional[int]) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–µ–∫—É—â–∏–π –º–∞—Å—à—Ç–∞–± —Å–º–µ—â–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–∑–æ–≥—Ä–µ–≤–∞
        
        –õ–æ–≥–∏–∫–∞:
        - –ü–µ—Ä–≤—ã–µ warmup_steps: –ø–æ–ª–Ω—ã–π scale
        - –î–∞–ª–µ–µ: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–±—ã–≤–∞–Ω–∏–µ scale *= decay –∫–∞–∂–¥—ã–µ update_interval —à–∞–≥–æ–≤
        - –ú–∏–Ω–∏–º—É–º: scale_min (–Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è –º–æ–¥–µ–ª–∏)
        
        Args:
            global_training_step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            current_scale: –¢–µ–∫—É—â–∏–π –º–∞—Å—à—Ç–∞–± —Å–º–µ—â–µ–Ω–∏–π
        """
        if global_training_step is None or global_training_step < self.config.displacement_warmup_steps:
            # –§–∞–∑–∞ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞: –ø–æ–ª–Ω—ã–π scale
            return self.config.displacement_scale
        
        # –§–∞–∑–∞ —É–±—ã–≤–∞–Ω–∏—è: —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –ø–æ—Å–ª–µ warmup
        steps_after_warmup = global_training_step - self.config.displacement_warmup_steps
        decay_intervals = steps_after_warmup // self.config.displacement_scale_update_interval
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É–±—ã–≤–∞–Ω–∏–µ
        current_scale = self.config.displacement_scale * (self.config.displacement_scale_decay ** decay_intervals)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º—É–º–æ–º
        current_scale = max(current_scale, self.config.displacement_scale_min)
        
        return current_scale
    
    def init_hidden(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è GRU —Å –Ω–µ–±–æ–ª—å—à–∏–º —à—É–º–æ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        hidden = torch.randn(
            self.num_layers, batch_size, self.hidden_size,
            device=device, dtype=torch.float32
        ) * 0.01  # –ú–∞–ª–µ–Ω—å–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        logger.debug_init(f"üé≤ Initialized GRU hidden state with noise: std=0.01, shape={hidden.shape}")
        return hidden
    
    # –£–î–ê–õ–ï–ù: check_energy_level() - –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç 
    # –ø–æ—Ç–æ–∫–∏ –Ω–µ —É–º–∏—Ä–∞—é—Ç –æ—Ç "–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —ç–Ω–µ—Ä–≥–∏–∏". –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ - —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ, –∞ –Ω–µ —ç–Ω–µ—Ä–≥–∏—è.


def create_energy_carrier(config=None) -> EnergyCarrier:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è EnergyCarrier"""
    return EnergyCarrier(config)