"""
Flow Processor - –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏
=================================================

–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Å–µ—Ö —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤.
–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É SimpleNeuron –∏ EnergyCarrier.
"""

import torch
import torch.nn as nn
from typing import List, Dict, Optional, Tuple
import time

from ..utils.logging import get_logger, log_memory_state
from ..config import get_energy_config, create_debug_config, set_energy_config
from ..utils.device_manager import get_device_manager
from .simple_neuron import SimpleNeuron, create_simple_neuron
from .energy_carrier import EnergyCarrier, create_energy_carrier
from .energy_lattice import EnergyLattice, create_energy_lattice

logger = get_logger(__name__)


class FlowProcessor(nn.Module):
    """
    –ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏ —á–µ—Ä–µ–∑ —Ä–µ—à–µ—Ç–∫—É
    
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ SimpleNeuron –∏ EnergyCarrier
    - –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø–æ—Ç–æ–∫–æ–≤
    """
    
    def __init__(self, config=None):
        """
        Args:
            config: EnergyConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        """
        super().__init__()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config is None:
            config = create_debug_config()
            set_energy_config(config)
        self.config = config
        
        # Device manager
        self.device_manager = get_device_manager()
        self.device = self.device_manager.device
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.lattice = create_energy_lattice(config)
        self.neuron = create_simple_neuron(config)
        self.carrier = create_energy_carrier(config)
        
        # Embedding mapper - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        from .embedding_mapper import EnergyFlowMapper
        self.mapper = EnergyFlowMapper(config)
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º mapper –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.mapper.input_mapper = self.mapper.input_mapper.to(self.device)
        self.mapper.output_collector = self.mapper.output_collector.to(self.device)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.lattice = self.lattice.to(self.device)
        self.neuron = self.neuron.to(self.device)
        self.carrier = self.carrier.to(self.device)
        # Mapper —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.perf_stats = {
            'step_times': [],
            'flows_per_step': [],
            'gpu_memory_usage': []
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–±–∏–π—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤
        self.stats = {
            'flows_killed_backward': 0,
            'flows_killed_bounds': 0,
            'flows_killed_energy': 0
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        self.convergence_stats = {
            'completed_count_history': [],
            'no_improvement_steps': 0,
            'best_completed_count': 0
        }
        
        # –°—á–µ—Ç—á–∏–∫ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        self.total_flows_created = 0
        
        logger.info(f"FlowProcessor initialized on {self.device}")
        logger.info(f"Components: Lattice {config.lattice_width}x{config.lattice_height}x{config.lattice_depth}, "
                   f"SimpleNeuron, EnergyCarrier")
        
        if config.convergence_enabled:
            logger.info(f"Adaptive convergence enabled: threshold={config.convergence_threshold:.2f}, "
                       f"min_steps={config.convergence_min_steps}, patience={config.convergence_patience}")
    
    def forward(self, input_embeddings: torch.Tensor, max_steps: Optional[int] = None, 
                global_training_step: Optional[int] = None) -> torch.Tensor:
        """
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —ç–Ω–µ—Ä–≥–∏–∏ —á–µ—Ä–µ–∑ —Ä–µ—à–µ—Ç–∫—É
        
        Args:
            input_embeddings: [batch, 768] - –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–µ—Å–ª–∏ None - depth —Ä–µ—à–µ—Ç–∫–∏)
            global_training_step: –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è curriculum learning
            
        Returns:
            output_embeddings: [batch, 768] - –≤—ã—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        """
        batch_size = input_embeddings.shape[0]
        
        # –†–∞–∑–º–µ—â–∞–µ–º –≤—Ö–æ–¥–Ω—É—é —ç–Ω–µ—Ä–≥–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–ø–ø–µ—Ä–∞
        self.lattice.reset()
        flow_ids = self.lattice.place_initial_energy(input_embeddings, self.mapper)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Ç–æ–∫–æ–≤
        self.total_flows_created = len(flow_ids)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        if max_steps is None:
            max_steps = self.config.lattice_depth
        
        logger.info(f"Starting energy propagation: {len(flow_ids)} initial flows, max {max_steps} steps")
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤
        initial_flows = self.lattice.get_active_flows()
        if initial_flows:
            initial_z_positions = torch.stack([flow.position[2] for flow in initial_flows[:10]])  # –ü–µ—Ä–≤—ã–µ 10
            logger.debug_energy(f"üèÅ INITIAL positions (first 10): Z-coords = {initial_z_positions.tolist()}")
            if torch.any(initial_z_positions != 0):
                logger.debug_energy(f"—Ç–µ–ø–µ—Ä—å —ç—Ç–æ –Ω–µ –æ—à–∏–±–∫–∞: Initial flows do NOT start at Z=0! Found Z = {initial_z_positions.unique().tolist()}")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        initial_flows_count = len(flow_ids)
        self.convergence_stats = {
            'completed_count_history': [],
            'no_improvement_steps': 0,
            'best_completed_count': 0
        }
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Å adaptive convergence
        actual_steps = 0
        for step in range(max_steps):
            actual_steps = step + 1
            active_flows = self.lattice.get_active_flows()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            if not active_flows:
                logger.info(f"No active flows at step {step}, stopping")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—é (adaptive max_steps)
            if self._check_convergence(step, initial_flows_count):
                logger.log(20, f"Early convergence at step {step}/{max_steps}")
                break
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
            if active_flows:
                self.step(active_flows, global_training_step=global_training_step)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –≤ –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏
            if step % self.config.log_interval == 0:
                stats = self.lattice.get_statistics()
                completion_rate = stats['total_completed'] / initial_flows_count if initial_flows_count > 0 else 0
                logger.info(f"Step {step}: {stats['current_active']} active flows, "
                          f"{stats['total_completed']} completed ({completion_rate:.2f})")
                
                # –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 —à–∞–≥–æ–≤
                if step <= 5 and active_flows:
                    # –°–æ–±–∏—Ä–∞–µ–º Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
                    z_positions = torch.stack([flow.position[2] for flow in active_flows])
                    logger.info(f"üìä Step {step} Z-distribution: "
                              f"min={z_positions.min():.2f}, max={z_positions.max():.2f}, "
                              f"mean={z_positions.mean():.2f}, std={z_positions.std():.2f}")
                    
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –æ–∂–∏–¥–∞–µ–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                    max_valid_z = self.config.lattice_depth - 1  # 59 –¥–ª—è depth=60
                    out_of_bounds_flows = (z_positions > max_valid_z * 2).sum().item()  # –ë–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ
                    if out_of_bounds_flows > 0:
                        logger.error(f"üö´ CRITICAL BOUNDS ERROR: {out_of_bounds_flows}/{len(active_flows)} flows "
                                   f"have Z > {max_valid_z * 2} (expected max ‚âà {max_valid_z})")
                        logger.error(f"üîç Z-range in normalization: {self.config.normalization_manager.ranges.z_range}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ Z-—Å–ª–æ—è–º
                    z_int = z_positions.int()
                    unique_z, counts = torch.unique(z_int, return_counts=True)
                    z_distribution = {int(z.item()): int(count.item()) for z, count in zip(unique_z, counts)}
                    logger.info(f"üìä Step {step} Z-layers distribution: {z_distribution}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é —ç–Ω–µ—Ä–≥–∏—é –∏–∑ –±—É—Ñ–µ—Ä–∞ (–ë–ï–ó –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ 768D!)
        output_surface_embeddings, completed_flows = self._collect_final_surface_output()
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –≤—ã—Ö–æ–¥–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–∏ (surface —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å!)
        if output_surface_embeddings.shape[0] == 0:
            logger.warning("No flows reached output, returning zero surface embeddings")
            surface_dim = self.config.lattice_width * self.config.lattice_height
            output_surface_embeddings = torch.zeros(batch_size, surface_dim, device=self.device)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ä–∞–∑–º–µ—Ä—É –±–∞—Ç—á–∞
        if output_surface_embeddings.shape[0] < batch_size:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            padding = torch.zeros(batch_size - output_surface_embeddings.shape[0], 
                                output_surface_embeddings.shape[1], device=self.device)
            output_surface_embeddings = torch.cat([output_surface_embeddings, padding], dim=0)
        elif output_surface_embeddings.shape[0] > batch_size:
            # –û–±—Ä–µ–∑–∞–µ–º
            output_surface_embeddings = output_surface_embeddings[:batch_size]
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = self.lattice.get_statistics()
        killed_backward = self.stats['flows_killed_backward']
        killed_bounds = self.stats['flows_killed_bounds']
        killed_energy = self.stats['flows_killed_energy']
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ adaptive max_steps
        steps_saved = max_steps - actual_steps
        if self.config.convergence_enabled and steps_saved > 0:
            speedup = max_steps / actual_steps if actual_steps > 0 else 1.0
            logger.log(20, f"Adaptive convergence saved {steps_saved} steps ({speedup:.2f}x speedup)")
        
        logger.info(f"Energy propagation complete ({actual_steps}/{max_steps} steps): "
                   f"{final_stats['total_completed']} flows reached output, "
                   f"{final_stats['total_died']} died "
                   f"(energy: {killed_energy}, backward: {killed_backward}, bounds: {killed_bounds})")
        
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
        if killed_backward > initial_flows_count * 0.8:  # –ë–æ–ª–µ–µ 80% –ø–æ—Ç–æ–∫–æ–≤ —É–º–µ—Ä–ª–∏ –∏–∑-–∑–∞ backward –¥–≤–∏–∂–µ–Ω–∏—è
            logger.error(f"üö´ CRITICAL: {killed_backward}/{initial_flows_count} flows died from backward movement!")
            logger.error("üîç Possible causes: bias not applied, wrong normalization, or curriculum disabled")
            if global_training_step is not None:
                logger.error(f"üîç Current global_training_step: {global_training_step}")
            else:
                logger.error("üîç global_training_step is None - curriculum learning disabled!")
        
        return output_surface_embeddings
    
    def _collect_final_output(self) -> Tuple[torch.Tensor, List[int]]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —ç–Ω–µ—Ä–≥–∏–∏
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏ –±—É—Ñ–µ—Ä, —Å–æ–±–∏—Ä–∞–µ—Ç —ç–Ω–µ—Ä–≥–∏—é –∫–æ–≥–¥–∞ –≤—Å–µ –≥–æ—Ç–æ–≤–æ.
        
        Returns:
            output_embeddings: [batch, embedding_dim] - —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            completed_flows: ID –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        active_flows = self.lattice.get_active_flows()
        
        logger.debug(f"Final collection: {len(active_flows)} active flows")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ - –ø–æ–º–µ—á–∞–µ–º –∏—Ö –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ
        active_at_output = 0
        for flow in active_flows:
            z_pos = flow.position[2].item()
            if z_pos >= self.config.lattice_depth - 1:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –≤—ã—Ö–æ–¥–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
                self.lattice._mark_flow_completed_zdepth_plane(flow.id)
                active_at_output += 1
            elif z_pos <= 0:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –Ω–∞—á–∞–ª–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –Ω–∞ Z=0
                self.lattice._mark_flow_completed_z0_plane(flow.id)  
                active_at_output += 1
        
        if active_at_output > 0:
            logger.debug(f"Marked {active_at_output} remaining flows as completed")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –°–æ–±–∏—Ä–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏)
        output_embeddings, completed_flows = self.lattice.collect_completed_flows_direct()
        
        # –û—á–∏—â–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞
        if completed_flows:
            # –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            for flow_id in completed_flows:
                if flow_id in self.lattice.active_flows:
                    del self.lattice.active_flows[flow_id]
            logger.info(f"Collected and removed {len(completed_flows)} completed flows")
        
        return output_embeddings, completed_flows
    
    def _collect_final_surface_output(self) -> Tuple[torch.Tensor, List[int]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç surface embeddings –ë–ï–ó –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ 768D
        
        Returns:
            output_surface_embeddings: [batch, surface_dim] - surface embeddings
            completed_flows: ID –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        active_flows = self.lattice.get_active_flows()
        
        logger.debug(f"Surface collection: {len(active_flows)} active flows")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ - –ø–æ–º–µ—á–∞–µ–º –∏—Ö –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ
        active_at_output = 0
        for flow in active_flows:
            z_pos = flow.position[2].item()
            if z_pos >= self.config.lattice_depth - 1:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –≤—ã—Ö–æ–¥–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
                self.lattice._mark_flow_completed_zdepth_plane(flow.id)
                active_at_output += 1
            elif z_pos <= 0:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –Ω–∞—á–∞–ª–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –Ω–∞ Z=0
                self.lattice._mark_flow_completed_z0_plane(flow.id)
                active_at_output += 1
        
        if active_at_output > 0:
            logger.debug(f"Marked {active_at_output} remaining flows as completed")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –°–æ–±–∏—Ä–∞–µ–º surface embeddings –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏)
        output_surface_embeddings, completed_flows = self.lattice.collect_completed_flows_surface_direct()
        
        # –û—á–∏—â–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞
        if completed_flows:
            # –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            for flow_id in completed_flows:
                if flow_id in self.lattice.active_flows:
                    del self.lattice.active_flows[flow_id]
            logger.info(f"Collected and removed {len(completed_flows)} completed flows")
        
        return output_surface_embeddings, completed_flows
    
    def step(self, active_flows: Optional[List] = None, global_training_step: Optional[int] = None):
        """
        –û–¥–∏–Ω —à–∞–≥ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        –ü–û–õ–ù–ê–Ø –ü–ê–†–ê–õ–õ–ï–õ–ò–ó–ê–¶–ò–Ø: –≤—Å–µ –ø–æ—Ç–æ–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤–º–µ—Å—Ç–æ sequential batches
        
        Args:
            active_flows: –°–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–µ—Å–ª–∏ None - –ø–æ–ª—É—á–∞–µ–º –∏–∑ lattice)
        """
        start_time = time.time()
        
        if active_flows is None:
            active_flows = self.lattice.get_active_flows()
        
        if not active_flows:
            return
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —É–±–∏—Ä–∞–µ–º Sequential Processing Bottleneck!
        # –í–º–µ—Å—Ç–æ —Ü–∏–∫–ª–∞ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ batch'–∞–º–∏ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –ø–æ—Ç–æ–∫–∏ —Å—Ä–∞–∑—É
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç GPU cores —Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å 1000+ –ø–æ—Ç–æ–∫–∞–º–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        
        flows_count = len(active_flows)
        max_flows_per_step = self.config.max_active_flows  # RTX 5090 –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ä–∞–∑—É
        
        if flows_count <= max_flows_per_step:
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å–ª—É—á–∞–π: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –ø–æ—Ç–æ–∫–∏ –æ–¥–Ω–∏–º –±–æ–ª—å—à–∏–º batch'–µ–º
            self._process_flow_batch(active_flows, global_training_step=global_training_step)
        else:
            # Fallback: –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤ (>200K), –¥–µ–ª–∏–º –Ω–∞ –∫—Ä—É–ø–Ω—ã–µ chunk'–∏
            optimal_chunk_size = max_flows_per_step // 2  # 100K –ø–æ—Ç–æ–∫–æ–≤ –∑–∞ —Ä–∞–∑
            
            for i in range(0, flows_count, optimal_chunk_size):
                chunk_flows = active_flows[i:i + optimal_chunk_size]
                self._process_flow_batch(chunk_flows, global_training_step=global_training_step)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        step_time = time.time() - start_time
        self.perf_stats['step_times'].append(step_time)
        self.perf_stats['flows_per_step'].append(flows_count)
        
        if self.device.type == 'cuda':
            memory_info = self.device_manager.get_memory_info()
            self.perf_stats['gpu_memory_usage'].append(memory_info['gpu_allocated_gb'])
    
    def _process_flow_batch(self, flows, global_training_step: Optional[int] = None):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –ø–æ—Ç–æ–∫–æ–≤ —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏"""
        batch_size = len(flows)
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–æ–≤ –∏ ID
        positions = torch.stack([f.position for f in flows])  # [batch, 3]
        energies = torch.stack([f.energy for f in flows])     # [batch, embedding_dim]
        hidden_states = torch.stack([f.hidden_state for f in flows])  # [batch, layers, hidden]
        flow_ids = torch.tensor([f.id for f in flows], dtype=torch.long, device=positions.device)
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º hidden states –¥–ª—è GRU –∏ –¥–µ–ª–∞–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏
        hidden_states = hidden_states.transpose(0, 1).contiguous()  # [layers, batch, hidden]
        
        # 1. SimpleNeuron –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏ —ç–Ω–µ—Ä–≥–∏–∏
        neuron_output = self.neuron(positions, energies)  # [batch, neuron_output_dim]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–æ–∑—Ä–∞—Å—Ç–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è progressive bias
        ages = torch.tensor([flow.age for flow in flows], dtype=torch.float32, device=positions.device)
        
        # 2. EnergyCarrier –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å curriculum learning
        carrier_output, new_hidden = self.carrier(
            neuron_output, 
            energies,
            hidden_states,
            positions,
            flow_age=ages,
            global_training_step=global_training_step  # –ü–µ—Ä–µ–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥
        )
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –∏ –¥–µ–ª–∞–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏
        new_hidden = new_hidden.transpose(0, 1).contiguous()  # [batch, layers, hidden]
        
        # 3. –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._process_results_vectorized(flows, flow_ids, positions, carrier_output, new_hidden)
    
    def _process_results_vectorized(self, flows, flow_ids, current_positions, carrier_output, new_hidden):
        """–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ carrier_output —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
        batch_size = len(flows)
        device = current_positions.device
        
        # –£–î–ê–õ–ï–ù–û: energy_alive_mask - –≤ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø–æ—Ç–æ–∫–∏ –Ω–µ —É–º–∏—Ä–∞—é—Ç –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —ç–Ω–µ—Ä–≥–∏–∏
        # –í—Å–µ –ø–æ—Ç–æ–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –∂–∏–≤—ã–º–∏, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ termination_reasons
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º termination_reasons –∏–∑ EnergyCarrier
        termination_reasons = carrier_output.termination_reason
        is_terminated = carrier_output.is_terminated  # [batch]
        
        # –†–∞–∑–±–∏—Ä–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        reached_z0_count = sum(1 for reason in termination_reasons if reason == "reached_z0_plane")
        reached_zdepth_count = sum(1 for reason in termination_reasons if reason == "reached_zdepth_plane")
        reflection_needed_count = sum(1 for reason in termination_reasons if reason == "xy_reflection_needed")
        active_count = sum(1 for reason in termination_reasons if reason == "active")
        
        # –£–î–ê–õ–ï–ù–û: energy_dead_count - –ø–æ—Ç–æ–∫–∏ –±–æ–ª—å—à–µ –Ω–µ —É–º–∏—Ä–∞—é—Ç –æ—Ç —ç–Ω–µ—Ä–≥–∏–∏
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ termination_reasons
        
        logger.debug_energy(f"üéØ Termination breakdown: z0={reached_z0_count}, zdepth={reached_zdepth_count}, "
                           f"reflection={reflection_needed_count}, active={active_count}")
        
        # –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ª–æ–≥–∏–∫–∞ –Ω–∞–º–Ω–æ–≥–æ –ø—Ä–æ—â–µ - —Ç–æ–ª—å–∫–æ 3 —Ç–∏–ø–∞ –ø–æ—Ç–æ–∫–æ–≤:
        # 1. –î–æ—Å—Ç–∏–≥—à–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π (–±—É—Ñ–µ—Ä–∏–∑—É–µ–º)
        # 2. –¢—Ä–µ–±—É—é—â–∏–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è (–ø—Ä–∏–º–µ–Ω—è–µ–º –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ) 
        # 3. –ê–∫—Ç–∏–≤–Ω—ã–µ (–æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é)
        
        # –ú–∞—Å–∫–∞ –ø–æ—Ç–æ–∫–æ–≤, –¥–æ—Å—Ç–∏–≥—à–∏—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        output_reached_mask = is_terminated
        
        # –ú–∞—Å–∫–∞ –ø–æ—Ç–æ–∫–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
        reflection_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)
        for i, reason in enumerate(termination_reasons):
            if reason == "xy_reflection_needed":
                reflection_mask[i] = True
        
        # –ú–∞—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        active_mask = ~is_terminated
        
        # –î–í–£–•–£–†–û–í–ù–ï–í–ê–Ø –ü–†–û–ï–ö–¶–ò–û–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ç–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–¥–µ–ª–∞–ª–∏ depth/2 —à–∞–≥–æ–≤ –Ω–æ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        projection_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)
        depth_half = self.config.lattice_depth / 2
        
        for i, flow in enumerate(flows):
            # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö)
            if active_mask[i]:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–¥–µ–ª–∞–ª –ª–∏ –ø–æ—Ç–æ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —à–∞–≥–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏
                if flow.steps_taken >= depth_half:
                    projection_mask[i] = True
                    logger.debug_energy(f"üéØ Flow {flow.id} qualifies for projection: {flow.steps_taken} >= {depth_half} steps")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏
        if projection_mask.any():
            projected_count = projection_mask.sum().item()
            logger.info(f"üìä Projecting {projected_count} flows to nearest output surface (completed ‚â•{depth_half} steps)")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏
            projection_flow_ids = flow_ids[projection_mask]
            projection_positions = carrier_output.next_position[projection_mask]
            projection_energies = carrier_output.energy_value[projection_mask]
            projection_hidden = new_hidden[projection_mask]
            
            # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Ç–æ–∫ –Ω–∞ –±–ª–∏–∂–∞–π—à—É—é –≤—ã—Ö–æ–¥–Ω—É—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å
            for i, flow_id in enumerate(projection_flow_ids):
                flow_id_item = flow_id.item()
                if flow_id_item in self.lattice.active_flows:
                    flow = self.lattice.active_flows[flow_id_item]
                    current_pos = projection_positions[i]
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–ª–∏–∂–∞–π—à—É—é –≤—ã—Ö–æ–¥–Ω—É—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –ø–æ Z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ
                    z_current = current_pos[2].item()
                    distance_to_z0 = abs(z_current - 0)
                    distance_to_zdepth = abs(z_current - self.config.lattice_depth)
                    
                    # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –Ω–∞ –±–ª–∏–∂–∞–π—à—É—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å
                    if distance_to_z0 <= distance_to_zdepth:
                        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –Ω–∞ Z=0 –ø–ª–æ—Å–∫–æ—Å—Ç—å
                        projected_pos = current_pos.clone()
                        projected_pos[2] = 0
                        surface_type = "z0"
                        # –û–±–Ω–æ–≤–ª—è–µ–º projected_surface –≤ –ø–æ—Ç–æ–∫–µ
                        flow.projected_surface = "z0_plane"
                    else:
                        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –Ω–∞ Z=depth –ø–ª–æ—Å–∫–æ—Å—Ç—å
                        projected_pos = current_pos.clone()
                        projected_pos[2] = self.config.lattice_depth
                        surface_type = "zdepth"
                        # –û–±–Ω–æ–≤–ª—è–µ–º projected_surface –≤ –ø–æ—Ç–æ–∫–µ
                        flow.projected_surface = "zdepth_plane"
                    
                    # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ï —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –≤–µ—Å–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
                    original_distance = min(distance_to_z0, distance_to_zdepth)
                    flow.distance_to_surface = original_distance
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ç–æ–∫ –∏ –±—É—Ñ–µ—Ä–∏–∑—É–µ–º –µ–≥–æ
                    flow.position = projected_pos
                    flow.energy = projection_energies[i]
                    flow.hidden_state = projection_hidden[i]
                    flow.age += 1
                    
                    # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ü–æ–º–µ—á–∞–µ–º –ø–æ—Ç–æ–∫ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏  
                    if surface_type == "z0":
                        self.lattice._mark_flow_completed_z0_plane(flow_id_item)
                    else:
                        self.lattice._mark_flow_completed_zdepth_plane(flow_id_item)
                    
                    logger.debug_energy(f"üéØ Projected flow {flow_id_item} to {surface_type} plane: "
                                      f"original_distance={original_distance:.3f}, steps={flow.steps_taken}")
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            active_mask = active_mask & ~projection_mask
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ —Å–º–µ—â–µ–Ω–∏—è (–ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π carrier_dropout)
        if self.config.enable_displacement_filtering:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—É—â–∏—Ö –∏ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–∑–∏—Ü–∏–π
            displacements = carrier_output.next_position - current_positions  # [batch, 3]
            displacement_lengths = torch.norm(displacements, dim=1)  # [batch]
            
            # –ú–∞—Å–∫–∞ –ø–æ—Ç–æ–∫–æ–≤ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏ ("—Ç–æ–ø—á—É—â–∏–µ—Å—è")
            small_displacement_mask = displacement_lengths < self.config.min_displacement_threshold
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if small_displacement_mask.any():
                filtered_count = small_displacement_mask.sum().item()
                logger.debug_relative(f"üîç Filtered {filtered_count}/{batch_size} flows with small displacements "
                                     f"(< {self.config.min_displacement_threshold:.2f})")
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ—Ç–æ–∫–∏ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            active_mask = active_mask & ~small_displacement_mask
        
        # 2. –ë—É—Ñ–µ—Ä–∏–∑—É–µ–º –ø–æ—Ç–æ–∫–∏, –¥–æ—Å—Ç–∏–≥—à–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        if output_reached_mask.any():
            output_flow_ids = flow_ids[output_reached_mask]
            output_positions = carrier_output.next_position[output_reached_mask]
            output_energies = carrier_output.energy_value[output_reached_mask]
            output_hidden = new_hidden[output_reached_mask]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ –±—É—Ñ–µ—Ä–∏–∑—É–µ–º
            for i, flow_id in enumerate(output_flow_ids):
                flow_id_item = flow_id.item()
                new_position = output_positions[i]
                new_energy = output_energies[i]
                new_hidden_state = output_hidden[i]
                
                # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ü–æ–º–µ—á–∞–µ–º –ø–æ—Ç–æ–∫ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏
                z_pos = new_position[2].item()
                if z_pos <= 0:
                    self.lattice._mark_flow_completed_z0_plane(flow_id_item)
                elif z_pos >= self.config.lattice_depth:
                    self.lattice._mark_flow_completed_zdepth_plane(flow_id_item)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ç–æ–∫ –ø–µ—Ä–µ–¥ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–µ–π
                if flow_id_item in self.lattice.active_flows:
                    self.lattice.active_flows[flow_id_item].position = new_position
                    self.lattice.active_flows[flow_id_item].energy = new_energy
                    self.lattice.active_flows[flow_id_item].hidden_state = new_hidden_state
                    self.lattice.active_flows[flow_id_item].age += 1
        
        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if reflection_mask.any() and self.config.boundary_reflection_enabled:
            reflection_flow_ids = flow_ids[reflection_mask]
            reflection_count = reflection_mask.sum().item()
            
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –ø–æ—Ç–æ–∫–∏ –¥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è —Å –∏—Ö ID
            reflection_positions_before = carrier_output.next_position[reflection_mask]
            logger.debug_reflection(f"üîÑ BEFORE reflection: {reflection_count} flows need reflection")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
            for i in range(min(3, reflection_count)):
                flow_id = reflection_flow_ids[i].item()
                pos = reflection_positions_before[i]
                logger.debug_reflection(f"üîÑ Flow {flow_id} before: X={pos[0].item():.6f}, Y={pos[1].item():.6f}, Z={pos[2].item():.6f}")
            
            reflection_positions = self.reflect_boundaries(reflection_positions_before, reflection_flow_ids)
            reflection_energies = carrier_output.energy_value[reflection_mask]
            reflection_hidden = new_hidden[reflection_mask]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ç–æ–∫–∏ —Å –æ—Ç—Ä–∞–∂–µ–Ω–Ω—ã–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏
            self.lattice.batch_update_flows(
                reflection_flow_ids,
                reflection_positions,
                reflection_energies,
                reflection_hidden
            )
        
        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
        final_active_mask = active_mask
        if reflection_mask.any() and not self.config.boundary_reflection_enabled:
            # –ï—Å–ª–∏ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ, –ø–æ—Ç–æ–∫–∏ —Å xy_reflection_needed –æ—Å—Ç–∞—é—Ç—Å—è –∞–∫—Ç–∏–≤–Ω—ã–º–∏
            final_active_mask = active_mask | reflection_mask
        
        if final_active_mask.any():
            active_flow_ids = flow_ids[final_active_mask]
            active_positions = carrier_output.next_position[final_active_mask]
            active_energies = carrier_output.energy_value[final_active_mask]
            active_hidden = new_hidden[final_active_mask]
            
            # Batch update all active flows
            self.lattice.batch_update_flows(
                active_flow_ids,
                active_positions,
                active_energies,
                active_hidden
            )
        
        # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ spawn –ø–æ—Ç–æ–∫–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º spawn –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–º–µ—â–µ–Ω–∏—è, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.config.movement_based_spawn:
            movement_spawn_info = self._check_movement_spawn(current_positions, carrier_output.next_position, flow_ids)
            if movement_spawn_info:
                carrier_output.spawn_info.extend(movement_spawn_info)
                logger.debug_spawn_movement(f"üéÜ Added {len(movement_spawn_info)} movement-based spawns")
        
        self._process_spawns_optimized(flows, carrier_output, final_active_mask)
    
    def reflect_boundaries(self, position: torch.Tensor, flow_ids: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ [-1, 1]
        
        Args:
            position: [batch, 3] - –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
            flow_ids: [batch] - ID –ø–æ—Ç–æ–∫–æ–≤ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            reflected_position: [batch, 3] - –ø–æ–∑–∏—Ü–∏–∏ —Å –æ—Ç—Ä–∞–∂–µ–Ω–Ω—ã–º–∏ X/Y –≤ [-1, 1]
        """
        reflected_pos = position.clone()
        x, y, z = reflected_pos[:, 0], reflected_pos[:, 1], reflected_pos[:, 2]
        
        # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ X –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        x = torch.where(x < -1.0, -2.0 - x, x)  # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç –ª–µ–≤–æ–π –≥—Ä–∞–Ω–∏—Ü—ã -1
        x = torch.where(x > 1.0, 2.0 - x, x)    # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç –ø—Ä–∞–≤–æ–π –≥—Ä–∞–Ω–∏—Ü—ã 1
        
        # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ Y –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        y = torch.where(y < -1.0, -2.0 - y, y)
        y = torch.where(y > 1.0, 2.0 - y, y)
        
        # Z –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–¥–≤–∏–∂–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –ø–ª–æ—Å–∫–æ—Å—Ç—è–º)
        
        reflected_pos[:, 0] = x
        reflected_pos[:, 1] = y
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
        num_reflected = position.shape[0]
        
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏–π –ø–æ –æ—Å—è–º
        x_reflected_left = (position[:, 0] < -1.0).sum().item()
        x_reflected_right = (position[:, 0] > 1.0).sum().item()
        y_reflected_left = (position[:, 1] < -1.0).sum().item()
        y_reflected_right = (position[:, 1] > 1.0).sum().item()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–≤—ã—Ö 3-—Ö –æ—Ç—Ä–∞–∂–µ–Ω–∏–π
        reflection_examples = []
        for i in range(min(3, num_reflected)):
            orig_x, orig_y, orig_z = position[i, 0].item(), position[i, 1].item(), position[i, 2].item()
            new_x, new_y = x[i].item(), y[i].item()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
            reflection_type = []
            if orig_x < -1.0:
                reflection_type.append(f"X<-1({orig_x:.3f}‚Üí{new_x:.3f})")
            elif orig_x > 1.0:
                reflection_type.append(f"X>1({orig_x:.3f}‚Üí{new_x:.3f})")
            if orig_y < -1.0:
                reflection_type.append(f"Y<-1({orig_y:.3f}‚Üí{new_y:.3f})")
            elif orig_y > 1.0:
                reflection_type.append(f"Y>1({orig_y:.3f}‚Üí{new_y:.3f})")
            
            if reflection_type:
                reflection_examples.append(f"flow_{i}[{','.join(reflection_type)}]")
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.debug_reflection(f"üîÑ Reflected {num_reflected} positions: "
                               f"X_left={x_reflected_left}, X_right={x_reflected_right}, "
                               f"Y_left={y_reflected_left}, Y_right={y_reflected_right}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        if reflection_examples:
            logger.debug_reflection(f"üîÑ Examples: {', '.join(reflection_examples)}")
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        logger.debug_reflection(f"üîÑ Final ranges: X[{x.min().item():.3f}, {x.max().item():.3f}], "
                               f"Y[{y.min().item():.3f}, {y.max().item():.3f}]")
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –ø–æ—Ç–æ–∫–æ–≤
        if flow_ids is not None:
            logger.debug_reflection(f"üîÑ AFTER reflection examples:")
            for i in range(min(3, len(position))):
                flow_id = flow_ids[i].item()
                orig_pos = position[i]
                new_pos = reflected_pos[i]
                logger.debug_reflection(f"üîÑ Flow {flow_id} after: X={new_pos[0].item():.6f}, Y={new_pos[1].item():.6f}, Z={new_pos[2].item():.6f} "
                                       f"(changed: ŒîX={new_pos[0].item() - orig_pos[0].item():.6f}, "
                                       f"ŒîY={new_pos[1].item() - orig_pos[1].item():.6f})")
        
        return reflected_pos
    
    def _check_movement_spawn(self, current_positions: torch.Tensor, 
                             next_positions: torch.Tensor, 
                             flow_ids: torch.Tensor) -> List:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç spawn –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–º–µ—â–µ–Ω–∏—è
        
        Args:
            current_positions: [batch, 3] - —Ç–µ–∫—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
            next_positions: [batch, 3] - —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
            flow_ids: [batch] - ID –ø–æ—Ç–æ–∫–æ–≤
            
        Returns:
            spawn_info: –°–ø–∏—Å–æ–∫ SpawnInfo –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏—è
        displacement = next_positions - current_positions  # [batch, 3]
        displacement_lengths = torch.norm(displacement, dim=1)  # [batch]
        
        # –ü–æ—Ä–æ–≥ –¥–ª—è spawn –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        threshold = self.config.spawn_movement_threshold_ratio  # –ü—Ä—è–º–æ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
        
        # –ú–∞—Å–∫–∞ –¥–ª—è spawn
        spawn_mask = displacement_lengths > threshold
        
        if not spawn_mask.any():
            return []
        
        spawn_info_list = []
        total_candidates = spawn_mask.sum().item()
        total_potential_spawns = 0
        total_actual_spawns = 0
        total_limited_spawns = 0
        spawn_examples = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω —Å–æ–∑–¥–∞—Ç—å spawn
        spawn_indices = torch.where(spawn_mask)[0]
        for idx in spawn_indices:
            idx_val = idx.item()
            delta_length = displacement_lengths[idx].item()
            flow_id = flow_ids[idx].item()
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞)
            potential_spawns = int(delta_length / threshold) - 1
            actual_spawns = min(potential_spawns, self.config.max_spawn_per_step)
            
            total_potential_spawns += potential_spawns
            
            if actual_spawns > 0:
                # –ü–æ–ª—É—á–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞
                if flow_id in self.lattice.active_flows:
                    parent_energy = self.lattice.active_flows[flow_id].energy
                    spawn_energies = [parent_energy.clone() for _ in range(actual_spawns)]
                    
                    # –°–æ–∑–¥–∞–µ–º SpawnInfo —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞)
                    from .energy_carrier import SpawnInfo
                    spawn_info = SpawnInfo(
                        energies=spawn_energies,
                        parent_batch_idx=idx_val  # –ò–Ω–¥–µ–∫—Å –≤ –±–∞—Ç—á–µ
                    )
                    spawn_info_list.append(spawn_info)
                    
                    total_actual_spawns += actual_spawns
                    if potential_spawns > actual_spawns:
                        total_limited_spawns += (potential_spawns - actual_spawns)
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3-—Ö spawn'–æ–≤
                    if len(spawn_examples) < 3:
                        spawn_examples.append(f"flow_{flow_id}[disp={delta_length:.3f}‚Üí{actual_spawns}spawns]")
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if total_candidates > 0:
            logger.debug_spawn_movement(f"üéÜ Movement spawn summary: {total_candidates} candidates, "
                                       f"{total_potential_spawns} potential ‚Üí {total_actual_spawns} actual spawns")
            if total_limited_spawns > 0:
                logger.debug_spawn_movement(f"üéÜ Limited by config: {total_limited_spawns} spawns restricted by max_spawn_per_step={self.config.max_spawn_per_step}")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            if spawn_examples:
                logger.debug_spawn_movement(f"üéÜ Examples: {', '.join(spawn_examples)}")
        
        return spawn_info_list
    
    def _process_spawns_optimized(self, flows, carrier_output, alive_mask):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ spawn –ø–æ—Ç–æ–∫–æ–≤"""
        if not carrier_output.spawn_info:
            return
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å spawn_info –ø–æ parent_batch_idx –¥–ª—è O(1) –ø–æ–∏—Å–∫–∞
        spawn_by_idx = {}
        for spawn_info in carrier_output.spawn_info:
            spawn_by_idx[spawn_info.parent_batch_idx] = spawn_info
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ spawn'–æ–≤
        total_spawn_requests = len(spawn_by_idx)
        total_spawned = 0
        spawn_examples = []
        parent_flows = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º spawn'—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –∂–∏–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        alive_indices = torch.where(alive_mask)[0] if alive_mask.any() else torch.tensor([], dtype=torch.long)
        for idx in alive_indices:
            idx_val = idx.item()
            if idx_val in spawn_by_idx:
                spawn_info = spawn_by_idx[idx_val]
                if spawn_info.energies:
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ spawn'–æ–≤ –∫–æ–Ω—Ñ–∏–≥–æ–º
                    spawn_energies = spawn_info.energies[:self.config.max_spawn_per_step]
                    flow_id = flows[idx_val].id
                    new_flow_ids = self.lattice.spawn_flows(flow_id, spawn_energies)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
                    self.total_flows_created += len(spawn_energies)
                    total_spawned += len(spawn_energies)
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3-—Ö spawn'–æ–≤
                    if len(spawn_examples) < 3:
                        spawn_examples.append(f"parent_{flow_id}‚Üí{len(spawn_energies)}flows")
                    parent_flows.append(flow_id)
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if total_spawn_requests > 0:
            logger.debug_spawn(f"üéÜ Spawn summary: {total_spawn_requests} requests ‚Üí {total_spawned} new flows created")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            if spawn_examples:
                logger.debug_spawn(f"üéÜ Examples: {', '.join(spawn_examples)}")
            
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ spawn'—ã –≤ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ
            if len(parent_flows) > 3:
                other_parents = parent_flows[3:]
                logger.debug_spawn(f"üéÜ Additional parents: {len(other_parents)} flows (ids: {other_parents[:5]}{'...' if len(other_parents) > 5 else ''})")
    
    def _check_convergence(self, step: int, initial_flows_count: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—è –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –¥–ª—è adaptive max_steps
        
        Args:
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥
            initial_flows_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            
        Returns:
            True –µ—Å–ª–∏ —Å–ª–µ–¥—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
        """
        if not self.config.convergence_enabled:
            return False
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        if step < self.config.convergence_min_steps:
            return False
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = self.lattice.get_statistics()
        completed_count = stats['total_completed']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.convergence_stats['completed_count_history'].append(completed_count)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ (—É—á–∏—Ç—ã–≤–∞—è –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏, –≤–∫–ª—é—á–∞—è spawn'—ã)
        completion_rate = completed_count / self.total_flows_created if self.total_flows_created > 0 else 0
        
        logger.log(20, f"Convergence check step {step}: {completed_count}/{self.total_flows_created} "
                      f"flows completed ({completion_rate:.2f})")
        
        # –£—Å–ª–æ–≤–∏–µ 1: –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ø–æ—Ä–æ–≥ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        if completion_rate >= self.config.convergence_threshold:
            logger.log(20, f"Convergence threshold reached: {completion_rate:.2f} >= {self.config.convergence_threshold:.2f}")
            return True
        
        # –£—Å–ª–æ–≤–∏–µ 2: Patience - –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ç–µ—á–µ–Ω–∏–µ N —à–∞–≥–æ–≤
        if completed_count > self.convergence_stats['best_completed_count']:
            self.convergence_stats['best_completed_count'] = completed_count
            self.convergence_stats['no_improvement_steps'] = 0
        else:
            self.convergence_stats['no_improvement_steps'] += 1
        
        if self.convergence_stats['no_improvement_steps'] >= self.config.convergence_patience:
            logger.log(20, f"Convergence patience exceeded: {self.convergence_stats['no_improvement_steps']} "
                          f">= {self.config.convergence_patience}")
            return True
        
        return False
    
    def get_performance_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.perf_stats['step_times']:
            return {}
        
        import numpy as np
        
        stats = {
            'avg_step_time': np.mean(self.perf_stats['step_times']),
            'max_step_time': np.max(self.perf_stats['step_times']),
            'avg_flows_per_step': np.mean(self.perf_stats['flows_per_step']),
            'max_flows_per_step': np.max(self.perf_stats['flows_per_step']),
            'avg_gpu_memory_gb': np.mean(self.perf_stats['gpu_memory_usage']) if self.perf_stats['gpu_memory_usage'] else 0,
            'lattice_stats': self.lattice.get_statistics()
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        if self.config.convergence_enabled and self.convergence_stats['completed_count_history']:
            stats['convergence_stats'] = {
                'best_completion_count': self.convergence_stats['best_completed_count'],
                'final_completion_count': self.convergence_stats['completed_count_history'][-1] if self.convergence_stats['completed_count_history'] else 0,
                'completion_trend': len(self.convergence_stats['completed_count_history'])
            }
        
        return stats
    
    def visualize_flow_state(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤"""
        active_flows = self.lattice.get_active_flows()
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ —ç–Ω–µ—Ä–≥–∏–∏
        positions = []
        energies = []
        ages = []
        
        for flow in active_flows:
            positions.append(flow.position.cpu().numpy())
            energy_norm = torch.norm(flow.energy).item()
            energies.append(energy_norm)
            ages.append(flow.age)
        
        return {
            'positions': positions,
            'energies': energies,
            'ages': ages,
            'total_flows': len(active_flows),
            'lattice_dims': (self.config.lattice_width, 
                           self.config.lattice_height,
                           self.config.lattice_depth)
        }


def create_flow_processor(config=None) -> FlowProcessor:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è FlowProcessor"""
    return FlowProcessor(config)