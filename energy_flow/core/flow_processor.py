"""
Flow Processor - –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏
=================================================

–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Å–µ—Ö —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤.
–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É SimpleNeuron –∏ EnergyCarrier.
"""

import torch
import torch.nn as nn
from typing import List, Dict, Optional, Tuple
import time

from ..utils.logging import get_logger, log_memory_state, gated_log, summarize_step, format_first_n, DEBUG_PERFORMANCE
from ..config import get_energy_config, create_debug_config, set_energy_config
from ..utils.device_manager import get_device_manager
from .simple_neuron import SimpleNeuron, create_simple_neuron
from .energy_carrier import EnergyCarrier, create_energy_carrier
from .energy_lattice import EnergyLattice, create_energy_lattice

logger = get_logger(__name__)


class FlowProcessor(nn.Module):
    """
    –ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏ —á–µ—Ä–µ–∑ —Ä–µ—à–µ—Ç–∫—É
    
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ SimpleNeuron –∏ EnergyCarrier
    - –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø–æ—Ç–æ–∫–æ–≤
    """
    
    def __init__(self, config=None):
        """
        Args:
            config: EnergyConfig —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        """
        super().__init__()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config is None:
            config = create_debug_config()
            set_energy_config(config)
        self.config = config
        
        # Device manager
        self.device_manager = get_device_manager()
        self.device = self.device_manager.device
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.lattice = create_energy_lattice(config)
        self.neuron = create_simple_neuron(config)
        self.carrier = create_energy_carrier(config)
        
        # Embedding mapper - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        from .embedding_mapper import EnergyFlowMapper
        self.mapper = EnergyFlowMapper(config)
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º mapper –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.mapper.input_mapper = self.mapper.input_mapper.to(self.device)
        self.mapper.output_collector = self.mapper.output_collector.to(self.device)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.lattice = self.lattice.to(self.device)
        self.neuron = self.neuron.to(self.device)
        self.carrier = self.carrier.to(self.device)
        # Mapper —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.perf_stats = {
            'step_times': [],
            'flows_per_step': [],
            'gpu_memory_usage': []
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–±–∏–π—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤
        self.stats = {
            'flows_killed_backward': 0,
            'flows_killed_bounds': 0,
            'flows_killed_energy': 0
        }
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ —Å–æ —Å–∫–æ–ª—å–∑—è—â–∏–º –æ–∫–Ω–æ–º
        self.convergence_stats = {
            'completed_count_history': [],
            'no_improvement_steps': 0,
            'best_completed_count': 0,
            'moving_average_window': 5,  # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
            'last_moving_avg': 0.0,
            'improvement_threshold': 0.01  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è —Å—á–∏—Ç—ã–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        }
        
        # –°—á–µ—Ç—á–∏–∫ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        self.total_flows_created = 0
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
        self.memory_cleanup_interval = 10  # –ö–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
        self.memory_threshold_gb = 20.0    # –ü–æ—Ä–æ–≥ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ GPU –∫—ç—à–∞ (20GB –¥–ª—è RTX 5090)
        self.step_counter = 0
        
        logger.info(f"FlowProcessor initialized on {self.device}")
        logger.info(f"Components: Lattice {config.lattice_width}x{config.lattice_height}x{config.lattice_depth}, "
                   f"SimpleNeuron, EnergyCarrier")
        
        if config.convergence_enabled:
            logger.info(f"Adaptive convergence enabled: threshold={config.convergence_threshold:.2f}, "
                       f"min_steps={config.convergence_min_steps}, patience={config.convergence_patience}")
        
        logger.info(f"Memory management: cleanup every {self.memory_cleanup_interval} steps, threshold={self.memory_threshold_gb}GB")
    
    def forward(self, input_embeddings: torch.Tensor, max_steps: Optional[int] = None, 
                global_training_step: Optional[int] = None) -> torch.Tensor:
        """
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —ç–Ω–µ—Ä–≥–∏–∏ —á–µ—Ä–µ–∑ —Ä–µ—à–µ—Ç–∫—É
        
        Args:
            input_embeddings: [batch, 768] - –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–µ—Å–ª–∏ None - depth —Ä–µ—à–µ—Ç–∫–∏)
            global_training_step: –ì–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è curriculum learning
            
        Returns:
            output_embeddings: [batch, 768] - –≤—ã—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        """
        batch_size = input_embeddings.shape[0]
        
        # –†–∞–∑–º–µ—â–∞–µ–º –≤—Ö–æ–¥–Ω—É—é —ç–Ω–µ—Ä–≥–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–ø–ø–µ—Ä–∞
        self.lattice.reset()
        flow_ids = self.lattice.place_initial_energy(input_embeddings, self.mapper)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Ç–æ–∫–æ–≤
        self.total_flows_created = len(flow_ids)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        if max_steps is None:
            max_steps = self.config.lattice_depth // 2  # –ü–æ–ª–æ–≤–∏–Ω–∞ –≥–ª—É–±–∏–Ω—ã —Ä–µ—à–µ—Ç–∫–∏

        logger.info(f"Starting energy propagation: {len(flow_ids)} initial flows, max {max_steps} steps")
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤
        initial_flows = self.lattice.get_active_flows()
        if initial_flows:
            initial_z_positions = torch.stack([flow.position[2] for flow in initial_flows])
            gated_log(
                logger,
                'DEBUG_ENERGY',
                step=0,
                key='initial_positions',
                msg_or_factory=lambda: f"üèÅ INITIAL positions: Z={format_first_n(initial_z_positions, n=10)}",
                first_n_steps=1,
                every=0,
            )
            if torch.any(initial_z_positions != 0):
                logger.debug_energy(
                    f"—Ç–µ–ø–µ—Ä—å —ç—Ç–æ –Ω–µ –æ—à–∏–±–∫–∞: Initial flows do NOT start at Z=0! Found Z = {initial_z_positions.unique().tolist()}"
                )
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–∫–Ω–∞/–ø–æ—Ä–æ–≥–æ–≤
        initial_flows_count = len(flow_ids)
        # –ù–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –∫–ª—é—á–∏ –∫–∞–∫ moving_average_window
        self.convergence_stats.update({
            'completed_count_history': [],
            'no_improvement_steps': 0,
            'best_completed_count': 0,
            'last_moving_avg': 0.0,
        })
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π (–Ω–∞ —Å–ª—É—á–∞–π —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤/—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–π)
        if 'moving_average_window' not in self.convergence_stats:
            self.convergence_stats['moving_average_window'] = 5
        if 'improvement_threshold' not in self.convergence_stats:
            self.convergence_stats['improvement_threshold'] = 0.01
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Å adaptive convergence
        actual_steps = 0
        for step in range(max_steps):
            actual_steps = step + 1
            active_flows = self.lattice.get_active_flows()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            if not active_flows:
                logger.info(f"No active flows at step {step}, stopping")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—é (adaptive max_steps)
            if self._check_convergence(step, initial_flows_count):
                logger.log(20, f"Early convergence at step {step}/{max_steps}")
                break
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
            if active_flows:
                self.step(active_flows, global_training_step=global_training_step)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            self.cleanup_memory_safe()
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –≥–µ–π—Ç –∏ –ª–µ–Ω–∏–≤–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ
            stats = self.lattice.get_statistics()
            completion_rate = stats['total_completed'] / initial_flows_count if initial_flows_count > 0 else 0
            gated_log(
                logger,
                'INFO',
                step=step,
                key='flow_step_progress',
                msg_or_factory=lambda: summarize_step({
                    'active': stats['current_active'],
                    'completed': stats['total_completed'],
                    'rate': completion_rate,
                }, step=step, prefix='FLOW'),
                first_n_steps=5,
                every=getattr(self.config, 'log_interval', 10),
            )
            
            # –î–ï–¢–ê–õ–ò (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö —à–∞–≥–æ–≤) —á–µ—Ä–µ–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –≥–µ–π—Ç
            if active_flows:
                z_positions = torch.stack([flow.position[2] for flow in active_flows])
                def _detail_stats():
                    boundary_stats = {
                        'z_min_boundary': (z_positions <= -0.95).sum().item(),
                        'z_max_boundary': (z_positions >= 0.95).sum().item(),
                        'z_center': ((z_positions > -0.2) & (z_positions < 0.2)).sum().item(),
                        'total': len(active_flows)
                    }
                    return (
                        f"üìä Z: min={z_positions.min():.2f}, max={z_positions.max():.2f}, "
                        f"mean={z_positions.mean():.2f}, std={z_positions.std():.2f}; "
                        f"bounds: z0={boundary_stats['z_min_boundary']}, "
                        f"zdepth={boundary_stats['z_max_boundary']}, center={boundary_stats['z_center']}, "
                        f"total={boundary_stats['total']}"
                    )
                gated_log(
                    logger,
                    'DEBUG_CONVERGENCE',
                    step=step,
                    key='flow_step_details',
                    msg_or_factory=_detail_stats,
                    first_n_steps=5,
                    every=0,
                )
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                max_valid_z = self.config.lattice_depth - 1
                out_of_bounds_flows = (z_positions > max_valid_z * 2).sum().item()
                if out_of_bounds_flows > 0:
                    logger.error(
                        f"üö´ CRITICAL BOUNDS ERROR: {out_of_bounds_flows}/{len(active_flows)} flows have Z > {max_valid_z * 2} (expected max ‚âà {max_valid_z})"
                    )
                    logger.error(f"üîç Z-range in normalization: {self.config.normalization_manager.ranges.z_range}")
                # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏ –ø–æ –≥–µ–π—Ç—É
                def _hist_msg():
                    bins = 20
                    hist = torch.histc(z_positions, bins=bins, min=-1.0, max=1.0)
                    return f"üìä Z histogram (norm, bins={bins}): {format_first_n(hist, n=20)}"
                gated_log(
                    logger,
                    'INFO',
                    step=step,
                    key='flow_step_hist',
                    msg_or_factory=_hist_msg,
                    first_n_steps=3,
                    every=0,
                )
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é —ç–Ω–µ—Ä–≥–∏—é –∏–∑ –±—É—Ñ–µ—Ä–∞ (–ë–ï–ó –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ 768D!)
        output_surface_embeddings, completed_flows = self._collect_final_surface_output()
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –≤—ã—Ö–æ–¥–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–∏ (surface —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å!)
        if output_surface_embeddings.shape[0] == 0:
            logger.warning("No flows reached output, returning zero surface embeddings")
            surface_dim = self.config.lattice_width * self.config.lattice_height
            output_surface_embeddings = torch.zeros(batch_size, surface_dim, device=self.device)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ä–∞–∑–º–µ—Ä—É –±–∞—Ç—á–∞
        if output_surface_embeddings.shape[0] < batch_size:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            padding = torch.zeros(batch_size - output_surface_embeddings.shape[0], 
                                output_surface_embeddings.shape[1], device=self.device)
            output_surface_embeddings = torch.cat([output_surface_embeddings, padding], dim=0)
        elif output_surface_embeddings.shape[0] > batch_size:
            # –û–±—Ä–µ–∑–∞–µ–º
            output_surface_embeddings = output_surface_embeddings[:batch_size]
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = self.lattice.get_statistics()
        killed_backward = self.stats['flows_killed_backward']
        killed_bounds = self.stats['flows_killed_bounds']
        killed_energy = self.stats['flows_killed_energy']
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ adaptive max_steps
        steps_saved = max_steps - actual_steps
        if self.config.convergence_enabled and steps_saved > 0:
            speedup = max_steps / actual_steps if actual_steps > 0 else 1.0
            logger.log(20, f"Adaptive convergence saved {steps_saved} steps ({speedup:.2f}x speedup)")
        
        # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Å–≤–æ–¥ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —à–∞–≥–∞
        summary = summarize_step({
            'completed': final_stats['total_completed'],
            'died': final_stats['total_died'],
            'killed_energy': killed_energy,
            'killed_backward': killed_backward,
            'killed_bounds': killed_bounds,
        }, step=actual_steps, prefix='FLOW-END')
        logger.info(f"Energy propagation complete ({actual_steps}/{max_steps} steps). {summary}")
        
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
        if killed_backward > initial_flows_count * 0.8:  # –ë–æ–ª–µ–µ 80% –ø–æ—Ç–æ–∫–æ–≤ —É–º–µ—Ä–ª–∏ –∏–∑-–∑–∞ backward –¥–≤–∏–∂–µ–Ω–∏—è
            logger.error(f"üö´ CRITICAL: {killed_backward}/{initial_flows_count} flows died from backward movement!")
            logger.error("üîç Possible causes: bias not applied, wrong normalization, or curriculum disabled")
            if global_training_step is not None:
                logger.error(f"üîç Current global_training_step: {global_training_step}")
            else:
                logger.error("üîç global_training_step is None - curriculum learning disabled!")
        
        return output_surface_embeddings
    
    def _collect_final_output(self, batch_size: int = None) -> Tuple[torch.Tensor, List[int]]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —ç–Ω–µ—Ä–≥–∏–∏
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏ –±—É—Ñ–µ—Ä, —Å–æ–±–∏—Ä–∞–µ—Ç —ç–Ω–µ—Ä–≥–∏—é –∫–æ–≥–¥–∞ –≤—Å–µ –≥–æ—Ç–æ–≤–æ.
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            output_embeddings: [batch, embedding_dim] - —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            completed_flows: ID –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        active_flows = self.lattice.get_active_flows()
        
        logger.debug(f"Final collection: {len(active_flows)} active flows")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ - –ø–æ–º–µ—á–∞–µ–º –∏—Ö –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ
        active_at_output = 0
        for flow in active_flows:
            z_pos = flow.position[2].item()
            if z_pos >= self.config.lattice_depth - 1:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –≤—ã—Ö–æ–¥–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
                self.lattice._mark_flow_completed_zdepth_plane(flow.id)
                active_at_output += 1
            elif z_pos <= 0:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –Ω–∞—á–∞–ª–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –Ω–∞ Z=0
                self.lattice._mark_flow_completed_z0_plane(flow.id)  
                active_at_output += 1
        
        if active_at_output > 0:
            logger.debug(f"Marked {active_at_output} remaining flows as completed")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º batch_size –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if batch_size is None:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ config –∏–ª–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            batch_size = self.config.batch_size
            if batch_size is None:
                logger.error(f"Cannot determine batch_size")

        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –°–æ–±–∏—Ä–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏)
        # –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï 768D –ß–ï–†–ï–ó MAPPER: –∏—Å–ø–æ–ª—å–∑—É–µ–º surface-–∞–≥—Ä–µ–≥–∞—Ü–∏—é –∏ –æ–±—Ä–∞—Ç–Ω—ã–π –º–∞–ø–ø–µ—Ä
        output_embeddings, completed_flows = self.lattice.collect_completed_flows_direct(self.mapper, expected_batch_size=batch_size)
        
        # –û—á–∏—â–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞
        if completed_flows:
            # –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            for flow_id in completed_flows:
                if flow_id in self.lattice.active_flows:
                    del self.lattice.active_flows[flow_id]
            logger.info(f"Collected and removed {len(completed_flows)} completed flows")
        
        return output_embeddings, completed_flows
    
    def _collect_final_surface_output(self) -> Tuple[torch.Tensor, List[int]]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç surface embeddings –ë–ï–ó –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ 768D
        
        Returns:
            output_surface_embeddings: [batch, surface_dim] - surface embeddings
            completed_flows: ID –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        active_flows = self.lattice.get_active_flows()
        
        logger.debug(f"Surface collection: {len(active_flows)} active flows")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ - –ø–æ–º–µ—á–∞–µ–º –∏—Ö –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ
        active_at_output = 0
        for flow in active_flows:
            z_pos = flow.position[2].item()
            if z_pos >= self.config.lattice_depth - 1:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –≤—ã—Ö–æ–¥–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
                self.lattice._mark_flow_completed_zdepth_plane(flow.id)
                active_at_output += 1
            elif z_pos <= 0:
                # –ü–æ—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥ –Ω–∞—á–∞–ª–∞ - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –Ω–∞ Z=0
                self.lattice._mark_flow_completed_z0_plane(flow.id)
                active_at_output += 1
        
        if active_at_output > 0:
            logger.debug(f"Marked {active_at_output} remaining flows as completed")
        
        # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –°–æ–±–∏—Ä–∞–µ–º surface embeddings –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏)
        if self.lattice.tensor_storage is not None:
            output_surface_embeddings, completed_flows = self.lattice.collect_completed_flows_surface_direct_tensorized()
        else:
            output_surface_embeddings, completed_flows = self.lattice.collect_completed_flows_surface_direct()
        
        # –û—á–∏—â–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞
        if completed_flows:
            # –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
            for flow_id in completed_flows:
                if flow_id in self.lattice.active_flows:
                    del self.lattice.active_flows[flow_id]
            logger.info(f"Collected and removed {len(completed_flows)} completed flows")
        
        return output_surface_embeddings, completed_flows
    
    def step(self, active_flows: Optional[List] = None, global_training_step: Optional[int] = None):
        """
        –û–¥–∏–Ω —à–∞–≥ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        –ü–û–õ–ù–ê–Ø –ü–ê–†–ê–õ–õ–ï–õ–ò–ó–ê–¶–ò–Ø: –≤—Å–µ –ø–æ—Ç–æ–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤–º–µ—Å—Ç–æ sequential batches
        
        Args:
            active_flows: –°–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–µ—Å–ª–∏ None - –ø–æ–ª—É—á–∞–µ–º –∏–∑ lattice)
        """
        start_time = time.time()
        phase = 'tensorized' if self.lattice.tensor_storage is not None else 'list'
        
        if self.lattice.tensor_storage is not None:
            # Tensorized fast path
            positions, energies, hidden_states, flow_ids, ages, steps_taken = self.lattice.tensor_storage.get_active_data()
            if positions.numel() == 0:
                return
            flows_count = positions.shape[0]
            t0 = time.time()
            self._process_flow_batch_tensorized(
                positions,
                energies,
                hidden_states,  # [batch, layers, hidden]
                flow_ids,
                ages,
                steps_taken,
                global_training_step=global_training_step
            )
            proc_time = (time.time() - t0) * 1000.0
            if logger.isEnabledFor(10):
                logger.log(DEBUG_PERFORMANCE, f"STEP[{phase}] flows={flows_count}: process_batch_tensorized={proc_time:.1f}ms")
            # Lazy consistency logs for first few flows
            try:
                if logger.isEnabledFor(10):  # DEBUG
                    # 1) –ü–µ—Ä–≤—ã–µ K –ø–æ—Ç–æ–∫–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π/—ç–Ω–µ—Ä–≥–∏–π
                    k = min(3, flow_ids.shape[0])
                    for i in range(k):
                        fid = flow_ids[i].item()
                        pos_s = positions[i].tolist()
                        eng_s = energies[i].view(-1).item() if energies[i].numel()==1 else energies[i][0].item()
                        if fid in self.lattice.active_flows:
                            f = self.lattice.active_flows[fid]
                            pos_l = [round(v.item(), 6) for v in f.position]
                            eng_l = f.energy.item() if f.energy.numel()==1 else f.energy[0].item()
                            logger.debug(f"CONSISTENCY flow_id={fid}: pos_storage={list(map(lambda x: round(x,6), pos_s))} pos_lattice={pos_l}; energy_storage={eng_s:.6f} energy_lattice={eng_l:.6f}")
                    # 2) –°–≤–æ–¥–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏ –ø–µ—Ä–≤—ã–µ ID
                    active_lattice_ids = [fid for fid, fl in self.lattice.active_flows.items() if fl.is_active]
                    count_lattice = len(active_lattice_ids)
                    count_storage = flow_ids.shape[0]
                    sample_ids_l = active_lattice_ids[:3]
                    sample_ids_s = flow_ids[:3].detach().cpu().tolist()
                    logger.debug(f"CONSISTENCY counts: lattice_active={count_lattice}, storage_active={count_storage}; sample_lattice_ids={sample_ids_l}, sample_storage_ids={sample_ids_s}")
            except Exception as e:
                logger.debug(f"Consistency log skipped due to error: {e}")
        else:
            if active_flows is None:
                active_flows = self.lattice.get_active_flows()
            
            if not active_flows:
                return
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —É–±–∏—Ä–∞–µ–º Sequential Processing Bottleneck!
            # –í–º–µ—Å—Ç–æ —Ü–∏–∫–ª–∞ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ batch'–∞–º–∏ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –ø–æ—Ç–æ–∫–∏ —Å—Ä–∞–∑—É
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç GPU cores —Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å 1000+ –ø–æ—Ç–æ–∫–∞–º–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
            
            flows_count = len(active_flows)
            max_flows_per_step = self.config.max_active_flows  # RTX 5090 –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ä–∞–∑—É
            if flows_count <= max_flows_per_step:
                # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å–ª—É—á–∞–π: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –ø–æ—Ç–æ–∫–∏ –æ–¥–Ω–∏–º –±–æ–ª—å—à–∏–º batch'–µ–º
                t0 = time.time()
                self._process_flow_batch(active_flows, global_training_step=global_training_step)
                proc_time = (time.time() - t0) * 1000.0
                if logger.isEnabledFor(10):
                    logger.log(DEBUG_PERFORMANCE, f"STEP[{phase}] flows={flows_count}: process_flow_batch={proc_time:.1f}ms")
            else:
                # Fallback: –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤ (>200K), –¥–µ–ª–∏–º –Ω–∞ –∫—Ä—É–ø–Ω—ã–µ chunk'–∏
                optimal_chunk_size = max_flows_per_step // 2  # 100K –ø–æ—Ç–æ–∫–æ–≤ –∑–∞ —Ä–∞–∑
                
                for i in range(0, flows_count, optimal_chunk_size):
                    chunk_flows = active_flows[i:i + optimal_chunk_size]
                    t0 = time.time()
                    self._process_flow_batch(chunk_flows, global_training_step=global_training_step)
                    proc_time = (time.time() - t0) * 1000.0
                    if logger.isEnabledFor(10):
                        logger.log(DEBUG_PERFORMANCE, f"STEP[{phase}] chunk[{i}:{i+len(chunk_flows)}]: process_flow_batch={proc_time:.1f}ms")
                        logger.log(DEBUG_PERFORMANCE, f"STEP[{phase}] chunk[{i}:{i+len(chunk_flows)}]: process_flow_batch={proc_time:.1f}ms")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        step_time = time.time() - start_time
        self.perf_stats['step_times'].append(step_time)
        self.perf_stats['flows_per_step'].append(flows_count)
        
        if self.device.type == 'cuda':
            memory_info = self.device_manager.get_memory_info()
            self.perf_stats['gpu_memory_usage'].append(memory_info['gpu_allocated_gb'])
    
    def _sanitize_tensor(self, t: torch.Tensor, clip_value: float = 10.0, clamp_only: bool = False) -> torch.Tensor:
        """–ó–∞–º–µ–Ω—è–µ—Ç NaN/Inf –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ RNN."""
        if t is None:
            return t
        t = torch.nan_to_num(t, nan=0.0, posinf=clip_value, neginf=-clip_value)
        if not clamp_only:
            return t.clamp_(-clip_value, clip_value)
        return t

    def _process_flow_batch(self, flows, global_training_step: Optional[int] = None):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –ø–æ—Ç–æ–∫–æ–≤ —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏"""
        batch_size = len(flows)
        t_collect = time.time()
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–æ–≤ –∏ ID
        positions = torch.stack([f.position for f in flows])  # [batch, 3]
        energies = torch.stack([f.energy for f in flows])     # [batch, embedding_dim]
        hidden_states = torch.stack([f.hidden_state for f in flows])  # [batch, layers, hidden]

        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –∏ —ç–Ω–µ—Ä–≥–∏–∏)
        positions = self._sanitize_tensor(positions, clip_value=1.0)  # clamp –∫ [-1,1]
        energies = self._sanitize_tensor(energies, clip_value=10.0)
        flow_ids = torch.tensor([f.id for f in flows], dtype=torch.long, device=positions.device)
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º hidden states –¥–ª—è GRU –∏ –¥–µ–ª–∞–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏
        hidden_states = hidden_states.transpose(0, 1).contiguous()  # [layers, batch, hidden]
        
        collect_ms = (time.time() - t_collect) * 1000.0
        # 1. SimpleNeuron –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏ —ç–Ω–µ—Ä–≥–∏–∏
        t_neuron = time.time()
        neuron_output = self.neuron(positions, energies)  # [batch, neuron_output_dim]
        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞ –Ω–µ–π—Ä–æ–Ω–∞
        neuron_output = self._sanitize_tensor(neuron_output, clip_value=10.0)
        neuron_ms = (time.time() - t_neuron) * 1000.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–æ–∑—Ä–∞—Å—Ç–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è progressive bias
        ages = torch.tensor([flow.age for flow in flows], dtype=torch.float32, device=positions.device)
        
        # 2. EnergyCarrier –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å curriculum learning
        t_carrier = time.time()
        carrier_output, new_hidden = self.carrier(
            neuron_output, 
            energies,
            hidden_states,
            positions,
            flow_age=ages,
            global_training_step=global_training_step  # –ü–µ—Ä–µ–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥
        )
        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ carrier
        try:
            carrier_output.next_position = self._sanitize_tensor(getattr(carrier_output, 'next_position', None), clip_value=1.0)
            carrier_output.energy_value = self._sanitize_tensor(getattr(carrier_output, 'energy_value', None), clip_value=10.0)
            if hasattr(carrier_output, 'raw_next_position') and carrier_output.raw_next_position is not None:
                carrier_output.raw_next_position = self._sanitize_tensor(carrier_output.raw_next_position, clip_value=1.0)
        except Exception:
            pass
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –∏ –¥–µ–ª–∞–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏
        new_hidden = new_hidden.transpose(0, 1).contiguous()  # [batch, layers, hidden]
        carrier_ms = (time.time() - t_carrier) * 1000.0
        
        # 3. –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        t_results = time.time()
        self._process_results_vectorized(flows, flow_ids, positions, carrier_output, new_hidden)
        results_ms = (time.time() - t_results) * 1000.0
        if logger.isEnabledFor(10):
            logger.log(DEBUG_PERFORMANCE, f"BATCH timings: collect={collect_ms:.1f}ms, neuron={neuron_ms:.1f}ms, carrier={carrier_ms:.1f}ms, results={results_ms:.1f}ms (batch={batch_size})")
        # Throughput metric (flows per second) for this batch
        try:
            fps = (batch_size / ((collect_ms + neuron_ms + carrier_ms + results_ms) / 1000.0)) if (collect_ms + neuron_ms + carrier_ms + results_ms)  0 else 0.0
            logger.log(DEBUG_PERFORMANCE, f"Throughput[step_batch]: {fps:.1f} flows/s (batch={batch_size})")
        except Exception:
            pass
    
    def _process_results_vectorized(self, flows, flow_ids, current_positions, carrier_output, new_hidden):
        """–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ carrier_output —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
        batch_size = len(flows)
        device = current_positions.device
        
        # –£–î–ê–õ–ï–ù–û: energy_alive_mask - –≤ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø–æ—Ç–æ–∫–∏ –Ω–µ —É–º–∏—Ä–∞—é—Ç –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —ç–Ω–µ—Ä–≥–∏–∏
        # –í—Å–µ –ø–æ—Ç–æ–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –∂–∏–≤—ã–º–∏, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ termination_reasons
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º termination_reasons –∏–∑ EnergyCarrier
        termination_reasons = carrier_output.termination_reason
        is_terminated = carrier_output.is_terminated  # [batch]
        
        # –†–∞–∑–±–∏—Ä–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        reached_z0_count = sum(1 for reason in termination_reasons if reason == "reached_z0_plane")
        reached_zdepth_count = sum(1 for reason in termination_reasons if reason == "reached_zdepth_plane")
        reflection_needed_count = sum(1 for reason in termination_reasons if reason == "xy_reflection_needed")
        active_count = sum(1 for reason in termination_reasons if reason == "active")
        
        # –£–î–ê–õ–ï–ù–û: energy_dead_count - –ø–æ—Ç–æ–∫–∏ –±–æ–ª—å—à–µ –Ω–µ —É–º–∏—Ä–∞—é—Ç –æ—Ç —ç–Ω–µ—Ä–≥–∏–∏
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ termination_reasons
        
        logger.debug_energy(f"üéØ Termination breakdown: z0={reached_z0_count}, zdepth={reached_zdepth_count}, "
                           f"reflection={reflection_needed_count}, active={active_count}")
        
        # –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ª–æ–≥–∏–∫–∞ –Ω–∞–º–Ω–æ–≥–æ –ø—Ä–æ—â–µ - —Ç–æ–ª—å–∫–æ 3 —Ç–∏–ø–∞ –ø–æ—Ç–æ–∫–æ–≤:
        # 1. –î–æ—Å—Ç–∏–≥—à–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π (–±—É—Ñ–µ—Ä–∏–∑—É–µ–º)
        # 2. –¢—Ä–µ–±—É—é—â–∏–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è (–ø—Ä–∏–º–µ–Ω—è–µ–º –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ) 
        # 3. –ê–∫—Ç–∏–≤–Ω—ã–µ (–æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é)
        
        # –ú–∞—Å–∫–∞ –ø–æ—Ç–æ–∫–æ–≤, –¥–æ—Å—Ç–∏–≥—à–∏—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        output_reached_mask = is_terminated
        
        # –ú–∞—Å–∫–∞ –ø–æ—Ç–æ–∫–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç—Ä–∞–∂–µ–Ω–∏—è (–ø–æ raw_next_position –¥–æ clamp)
        reflection_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)
        raw_next = getattr(carrier_output, 'raw_next_position', None)
        if raw_next is not None:
            reflection_mask = (raw_next[:, 0] < -1.0) | (raw_next[:, 0] > 1.0) | (raw_next[:, 1] < -1.0) | (raw_next[:, 1] > 1.0)
        else:
            for i, reason in enumerate(termination_reasons):
                if reason == "xy_reflection_needed":
                    reflection_mask[i] = True
        
        # –ú–∞—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        active_mask = ~is_terminated
        
        # –î–í–£–•–£–†–û–í–ù–ï–í–ê–Ø –ü–†–û–ï–ö–¶–ò–û–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ç–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–¥–µ–ª–∞–ª–∏ depth/2 —à–∞–≥–æ–≤ –Ω–æ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        t_projmask = time.time()
        depth_half = self.config.lattice_depth // 2
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ projection_mask (–≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–∞)
        try:
            steps_taken_tensor = torch.tensor([f.steps_taken for f in flows], device=device)
            projection_mask = active_mask & (steps_taken_tensor >= depth_half)
        except Exception:
            # Fallback: –ø—É—Å—Ç–∞—è –º–∞—Å–∫–∞
            projection_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)
        projmask_ms = (time.time() - t_projmask) * 1000.0
        if logger.isEnabledFor(10):
            logger.log(DEBUG_PERFORMANCE, f"RESULTS: build_masks: projection_mask={projmask_ms:.1f}ms (count={int(projection_mask.sum().item())})")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏
        t_project = time.time()
        if projection_mask.any():
            projected_count = projection_mask.sum().item()
            logger.info(f"üìä Projecting {projected_count} flows to nearest output surface (completed ‚â•{depth_half} steps)")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏
            projection_flow_ids = flow_ids[projection_mask]
            projection_positions = carrier_output.next_position[projection_mask]
            projection_energies = carrier_output.energy_value[projection_mask]
            projection_hidden = new_hidden[projection_mask]
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –±–ª–∏–∂–∞–π—à—É—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å (–≤ –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–û–ú Z)
            proj_z = projection_positions[:, 2]
            dist_z0 = (proj_z - self.lattice.norm_z0).abs()
            dist_zd = (proj_z - self.lattice.norm_zdepth).abs()
            to_z0_mask = dist_z0 <= dist_zd
            to_zd_mask = ~to_z0_mask
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
            projected_positions = projection_positions.clone()
            projected_positions[to_z0_mask, 2] = self.lattice.norm_z0
            projected_positions[to_zd_mask, 2] = self.lattice.norm_zdepth
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –±–∞—Ç—á–µ–º
            self.lattice.batch_update_flows(
                projection_flow_ids,
                projected_positions,
                projection_energies,
                projection_hidden,
            )
            
            # –ü–æ–º–µ—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–º–∏ –±–∞—Ç—á–µ–º
            if to_z0_mask.any():
                self.lattice.mark_flows_completed_z0_batch(projection_flow_ids[to_z0_mask])
            if to_zd_mask.any():
                self.lattice.mark_flows_completed_zdepth_batch(projection_flow_ids[to_zd_mask])
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            active_mask = active_mask & ~projection_mask
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ —Å–º–µ—â–µ–Ω–∏—è (–ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π carrier_dropout)
        if self.config.enable_displacement_filtering:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—É—â–∏—Ö –∏ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–∑–∏—Ü–∏–π
            displacements = carrier_output.next_position - current_positions  # [batch, 3]
            displacement_lengths = torch.norm(displacements, dim=1)  # [batch]
            
            # –ú–∞—Å–∫–∞ –ø–æ—Ç–æ–∫–æ–≤ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏ ("—Ç–æ–ø—á—É—â–∏–µ—Å—è")
            small_displacement_mask = displacement_lengths < self.config.min_displacement_threshold
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if small_displacement_mask.any():
                filtered_count = small_displacement_mask.sum().item()
                logger.debug_relative(f"üîç Filtered {filtered_count}/{batch_size} flows with small displacements "
                                     f"(< {self.config.min_displacement_threshold:.2f})")
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ—Ç–æ–∫–∏ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏ –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            active_mask = active_mask & ~small_displacement_mask
        
        # 2. –ë—É—Ñ–µ—Ä–∏–∑—É–µ–º –ø–æ—Ç–æ–∫–∏, –¥–æ—Å—Ç–∏–≥—à–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π
        if output_reached_mask.any():
            output_flow_ids = flow_ids[output_reached_mask]
            output_positions = carrier_output.next_position[output_reached_mask]
            output_energies = carrier_output.energy_value[output_reached_mask]
            output_hidden = new_hidden[output_reached_mask]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ –±—É—Ñ–µ—Ä–∏–∑—É–µ–º
            for i, flow_id in enumerate(output_flow_ids):
                flow_id_item = flow_id.item()
                new_position = output_positions[i]
                new_energy = output_energies[i]
                new_hidden_state = output_hidden[i]
                
                # –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ü–æ–º–µ—á–∞–µ–º –ø–æ—Ç–æ–∫ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –±–µ–∑ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏
                z_pos = new_position[2].item()
                if z_pos <= 0:
                    self.lattice._mark_flow_completed_z0_plane(flow_id_item)
                elif z_pos >= self.config.lattice_depth:
                    self.lattice._mark_flow_completed_zdepth_plane(flow_id_item)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ç–æ–∫ –ø–µ—Ä–µ–¥ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–µ–π
                if flow_id_item in self.lattice.active_flows:
                    self.lattice.active_flows[flow_id_item].position = new_position
                    self.lattice.active_flows[flow_id_item].energy = new_energy
                    self.lattice.active_flows[flow_id_item].hidden_state = new_hidden_state
                    self.lattice.active_flows[flow_id_item].age += 1
        
        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if reflection_mask.any() and self.config.boundary_reflection_enabled:
            reflection_flow_ids = flow_ids[reflection_mask]
            reflection_count = reflection_mask.sum().item()
            
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ª–æ–≥–∏—Ä—É–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã –¥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
            reflection_positions_before = (raw_next if raw_next is not None else carrier_output.next_position)[reflection_mask]
            x_left = (reflection_positions_before[:, 0] < -1.0).sum().item()
            x_right = (reflection_positions_before[:, 0] > 1.0).sum().item()
            y_left = (reflection_positions_before[:, 1] < -1.0).sum().item()
            y_right = (reflection_positions_before[:, 1] > 1.0).sum().item()
            logger.debug_reflection(f"üîÑ BEFORE reflection: {reflection_count} flows need reflection | X_left={x_left}, X_right={x_right}, Y_left={y_left}, Y_right={y_right}")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
            for i in range(min(3, reflection_count)):
                flow_id = reflection_flow_ids[i].item()
                pos = reflection_positions_before[i]
                logger.debug_reflection(f"üîÑ Flow {flow_id} before: X={pos[0].item():.6f}, Y={pos[1].item():.6f}, Z={pos[2].item():.6f}")
            
            reflection_positions = self.reflect_boundaries(reflection_positions_before, reflection_flow_ids)
            reflection_energies = carrier_output.energy_value[reflection_mask]
            reflection_hidden = new_hidden[reflection_mask]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ç–æ–∫–∏ —Å –æ—Ç—Ä–∞–∂–µ–Ω–Ω—ã–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏
            self.lattice.batch_update_flows(
                reflection_flow_ids,
                reflection_positions,
                reflection_energies,
                reflection_hidden
            )
        
        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ—Ç–æ–∫–∏
        final_active_mask = active_mask
        if reflection_mask.any() and not self.config.boundary_reflection_enabled:
            # –ï—Å–ª–∏ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ, –ø–æ—Ç–æ–∫–∏ —Å xy_reflection_needed –æ—Å—Ç–∞—é—Ç—Å—è –∞–∫—Ç–∏–≤–Ω—ã–º–∏
            final_active_mask = active_mask | reflection_mask
        
        if final_active_mask.any():
            active_flow_ids = flow_ids[final_active_mask]
            active_positions = carrier_output.next_position[final_active_mask]
            active_energies = carrier_output.energy_value[final_active_mask]
            active_hidden = new_hidden[final_active_mask]
            
            # Batch update all active flows
            self.lattice.batch_update_flows(
                active_flow_ids,
                active_positions,
                active_energies,
                active_hidden
            )
        
        # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ spawn –ø–æ—Ç–æ–∫–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º spawn –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–º–µ—â–µ–Ω–∏—è, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.config.movement_based_spawn:
            movement_spawn_info = self._check_movement_spawn(current_positions, carrier_output.next_position, flow_ids)
            if movement_spawn_info:
                carrier_output.spawn_info.extend(movement_spawn_info)
                logger.debug_spawn_movement(f"üéÜ Added {len(movement_spawn_info)} movement-based spawns")
        
        self._process_spawns_optimized(flows, carrier_output, final_active_mask, current_positions)
    
    def _process_flow_batch_tensorized(self,
                                      positions: torch.Tensor,
                                      energies: torch.Tensor,
                                      hidden_states: torch.Tensor,
                                      flow_ids: torch.Tensor,
                                      ages: torch.Tensor,
                                      steps_taken: torch.Tensor,
                                      global_training_step: Optional[int] = None):
        """–ü–æ–ª–Ω–æ—Å—Ç—å—é —Ç–µ–Ω–∑–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ –ø–æ—Ç–æ–∫–æ–≤"""
        batch_size = positions.shape[0]
        device = positions.device
        
        # –ü—Ä–∏–≤–æ–¥–∏–º hidden_states –∫ [layers, batch, hidden]
        hidden_for_gru = hidden_states.transpose(0, 1).contiguous()
        
        # 1. SimpleNeuron
        neuron_output = self.neuron(positions, energies)
        
        # 2. EnergyCarrier
        carrier_output, new_hidden = self.carrier(
            neuron_output,
            energies,
            hidden_for_gru,
            positions,
            flow_age=ages.to(device),
            global_training_step=global_training_step
        )
        new_hidden_bt = new_hidden.transpose(0, 1).contiguous()  # [batch, layers, hidden]
        
        # Termination masks
        is_terminated = carrier_output.is_terminated
        termination_reasons = carrier_output.termination_reason
        
        # Reflection mask by raw positions if provided; fallback to reasons list
        reflection_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)
        raw_next = getattr(carrier_output, 'raw_next_position', None)
        if raw_next is not None:
            reflection_mask = (raw_next[:, 0] < -1.0) | (raw_next[:, 0] > 1.0) | (raw_next[:, 1] < -1.0) | (raw_next[:, 1] > 1.0)
        else:
            for i, reason in enumerate(termination_reasons):
                if reason == "xy_reflection_needed":
                    reflection_mask[i] = True
        
        active_mask = ~is_terminated
        
        # Projection mask using steps_taken
        projection_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)
        depth_half = self.config.lattice_depth // 2
        projection_mask = active_mask & (steps_taken.to(device) >= depth_half)
        
        # Handle projections
        if projection_mask.any():
            proj_ids = flow_ids[projection_mask]
            proj_pos = carrier_output.next_position[projection_mask]
            proj_energy = carrier_output.energy_value[projection_mask]
            proj_hidden = new_hidden_bt[projection_mask]
            # Decide nearest surface by Z distance in raw space assumption consistent with existing logic
            z_curr = proj_pos[:, 2]
            dist_z0 = (z_curr - 0).abs()
            dist_zd = (z_curr - self.config.lattice_depth).abs()
            to_z0 = dist_z0 <= dist_zd
            # For logging and marking
            ids_list = proj_ids.detach().cpu().tolist()
            z0_ids = [ids_list[i] for i, flag in enumerate(to_z0.detach().cpu().tolist()) if flag]
            zd_ids = [ids_list[i] for i, flag in enumerate((~to_z0).detach().cpu().tolist()) if flag]
            if z0_ids:
                self.lattice._mark_flow_completed_z0_plane_batch = getattr(self.lattice, "_mark_flow_completed_z0_plane", None)
                for fid in z0_ids:
                    self.lattice._mark_flow_completed_z0_plane(fid)
            if zd_ids:
                for fid in zd_ids:
                    self.lattice._mark_flow_completed_zdepth_plane(fid)
            active_mask = active_mask & (~projection_mask)
        
        # Output reached processing
        if is_terminated.any():
            out_ids = flow_ids[is_terminated]
            out_pos = carrier_output.next_position[is_terminated]
            out_energy = carrier_output.energy_value[is_terminated]
            out_hidden = new_hidden_bt[is_terminated]
            # Mark completed depending on normalized Z
            z_vals = out_pos[:, 2]
            to_z0_mask = (z_vals - self.lattice.norm_z0).abs() <= (z_vals - self.lattice.norm_zdepth).abs()
            to_zd_mask = ~to_z0_mask
            # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º Z –∫ –ø–ª–æ—Å–∫–æ—Å—Ç—è–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            out_pos[to_z0_mask, 2] = self.lattice.norm_z0
            out_pos[to_zd_mask, 2] = self.lattice.norm_zdepth
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ —Å—Ä–∞–∑—É
            self.lattice.batch_update_flows(out_ids, out_pos, out_energy, out_hidden)
            # –û—Ç–º–µ—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–º–∏ –±–∞—Ç—á–µ–º
            if to_z0_mask.any():
                self.lattice.mark_flows_completed_z0_batch(out_ids[to_z0_mask])
            if to_zd_mask.any():
                self.lattice.mark_flows_completed_zdepth_batch(out_ids[to_zd_mask])
        
        # Reflection
        if reflection_mask.any() and self.config.boundary_reflection_enabled:
            refl_ids = flow_ids[reflection_mask]
            refl_pos_before = (raw_next if raw_next is not None else carrier_output.next_position)[reflection_mask]
            refl_pos = self.reflect_boundaries(refl_pos_before, refl_ids)
            refl_energy = carrier_output.energy_value[reflection_mask]
            refl_hidden = new_hidden_bt[reflection_mask]
            self.lattice.batch_update_flows(refl_ids, refl_pos, refl_energy, refl_hidden)
        
        # Active updates
        final_active = active_mask | (reflection_mask & ~self.config.boundary_reflection_enabled)
        if final_active.any():
            act_ids = flow_ids[final_active]
            act_pos = carrier_output.next_position[final_active]
            act_energy = carrier_output.energy_value[final_active]
            act_hidden = new_hidden_bt[final_active]
            self.lattice.batch_update_flows(act_ids, act_pos, act_energy, act_hidden)
        
        # Movement-based spawns
        if self.config.movement_based_spawn:
            mv_info = self._check_movement_spawn(positions, carrier_output.next_position, flow_ids)
            if mv_info:
                # Reuse existing spawn handler that expects flows and indices; build minimal flows proxy
                class _F: pass
                flows_proxy = []
                id_to_idx = {fid.item(): i for i, fid in enumerate(flow_ids)}
                for fid in flow_ids.detach().cpu().tolist():
                    f = _F()
                    f.id = fid
                    flows_proxy.append(f)
                alive_mask = final_active
                self._process_spawns_optimized(flows_proxy, type("O", (), {"spawn_info": mv_info})(), alive_mask, positions)
        
        return
    
    def reflect_boundaries(self, position: torch.Tensor, flow_ids: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ [-1, 1]
        
        Args:
            position: [batch, 3] - –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
            flow_ids: [batch] - ID –ø–æ—Ç–æ–∫–æ–≤ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            reflected_position: [batch, 3] - –ø–æ–∑–∏—Ü–∏–∏ —Å –æ—Ç—Ä–∞–∂–µ–Ω–Ω—ã–º–∏ X/Y –≤ [-1, 1]
        """
        reflected_pos = position.clone()
        x, y, z = reflected_pos[:, 0], reflected_pos[:, 1], reflected_pos[:, 2]
        
        # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ X –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        x = torch.where(x < -1.0, -2.0 - x, x)  # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç –ª–µ–≤–æ–π –≥—Ä–∞–Ω–∏—Ü—ã -1
        x = torch.where(x > 1.0, 2.0 - x, x)    # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç –ø—Ä–∞–≤–æ–π –≥—Ä–∞–Ω–∏—Ü—ã 1
        
        # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ Y –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        y = torch.where(y < -1.0, -2.0 - y, y)
        y = torch.where(y > 1.0, 2.0 - y, y)
        
        # Z –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–¥–≤–∏–∂–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –ø–ª–æ—Å–∫–æ—Å—Ç—è–º)
        
        reflected_pos[:, 0] = x
        reflected_pos[:, 1] = y

        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
        num_reflected = position.shape[0]

        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏–π –ø–æ –æ—Å—è–º –ü–û –°–´–†–´–ú –í–•–û–î–ù–´–ú –ø–æ–∑–∏—Ü–∏—è–º
        x_reflected_left = (position[:, 0] < -1.0).sum().item()
        x_reflected_right = (position[:, 0] > 1.0).sum().item()
        y_reflected_left = (position[:, 1] < -1.0).sum().item()
        y_reflected_right = (position[:, 1] > 1.0).sum().item()

        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–≤—ã—Ö 3-—Ö –æ—Ç—Ä–∞–∂–µ–Ω–∏–π
        reflection_examples = []
        for i in range(min(3, num_reflected)):
            orig_x, orig_y, orig_z = position[i, 0].item(), position[i, 1].item(), position[i, 2].item()
            new_x, new_y = x[i].item(), y[i].item()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
            reflection_type = []
            if orig_x < -1.0:
                reflection_type.append(f"X<-1({orig_x:.3f}‚Üí{new_x:.3f})")
            elif orig_x > 1.0:
                reflection_type.append(f"X>1({orig_x:.3f}‚Üí{new_x:.3f})")
            if orig_y < -1.0:
                reflection_type.append(f"Y<-1({orig_y:.3f}‚Üí{new_y:.3f})")
            elif orig_y > 1.0:
                reflection_type.append(f"Y>1({orig_y:.3f}‚Üí{new_y:.3f})")

            if reflection_type:
                reflection_examples.append(f"flow_{i}[{','.join(reflection_type)}]")

        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.debug_reflection(
            f"üîÑ Reflected candidates: {num_reflected} | X_left={x_reflected_left}, X_right={x_reflected_right}, "
            f"Y_left={y_reflected_left}, Y_right={y_reflected_right}"
        )

        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        if reflection_examples:
            logger.debug_reflection(f"üîÑ Examples: {', '.join(reflection_examples)}")

        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        logger.debug_reflection(
            f"üîÑ Post-reflection ranges: X[{x.min().item():.3f}, {x.max().item():.3f}], "
            f"Y[{y.min().item():.3f}, {y.max().item():.3f}]"
        )

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –ø–æ—Ç–æ–∫–æ–≤
        if flow_ids is not None:
            logger.debug_reflection(f"üîÑ AFTER reflection examples:")
            for i in range(min(3, len(position))):
                flow_id = flow_ids[i].item()
                orig_pos = position[i]
                new_pos = reflected_pos[i]
                logger.debug_reflection(
                    f"üîÑ Flow {flow_id} after: X={new_pos[0].item():.6f}, Y={new_pos[1].item():.6f}, Z={new_pos[2].item():.6f} "
                    f"(ŒîX={new_pos[0].item() - orig_pos[0].item():.6f}, ŒîY={new_pos[1].item() - orig_pos[1].item():.6f})"
                )

        return reflected_pos
    
    def _check_movement_spawn(self, current_positions: torch.Tensor, 
                             next_positions: torch.Tensor, 
                             flow_ids: torch.Tensor) -> List:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç spawn –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–º–µ—â–µ–Ω–∏—è
        
        Args:
            current_positions: [batch, 3] - —Ç–µ–∫—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
            next_positions: [batch, 3] - —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
            flow_ids: [batch] - ID –ø–æ—Ç–æ–∫–æ–≤
            
        Returns:
            spawn_info: –°–ø–∏—Å–æ–∫ SpawnInfo –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        """
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–º–µ—â–µ–Ω–∏—è
        displacement = next_positions - current_positions  # [batch, 3]
        displacement_lengths = torch.norm(displacement, dim=1)  # [batch]
        
        # –ü–æ—Ä–æ–≥ –¥–ª—è spawn –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ [-1, 1]
        threshold = self.config.spawn_movement_threshold_ratio  # –ü—Ä—è–º–æ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
        
        # –ú–∞—Å–∫–∞ –¥–ª—è spawn
        spawn_mask = displacement_lengths > threshold
        
        if not spawn_mask.any():
            return []
        
        spawn_info_list = []
        total_candidates = spawn_mask.sum().item()
        total_potential_spawns = 0
        total_actual_spawns = 0
        total_limited_spawns = 0
        spawn_examples = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω —Å–æ–∑–¥–∞—Ç—å spawn
        spawn_indices = torch.where(spawn_mask)[0]
        for idx in spawn_indices:
            idx_val = idx.item()
            delta_length = displacement_lengths[idx].item()
            flow_id = flow_ids[idx].item()
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞)
            potential_spawns = int(delta_length / threshold) - 1
            actual_spawns = min(potential_spawns, self.config.max_spawn_per_step)
            
            total_potential_spawns += potential_spawns
            
            if actual_spawns > 0:
                # –ü–æ–ª—É—á–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞
                if flow_id in self.lattice.active_flows:
                    parent_energy = self.lattice.active_flows[flow_id].energy
                    spawn_energies = [parent_energy.clone() for _ in range(actual_spawns)]
                    
                    # –°–æ–∑–¥–∞–µ–º SpawnInfo —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞)
                    from .energy_carrier import SpawnInfo
                    spawn_info = SpawnInfo(
                        energies=spawn_energies,
                        parent_batch_idx=idx_val  # –ò–Ω–¥–µ–∫—Å –≤ –±–∞—Ç—á–µ
                    )
                    spawn_info_list.append(spawn_info)
                    
                    total_actual_spawns += actual_spawns
                    if potential_spawns > actual_spawns:
                        total_limited_spawns += (potential_spawns - actual_spawns)
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3-—Ö spawn'–æ–≤
                    if len(spawn_examples) < 3:
                        spawn_examples.append(f"flow_{flow_id}[disp={delta_length:.3f}‚Üí{actual_spawns}spawns]")
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if total_candidates > 0:
            logger.debug_spawn_movement(f"üéÜ Movement spawn summary: {total_candidates} candidates, "
                                       f"{total_potential_spawns} potential ‚Üí {total_actual_spawns} actual spawns")
            if total_limited_spawns > 0:
                logger.debug_spawn_movement(f"üéÜ Limited by config: {total_limited_spawns} spawns restricted by max_spawn_per_step={self.config.max_spawn_per_step}")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            if spawn_examples:
                logger.debug_spawn_movement(f"üéÜ Examples: {', '.join(spawn_examples)}")
        
        return spawn_info_list
    
    def _process_spawns_optimized(self, flows, carrier_output, alive_mask, current_positions):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ spawn –ø–æ—Ç–æ–∫–æ–≤"""
        if not carrier_output.spawn_info:
            return
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å spawn_info –ø–æ parent_batch_idx –¥–ª—è O(1) –ø–æ–∏—Å–∫–∞
        spawn_by_idx = {}
        for spawn_info in carrier_output.spawn_info:
            spawn_by_idx[spawn_info.parent_batch_idx] = spawn_info
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ spawn'–æ–≤
        total_spawn_requests = len(spawn_by_idx)
        total_spawned = 0
        spawn_examples = []
        parent_flows = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º spawn'—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –∂–∏–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        alive_indices = torch.where(alive_mask)[0] if alive_mask.any() else torch.tensor([], dtype=torch.long)
        for idx in alive_indices:
            idx_val = idx.item()
            if idx_val in spawn_by_idx:
                spawn_info = spawn_by_idx[idx_val]
                if spawn_info.energies:
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ spawn'–æ–≤ –∫–æ–Ω—Ñ–∏–≥–æ–º
                    spawn_energies = spawn_info.energies[:self.config.max_spawn_per_step]
                    flow_id = flows[idx_val].id
                    # –°–ø–∞–≤–Ω–∏–º –Ω–æ–≤—ã–µ –ø–æ—Ç–æ–∫–∏ –í –¢–û–ô –ñ–ï –¢–û–ß–ö–ï, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏–ª—Å—è —Ä–æ–¥–∏—Ç–µ–ª—å –î–û –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è
                    start_pos = current_positions[idx_val]
                    new_flow_ids = self.lattice.spawn_flows(flow_id, spawn_energies, start_position=start_pos)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
                    self.total_flows_created += len(spawn_energies)
                    total_spawned += len(spawn_energies)
                    
                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3-—Ö spawn'–æ–≤
                    if len(spawn_examples) < 3:
                        spawn_examples.append(f"parent_{flow_id}‚Üí{len(spawn_energies)}flows")
                    parent_flows.append(flow_id)
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if total_spawn_requests > 0:
            logger.debug_spawn(f"üéÜ Spawn summary: {total_spawn_requests} requests ‚Üí {total_spawned} new flows created")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            if spawn_examples:
                logger.debug_spawn(f"üéÜ Examples: {', '.join(spawn_examples)}")
            
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ spawn'—ã –≤ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ
            if len(parent_flows) > 3:
                other_parents = parent_flows[3:]
                logger.debug_spawn(f"üéÜ Additional parents: {len(other_parents)} flows (ids: {other_parents[:5]}{'...' if len(other_parents) > 5 else ''})")
    
    def cleanup_memory_safe(self):
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        –û—á–∏—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –∏ GPU –∫—ç—à –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: —Ä–µ–∂–µ, –±–∞—Ç—á–µ–≤–æ, —É—Å–ª–æ–≤–Ω–æ.
        """
        self.step_counter += 1
        
        # –ú–µ–Ω–µ–µ —á–∞—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞
        if self.step_counter % (self.memory_cleanup_interval * 2) != 0:
            return
        
        # 1. –ë–∞—Ç—á-—É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        completed_ids = [fid for fid, flow in self.lattice.active_flows.items() if not flow.is_active]
        if completed_ids:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pop —Å default, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å KeyError –ø—Ä–∏ –≥–æ–Ω–∫–∞—Ö
            for fid in completed_ids:
                self.lattice.active_flows.pop(fid, None)
            logger.debug(f"üßπ Cleaned {len(completed_ids)} completed flows")
        
        # 2. –£—Å–ª–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –ø–∞–º—è—Ç—å—é GPU —Ä–µ–∂–µ –∏ –ø—Ä–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–º –ø–æ—Ä–æ–≥–µ
        if self.device.type == 'cuda':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ, –∞ –µ—â–µ —Ä–µ–∂–µ
            if self.step_counter % (self.memory_cleanup_interval * 4) == 0:
                mem_allocated = torch.cuda.memory_allocated() / 1e9  # GB
                mem_reserved = torch.cuda.memory_reserved() / 1e9    # GB
                
                if mem_allocated > self.memory_threshold_gb * 1.5:
                    torch.cuda.empty_cache()
                    try:
                        torch.cuda.reset_peak_memory_stats()
                    except Exception:
                        pass
                    mem_allocated_after = torch.cuda.memory_allocated() / 1e9
                    mem_freed = mem_allocated - mem_allocated_after
                    if mem_freed > 0.1:
                        logger.info(f"üßπ GPU memory cleanup: freed {mem_freed:.2f}GB (alloc {mem_allocated:.2f}‚Üí{mem_allocated_after:.2f}GB, reserved {mem_reserved:.2f}GB)")
                else:
                    logger.debug(f"üíæ Memory OK: allocated={mem_allocated:.2f}GB < threshold={self.memory_threshold_gb*1.5:.2f}GB")
        
    def _check_convergence(self, step: int, initial_flows_count: int) -> bool:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ —Å–æ —Å–∫–æ–ª—å–∑—è—â–∏–º –æ–∫–Ω–æ–º
        
        Args:
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥
            initial_flows_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            
        Returns:
            True –µ—Å–ª–∏ —Å–ª–µ–¥—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
        """
        if not self.config.convergence_enabled:
            return False
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        if step < self.config.convergence_min_steps:
            return False
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = self.lattice.get_statistics()
        completed_count = stats['total_completed']
        active_count = stats['current_active']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.convergence_stats['completed_count_history'].append(completed_count)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ (—É—á–∏—Ç—ã–≤–∞—è –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏, –≤–∫–ª—é—á–∞—è spawn'—ã)
        completion_rate = completed_count / self.total_flows_created if self.total_flows_created > 0 else 0
        
        # –£–õ–£–ß–®–ï–ù–ò–ï 1: –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        window_size = self.convergence_stats.get('moving_average_window', 5)
        if len(self.convergence_stats['completed_count_history']) >= window_size:
            recent_counts = self.convergence_stats['completed_count_history'][-window_size:]
            moving_avg = sum(recent_counts) / window_size
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
            last_moving_avg = self.convergence_stats.get('last_moving_avg', 0.0)
            improvement_threshold = self.convergence_stats.get('improvement_threshold', 0.01)
            if last_moving_avg > 0:
                improvement = (moving_avg - last_moving_avg) / last_moving_avg
                if improvement < improvement_threshold:
                    self.convergence_stats['no_improvement_steps'] += 1
                else:
                    self.convergence_stats['no_improvement_steps'] = 0
            
            self.convergence_stats['last_moving_avg'] = moving_avg
            
            logger.log(20, f"Convergence check step {step}: {completed_count}/{self.total_flows_created} completed "
                          f"(rate={completion_rate:.2f}, moving_avg={moving_avg:.1f}, active={active_count})")
        else:
            logger.log(20, f"Convergence check step {step}: {completed_count}/{self.total_flows_created} completed "
                          f"(rate={completion_rate:.2f}, active={active_count}) - building history")
        
        # –£—Å–ª–æ–≤–∏–µ 1: –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ø–æ—Ä–æ–≥ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        if completion_rate >= self.config.convergence_threshold:
            logger.log(20, f"‚úÖ Convergence threshold reached: {completion_rate:.2f} >= {self.config.convergence_threshold:.2f}")
            return True
        
        # –£–õ–£–ß–®–ï–ù–ò–ï 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–≥–Ω–∞—Ü–∏—é - –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –∏ –º–∞–ª–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö
        if active_count == 0 and completion_rate < 0.5:
            logger.log(20, f"‚ö†Ô∏è Stagnation detected: no active flows, only {completion_rate:.2f} completion rate")
            return True
        
        # –£—Å–ª–æ–≤–∏–µ 2: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π Patience –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
        if completed_count > self.convergence_stats['best_completed_count']:
            self.convergence_stats['best_completed_count'] = completed_count
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —É–ª—É—á—à–µ–Ω–∏–∏
            if self.convergence_stats['best_completed_count'] - completed_count > 1:
                self.convergence_stats['no_improvement_steps'] = 0
        
        if self.convergence_stats['no_improvement_steps'] >= self.config.convergence_patience:
            logger.log(20, f"‚è∏Ô∏è Convergence patience exceeded: {self.convergence_stats['no_improvement_steps']} "
                          f">= {self.config.convergence_patience} steps without improvement")
            return True
        
        # –£–õ–£–ß–®–ï–ù–ò–ï 3: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if step > 10 and completion_rate > 0.8:
            # –ï—Å–ª–∏ —É–∂–µ –¥–æ—Å—Ç–∏–≥–ª–∏ 80% –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–º–µ–¥–ª–∏–ª—Å—è
            if self.convergence_stats['no_improvement_steps'] > self.config.convergence_patience // 2:
                logger.log(20, f"üéØ High efficiency early stop: {completion_rate:.2f} completion with slowing progress")
                return True
        
        return False
    
    def get_performance_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.perf_stats['step_times']:
            return {}
        
        import numpy as np
        
        stats = {
            'avg_step_time': np.mean(self.perf_stats['step_times']),
            'max_step_time': np.max(self.perf_stats['step_times']),
            'avg_flows_per_step': np.mean(self.perf_stats['flows_per_step']),
            'max_flows_per_step': np.max(self.perf_stats['flows_per_step']),
            'avg_gpu_memory_gb': np.mean(self.perf_stats['gpu_memory_usage']) if self.perf_stats['gpu_memory_usage'] else 0,
            'lattice_stats': self.lattice.get_statistics()
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        if self.config.convergence_enabled and self.convergence_stats['completed_count_history']:
            stats['convergence_stats'] = {
                'best_completion_count': self.convergence_stats['best_completed_count'],
                'final_completion_count': self.convergence_stats['completed_count_history'][-1] if self.convergence_stats['completed_count_history'] else 0,
                'completion_trend': len(self.convergence_stats['completed_count_history'])
            }
        
        return stats
    
    def visualize_flow_state(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤"""
        active_flows = self.lattice.get_active_flows()
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ —ç–Ω–µ—Ä–≥–∏–∏
        positions = []
        energies = []
        ages = []
        
        for flow in active_flows:
            positions.append(flow.position.cpu().numpy())
            embedding_magnitude = torch.norm(flow.energy).item()
            energies.append(embedding_magnitude)
            ages.append(flow.age)
        
        return {
            'positions': positions,
            'energies': energies,
            'ages': ages,
            'total_flows': len(active_flows),
            'lattice_dims': (self.config.lattice_width, 
                           self.config.lattice_height,
                           self.config.lattice_depth)
        }


def create_flow_processor(config=None) -> FlowProcessor:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è FlowProcessor"""
    return FlowProcessor(config)