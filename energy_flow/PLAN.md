# Energy Flow Architecture - Подробный план реализации

## Концепция

Энергетическая архитектура на основе 3D решетки, где энергия (представленная RNN-моделями) течет через простые нейроны-автоматы. Ключевая идея - параллельная обработка независимых энергетических потоков вместо последовательной обработки клеток.

## Архитектура системы

### Структура проекта

```
energy_flow/
├── config/
│   ├── __init__.py
│   ├── energy_config.py       # Конфигурация энергетической системы
│   └── base_config.py         # Базовая конфигурация (адаптированная из new_rebuild)
├── core/
│   ├── __init__.py
│   ├── energy_carrier.py      # RNN-based энергетические потоки
│   ├── simple_neuron.py       # Простой нейрон-автомат
│   ├── energy_lattice.py      # 3D решетка для потоков
│   └── flow_processor.py      # Механизм распространения энергии
├── training/
│   ├── __init__.py
│   └── energy_trainer.py      # Тренировочный цикл адаптируем из new_rebuild\core\training\embedding_trainer.py
├── utils/
│   ├── __init__.py
│   ├── logging.py            # (копируем из new_rebuild)
│   ├── device_manager.py     # (адаптируем из new_rebuild)
│   └── helpers.py            # Вспомогательные функции
└── examples/
    └── simple_training.py     # Пример использования
```

координаты и часть входного эмбединга передаются в SimpleNeuron - он на основе этого и своих весов высчитывает некоторое значение, которое мы передаем RNN и так же передаем значение входного эмбединга для этого шага - она так же на основании своих весов высчитывает определенное значение - это значение должно содержать изменение сигнала (или энергии или части эмбединга), который мы изначально запустили от обучающей модели, можно называть по разному для лучшего понимания, так же должен содержать координаты для следующего нейрона(с которым будет взаимодействовать эта RNN) и набор возможных новых RNN, которые пойдут из этого нейрона с возможностью ограничения их количества через конфиг. получается, что вывод должен быть структурированный, а размера выхода SimpleNeuron, должен соответствовать входу RNN. так же нам нужно уделить внимание и проверить размерности нейронок, что бы они сочетались по входным и выходным параметрам

## Компоненты системы

### 1. EnergyCarrier (energy_carrier.py)

**Назначение**: Представление энергии в виде GRU-модели

**Характеристики**:

- ~10M параметров
- GRU с hidden_size=1024, num_layers=3
- Входной эмбеддинг: часть от входного эмбединга обучающей модели можно посмотреть как тут реализовано new_rebuild\core\training\embedding_lattice_mapper.py, но нам не нужно состояние всех клеток, а мы работаем только с входной и выходной стороной
- Внутреннее состояние: скрытое состояние GRU
- Выход: структурированный вывод через projection head
- Общие веса для всех GRU в решетке

**Структура вывода EnergyCarrier**:

```python
{
    'energy_value': torch.Tensor,      # Текущая энергия/эмбеддинг
    'next_position': torch.Tensor,     # Координаты следующей клетки (3D)
    'spawn_energies': List[torch.Tensor],  # Энергии для новых потоков
    'spawn_count': int                 # Количество новых потоков
}
```

**Ключевые методы**:

- `__init__(hidden_size=1024, num_layers=3)`
- `forward(neuron_output, hidden_state)` → structured_output, new_hidden_state
- `can_spawn(energy_level)` → bool
- `spawn(parent_energy)` → новый EnergyCarrier с частью энергии

### 2. SimpleNeuron (simple_neuron.py)

**Назначение**: Простой нейрон-автомат в каждой клетке решетки

**Характеристики**:

- ~1000 параметров
- Архитектура: координаты (3D) + энергия части входного эмбединга → 64 скрытых → выход для RNN
- Общие веса для всех нейронов в решетке
- Выходной размер должен соответствовать входу GRU

**Входные данные**:

- Координаты клетки (x, y, z) - 3 значения

**Ключевые методы**:

- `__init__(coord_dim=3, extra_features=0, hidden_dim=64, output_dim=128)`
- `forward(position, extra_features=None)` → neuron_output (для передачи в GRU)
- `compute_features(position, lattice_state)` → extra_features

### 3. EnergyLattice (energy_lattice.py)

**Назначение**: 3D решетка для управления потоками

**Характеристики**:

- Размеры: width × height × depth
- Хранит активные потоки и их позиции
- Управляет бюджетом потоков

**Размеры по режимам**:

- DEBUG: 20×20×10 (4,000 клеток)
- EXPERIMENT: 50×50×20 (50,000 клеток)
- OPTIMIZED: 100×100×50 (500,000 клеток)

**Ключевые методы**:

- `__init__(width, height, depth, max_flows=1000)`
- `place_initial_energy(embeddings)` → размещение на входной стороне
- `get_active_flows()` → список активных потоков
- `collect_output_energy()` → сбор энергии с выходной стороны
- тут нужно будет подумать о том, как отливливать координаты за пределами выходной плоскости куба - например, если координата больше - то мы можем атоматически присваивать ближайшей выходной точке на выходной стороне, что бы потом формировать общий выходной эмбединг.

### 4. FlowProcessor (flow_processor.py)

**Назначение**: Механизм распространения энергии

**Ключевые особенности**:

- Энергия может двигаться только вперед (по оси Z)
- Может прыгать на любую клетку впереди
- При высокой энергии может создавать новые потоки
- Параллельная обработка всех потоков

**Ключевые методы**:

- `__init__(lattice, neuron, device_manager)`
- `step()` → один шаг распространения всех потоков
- `propagate_flow(flow, position)` → новая позиция или None (затухание)

## Конфигурация системы

### EnergyConfig (energy_config.py)

```python
@dataclass
class EnergyConfig:
    # Размеры решетки
    lattice_width: int
    lattice_height: int
    lattice_depth: int

    # Параметры энергии
    max_active_flows: int = 1000
    energy_threshold: float = 0.1  # Минимальная энергия для продолжения
    spawn_threshold: float = 0.8   # Порог для создания новых потоков
    max_spawn_per_step: int = 10   # Максимум новых потоков за шаг

    # Параметры моделей
    carrier_hidden_size: int = 1024
    carrier_num_layers: int = 3
    neuron_hidden_dim: int = 64

    # Обучение
    learning_rate: float = 1e-4
    batch_size: int = 32
```

### Режимы работы

```python
DEBUG_CONFIG = EnergyConfig(
    lattice_width=20, lattice_height=20, lattice_depth=10,
    max_active_flows=100, batch_size=8
)

EXPERIMENT_CONFIG = EnergyConfig(
    lattice_width=50, lattice_height=50, lattice_depth=20,
    max_active_flows=500, batch_size=16
)

OPTIMIZED_CONFIG = EnergyConfig(
    lattice_width=100, lattice_height=100, lattice_depth=50,
    max_active_flows=1000, batch_size=32
)
```

## Механизм взаимодействия (детальный)

### Пошаговое взаимодействие для одной клетки:

1. **SimpleNeuron получает координаты**:

   ```python
   position = (x, y, z)
   neuron_output = simple_neuron(position)  # Размер: 128
   ```

2. **EnergyCarrier обрабатывает выход нейрона**:

   ```python
   structured_output, new_hidden = energy_carrier(
       neuron_output,
       current_hidden_state
   )
   ```

3. **Структурированный вывод содержит**:

   - `energy_value`: текущая энергия/эмбеддинг (768D - это допустим входной эмбединг от обучающей модели - мы его равномерно распределяем по неронам входной стороны куба, тогда у нас от каждого нейрона отходит GRU со своей энергией. для уточнения смотри new_rebuild\core\training\embedding_lattice_mapper.py - но нам не нужно состояние всех клеток, а мы работаем только с входной и выходной стороной
   - `next_position`: координаты следующей клетки (должны быть впереди)
   - `spawn_energies`: энергии для новых потоков (если есть)
   - `spawn_count`: количество новых потоков (ограничено конфигом)

4. **Создание новых потоков** (если spawn_count > 0):
   - Каждый новый поток получает часть энергии родителя
   - Новые потоки начинают с текущей позиции
   - У каждого свое скрытое состояние GRU

## Механизм работы

### 1. Инициализация

- Создаем решетку заданного размера
- Инициализируем общий SimpleNeuron
- Подготавливаем пул для EnergyCarrier

### 2. Прямой проход

1. Размещаем входные эмбеддинги на входной стороне куба (z=0) new_rebuild\core\training\embedding_lattice_mapper.py - для примера, но нам не нужно состояние всех клеток, а мы работаем только с входной и выходной стороной
2. Для каждого эмбеддинга создаем EnergyCarrier
3. На каждом шаге:
   - Все активные потоки обрабатываются параллельно
   - SimpleNeuron преобразует координаты и текущий входной эмбединг на текущем шаге в features
   - EnergyCarrier принимает features и текущий входной эмбединг и выдает структурированный вывод
   - Проверяем валидность следующей позиции (только вперед по Z)
   - Создаем новые потоки если необходимо (с учетом бюджета)
   - Обновляем позиции и состояния всех потоков
4. Собираем энергию с выходной стороны (z=depth-1). если координаты выходят за предели размеров куба - можно присваивать ближайшим координатам выходной стороны. если несколько значений - мжно делать умное усреднение.

### 3. Обучение

- Сравниваем выходные эмбеддинги с целевыми
- Вычисляем loss (MSE или cosine similarity)
- Обратное распространение через:
  - Веса всех активных EnergyCarrier (GRU)
  - Веса общего SimpleNeuron
- Оптимизация через Adam
- для примера можно посмотреть new_rebuild\core\training\embedding_trainer.py. так же там реализовано преобразование выходного эмбединга в текст, что бы дополнительно проверять, насколько продвинулось обучение

## Особенности реализации

### Параллелизм

- Все EnergyCarrier обрабатываются параллельно в батчах
- Используем torch.nn.parallel для эффективной обработки
- Векторизованные операции где возможно

### Управление памятью

- Пул предаллоцированных EnergyCarrier
- Переиспользование неактивных потоков
- Автоматическая очистка по порогу энергии

### Эмерджентность

- Минимум жестко заданных правил
- Путь энергии определяется обучением
- Естественное формирование "каналов" через решетку

## Интеграция с существующими компонентами

### Из new_rebuild переиспользуем:

1. **Система логирования** (utils/logging.py)

   - Custom debug levels
   - Контекстное логирование

2. **DeviceManager** (адаптированный)

   - Управление GPU/CPU
   - Мониторинг памяти

3. **Базовые хелперы**
   - Position3D для навигации
   - Batch processing утилиты

## Примерный API использования

```python
from energy_flow.config import create_experiment_config
from energy_flow.core import EnergyLattice, SimpleNeuron, FlowProcessor
from energy_flow.training import EnergyTrainer

# Конфигурация
config = create_experiment_config()

# Создание компонентов
lattice = EnergyLattice(config)
neuron = SimpleNeuron(config)
processor = FlowProcessor(lattice, neuron, config)

# Обучение
trainer = EnergyTrainer(processor, config)
trainer.train(input_embeddings, target_embeddings, epochs=100)
```

1.  Работаем только с входной и выходной сторонами куба (не со всеми клетками)
2.  SimpleNeuron получает координаты И часть входного эмбеддинга
3.  GRU получает выход SimpleNeuron И также часть входного эмбеддинга
4.  Размерности должны быть согласованы между компонентами

## Метрики и мониторинг

### Ключевые метрики:

- Количество активных потоков
- Средняя энергия потоков
- Процент достигших выхода
- Loss (MSE/cosine similarity)
- Утилизация GPU памяти

### Логирование:

- DEBUG_ENERGY: детали распространения энергии
- DEBUG_SPAWN: создание новых потоков
- DEBUG_CONVERGENCE: статистика достижения выхода

## Этапы реализации

1. **Базовая инфраструктура** (config, logging, device)
2. **SimpleNeuron** - простейший компонент
3. **EnergyCarrier** - ядро системы
4. **EnergyLattice** - управление пространством
5. **FlowProcessor** - механизм распространения
6. **EnergyTrainer** - обучение
7. **Простой пример** - проверка работоспособности

## Оптимизации (на будущее)

1. **Batch processing**: группировка потоков по позициям
2. **Sparse operations**: только активные клетки
3. **Multi-GPU**: распределение решетки по устройствам. вообще у нас пока что одна gpu 5090 32gb, так что для исследования мы оптимизируем под этот вариант
4. **Checkpoint/restore**: сохранение состояния обучения
5. **Адаптивный бюджет**: динамическое управление max_flows
