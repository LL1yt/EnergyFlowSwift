"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
===========================================

–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤.
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π, –ø–æ—Ä–æ–≥–∏ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Ç.–¥.
"""

from dataclasses import dataclass, field
from typing import Optional, Dict, Any
import torch

# –ò–º–ø–æ—Ä—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (delayed –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤)
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from ..utils.normalization import NormalizationManager

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º GPU –∫–∞–∫ default device –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
if torch.cuda.is_available():
    torch.set_default_device('cuda')
    torch.set_default_dtype(torch.float32)
    print(f"üöÄ Energy Flow: Default device set to CUDA ({torch.cuda.get_device_name()})")
else:
    print("‚ö†Ô∏è Energy Flow: CUDA not available, using CPU")


@dataclass
class EnergyConfig:
    """–û—Å–Ω–æ–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    
    # –†–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏
    lattice_width: int
    lattice_height: int 
    lattice_depth: int
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–Ω–µ—Ä–≥–∏–∏ (–¥–ª—è —Å–∫–∞–ª—è—Ä–Ω–æ–π —ç–Ω–µ—Ä–≥–∏–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1])
    max_active_flows: int = 1000
    energy_threshold: float = 0.05  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è (–Ω–∏–∑–∫–∏–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
    spawn_threshold: float = 0.6    # –ü–æ—Ä–æ–≥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (—É–º–µ—Ä–µ–Ω–Ω—ã–π)
    max_spawn_per_step: int = 3     # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ spawn'–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π
    # GRU (EnergyCarrier)
    carrier_hidden_size: int = 1024
    carrier_num_layers: int = 3
    carrier_dropout: float = 0.1
    
    # SimpleNeuron
    neuron_hidden_dim: int = 32
    neuron_output_dim: int = 64  # –î–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤—Ö–æ–¥–æ–º GRU
    
    # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    input_embedding_dim_from_teacher: int = 768  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ—Ç language models
    output_embedding_dim_to_teacher: int = input_embedding_dim_from_teacher # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    # embedding_per_cell –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ embedding_mapper –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ä–µ—à–µ—Ç–∫–∏
    
    # –û–±—É—á–µ–Ω–∏–µ
    learning_rate: float = 1e-4
    batch_size: int = 32
    gradient_clip: float = 1.0
    weight_decay: float = 1e-5
    
    # Device settings
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    dtype: torch.dtype = torch.float32
    
    # Logging
    log_interval: int = 10
    checkpoint_interval: int = 100
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (experimental features)
    initial_z_bias: float = 1.0  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π bias –¥–ª—è Z –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–ø–æ–º–æ—â—å –Ω–µ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏)
    use_forward_movement_bias: bool = True  # –í–∫–ª—é—á–∞—Ç—å –ª–∏ bias –¥–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –≤–ø–µ—Ä–µ–¥
    progressive_z_multiplier: float = 2.1  # –ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ bias'–∞
    exploration_noise: float = 0.5  # –°–ª—É—á–∞–π–Ω—ã–π —à—É–º –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—É—Ç–µ–π
    use_exploration_noise: bool = True  # –í–∫–ª—é—á–∞—Ç—å –ª–∏ exploration noise
    
    def __post_init__(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
        assert self.lattice_width > 0, "lattice_width –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å > 0"
        assert self.lattice_height > 0, "lattice_height –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å > 0"
        assert self.lattice_depth > 0, "lattice_depth –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å > 0"
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–µ—Ç–æ–∫ –Ω–∞ –≤—Ö–æ–¥–Ω–æ–π/–≤—ã—Ö–æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ
        self.input_cells = self.lattice_width * self.lattice_height
        self.output_cells = self.lattice_width * self.lattice_height
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –≤ embedding_mapper –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —ç–Ω–µ—Ä–≥–∏–∏
        assert 0 < self.energy_threshold < 1, "energy_threshold –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ (0, 1)"
        assert 0 < self.spawn_threshold <= 1, "spawn_threshold –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ (0, 1]"
        assert self.max_spawn_per_step > 0, "max_spawn_per_step –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π
        assert self.carrier_hidden_size > 0, "carrier_hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0"
        assert self.carrier_num_layers > 0, "carrier_num_layers –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0"
        assert self.neuron_hidden_dim > 0, "neuron_hidden_dim –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0"
        assert self.neuron_output_dim > 0, "neuron_output_dim –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0"
        
        # –°–æ–∑–¥–∞–µ–º NormalizationManager
        self._normalization_manager = None  # Lazy initialization
    
    @property
    def total_cells(self) -> int:
        """–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–µ—Ç–æ–∫ –≤ —Ä–µ—à–µ—Ç–∫–µ"""
        return self.lattice_width * self.lattice_height * self.lattice_depth
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        return {
            k: v for k, v in self.__dict__.items() 
            if not k.startswith('_') and not callable(v)
        }
    
    @property
    def normalization_manager(self) -> 'NormalizationManager':
        """–ü–æ–ª—É—á–µ–Ω–∏–µ NormalizationManager (lazy initialization)"""
        if self._normalization_manager is None:
            from ..utils.normalization import create_normalization_manager
            self._normalization_manager = create_normalization_manager(
                self.lattice_width, self.lattice_height, self.lattice_depth
            )
        return self._normalization_manager


# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤

def create_debug_config() -> EnergyConfig:
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    return EnergyConfig(
        lattice_width=20,
        lattice_height=20,
        lattice_depth=10,
        max_active_flows=1000,
        energy_threshold=0.01,  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (—Å–∫–∞–ª—è—Ä–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è)
        spawn_threshold=0.7,    # –ù–µ–º–Ω–æ–≥–æ –≤—ã—à–µ –±–∞–∑–æ–≤–æ–≥–æ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è spawn'–æ–≤
        max_spawn_per_step=2,   # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π spawn –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        batch_size=8,
        carrier_hidden_size=256,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        carrier_num_layers=2,
        carrier_dropout=0.05,   # –ù–∏–∑–∫–∏–π dropout –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        log_interval=1
    )


def create_experiment_config() -> EnergyConfig:
    """–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    return EnergyConfig(
        lattice_width=50,
        lattice_height=50,
        lattice_depth=20,
        max_active_flows=500,
        batch_size=16,
        carrier_hidden_size=512,
        carrier_num_layers=2
    )


def create_optimized_config() -> EnergyConfig:
    """–ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è RTX 5090"""
    return EnergyConfig(
        lattice_width=100,
        lattice_height=100,
        lattice_depth=50,
        max_active_flows=1000,
        batch_size=32,
        carrier_hidden_size=1024,
        carrier_num_layers=3
    )


# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
_global_config: Optional[EnergyConfig] = None


def set_energy_config(config: EnergyConfig):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    global _global_config
    _global_config = config


def get_energy_config() -> EnergyConfig:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    if _global_config is None:
        raise RuntimeError("Energy config not set. Call set_energy_config() first.")
    return _global_config