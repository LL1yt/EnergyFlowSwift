#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π Meta-LLaMA-3-8B –º–æ–¥–µ–ª–∏
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
sys.path.append(str(Path(__file__).parent))

from data.embedding_loader.format_handlers import SUPPORTED_LLM_MODELS, create_llm_handler

def test_llama_model():
    """–¢–µ—Å—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π LLaMA –º–æ–¥–µ–ª–∏"""
    
    print("ü¶ô Testing Local LLaMA-3-8B Model Availability")
    print("=" * 50)
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏
    print("üìã Supported LLM Models:")
    for key, path in SUPPORTED_LLM_MODELS.items():
        print(f"   {key}: {path}")
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    local_path = r"C:\Users\n0n4a\Meta-Llama-3-8B"
    print(f"\nüìÅ Checking local path: {local_path}")
    
    if os.path.exists(local_path):
        print("‚úÖ Local model directory exists")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        contents = list(os.listdir(local_path))
        print(f"üìÇ Directory contents ({len(contents)} items):")
        for item in contents[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            print(f"   - {item}")
        if len(contents) > 10:
            print(f"   ... and {len(contents) - 10} more items")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏
        key_files = ['config.json', 'tokenizer.json', 'pytorch_model.bin', 'model.safetensors']
        for key_file in key_files:
            if key_file in contents:
                print(f"‚úÖ Found {key_file}")
            else:
                print(f"‚ùå Missing {key_file}")
    else:
        print("‚ùå Local model directory does not exist")
        return False
    
    # 3. –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å LLM handler
    print(f"\nüß† Testing LLM Handler creation:")
    
    try:
        print("   Creating handler for 'llama3-8b-local'...")
        handler = create_llm_handler("llama3-8b-local")
        print("‚úÖ Handler created successfully")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏)
        print(f"   Model name: {handler.model_name}")
        print(f"   Device: {handler._device}")
        
        # –ü—Ä–æ–±—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–∏–Ω–≥
        print("   Generating test embedding...")
        test_embedding = handler.generate_embeddings(["Hello world test"])
        print(f"‚úÖ Test embedding generated: {test_embedding.shape}")
        print(f"   Dtype: {test_embedding.dtype}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error creating LLM handler: {e}")
        print(f"   Error type: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_llama_model()
    
    if success:
        print("\nüéâ Local LLaMA-3-8B model is accessible!")
    else:
        print("\nüîß Local LLaMA-3-8B model needs configuration fixes") 