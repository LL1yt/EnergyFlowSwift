"""
üéØ QUALITY OPTIMIZER - Stage 2.3 Production Integration
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è GenerativeDecoder

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (BLEU, ROUGE, BERTScore)
- Fine-tuning —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–ª—è RET v2.1
- Production-ready quality monitoring
"""

import torch
import torch.nn as nn
import numpy as np
import time
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from pathlib import Path
import json
import math

# –í–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from rouge_score import rouge_scorer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    logging.warning("NLTK/ROUGE –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMER_AVAILABLE = False
    logging.warning("SentenceTransformer –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. BERTScore –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

logger = logging.getLogger(__name__)


@dataclass
class QualityMetrics:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    bleu_score: float = 0.0
    rouge_1: float = 0.0
    rouge_2: float = 0.0
    rouge_l: float = 0.0
    bert_score: float = 0.0
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    semantic_similarity: float = 0.0
    coherence_score: float = 0.0
    fluency_score: float = 0.0
    diversity_score: float = 0.0
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    generation_time: float = 0.0
    tokens_per_second: float = 0.0
    
    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    overall_quality: float = 0.0
    production_readiness: float = 0.0
    
    def to_dict(self) -> Dict[str, float]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return {
            'bleu_score': self.bleu_score,
            'rouge_1': self.rouge_1,
            'rouge_2': self.rouge_2,
            'rouge_l': self.rouge_l,
            'bert_score': self.bert_score,
            'semantic_similarity': self.semantic_similarity,
            'coherence_score': self.coherence_score,
            'fluency_score': self.fluency_score,
            'diversity_score': self.diversity_score,
            'generation_time': self.generation_time,
            'tokens_per_second': self.tokens_per_second,
            'overall_quality': self.overall_quality,
            'production_readiness': self.production_readiness
        }


@dataclass
class OptimizationConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    
    # –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Stage 2.3
    target_bleu: float = 0.45          # –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ü–µ–ª—å –¥–ª—è Stage 2.3
    target_rouge_l: float = 0.35       # ROUGE-L —Ü–µ–ª—å
    target_bert_score: float = 0.70    # BERTScore —Ü–µ–ª—å
    target_coherence: float = 0.75     # Coherence —Ü–µ–ª—å
    target_fluency: float = 0.80       # Fluency —Ü–µ–ª—å
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    max_optimization_iterations: int = 50
    patience: int = 5                  # Early stopping patience
    improvement_threshold: float = 0.01  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    
    # –î–∏–∞–ø–∞–∑–æ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è fine-tuning
    temperature_range: Tuple[float, float] = (0.3, 1.2)
    top_k_range: Tuple[int, int] = (10, 100)
    top_p_range: Tuple[float, float] = (0.7, 0.95)
    repetition_penalty_range: Tuple[float, float] = (1.0, 1.5)
    
    # Production settings
    quality_monitoring: bool = True
    save_best_params: bool = True
    detailed_logging: bool = True


class AdvancedQualityAssessment:
    """
    üî¨ –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –°–ò–°–¢–ï–ú–ê –û–¶–ï–ù–ö–ò –ö–ê–ß–ï–°–¢–í–ê
    
    Stage 2.3 enhancement –¥–ª—è GenerativeDecoder:
    - –†–µ–∞–ª—å–Ω—ã–µ BLEU/ROUGE/BERTScore –º–µ—Ç—Ä–∏–∫–∏
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è coherence –∏ fluency –æ—Ü–µ–Ω–∫–∞
    - Production-ready –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    """
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ROUGE scorer
        if NLTK_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
            self.bleu_smoother = SmoothingFunction().method4
        else:
            self.rouge_scorer = None
            self.bleu_smoother = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERTScore model
        if SENTENCE_TRANSFORMER_AVAILABLE:
            try:
                self.bert_model = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("üéØ BERTScore model –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å BERTScore model: {e}")
                self.bert_model = None
        else:
            self.bert_model = None
        
        logger.info("üî¨ AdvancedQualityAssessment –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def assess_comprehensive_quality(self, 
                                   generated_text: str, 
                                   reference_text: str,
                                   generation_time: float = 0.0) -> QualityMetrics:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Args:
            generated_text: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            reference_text: –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            generation_time: –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            QualityMetrics: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        
        metrics = QualityMetrics()
        
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if not generated_text or not reference_text:
            logger.warning("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
            return metrics
        
        # 1. BLEU Score
        metrics.bleu_score = self._calculate_bleu(generated_text, reference_text)
        
        # 2. ROUGE Scores
        rouge_scores = self._calculate_rouge(generated_text, reference_text)
        metrics.rouge_1 = rouge_scores.get('rouge1', 0.0)
        metrics.rouge_2 = rouge_scores.get('rouge2', 0.0)
        metrics.rouge_l = rouge_scores.get('rougeL', 0.0)
        
        # 3. BERTScore (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        metrics.bert_score = self._calculate_bert_score(generated_text, reference_text)
        
        # 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        metrics.semantic_similarity = self._calculate_semantic_similarity(generated_text, reference_text)
        
        # 5. Coherence –æ—Ü–µ–Ω–∫–∞
        metrics.coherence_score = self._assess_coherence(generated_text)
        
        # 6. Fluency –æ—Ü–µ–Ω–∫–∞
        metrics.fluency_score = self._assess_fluency(generated_text)
        
        # 7. Diversity –æ—Ü–µ–Ω–∫–∞
        metrics.diversity_score = self._assess_diversity(generated_text)
        
        # 8. Performance –º–µ—Ç—Ä–∏–∫–∏
        metrics.generation_time = generation_time
        if generation_time > 0:
            tokens = len(generated_text.split())
            metrics.tokens_per_second = tokens / generation_time
        
        # 9. –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics.overall_quality = self._calculate_overall_quality(metrics)
        metrics.production_readiness = self._calculate_production_readiness(metrics)
        
        return metrics
    
    def _calculate_bleu(self, generated: str, reference: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å BLEU score"""
        if not NLTK_AVAILABLE:
            return self._simple_bleu_approximation(generated, reference)
        
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            generated_tokens = generated.lower().split()
            reference_tokens = [reference.lower().split()]
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BLEU —Å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
            bleu = sentence_bleu(reference_tokens, generated_tokens, 
                               smoothing_function=self.bleu_smoother)
            return min(max(bleu, 0.0), 1.0)
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤ BLEU —Ä–∞—Å—á–µ—Ç–µ: {e}")
            return self._simple_bleu_approximation(generated, reference)
    
    def _simple_bleu_approximation(self, generated: str, reference: str) -> float:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è BLEU –¥–ª—è fallback"""
        gen_words = set(generated.lower().split())
        ref_words = set(reference.lower().split())
        
        if not gen_words or not ref_words:
            return 0.0
        
        intersection = len(gen_words & ref_words)
        union = len(gen_words | ref_words)
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_rouge(self, generated: str, reference: str) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å ROUGE scores"""
        if not self.rouge_scorer:
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
        
        try:
            scores = self.rouge_scorer.score(generated, reference)
            return {
                'rouge1': scores['rouge1'].fmeasure,
                'rouge2': scores['rouge2'].fmeasure,
                'rougeL': scores['rougeL'].fmeasure
            }
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤ ROUGE —Ä–∞—Å—á–µ—Ç–µ: {e}")
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
    
    def _calculate_bert_score(self, generated: str, reference: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å BERTScore (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)"""
        if not self.bert_model:
            return 0.0
        
        try:
            embeddings = self.bert_model.encode([generated, reference])
            similarity = np.dot(embeddings[0], embeddings[1]) / (
                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
            )
            return float(max(0.0, min(1.0, similarity)))
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤ BERTScore —Ä–∞—Å—á–µ—Ç–µ: {e}")
            return 0.0
    
    def _calculate_semantic_similarity(self, generated: str, reference: str) -> float:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —á–µ—Ä–µ–∑ word overlap –∏ length similarity"""
        gen_words = generated.lower().split()
        ref_words = reference.lower().split()
        
        if not gen_words or not ref_words:
            return 0.0
        
        # Word overlap similarity
        word_intersection = len(set(gen_words) & set(ref_words))
        word_union = len(set(gen_words) | set(ref_words))
        word_similarity = word_intersection / word_union if word_union > 0 else 0.0
        
        # Length similarity
        length_ratio = min(len(gen_words), len(ref_words)) / max(len(gen_words), len(ref_words))
        
        return (word_similarity * 0.7 + length_ratio * 0.3)
    
    def _assess_coherence(self, text: str) -> float:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ coherence"""
        if not text:
            return 0.0
        
        words = text.split()
        if len(words) < 3:
            return 0.1
        
        coherence_score = 0.0
        factors = 0
        
        # 1. –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (optimal range)
        if 5 <= len(words) <= 25:
            coherence_score += 0.3
        elif 3 <= len(words) <= 40:
            coherence_score += 0.15
        factors += 1
        
        # 2. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'to', 'of', 'in', 'on', 'at'}
        function_word_count = sum(1 for word in words if word.lower() in function_words)
        function_ratio = function_word_count / len(words)
        if 0.2 <= function_ratio <= 0.6:
            coherence_score += 0.25
        factors += 1
        
        # 3. Repetition analysis
        word_counts = {}
        for word in words:
            word_counts[word.lower()] = word_counts.get(word.lower(), 0) + 1
        
        max_repetition = max(word_counts.values())
        if max_repetition <= max(2, len(words) // 4):
            coherence_score += 0.25
        factors += 1
        
        # 4. Vocabulary diversity
        unique_ratio = len(set(words)) / len(words)
        if unique_ratio >= 0.6:
            coherence_score += 0.2
        elif unique_ratio >= 0.4:
            coherence_score += 0.1
        factors += 1
        
        return coherence_score
    
    def _assess_fluency(self, text: str) -> float:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ fluency"""
        if not text:
            return 0.0
        
        words = text.split()
        if not words:
            return 0.0
        
        fluency_score = 0.0
        
        # 1. Basic structure (30%)
        if 3 <= len(words) <= 30:
            fluency_score += 0.3
        elif 1 <= len(words) <= 50:
            fluency_score += 0.15
        
        # 2. Grammar patterns (25%)
        text_lower = ' ' + text.lower() + ' '
        grammar_patterns = [
            ' the ', ' a ', ' an ', ' and ', ' or ', ' but ',
            ' is ', ' are ', ' was ', ' were ', ' have ', ' has ',
            ' to ', ' of ', ' in ', ' on ', ' at ', ' for '
        ]
        pattern_matches = sum(1 for pattern in grammar_patterns if pattern in text_lower)
        if pattern_matches >= 2:
            fluency_score += 0.25
        elif pattern_matches >= 1:
            fluency_score += 0.15
        
        # 3. Word flow (20%)
        if len(words) >= 3:
            # Check for reasonable word transitions
            transition_score = 0.0
            consecutive_same = 0
            for i in range(1, len(words)):
                if words[i] == words[i-1]:
                    consecutive_same += 1
                else:
                    consecutive_same = 0
                
                if consecutive_same == 0:
                    transition_score += 1
            
            transition_ratio = transition_score / (len(words) - 1)
            fluency_score += transition_ratio * 0.2
        
        # 4. Length appropriateness (15%)
        if 5 <= len(words) <= 20:
            fluency_score += 0.15
        elif 3 <= len(words) <= 35:
            fluency_score += 0.1
        
        # 5. Capitalization and punctuation patterns (10%)
        if text[0].isupper() if text else False:
            fluency_score += 0.05
        if any(punct in text for punct in '.!?'):
            fluency_score += 0.05
        
        return min(fluency_score, 1.0)
    
    def _assess_diversity(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è vocabulary"""
        words = text.split()
        if len(words) <= 1:
            return 0.0
        
        unique_words = len(set(word.lower() for word in words))
        diversity_ratio = unique_words / len(words)
        
        # Ideal diversity ratio is around 0.7-0.9
        if diversity_ratio >= 0.7:
            return 1.0
        elif diversity_ratio >= 0.5:
            return 0.8
        elif diversity_ratio >= 0.3:
            return 0.6
        else:
            return 0.3
    
    def _calculate_overall_quality(self, metrics: QualityMetrics) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞"""
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        weights = {
            'bleu': 0.25,
            'rouge_l': 0.20,
            'bert_score': 0.15,
            'semantic_similarity': 0.10,
            'coherence': 0.15,
            'fluency': 0.15
        }
        
        overall = (
            metrics.bleu_score * weights['bleu'] +
            metrics.rouge_l * weights['rouge_l'] +
            metrics.bert_score * weights['bert_score'] +
            metrics.semantic_similarity * weights['semantic_similarity'] +
            metrics.coherence_score * weights['coherence'] +
            metrics.fluency_score * weights['fluency']
        )
        
        return float(min(max(overall, 0.0), 1.0))
    
    def _calculate_production_readiness(self, metrics: QualityMetrics) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ production"""
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ production readiness –¥–ª—è Stage 2.3 (SOFTENED)
        criteria_met = 0.0
        total_criteria = 6
        
        # 1. BLEU Score (graduated scoring)
        if metrics.bleu_score >= 0.35:
            criteria_met += 1.0
        elif metrics.bleu_score >= 0.25:
            criteria_met += 0.7
        elif metrics.bleu_score >= 0.15:
            criteria_met += 0.4
        elif metrics.bleu_score >= 0.05:
            criteria_met += 0.2
        
        # 2. ROUGE-L Score (graduated scoring)
        if metrics.rouge_l >= 0.25:
            criteria_met += 1.0
        elif metrics.rouge_l >= 0.18:
            criteria_met += 0.7
        elif metrics.rouge_l >= 0.12:
            criteria_met += 0.4
        elif metrics.rouge_l >= 0.05:
            criteria_met += 0.2
        
        # 3. Coherence Score (graduated scoring)
        if metrics.coherence_score >= 0.65:
            criteria_met += 1.0
        elif metrics.coherence_score >= 0.55:
            criteria_met += 0.7
        elif metrics.coherence_score >= 0.45:
            criteria_met += 0.4
        elif metrics.coherence_score >= 0.25:
            criteria_met += 0.2
        
        # 4. Fluency Score (graduated scoring)
        if metrics.fluency_score >= 0.70:
            criteria_met += 1.0
        elif metrics.fluency_score >= 0.60:
            criteria_met += 0.7
        elif metrics.fluency_score >= 0.50:
            criteria_met += 0.4
        elif metrics.fluency_score >= 0.30:
            criteria_met += 0.2
        
        # 5. Overall Quality (graduated scoring)
        if metrics.overall_quality >= 0.60:
            criteria_met += 1.0
        elif metrics.overall_quality >= 0.45:
            criteria_met += 0.7
        elif metrics.overall_quality >= 0.30:
            criteria_met += 0.4
        elif metrics.overall_quality >= 0.15:
            criteria_met += 0.2
        
        # 6. Performance (if available)
        if metrics.generation_time > 0:
            if metrics.generation_time <= 0.5:  # Very fast
                criteria_met += 1.0
            elif metrics.generation_time <= 1.0:  # Fast
                criteria_met += 0.7
            elif metrics.generation_time <= 2.0:  # Acceptable
                criteria_met += 0.4
            elif metrics.generation_time <= 5.0:  # Slow but usable
                criteria_met += 0.2
        else:  # Performance not measured
            criteria_met += 0.5  # Partial credit
        
        return float(criteria_met / total_criteria)


class GenerationParameterOptimizer:
    """
    ‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† –ü–ê–†–ê–ú–ï–¢–†–û–í –ì–ï–ù–ï–†–ê–¶–ò–ò
    
    Stage 2.3 optimization –¥–ª—è fine-tuning –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ GenerativeDecoder:
    - –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è temperature, top_k, top_p
    - Evolutionary search —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    - Production-ready parameter sets
    """
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.quality_assessor = AdvancedQualityAssessment(config)
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.optimization_history = []
        self.best_params = None
        self.best_score = 0.0
        
        logger.info("‚ö° GenerationParameterOptimizer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def optimize_parameters(self, 
                          model,
                          test_embeddings: List[torch.Tensor],
                          reference_texts: List[str],
                          max_iterations: Optional[int] = None) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Args:
            model: GenerativeDecoder model
            test_embeddings: –¢–µ—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            reference_texts: –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            max_iterations: –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            
        Returns:
            Dict: –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –º–µ—Ç—Ä–∏–∫–∏
        """
        
        max_iterations = max_iterations or self.config.max_optimization_iterations
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (max_iterations={max_iterations})")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        best_params = self._get_initial_parameters()
        best_metrics = self._evaluate_parameters(model, best_params, test_embeddings, reference_texts)
        best_score = best_metrics.overall_quality
        
        patience_counter = 0
        
        for iteration in range(max_iterations):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º candidate parameters
            candidate_params = self._generate_candidate_parameters(best_params, iteration)
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            candidate_metrics = self._evaluate_parameters(model, candidate_params, test_embeddings, reference_texts)
            candidate_score = candidate_metrics.overall_quality
            
            # –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ?
            if candidate_score > best_score + self.config.improvement_threshold:
                best_params = candidate_params
                best_metrics = candidate_metrics
                best_score = candidate_score
                patience_counter = 0
                
                logger.info(f"üéØ Iteration {iteration}: –ù–æ–≤—ã–µ –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã! Score: {best_score:.4f}")
                
            else:
                patience_counter += 1
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.optimization_history.append({
                'iteration': iteration,
                'params': candidate_params.copy(),
                'metrics': candidate_metrics.to_dict(),
                'score': candidate_score,
                'is_best': candidate_score > best_score
            })
            
            # Early stopping
            if patience_counter >= self.config.patience:
                logger.info(f"üõë Early stopping –Ω–∞ iteration {iteration} (patience={self.config.patience})")
                break
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.best_params = best_params
        self.best_score = best_score
        
        optimization_result = {
            'best_params': best_params,
            'best_metrics': best_metrics.to_dict(),
            'best_score': best_score,
            'total_iterations': len(self.optimization_history),
            'optimization_history': self.optimization_history
        }
        
        logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –õ—É—á—à–∏–π score: {best_score:.4f}")
        logger.info(f"üìä –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
        
        return optimization_result
    
    def _get_initial_parameters(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
        return {
            'temperature': 0.8,
            'top_k': 50,
            'top_p': 0.9,
            'repetition_penalty': 1.1,
            'length_penalty': 1.0
        }
    
    def _generate_candidate_parameters(self, base_params: Dict[str, Any], iteration: int) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å candidate –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
        candidate = base_params.copy()
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º—É—Ç–∞—Ü–∏—è (stronger early, gentler later)
        mutation_strength = max(0.1, 1.0 - iteration / 50.0)
        
        # Temperature optimization
        temp_delta = np.random.normal(0, 0.1 * mutation_strength)
        candidate['temperature'] = np.clip(
            candidate['temperature'] + temp_delta,
            self.config.temperature_range[0],
            self.config.temperature_range[1]
        )
        
        # Top-k optimization
        k_delta = int(np.random.normal(0, 10 * mutation_strength))
        candidate['top_k'] = np.clip(
            candidate['top_k'] + k_delta,
            self.config.top_k_range[0],
            self.config.top_k_range[1]
        )
        
        # Top-p optimization
        p_delta = np.random.normal(0, 0.05 * mutation_strength)
        candidate['top_p'] = np.clip(
            candidate['top_p'] + p_delta,
            self.config.top_p_range[0],
            self.config.top_p_range[1]
        )
        
        # Repetition penalty optimization
        rep_delta = np.random.normal(0, 0.05 * mutation_strength)
        candidate['repetition_penalty'] = np.clip(
            candidate['repetition_penalty'] + rep_delta,
            self.config.repetition_penalty_range[0],
            self.config.repetition_penalty_range[1]
        )
        
        return candidate
    
    def _evaluate_parameters(self, 
                           model,
                           params: Dict[str, Any],
                           embeddings: List[torch.Tensor],
                           reference_texts: List[str]) -> QualityMetrics:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        
        total_metrics = QualityMetrics()
        valid_evaluations = 0
        
        for embedding, reference in zip(embeddings, reference_texts):
            try:
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                start_time = time.time()
                result = model.generate(
                    embedding,
                    temperature=params['temperature'],
                    top_k=params['top_k'],
                    top_p=params['top_p'],
                    **{k: v for k, v in params.items() if k not in ['temperature', 'top_k', 'top_p']}
                )
                generation_time = time.time() - start_time
                
                generated_text = result.get('text', '')
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                metrics = self.quality_assessor.assess_comprehensive_quality(
                    generated_text, reference, generation_time
                )
                
                # –ê–∫–∫—É–º—É–ª—è—Ü–∏—è –º–µ—Ç—Ä–∏–∫
                total_metrics.bleu_score += metrics.bleu_score
                total_metrics.rouge_1 += metrics.rouge_1
                total_metrics.rouge_2 += metrics.rouge_2
                total_metrics.rouge_l += metrics.rouge_l
                total_metrics.bert_score += metrics.bert_score
                total_metrics.semantic_similarity += metrics.semantic_similarity
                total_metrics.coherence_score += metrics.coherence_score
                total_metrics.fluency_score += metrics.fluency_score
                total_metrics.diversity_score += metrics.diversity_score
                total_metrics.generation_time += metrics.generation_time
                total_metrics.tokens_per_second += metrics.tokens_per_second
                total_metrics.overall_quality += metrics.overall_quality
                total_metrics.production_readiness += metrics.production_readiness
                
                valid_evaluations += 1
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
                continue
        
        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        if valid_evaluations > 0:
            for attr in total_metrics.__dict__:
                setattr(total_metrics, attr, getattr(total_metrics, attr) / valid_evaluations)
        
        return total_metrics
    
    def save_optimization_results(self, filepath: Union[str, Path]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        results = {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'optimization_history': self.optimization_history,
            'config': {
                'target_bleu': self.config.target_bleu,
                'target_rouge_l': self.config.target_rouge_l,
                'target_coherence': self.config.target_coherence,
                'target_fluency': self.config.target_fluency
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")


# Factory function
def create_quality_optimizer(target_bleu: float = 0.45, 
                           target_rouge_l: float = 0.35,
                           max_iterations: int = 50) -> GenerationParameterOptimizer:
    """
    Factory function –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Stage 2.3
    
    Args:
        target_bleu: –¶–µ–ª–µ–≤–æ–π BLEU score (Stage 2.3 = 0.45)
        target_rouge_l: –¶–µ–ª–µ–≤–æ–π ROUGE-L score (Stage 2.3 = 0.35)
        max_iterations: –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        
    Returns:
        GenerationParameterOptimizer: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    """
    
    config = OptimizationConfig(
        target_bleu=target_bleu,
        target_rouge_l=target_rouge_l,
        max_optimization_iterations=max_iterations
    )
    
    return GenerationParameterOptimizer(config)


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    print("üî¨ Testing Quality Optimization System...")
    
    # Test quality assessment
    assessor = AdvancedQualityAssessment(OptimizationConfig())
    
    test_generated = "The quick brown fox jumps over the lazy dog."
    test_reference = "A fast brown fox leaps over a sleeping dog."
    
    metrics = assessor.assess_comprehensive_quality(test_generated, test_reference, 0.1)
    
    print(f"üìä Quality Metrics:")
    print(f"   BLEU: {metrics.bleu_score:.3f}")
    print(f"   ROUGE-L: {metrics.rouge_l:.3f}")
    print(f"   Coherence: {metrics.coherence_score:.3f}")
    print(f"   Fluency: {metrics.fluency_score:.3f}")
    print(f"   Overall: {metrics.overall_quality:.3f}")
    print(f"   Production Ready: {metrics.production_readiness:.3f}")
    
    print("‚úÖ Quality Optimization System - READY!")