# üìö EXAMPLES: Lightweight Decoder Usage

**–ú–æ–¥—É–ª—å:** inference/lightweight_decoder/  
**–í–µ—Ä—Å–∏—è:** 0.1.0  
**–°—Ç–∞—Ç—É—Å:** üÜï –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è Phase 2.7 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

---

## üéØ –ë–ê–ó–û–í–´–ï –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø

### 1. Simple PhraseBankDecoder

```python
from inference.lightweight_decoder import PhraseBankDecoder
import torch

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
decoder = PhraseBankDecoder(
    embedding_dim=768,
    phrase_bank_size=50000,
    similarity_threshold=0.8
)

# –ó–∞–≥—Ä—É–∑–∫–∞ phrase bank
decoder.load_phrase_bank("data/phrase_banks/common_phrases.pkl")

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
input_embedding = torch.randn(768)  # –û—Ç EmbeddingProcessor
generated_text = decoder.decode(input_embedding)

print(f"Generated: {generated_text}")
# Output: "The quick brown fox jumps over the lazy dog"
```

### 2. GenerativeDecoder —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

```python
from inference.lightweight_decoder import GenerativeDecoder

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
decoder = GenerativeDecoder(
    embedding_dim=768,
    vocab_size=32000,
    hidden_size=1024,
    num_layers=4,
    num_heads=8
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ temperature –¥–ª—è creativity control
decoder.set_temperature(0.7)  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
input_embedding = torch.randn(768)
generated_text = decoder.generate(
    input_embedding,
    max_length=100
)

print(f"Generated: {generated_text}")
# Output: "Artificial intelligence represents a fascinating frontier..."
```

### 3. HybridDecoder - –ª—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤

```python
from inference.lightweight_decoder import HybridDecoder, PhraseBankDecoder, GenerativeDecoder

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
phrase_decoder = PhraseBankDecoder(768, 50000, 0.8)
generative_decoder = GenerativeDecoder(768, 32000, 1024, 4)

# –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥
hybrid_decoder = HybridDecoder(
    phrase_decoder=phrase_decoder,
    generative_decoder=generative_decoder,
    confidence_threshold=0.75
)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
input_embedding = torch.randn(768)
result = hybrid_decoder.decode(input_embedding)

print(f"Generated: {result}")
print(f"Confidence: {hybrid_decoder.get_confidence(input_embedding)}")
```

---

## üîß –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –ü–†–ò–ú–ï–†–´

### 1. DecoderFactory - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é

```python
from inference.lightweight_decoder import DecoderFactory

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = {
    "type": "hybrid",
    "phrase_bank": {
        "size": 50000,
        "threshold": 0.8
    },
    "generative": {
        "hidden_size": 1024,
        "num_layers": 4,
        "temperature": 0.7
    }
}

decoder = DecoderFactory.create_decoder("hybrid", config)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
input_embedding = torch.randn(768)
output = decoder.decode(input_embedding)
```

### 2. Switching –º–µ–∂–¥—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏

```python
# Runtime –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
hybrid_decoder.set_strategy("phrase")      # –¢–æ–ª—å–∫–æ phrase bank
result1 = hybrid_decoder.decode(embedding)

hybrid_decoder.set_strategy("generative")  # –¢–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
result2 = hybrid_decoder.decode(embedding)

hybrid_decoder.set_strategy("hybrid")      # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä
result3 = hybrid_decoder.decode(embedding)

print(f"Phrase only: {result1}")
print(f"Generative only: {result2}")
print(f"Hybrid approach: {result3}")
```

---

## üåä –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° –ü–û–õ–ù–û–ô –°–ò–°–¢–ï–ú–û–ô

### 1. End-to-End Pipeline

```python
from data.embedding_loader import EmbeddingLoader
from core.embedding_processor import EmbeddingProcessor
from inference.lightweight_decoder import HybridDecoder

# –ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Module 1 + 2 + 3
class CompleteCognitiveSystem:
    def __init__(self):
        # Module 1: Teacher LLM Encoder
        self.encoder = EmbeddingLoader(
            model_name="llama3-8b",
            cache_enabled=True
        )

        # Module 2: 3D Cubic Core
        self.processor = EmbeddingProcessor(
            lattice_size=(8, 8, 8),
            propagation_steps=10
        )

        # Module 3: Lightweight Decoder
        self.decoder = HybridDecoder.from_config("config/decoder.yaml")

    def process_text(self, input_text: str) -> str:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —Ç—Ä–∏ –º–æ–¥—É–ª—è"""

        # –¢–µ–∫—Å—Ç ‚Üí –≠–º–±–µ–¥–∏–Ω–≥ (Module 1)
        embedding = self.encoder.encode_text(input_text)
        print(f"Embedding shape: {embedding.shape}")

        # –≠–º–±–µ–¥–∏–Ω–≥ ‚Üí –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥ (Module 2)
        processed = self.processor.process(embedding)
        print(f"Processing complete, similarity: {self.processor.last_similarity}")

        # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥ ‚Üí –¢–µ–∫—Å—Ç (Module 3)
        output_text = self.decoder.decode(processed)
        print(f"Decoding complete")

        return output_text

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
system = CompleteCognitiveSystem()

# Autoencoder —Ä–µ–∂–∏–º
input_text = "Hello, how are you today?"
output = system.process_text(input_text)
print(f"Input: {input_text}")
print(f"Output: {output}")

# Dialogue —Ä–µ–∂–∏–º
input_text = "What is artificial intelligence?"
output = system.process_text(input_text)
print(f"Question: {input_text}")
print(f"Answer: {output}")
```

### 2. Batch Processing –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

```python
import torch

# Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö inputs
def batch_decode(decoder, embeddings_batch):
    """–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞"""

    results = []
    batch_size = embeddings_batch.shape[0]

    print(f"Processing batch of {batch_size} embeddings...")

    for i, embedding in enumerate(embeddings_batch):
        result = decoder.decode(embedding)
        results.append(result)

        if (i + 1) % 10 == 0:
            print(f"Processed {i + 1}/{batch_size}")

    return results

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
embeddings_batch = torch.randn(50, 768)  # 50 —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
decoder = HybridDecoder.from_config("config/decoder.yaml")

decoded_texts = batch_decode(decoder, embeddings_batch)

for i, text in enumerate(decoded_texts[:5]):  # –ü–µ—Ä–≤—ã–µ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"Text {i+1}: {text}")
```

---

## üß™ EVALUATION –ò TESTING

### 1. Quality Assessment

```python
from inference.lightweight_decoder.utils import calculate_bleu, semantic_similarity

# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
def evaluate_decoder_quality(decoder, test_embeddings, reference_texts):
    """Comprehensive quality evaluation"""

    results = {
        'bleu_scores': [],
        'semantic_similarities': [],
        'generation_times': []
    }

    for embedding, reference in zip(test_embeddings, reference_texts):
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏
        import time
        start_time = time.time()
        generated = decoder.decode(embedding)
        generation_time = time.time() - start_time

        # BLEU score
        bleu = calculate_bleu([reference], generated)

        # Semantic similarity
        similarity = semantic_similarity(reference, generated)

        results['bleu_scores'].append(bleu)
        results['semantic_similarities'].append(similarity)
        results['generation_times'].append(generation_time)

    # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    avg_bleu = sum(results['bleu_scores']) / len(results['bleu_scores'])
    avg_similarity = sum(results['semantic_similarities']) / len(results['semantic_similarities'])
    avg_time = sum(results['generation_times']) / len(results['generation_times'])

    print(f"Average BLEU: {avg_bleu:.3f}")
    print(f"Average Semantic Similarity: {avg_similarity:.3f}")
    print(f"Average Generation Time: {avg_time:.3f}s")

    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
test_embeddings = torch.randn(100, 768)
reference_texts = ["Reference text " + str(i) for i in range(100)]

results = evaluate_decoder_quality(decoder, test_embeddings, reference_texts)
```

### 2. Performance Benchmarking

```python
import torch
import time

def benchmark_decoders():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"""

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_embeddings = torch.randn(100, 768)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–µ–∫–æ–¥–µ—Ä–æ–≤
    phrase_decoder = PhraseBankDecoder(768, 50000, 0.8)
    generative_decoder = GenerativeDecoder(768, 32000, 1024, 4)
    hybrid_decoder = HybridDecoder(phrase_decoder, generative_decoder, 0.75)

    decoders = {
        'Phrase Bank': phrase_decoder,
        'Generative': generative_decoder,
        'Hybrid': hybrid_decoder
    }

    results = {}

    for name, decoder in decoders.items():
        print(f"\nBenchmarking {name} Decoder...")

        start_time = time.time()
        outputs = []

        for embedding in test_embeddings:
            output = decoder.decode(embedding)
            outputs.append(output)

        total_time = time.time() - start_time
        avg_time_per_decode = total_time / len(test_embeddings)

        results[name] = {
            'total_time': total_time,
            'avg_time_per_decode': avg_time_per_decode,
            'throughput': len(test_embeddings) / total_time
        }

        print(f"Total time: {total_time:.2f}s")
        print(f"Avg time per decode: {avg_time_per_decode:.4f}s")
        print(f"Throughput: {results[name]['throughput']:.2f} decodes/sec")

    return results

# –ó–∞–ø—É—Å–∫ benchmark
benchmark_results = benchmark_decoders()
```

---

## üîß DEBUGGING –ò TROUBLESHOOTING

### 1. Debugging Helpers

```python
def debug_decoder_pipeline(decoder, embedding, verbose=True):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"""

    print(f"Input embedding shape: {embedding.shape}")
    print(f"Input embedding norm: {torch.norm(embedding):.4f}")

    if hasattr(decoder, 'get_confidence'):
        confidence = decoder.get_confidence(embedding)
        print(f"Confidence score: {confidence:.4f}")

    if hasattr(decoder, 'phrase_decoder'):
        print("Using hybrid decoder with phrase bank fallback")

    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    result = decoder.decode(embedding)

    print(f"Generated text length: {len(result)}")
    print(f"Generated text: {result}")

    return result

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
debug_result = debug_decoder_pipeline(hybrid_decoder, torch.randn(768))
```

### 2. Error Recovery

```python
def robust_decode_with_fallback(decoder, embedding, max_retries=3):
    """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ fallback"""

    for attempt in range(max_retries):
        try:
            result = decoder.decode(embedding)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if len(result.strip()) == 0:
                raise ValueError("Empty generation result")

            return result

        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")

            if attempt == max_retries - 1:
                # –§–∏–Ω–∞–ª—å–Ω—ã–π fallback
                return "Unable to generate text from embedding"

            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
            if hasattr(decoder, 'set_strategy'):
                strategies = ['phrase', 'generative', 'hybrid']
                current_strategy = strategies[attempt % len(strategies)]
                decoder.set_strategy(current_strategy)
                print(f"Retrying with strategy: {current_strategy}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
safe_result = robust_decode_with_fallback(hybrid_decoder, torch.randn(768))
```

---

## üéØ EXPECTED OUTPUTS

### –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã—Ö–æ–¥–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤—Ö–æ–¥–æ–≤:

```python
# –ü—Ä–∏–º–µ—Ä 1: Factual Query
input_text = "What is machine learning?"
# Expected output: "Machine learning is a subset of artificial intelligence..."

# –ü—Ä–∏–º–µ—Ä 2: Creative Request
input_text = "Write a short poem about nature"
# Expected output: "Trees whisper secrets in the gentle breeze..."

# –ü—Ä–∏–º–µ—Ä 3: Technical Question
input_text = "Explain neural networks"
# Expected output: "Neural networks are computational models inspired by..."

# –ü—Ä–∏–º–µ—Ä 4: Conversation
input_text = "How are you feeling today?"
# Expected output: "I'm functioning well and ready to help with your questions..."
```

---

**üéØ –†–ï–ó–£–õ–¨–¢–ê–¢:** Comprehensive examples –≥–æ—Ç–æ–≤—ã –¥–ª—è Phase 2.7 implementation –∏ testing!
