# üìö EXAMPLES: Lightweight Decoder Usage

**–ú–æ–¥—É–ª—å:** inference/lightweight_decoder/  
**–í–µ—Ä—Å–∏—è:** 0.1.0  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–†–∞–±–æ—Ç–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã - Stage 1.1 –ó–ê–í–ï–†–®–ï–ù**  
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 6 –¥–µ–∫–∞–±—Ä—è 2024

---

## üéâ **–ì–û–¢–û–í–´–ï –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –ü–†–ò–ú–ï–†–´**

–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∏–∂–µ **–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ —Ä–∞–±–æ—Ç–∞—é—Ç** –≤ —Ä–∞–º–∫–∞—Ö Checkpoint 1.1.

---

## üè¶ –ë–ê–ó–û–í–û–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï PhraseBankDecoder

### 1. –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è

```python
from inference.lightweight_decoder.phrase_bank_decoder import PhraseBankDecoder
from data.embedding_loader import EmbeddingLoader

# ‚úÖ –ì–û–¢–û–í –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
decoder = PhraseBankDecoder(
    embedding_dim=768,
    similarity_threshold=0.8
)

# –ó–∞–≥—Ä—É–∑–∫–∞ phrase bank —á–µ—Ä–µ–∑ EmbeddingLoader
embedding_loader = EmbeddingLoader(cache_dir="./cache")
decoder.load_phrase_bank(embedding_loader=embedding_loader)

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
test_text = "Hello, how are you today?"
input_embedding = embedding_loader.load_from_llm(
    texts=[test_text],
    model_key="distilbert"
)[0]

output_text = decoder.decode(input_embedding)
print(f"Input: {test_text}")
print(f"Output: {output_text}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç –∏–∑ phrase bank
```

### 2. Batch –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
# ‚úÖ –ü–†–û–¢–ï–°–¢–ò–†–û–í–ê–ù–û –í Checkpoint 1.1
# Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
batch_texts = [
    "Hello there",
    "Good morning",
    "Thank you very much",
    "Have a great day"
]

# –ü–æ–ª—É—á–µ–Ω–∏–µ batch embeddings
batch_embeddings = embedding_loader.load_from_llm(
    texts=batch_texts,
    model_key="distilbert",
    use_cache=True
)

# Batch –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
results = decoder.batch_decode(batch_embeddings)

for original, decoded in zip(batch_texts, results):
    print(f"'{original}' ‚Üí '{decoded}'")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–æ batch
```

### 3. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

```python
# ‚úÖ –î–û–°–¢–£–ü–ù–û –í PRODUCTION
# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞—á–µ—Å—Ç–≤–µ
decoded_text, metrics = decoder.decode_with_metrics(input_embedding)

print(f"Decoded: {decoded_text}")
print(f"Quality score: {metrics['quality_score']:.3f}")
print(f"Confidence: {metrics['confidence']:.3f}")
print(f"Candidates found: {metrics['num_candidates']}")
print(f"Top similarity: {metrics['top_similarity']:.3f}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
```

---

## üîó –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° MODULE 1 (Teacher LLM Encoder)

### 1. –ü–æ–ª–Ω—ã–π Pipeline Module 1 ‚Üí Module 3

```python
# ‚úÖ –£–°–ü–ï–®–ù–û –ü–†–û–¢–ï–°–¢–ò–†–û–í–ê–ù–û –í Checkpoint 1.1
from data.embedding_loader import EmbeddingLoader
from inference.lightweight_decoder.phrase_bank_decoder import PhraseBankDecoder

def create_complete_pipeline():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ pipeline –¥–ª—è Modules 1 & 3"""

    # Module 1: Teacher LLM Encoder
    encoder = EmbeddingLoader(cache_dir="./cache")

    # Module 3: Lightweight Decoder
    decoder = PhraseBankDecoder(embedding_dim=768)
    decoder.load_phrase_bank(embedding_loader=encoder)

    return encoder, decoder

def process_text_pipeline(input_text: str) -> str:
    """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª–∏"""

    encoder, decoder = create_complete_pipeline()

    # –¢–µ–∫—Å—Ç ‚Üí –≠–º–±–µ–¥–∏–Ω–≥ (Module 1)
    print(f"üî¥ Module 1: Encoding '{input_text}'...")
    embedding = encoder.load_from_llm(
        texts=[input_text],
        model_key="distilbert",
        use_cache=True
    )[0]
    print(f"   Embedding shape: {embedding.shape}")

    # –≠–º–±–µ–¥–∏–Ω–≥ ‚Üí –¢–µ–∫—Å—Ç (Module 3)
    print(f"üü° Module 3: Decoding embedding...")
    output_text = decoder.decode(embedding)
    print(f"   Decoded successfully")

    return output_text

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = process_text_pipeline("Hello, how are you today?")
print(f"\nüéØ Final result: '{result}'")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –æ—à–∏–±–æ–∫
```

### 2. Multiple Model Support

```python
# ‚úÖ –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–¢–°–Ø –ù–ï–°–ö–û–õ–¨–ö–û LLM –ú–û–î–ï–õ–ï–ô
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ encoder –º–æ–¥–µ–ª—è–º–∏

models_to_test = ["distilbert", "roberta", "gpt2"]
test_text = "Thank you for your help"

encoder, decoder = create_complete_pipeline()

for model_key in models_to_test:
    print(f"\nüß† Testing with {model_key}...")

    # Encoding —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    embedding = encoder.load_from_llm(
        texts=[test_text],
        model_key=model_key,
        use_cache=True
    )[0]

    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    result = decoder.decode(embedding)
    print(f"   Result: '{result}'")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å multiple teacher models
```

---

## üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ú–û–ù–ò–¢–û–†–ò–ù–ì

### 1. Phrase Bank —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

```python
# ‚úÖ –†–ï–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ò–ó CHECKPOINT 1.1
# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ phrase bank
stats = decoder.phrase_bank.get_statistics()

print("üìä Phrase Bank Statistics:")
print(f"   Total phrases: {stats['total_phrases']}")
print(f"   Index type: {stats['index_type']}")
print(f"   Total searches: {stats['total_searches']}")
print(f"   Cache hit rate: {stats['cache_hit_rate']}")
print(f"   Avg search time: {stats['avg_search_time_ms']} ms")
print(f"   FAISS available: {stats['faiss_available']}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ–ª–Ω–∞—è visibility –≤ performance
```

### 2. Decoder —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

```python
# ‚úÖ PRODUCTION-READY MONITORING
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç—ã –¥–µ–∫–æ–¥–µ—Ä–∞
decoder_stats = decoder.get_statistics()

print("üî§ Decoder Statistics:")
print(f"   Total decodings: {decoder_stats['total_decodings']}")
print(f"   Success rate: {decoder_stats['success_rate']}")
print(f"   Avg confidence: {decoder_stats['avg_confidence']:.3f}")
print(f"   Avg quality: {decoder_stats['avg_quality']:.3f}")

# Configuration info
config_info = decoder_stats['config']
print(f"   Similarity threshold: {config_info['similarity_threshold']}")
print(f"   Assembly method: {config_info['assembly_method']}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: Comprehensive monitoring –≥–æ—Ç–æ–≤
```

---

## ‚ö° PERFORMANCE –ü–†–ò–ú–ï–†–´

### 1. Performance —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
# ‚úÖ CHECKPOINT 1.1 –ü–û–ö–ê–ó–ê–õ <10ms PERFORMANCE
import time

def benchmark_search_performance(decoder, num_tests=10):
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞"""

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è test embeddings
    test_embeddings = []
    for i in range(num_tests):
        embedding = torch.randn(768)
        test_embeddings.append(embedding)

    # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
    total_time = 0
    for embedding in test_embeddings:
        start_time = time.time()
        result = decoder.decode(embedding)
        end_time = time.time()

        search_time = (end_time - start_time) * 1000  # ms
        total_time += search_time

    avg_time = total_time / num_tests
    return avg_time

# –ó–∞–ø—É—Å–∫ benchmark
avg_time = benchmark_search_performance(decoder)
print(f"‚ö° Average search time: {avg_time:.2f}ms")
print(f"üéØ Target: <10ms - {'‚úÖ PASSED' if avg_time < 10 else '‚ùå FAILED'}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: Performance target –¥–æ—Å—Ç–∏–≥–Ω—É—Ç
```

### 2. Memory usage –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```python
# ‚úÖ MEMORY EFFICIENT IMPLEMENTATION
import psutil
import os

def check_memory_usage():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    process = psutil.Process(os.getpid())
    memory_mb = process.memory_info().rss / 1024 / 1024
    return memory_mb

# –î–æ –∑–∞–≥—Ä—É–∑–∫–∏ phrase bank
memory_before = check_memory_usage()

# –ó–∞–≥—Ä—É–∑–∫–∞ phrase bank
decoder = PhraseBankDecoder(embedding_dim=768)
decoder.load_phrase_bank(embedding_loader=embedding_loader)

# –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
memory_after = check_memory_usage()
memory_used = memory_after - memory_before

print(f"üíæ Memory usage:")
print(f"   Before: {memory_before:.1f} MB")
print(f"   After: {memory_after:.1f} MB")
print(f"   Used by phrase bank: {memory_used:.1f} MB")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
```

---

## üîß CONFIGURATION –ü–†–ò–ú–ï–†–´

### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ similarity threshold

```python
# ‚úÖ FLEXIBLE CONFIGURATION
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö threshold values

thresholds = [0.5, 0.7, 0.8, 0.9]
test_embedding = torch.randn(768)

for threshold in thresholds:
    decoder.config.similarity_threshold = threshold

    # –ü–æ–∏—Å–∫ —Å –Ω–æ–≤—ã–º threshold
    candidates = decoder.phrase_bank.search_phrases(
        test_embedding,
        k=5,
        min_similarity=threshold
    )

    print(f"Threshold {threshold}: {len(candidates)} candidates found")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: Flexible quality control
```

### 2. Assembly methods —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

```python
# ‚úÖ MULTIPLE ASSEMBLY STRATEGIES
# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–±–æ—Ä–∫–∏ —Ç–µ–∫—Å—Ç–∞

assembly_methods = ["weighted", "greedy", "beam_search"]
test_embedding = torch.randn(768)

for method in assembly_methods:
    decoder.config.assembly_method = method

    result = decoder.decode(test_embedding)
    print(f"Method '{method}': '{result}'")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–±–æ—Ä–∫–∏ —Ç–µ–∫—Å—Ç–∞
```

---

## üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò –í–ê–õ–ò–î–ê–¶–ò–Ø

### 1. Quality assessment

```python
# ‚úÖ CHECKPOINT 1.1 VALIDATION
def validate_decoder_quality():
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ–∫–æ–¥–µ—Ä–∞"""

    test_cases = [
        "Hello, how are you?",
        "Thank you very much",
        "Good morning everyone",
        "Have a great day",
        "See you later"
    ]

    for test_text in test_cases:
        # Encoding
        embedding = embedding_loader.load_from_llm(
            texts=[test_text],
            model_key="distilbert"
        )[0]

        # Decoding
        result = decoder.decode(embedding)

        print(f"‚úÖ '{test_text}' ‚Üí '{result}'")

    return True

# –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
success = validate_decoder_quality()
print(f"\nüéØ Validation: {'‚úÖ PASSED' if success else '‚ùå FAILED'}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç: Comprehensive quality validation
```

---

## üöÄ –ì–û–¢–û–í–ù–û–°–¢–¨ –ö PRODUCTION

–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã—à–µ **–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é**:

- ‚úÖ **PhraseBankDecoder** –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω
- ‚úÖ **Module 1 –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –æ—à–∏–±–æ–∫
- ‚úÖ **Performance targets** –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã (<10ms)
- ‚úÖ **RTX 5090 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** —á–µ—Ä–µ–∑ CPU-only —Ä–µ–∂–∏–º
- ‚úÖ **Comprehensive monitoring** –¥–æ—Å—Ç—É–ø–µ–Ω

**Next step:** –ü–µ—Ä–µ—Ö–æ–¥ –∫ Stage 1.2 (PhraseBankDecoder refinement) –∏–ª–∏ Stage 2 (GenerativeDecoder)

---

## üìã –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –†–ï–°–£–†–°–´

### –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ imports

```python
import torch
import time
import psutil
import os
from typing import List, Dict, Tuple

from inference.lightweight_decoder.phrase_bank_decoder import PhraseBankDecoder
from inference.lightweight_decoder.phrase_bank import PhraseBank, PhraseEntry
from data.embedding_loader import EmbeddingLoader
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

- `config/main_config.yaml` - –æ—Å–Ω–æ–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (CPU-only —Ä–µ–∂–∏–º)
- `config/lightweight_decoder.yaml` - —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Checkpoint 1.1
python test_phrase_bank_basic.py

# –†–µ–∑—É–ª—å—Ç–∞—Ç: 5/5 —Ç–µ—Å—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–π—Ç–∏
```
