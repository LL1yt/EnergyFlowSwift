"""
üî§ PHRASE BANK DECODER - –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ —Ñ—Ä–∞–∑

–†–µ–∞–ª–∏–∑—É–µ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏
–±–ª–∏–∑–∫–∏—Ö —Ñ—Ä–∞–∑ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–º phrase bank.

Phase 2.7.1 - PhraseBankDecoder Implementation
Phase 2.7.2 - STAGE 1.2 OPTIMIZATION ‚ú®
Phase 2.7.3 - STAGE 1.3 PRODUCTION READINESS [START]
"""

import torch
import numpy as np
import logging
from typing import List, Optional, Dict, Tuple, Set, Union
from dataclasses import dataclass, field
import re
from collections import defaultdict, OrderedDict
import hashlib
import json
from pathlib import Path
import time

from .phrase_bank import PhraseBank, PhraseEntry

@dataclass
class DecodingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    max_candidates: int = 10          # –ú–∞–∫—Å–∏–º—É–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
    similarity_threshold: float = 0.8  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π threshold similarity
    assembly_method: str = "context_aware"  # weighted, greedy, beam_search, context_aware
    coherence_weight: float = 0.3     # –í–µ—Å –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
    relevance_weight: float = 0.7     # –í–µ—Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    context_weight: float = 0.4       # –í–µ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    min_phrase_length: int = 3        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã (—Å–ª–æ–≤–∞)
    max_phrase_length: int = 50       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã (—Å–ª–æ–≤–∞)
    
    # Context-aware parameters
    context_window_size: int = 5      # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    category_bonus: float = 0.1       # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–∞—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    length_preference: str = "medium" # short, medium, long, adaptive
    
    # Post-processing parameters  
    enable_grammar_fix: bool = True   # –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    enable_coherence_boost: bool = True  # –ü–æ–≤—ã—à–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
    enable_redundancy_removal: bool = True  # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏
    
    # üÜï Stage 1.3: Production readiness parameters
    enable_caching: bool = True       # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    cache_size: int = 1000           # –†–∞–∑–º–µ—Ä –∫—ç—à–∞
    enable_fallbacks: bool = True     # –†–µ–∑–µ—Ä–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    max_retry_attempts: int = 3       # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø–æ–≤—Ç–æ—Ä–∞
    timeout_seconds: float = 5.0      # –¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π
    enable_performance_monitoring: bool = True  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    # Error handling parameters
    strict_mode: bool = False         # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º (–∏—Å–∫–ª—é—á–µ–Ω–∏—è vs fallbacks)
    default_fallback_text: str = "Unable to decode."  # –¢–µ–∫—Å—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    log_errors: bool = True          # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
    
    # Configuration validation
    validate_on_init: bool = True     # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    
    def __post_init__(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.validate_on_init:
            self.validate()
    
    def validate(self):
        """üÜï –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        errors = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        if not 0.0 <= self.similarity_threshold <= 1.0:
            errors.append("similarity_threshold must be between 0.0 and 1.0")
        
        if not 0.0 <= self.context_weight <= 1.0:
            errors.append("context_weight must be between 0.0 and 1.0")
            
        if self.max_candidates <= 0:
            errors.append("max_candidates must be positive")
            
        if self.cache_size <= 0:
            errors.append("cache_size must be positive")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        valid_assembly_methods = ["weighted", "greedy", "beam_search", "context_aware"]
        if self.assembly_method not in valid_assembly_methods:
            errors.append(f"assembly_method must be one of {valid_assembly_methods}")
        
        valid_length_preferences = ["short", "medium", "long", "adaptive"]
        if self.length_preference not in valid_length_preferences:
            errors.append(f"length_preference must be one of {valid_length_preferences}")
        
        if errors:
            raise ValueError(f"Configuration validation failed: {'; '.join(errors)}")

class PatternCache:
    """üÜï –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache: OrderedDict = OrderedDict()
        self.hit_count = 0
        self.miss_count = 0
    
    def _hash_embedding(self, embedding: torch.Tensor) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è —ç–º–±–µ–¥–∏–Ω–≥–∞"""
        # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 4 –∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        rounded = torch.round(embedding * 10000) / 10000
        return hashlib.md5(rounded.numpy().tobytes()).hexdigest()
    
    def get(self, embedding: torch.Tensor) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫—ç—à–∞"""
        key = self._hash_embedding(embedding)
        
        if key in self.cache:
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∫–æ–Ω–µ—Ü (LRU)
            result = self.cache.pop(key)
            self.cache[key] = result
            self.hit_count += 1
            return result
        
        self.miss_count += 1
        return None
    
    def put(self, embedding: torch.Tensor, result: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à"""
        key = self._hash_embedding(embedding)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –µ—Å–ª–∏ –∫—ç—à –ø–æ–ª–æ–Ω
        if len(self.cache) >= self.max_size:
            self.cache.popitem(last=False)  # –£–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å
        
        self.cache[key] = {
            'result': result,
            'timestamp': time.time()
        }
    
    def clear(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        self.cache.clear()
        self.hit_count = 0
        self.miss_count = 0
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': f"{hit_rate:.1f}%",
            'total_requests': total_requests
        }

class ErrorHandler:
    """üÜï –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
    
    def __init__(self, config: DecodingConfig):
        self.config = config
        self.error_counts = defaultdict(int)
        self.recent_errors: List[Dict] = []
        self.max_recent_errors = 50
    
    def handle_error(self, error: Exception, context: str, 
                    fallback_fn: Optional[callable] = None) -> Union[str, None]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        error_info = {
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context,
            'timestamp': time.time()
        }
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if self.config.log_errors:
            logging.error(f"Error in {context}: {error}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        self.error_counts[error_info['error_type']] += 1
        self.recent_errors.append(error_info)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–¥–∞–≤–Ω–∏—Ö –æ—à–∏–±–æ–∫
        if len(self.recent_errors) > self.max_recent_errors:
            self.recent_errors = self.recent_errors[-self.max_recent_errors:]
        
        # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if self.config.strict_mode:
            raise error
        
        if self.config.enable_fallbacks and fallback_fn:
            try:
                return fallback_fn()
            except Exception as fallback_error:
                if self.config.log_errors:
                    logging.error(f"Fallback failed: {fallback_error}")
        
        return self.config.default_fallback_text
    
    def get_error_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫"""
        return {
            'error_counts': dict(self.error_counts),
            'recent_errors_count': len(self.recent_errors),
            'total_errors': sum(self.error_counts.values())
        }

class PerformanceMonitor:
    """üÜï –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self.operation_times: Dict[str, List[float]] = defaultdict(list)
        self.operation_counts: Dict[str, int] = defaultdict(int)
    
    def time_operation(self, operation_name: str):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–ø–µ—Ä–∞—Ü–∏–π"""
        return self._OperationTimer(self, operation_name) if self.enabled else self._NoOpTimer()
    
    class _OperationTimer:
        def __init__(self, monitor, operation_name: str):
            self.monitor = monitor
            self.operation_name = operation_name
            self.start_time = None
        
        def __enter__(self):
            self.start_time = time.time()
            return self
        
        def __exit__(self, exc_type, exc_val, exc_tb):
            duration = time.time() - self.start_time
            self.monitor.operation_times[self.operation_name].append(duration)
            self.monitor.operation_counts[self.operation_name] += 1
    
    class _NoOpTimer:
        def __enter__(self):
            return self
        def __exit__(self, exc_type, exc_val, exc_tb):
            pass
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.enabled:
            return {'monitoring_disabled': True}
        
        stats = {}
        for operation, times in self.operation_times.items():
            if times:
                stats[operation] = {
                    'count': len(times),
                    'avg_time_ms': np.mean(times) * 1000,
                    'min_time_ms': min(times) * 1000,
                    'max_time_ms': max(times) * 1000,
                    'total_time_ms': sum(times) * 1000
                }
        
        return stats

class ContextAnalyzer:
    """üÜï –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ñ—Ä–∞–∑"""
    
    def __init__(self, config: DecodingConfig):
        self.config = config
        self.phrase_history: List[PhraseEntry] = []
        self.category_frequencies = defaultdict(int)
        self.length_preferences = {
            "short": (1, 5),
            "medium": (3, 15), 
            "long": (10, 50),
            "adaptive": (1, 50)
        }
    
    def analyze_context(self, candidates: List[Tuple[PhraseEntry, float]], 
                       embedding: torch.Tensor) -> List[Tuple[PhraseEntry, float]]:
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        if not candidates:
            return candidates
        
        # [TARGET] Context-aware scoring
        scored_candidates = []
        
        for phrase, base_similarity in candidates:
            context_score = self._calculate_context_score(phrase, embedding)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            final_score = (
                base_similarity * (1 - self.config.context_weight) +
                context_score * self.config.context_weight
            )
            
            scored_candidates.append((phrase, final_score))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É score
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        return scored_candidates
    
    def _calculate_context_score(self, phrase: PhraseEntry, embedding: torch.Tensor) -> float:
        """–†–∞—Å—á–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ score –¥–ª—è —Ñ—Ä–∞–∑—ã"""
        context_score = 0.0
        
        # 1. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        if phrase.category in self.category_frequencies:
            category_boost = min(0.2, self.category_frequencies[phrase.category] * 0.05)
            context_score += category_boost
        
        # 2. –î–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã –ø–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º
        length_score = self._score_phrase_length(phrase.length)
        context_score += length_score * 0.3
        
        # 3. –ò—Å—Ç–æ—Ä–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–∏–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–æ–≤)
        if phrase in self.phrase_history[-self.config.context_window_size:]:
            context_score -= 0.15  # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–¥–∞–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
        
        # 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ñ—Ä–∞–∑–∞–º–∏
        if self.phrase_history:
            coherence_score = self._calculate_semantic_coherence(phrase)
            context_score += coherence_score * 0.25
        
        return max(0.0, min(1.0, context_score))
    
    def _score_phrase_length(self, length: int) -> float:
        """–û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã —Ñ—Ä–∞–∑—ã —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º"""
        pref_range = self.length_preferences[self.config.length_preference]
        min_len, max_len = pref_range
        
        if min_len <= length <= max_len:
            # –í –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            return 1.0
        elif length < min_len:
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è
            return max(0.0, 0.5 - (min_len - length) * 0.1)
        else:
            # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è
            return max(0.0, 0.7 - (length - max_len) * 0.05)
    
    def _calculate_semantic_coherence(self, phrase: PhraseEntry) -> float:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ñ—Ä–∞–∑–∞–º–∏"""
        if not self.phrase_history:
            return 0.0
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        recent_phrases = self.phrase_history[-3:]
        
        coherence_scores = []
        for prev_phrase in recent_phrases:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–≤—ã—à–∞–µ—Ç —Å–≤—è–∑–Ω–æ—Å—Ç—å
            if phrase.category == prev_phrase.category:
                coherence_scores.append(0.8)
            else:
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
                coherence_scores.append(0.3)
        
        return np.mean(coherence_scores) if coherence_scores else 0.0
    
    def update_context(self, selected_phrase: PhraseEntry):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ —Ñ—Ä–∞–∑—ã"""
        self.phrase_history.append(selected_phrase)
        self.category_frequencies[selected_phrase.category] += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.phrase_history) > self.config.context_window_size * 2:
            self.phrase_history = self.phrase_history[-self.config.context_window_size:]
    
    def reset_context(self):
        """–°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏"""
        self.phrase_history.clear()
        self.category_frequencies.clear()

class TextPostProcessor:
    """üÜï –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"""
    
    def __init__(self, config: DecodingConfig):
        self.config = config
    
    def process_text(self, raw_text: str, confidence: float = 0.0) -> str:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        processed_text = raw_text
        
        if self.config.enable_grammar_fix:
            processed_text = self._fix_basic_grammar(processed_text)
        
        if self.config.enable_coherence_boost:
            processed_text = self._boost_coherence(processed_text, confidence)
        
        if self.config.enable_redundancy_removal:
            processed_text = self._remove_redundancy(processed_text)
        
        return processed_text.strip()
    
    def _fix_basic_grammar(self, text: str) -> str:
        """–ë–∞–∑–æ–≤—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        # –ö–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        text = re.sub(r'(^|[.!?]\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'\s+([.!?,:;])', r'\1', text)
        text = re.sub(r'([.!?])\s*([.!?])+', r'\1', text)
        
        return text
    
    def _boost_coherence(self, text: str, confidence: float) -> str:
        """–ü–æ–≤—ã—à–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å–º—è–≥—á–∞—é—â–∏–µ —Å–ª–æ–≤–∞
        if confidence < 0.6:
            coherence_boosters = ["perhaps", "possibly", "it seems", "likely"]
            if not any(booster in text.lower() for booster in coherence_boosters):
                text = f"It seems {text.lower()}"
        
        return text
    
    def _remove_redundancy(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏"""
        words = text.split()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –ø–æ–¥—Ä—è–¥
        cleaned_words = []
        prev_word = None
        
        for word in words:
            if word.lower() != prev_word:
                cleaned_words.append(word)
                prev_word = word.lower()
        
        return ' '.join(cleaned_words)

class TextAssembler:
    """–°–±–æ—Ä—â–∏–∫ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑"""
    
    def __init__(self, config: DecodingConfig):
        self.config = config
        self.context_analyzer = ContextAnalyzer(config)  # üÜï
        self.post_processor = TextPostProcessor(config)  # üÜï
    
    def assemble_weighted(self, candidates: List[Tuple[PhraseEntry, float]]) -> str:
        """–í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å–±–æ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ similarity scores"""
        if not candidates:
            return "No suitable phrases found."
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –±–µ—Ä–µ–º –ª—É—á—à—É—é —Ñ—Ä–∞–∑—É
        best_phrase, best_similarity = candidates[0]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if best_similarity < self.config.similarity_threshold:
            return "Low confidence phrase match."
        
        # üÜï –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.context_analyzer.update_context(best_phrase)
        
        return best_phrase.text
    
    def assemble_greedy(self, candidates: List[Tuple[PhraseEntry, float]]) -> str:
        """–ñ–∞–¥–Ω–∞—è —Å–±–æ—Ä–∫–∞ - –ø—Ä–æ—Å—Ç–æ –ª—É—á—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç"""
        if not candidates:
            return "No phrases available."
        
        best_phrase, _ = candidates[0]
        self.context_analyzer.update_context(best_phrase)
        return best_phrase.text
    
    def assemble_beam_search(self, candidates: List[Tuple[PhraseEntry, float]]) -> str:
        """Beam search —Å–±–æ—Ä–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        if not candidates:
            return "No beam candidates."
        
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è multi-phrase assembly
        filtered_candidates = [
            (phrase, similarity) for phrase, similarity in candidates
            if self.config.min_phrase_length <= phrase.length <= self.config.max_phrase_length
        ]
        
        if not filtered_candidates:
            # Fallback –∫ –ø–µ—Ä–≤–æ–º—É –∫–∞–Ω–¥–∏–¥–∞—Ç—É
            return candidates[0][0].text
        
        # –í–æ–∑—å–º–µ–º –ª—É—á—à–∏–π –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π
        best_phrase, _ = filtered_candidates[0]
        self.context_analyzer.update_context(best_phrase)
        return best_phrase.text
    
    def assemble_context_aware(self, candidates: List[Tuple[PhraseEntry, float]], 
                             embedding: torch.Tensor) -> str:
        """üÜï –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–∞—è —Å–±–æ—Ä–∫–∞"""
        if not candidates:
            return "No context-aware candidates."
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        context_candidates = self.context_analyzer.analyze_context(candidates, embedding)
        
        if not context_candidates:
            return "Context analysis failed."
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç
        best_phrase, confidence = context_candidates[0]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.context_analyzer.update_context(best_phrase)
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_text = self.post_processor.process_text(best_phrase.text, confidence)
        
        return processed_text
    
    def assemble(self, candidates: List[Tuple[PhraseEntry, float]], 
                embedding: Optional[torch.Tensor] = None) -> str:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∫–∏"""
        if self.config.assembly_method == "weighted":
            return self.assemble_weighted(candidates)
        elif self.config.assembly_method == "greedy":
            return self.assemble_greedy(candidates)
        elif self.config.assembly_method == "beam_search":
            return self.assemble_beam_search(candidates)
        elif self.config.assembly_method == "context_aware":
            if embedding is not None:
                return self.assemble_context_aware(candidates, embedding)
            else:
                logging.warning("Context-aware assembly requires embedding, falling back to weighted")
                return self.assemble_weighted(candidates)
        else:
            logging.warning(f"Unknown assembly method: {self.config.assembly_method}")
            return self.assemble_greedy(candidates)
    
    def reset_session(self):
        """üÜï –°–±—Ä–æ—Å —Å–µ—Å—Å–∏–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞"""
        self.context_analyzer.reset_context()

class QualityAssessor:
    """–û—Ü–µ–Ω—â–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, config: DecodingConfig):
        self.config = config
    
    def assess_candidates(self, candidates: List[Tuple[PhraseEntry, float]]) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        if not candidates:
            return {
                'quality_score': 0.0,
                'confidence': 0.0,
                'coherence': 0.0,
                'diversity': 0.0
            }
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        similarities = [similarity for _, similarity in candidates]
        
        quality_score = np.mean(similarities)
        confidence = max(similarities)
        coherence = self._assess_coherence(candidates)
        diversity = self._assess_diversity(candidates)
        
        return {
            'quality_score': float(quality_score),
            'confidence': float(confidence),
            'coherence': float(coherence),
            'diversity': float(diversity)
        }
    
    def _assess_coherence(self, candidates: List[Tuple[PhraseEntry, float]]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        if len(candidates) <= 1:
            return 1.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: —Å—Ö–æ–∂–µ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        categories = [phrase.category for phrase, _ in candidates]
        unique_categories = set(categories)
        
        # –ë–æ–ª—å—à–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π = –≤—ã—à–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å
        coherence = 1.0 - (len(unique_categories) - 1) / len(candidates)
        return max(0.0, coherence)
    
    def _assess_diversity(self, candidates: List[Tuple[PhraseEntry, float]]) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        if len(candidates) <= 1:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–ª–∏–Ω —Ñ—Ä–∞–∑
        lengths = [phrase.length for phrase, _ in candidates]
        diversity = np.std(lengths) / max(lengths) if max(lengths) > 0 else 0.0
        
        return min(1.0, diversity)

class PhraseBankDecoder:
    """[START] Production-ready –¥–µ–∫–æ–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ phrase bank (Stage 1.3)"""
    
    def __init__(self, 
                 embedding_dim: int = 768,
                 phrase_bank_size: int = 50000,
                 similarity_threshold: float = 0.8,
                 config: Optional[DecodingConfig] = None):
        
        self.embedding_dim = embedding_dim
        self.phrase_bank_size = phrase_bank_size
        self.similarity_threshold = similarity_threshold
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        self.config = config or DecodingConfig(similarity_threshold=similarity_threshold)
        
        # üÜï Production-ready –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.cache = PatternCache(max_size=self.config.cache_size) if self.config.enable_caching else None
        self.error_handler = ErrorHandler(self.config)
        self.performance_monitor = PerformanceMonitor(enabled=self.config.enable_performance_monitoring)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.phrase_bank = PhraseBank(
            embedding_dim=embedding_dim,
            similarity_threshold=similarity_threshold,
            max_phrases=phrase_bank_size
        )
        
        self.assembler = TextAssembler(self.config)
        self.quality_assessor = QualityAssessor(self.config)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_decodings': 0,
            'successful_decodings': 0,
            'cache_hits': 0,
            'fallback_uses': 0,
            'error_count': 0,
            'avg_confidence': 0.0,
            'avg_quality': 0.0,
            'avg_decode_time_ms': 0.0
        }
        
        self.ready = False
        
        logging.info(f"PhraseBankDecoder (Stage 1.3) initialized: dim={embedding_dim}, " +
                    f"threshold={similarity_threshold}, caching={self.config.enable_caching}")
    
    def load_phrase_bank(self, embedding_loader=None, bank_path: Optional[str] = None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ phrase bank"""
        if bank_path:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
            self.phrase_bank.load_bank(bank_path)
        elif embedding_loader:
            # –°–æ–∑–¥–∞–Ω–∏–µ sample bank –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            self.phrase_bank.load_sample_bank(embedding_loader)
        else:
            raise ValueError("Either embedding_loader or bank_path must be provided")
        
        self.ready = True
        logging.info("Phrase bank loaded successfully")
    
    def decode(self, embedding: torch.Tensor) -> str:
        """[START] Production-ready –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Stage 1.3 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        if not self.ready:
            error_msg = "Phrase bank not loaded. Call load_phrase_bank() first."
            return self.error_handler.handle_error(
                ValueError(error_msg), 
                "decode_readiness_check",
                fallback_fn=lambda: self.config.default_fallback_text
            )
        
        if embedding.dim() != 1 or embedding.size(0) != self.embedding_dim:
            error_msg = f"Expected embedding shape ({self.embedding_dim},), got {embedding.shape}"
            return self.error_handler.handle_error(
                ValueError(error_msg),
                "decode_input_validation",
                fallback_fn=lambda: self.config.default_fallback_text
            )
        
        # üÜï –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if self.cache:
            cached_result = self.cache.get(embedding)
            if cached_result:
                self.stats['cache_hits'] += 1
                self.stats['total_decodings'] += 1  # –£—á–∏—Ç—ã–≤–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã
                return cached_result['result']['decoded_text']
        
        try:
            with self.performance_monitor.time_operation("full_decode"):
                # –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
                with self.performance_monitor.time_operation("phrase_search"):
                    candidates = self.phrase_bank.search_phrases(
                        embedding, 
                        k=self.config.max_candidates,
                        min_similarity=self.config.similarity_threshold
                    )
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                with self.performance_monitor.time_operation("quality_assessment"):
                    quality_metrics = self.quality_assessor.assess_candidates(candidates)
                
                # –°–±–æ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞
                with self.performance_monitor.time_operation("text_assembly"):
                    decoded_text = self.assembler.assemble(candidates, embedding)
                
                # üÜï –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
                if self.cache:
                    cache_data = {
                        'decoded_text': decoded_text,
                        'quality_metrics': quality_metrics
                    }
                    self.cache.put(embedding, cache_data)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                self._update_stats(quality_metrics, len(candidates) > 0, decode_time=0.0)
                
                logging.debug(f"Decoded: {decoded_text} (confidence: {quality_metrics['confidence']:.3f})")
                
                return decoded_text
            
        except Exception as e:
            self.stats['error_count'] += 1
            
            # –†–µ–∑–µ—Ä–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
            def fallback_decode():
                self.stats['fallback_uses'] += 1
                # –ü—Ä–æ—Å—Ç–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
                try:
                    basic_candidates = self.phrase_bank.search_phrases(embedding, k=3, min_similarity=0.5)
                    if basic_candidates:
                        return basic_candidates[0][0].text
                    return "Fallback: basic phrase match failed."
                except:
                    return self.config.default_fallback_text
            
            return self.error_handler.handle_error(e, "decode_main", fallback_fn=fallback_decode)
    
    def decode_with_metrics(self, embedding: torch.Tensor) -> Tuple[str, Dict]:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        if not self.ready:
            raise ValueError("Phrase bank not loaded.")
        
        # –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = self.phrase_bank.search_phrases(
            embedding, 
            k=self.config.max_candidates,
            min_similarity=self.config.similarity_threshold
        )
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_metrics = self.quality_assessor.assess_candidates(candidates)
        
        # –°–±–æ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞
        decoded_text = self.assembler.assemble(candidates)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        detailed_metrics = {
            **quality_metrics,
            'num_candidates': len(candidates),
            'top_similarity': candidates[0][1] if candidates else 0.0,
            'phrase_categories': [phrase.category for phrase, _ in candidates[:3]],
            'phrase_bank_stats': self.phrase_bank.get_statistics()
        }
        
        self._update_stats(quality_metrics, len(candidates) > 0)
        
        return decoded_text, detailed_metrics
    
    def batch_decode(self, embeddings: torch.Tensor) -> List[str]:
        """Batch –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if embeddings.dim() != 2 or embeddings.size(1) != self.embedding_dim:
            raise ValueError(f"Expected embeddings shape (N, {self.embedding_dim}), got {embeddings.shape}")
        
        results = []
        for i, embedding in enumerate(embeddings):
            try:
                result = self.decode(embedding)
                results.append(result)
            except Exception as e:
                logging.warning(f"Batch decode failed for item {i}: {e}")
                results.append("Batch decode error.")
        
        return results
    
    def get_statistics(self) -> Dict:
        """[START] Production-ready —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–∫–æ–¥–µ—Ä–∞"""
        success_rate = (
            self.stats['successful_decodings'] / max(self.stats['total_decodings'], 1) * 100
        )
        
        cache_hit_rate = (
            self.stats['cache_hits'] / max(self.stats['total_decodings'], 1) * 100
        )
        
        base_stats = {
            **self.stats,
            'success_rate': f"{success_rate:.1f}%",
            'cache_hit_rate': f"{cache_hit_rate:.1f}%",
            'phrase_bank_stats': self.phrase_bank.get_statistics(),
            'config': {
                'similarity_threshold': self.config.similarity_threshold,
                'assembly_method': self.config.assembly_method,
                'max_candidates': self.config.max_candidates,
                'caching_enabled': self.config.enable_caching,
                'fallbacks_enabled': self.config.enable_fallbacks
            }
        }
        
        # üÜï –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É production –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        if self.cache:
            base_stats['cache_stats'] = self.cache.get_stats()
        
        base_stats['error_stats'] = self.error_handler.get_error_stats()
        base_stats['performance_stats'] = self.performance_monitor.get_stats()
        
        return base_stats
    
    def _update_stats(self, quality_metrics: Dict, success: bool, decode_time: float = 0.0):
        """[START] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.stats['total_decodings'] += 1
        
        if success:
            self.stats['successful_decodings'] += 1
        
        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        total = self.stats['total_decodings']
        
        self.stats['avg_confidence'] = (
            (self.stats['avg_confidence'] * (total - 1) + quality_metrics.get('confidence', 0.0)) / total
        )
        
        self.stats['avg_quality'] = (
            (self.stats['avg_quality'] * (total - 1) + quality_metrics.get('quality_score', 0.0)) / total
        )
        
        # üÜï –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        if decode_time > 0:
            self.stats['avg_decode_time_ms'] = (
                (self.stats['avg_decode_time_ms'] * (total - 1) + decode_time * 1000) / total
            )
    
    def set_config(self, **kwargs):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                logging.info(f"Updated config: {key} = {value}")
            else:
                logging.warning(f"Unknown config parameter: {key}")
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        self.assembler = TextAssembler(self.config)
        self.quality_assessor = QualityAssessor(self.config)
    
    def start_new_session(self):
        """üÜï –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.assembler.reset_session()
        logging.info("Started new decoding session")
    
    def get_context_info(self) -> Dict:
        """üÜï –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
        return {
            'phrase_history_length': len(self.assembler.context_analyzer.phrase_history),
            'category_frequencies': dict(self.assembler.context_analyzer.category_frequencies),
            'current_length_preference': self.config.length_preference,
            'assembly_method': self.config.assembly_method
        }
    
    def decode_with_context_reset(self, embedding: torch.Tensor, reset_context: bool = False) -> str:
        """üÜï –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Å–±—Ä–æ—Å–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if reset_context:
            self.start_new_session()
        
        return self.decode(embedding)
    
    def batch_decode_with_sessions(self, embeddings: torch.Tensor, 
                                 session_boundaries: Optional[List[int]] = None) -> List[str]:
        """üÜï Batch –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–µ—Å—Å–∏–π"""
        if embeddings.dim() != 2 or embeddings.size(1) != self.embedding_dim:
            raise ValueError(f"Expected embeddings shape (N, {self.embedding_dim}), got {embeddings.shape}")
        
        results = []
        session_boundaries = session_boundaries or []
        
        for i, embedding in enumerate(embeddings):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–±—Ä–æ—Å–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if i in session_boundaries:
                self.start_new_session()
                logging.debug(f"Reset context at position {i}")
            
            try:
                result = self.decode(embedding)
                results.append(result)
            except Exception as e:
                logging.warning(f"Batch decode failed for item {i}: {e}")
                results.append("Batch decode error.")
        
        return results
    
    def clear_cache(self):
        """üÜï –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        if self.cache:
            self.cache.clear()
            logging.info("Cache cleared")
    
    def save_config(self, filepath: str):
        """üÜï –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        config_dict = {
            'embedding_dim': self.embedding_dim,
            'phrase_bank_size': self.phrase_bank_size,
            'similarity_threshold': self.similarity_threshold,
            'assembly_method': self.config.assembly_method,
            'enable_caching': self.config.enable_caching,
            'cache_size': self.config.cache_size,
            'enable_fallbacks': self.config.enable_fallbacks,
            'strict_mode': self.config.strict_mode,
            'default_fallback_text': self.config.default_fallback_text
        }
        
        with open(filepath, 'w') as f:
            json.dump(config_dict, f, indent=2)
        
        logging.info(f"Configuration saved to {filepath}")
    
    def load_config(self, filepath: str):
        """üÜï –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(filepath, 'r') as f:
            config_dict = json.load(f)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        for key, value in config_dict.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.assembler = TextAssembler(self.config)
        self.quality_assessor = QualityAssessor(self.config)
        
        if self.config.enable_caching and not self.cache:
            self.cache = PatternCache(max_size=self.config.cache_size)
        elif not self.config.enable_caching and self.cache:
            self.cache = None
        
        logging.info(f"Configuration loaded from {filepath}")
    
    def get_health_status(self) -> Dict:
        """üÜï –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        health = {
            'status': 'healthy',
            'ready': self.ready,
            'components': {
                'phrase_bank': self.phrase_bank is not None,
                'assembler': self.assembler is not None,
                'quality_assessor': self.quality_assessor is not None,
                'cache': self.cache is not None,
                'error_handler': self.error_handler is not None,
                'performance_monitor': self.performance_monitor is not None
            },
            'error_rate': 0.0,
            'cache_efficiency': 0.0
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞—Å—Ç–æ—Ç—ã –æ—à–∏–±–æ–∫
        total_ops = self.stats['total_decodings']
        if total_ops > 0:
            error_rate = (self.stats['error_count'] / total_ops) * 100
            health['error_rate'] = error_rate
            
            if error_rate > 10:  # –ë–æ–ª—å—à–µ 10% –æ—à–∏–±–æ–∫
                health['status'] = 'degraded'
            elif error_rate > 25:  # –ë–æ–ª—å—à–µ 25% –æ—à–∏–±–æ–∫
                health['status'] = 'unhealthy'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—ç—à–∞
        if self.cache and total_ops > 0:
            cache_efficiency = (self.stats['cache_hits'] / total_ops) * 100
            health['cache_efficiency'] = cache_efficiency
        
        return health
    
    def optimize_for_production(self):
        """[START] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω —Ä–µ–∂–∏–º–∞"""
        optimizations = []
        
        # –í–∫–ª—é—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω–æ
        if not self.config.enable_caching:
            self.config.enable_caching = True
            self.cache = PatternCache(max_size=self.config.cache_size)
            optimizations.append("Enabled caching")
        
        # –í–∫–ª—é—á–∞–µ–º fallbacks
        if not self.config.enable_fallbacks:
            self.config.enable_fallbacks = True
            optimizations.append("Enabled fallbacks")
        
        # –û—Ç–∫–ª—é—á–∞–µ–º strict mode
        if self.config.strict_mode:
            self.config.strict_mode = False
            optimizations.append("Disabled strict mode")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
        if self.config.cache_size < 500:
            self.config.cache_size = 1000
            if self.cache:
                self.cache.max_size = 1000
            optimizations.append("Increased cache size to 1000")
        
        # –í–∫–ª—é—á–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if not self.config.enable_performance_monitoring:
            self.config.enable_performance_monitoring = True
            self.performance_monitor = PerformanceMonitor(enabled=True)
            optimizations.append("Enabled performance monitoring")
        
        logging.info(f"Production optimizations applied: {', '.join(optimizations)}")
        return optimizations

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logger = logging.getLogger(__name__) 