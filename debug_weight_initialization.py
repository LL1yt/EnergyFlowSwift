#!/usr/bin/env python3
"""
üîß –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ò –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò –í–ï–°–û–í

–ü—Ä–æ–±–ª–µ–º–∞: Loss = 0.0000 —Å—Ä–∞–∑—É (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 5-10 –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ)
–¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤ –≤ gMLP —Å–∏—Å—Ç–µ–º–µ
"""

import sys
import os
sys.path.insert(0, os.path.abspath('.'))

import torch
import torch.nn as nn
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from training.embedding_trainer.emergent_training_stage_3_1_4_1 import (
    EmergentCubeTrainer, EmergentTrainingConfig
)
from training.embedding_trainer.dialogue_dataset import create_dialogue_dataset

class WeightInitializationDiagnostics:
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def run_diagnostics(self):
        """–ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        
        print("üîß –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò –í–ï–°–û–í")
        print("="*50)
        
        # 1. –°–æ–∑–¥–∞–µ–º trainer
        trainer = self._create_trainer()
        if trainer is None:
            return
            
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        self._check_initial_weights(trainer)
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º forward pass
        self._check_forward_pass(trainer)
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º loss computation
        self._check_loss_computation(trainer)
        
        # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        self._apply_proper_initialization(trainer)
        
        # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        self._verify_after_fix(trainer)
        
    def _create_trainer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ trainer –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        try:
            config = EmergentTrainingConfig()
            config.teacher_model = "distilbert-base-uncased"
            config.cube_dimensions = (15, 15, 11)
            config.mixed_precision = False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            
            trainer = EmergentCubeTrainer(config, device=str(self.device))
            
            total_params = sum(p.numel() for p in trainer.parameters())
            print(f"‚úÖ Trainer —Å–æ–∑–¥–∞–Ω: {total_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
            
            return trainer
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è trainer: {e}")
            return None
    
    def _check_initial_weights(self, trainer):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤"""
        print("\nüîç –ü–†–û–í–ï–†–ö–ê –ù–ê–ß–ê–õ–¨–ù–´–• –í–ï–°–û–í:")
        
        weight_stats = {}
        zero_params = 0
        total_params = 0
        
        for name, param in trainer.named_parameters():
            if param.requires_grad:
                weight_data = param.data
                total_params += param.numel()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                mean_val = weight_data.mean().item()
                std_val = weight_data.std().item()
                min_val = weight_data.min().item()
                max_val = weight_data.max().item()
                zero_count = (weight_data == 0).sum().item()
                
                weight_stats[name] = {
                    'mean': mean_val,
                    'std': std_val,
                    'min': min_val,
                    'max': max_val,
                    'zero_count': zero_count,
                    'total': param.numel(),
                    'zero_percentage': (zero_count / param.numel()) * 100
                }
                
                zero_params += zero_count
                
                # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–∏
                if abs(mean_val) < 1e-6 and std_val < 1e-6:
                    print(f"üö® –ü–†–û–ë–õ–ï–ú–ê: {name}")
                    print(f"   Mean: {mean_val:.8f}, Std: {std_val:.8f}")
                    print(f"   –í—Å–µ –≤–µ—Å–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω—É–ª–∏!")
                elif zero_count > param.numel() * 0.9:
                    print(f"‚ö†Ô∏è –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–û: {name}")
                    print(f"   {zero_count}/{param.numel()} ({weight_stats[name]['zero_percentage']:.1f}%) –Ω—É–ª–µ–≤—ã—Ö –≤–µ—Å–æ–≤")
                else:
                    print(f"‚úÖ –ù–û–†–ú–ê–õ–¨–ù–û: {name}")
                    print(f"   Mean: {mean_val:.6f}, Std: {std_val:.6f}, Range: [{min_val:.6f}, {max_val:.6f}]")
        
        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        print(f"   –ù—É–ª–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {zero_params:,} ({(zero_params/total_params)*100:.1f}%)")
        
        return weight_stats
    
    def _check_forward_pass(self, trainer):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ forward pass"""
        print("\n‚ö° –ü–†–û–í–ï–†–ö–ê FORWARD PASS:")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π input
        batch_size = 2
        input_dim = 768  # DistilBERT
        test_input = torch.randn(batch_size, input_dim).to(self.device)
        
        print(f"   Input: shape={test_input.shape}, mean={test_input.mean().item():.6f}")
        
        with torch.no_grad():
            outputs = trainer.forward(test_input)
        
        for key, tensor in outputs.items():
            if torch.is_tensor(tensor):
                mean_val = tensor.mean().item()
                std_val = tensor.std().item()
                zero_count = (tensor == 0).sum().item()
                total_elements = tensor.numel()
                
                print(f"   {key}: shape={tensor.shape}")
                print(f"      Mean: {mean_val:.6f}, Std: {std_val:.6f}")
                print(f"      Zeros: {zero_count}/{total_elements} ({(zero_count/total_elements)*100:.1f}%)")
                
                if abs(mean_val) < 1e-6 and std_val < 1e-6:
                    print(f"      üö® –ü–†–û–ë–õ–ï–ú–ê: –í—ã—Ö–æ–¥ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω—É–ª–µ–≤–æ–π!")
                elif zero_count > total_elements * 0.9:
                    print(f"      ‚ö†Ô∏è –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–û: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω—É–ª–µ–π")
                else:
                    print(f"      ‚úÖ –ù–û–†–ú–ê–õ–¨–ù–û: –†–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        
        return outputs
    
    def _check_loss_computation(self, trainer):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss"""
        print("\nüí∞ –ü–†–û–í–ï–†–ö–ê LOSS COMPUTATION:")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        dialogue_pairs = [
            {"question": "What is AI?", "answer": "AI is artificial intelligence."}
        ]
        
        dataset = create_dialogue_dataset(
            dialogue_pairs,
            teacher_model="distilbert-base-uncased",
            cache_embeddings=False,
            validation_split=0.0,
            normalize_embeddings=True
        )
        
        sample = dataset[0]
        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ dataset - —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å tuple
        if isinstance(sample, tuple):
            question_emb, answer_emb = sample
            question_emb = question_emb.unsqueeze(0).to(self.device)
            answer_emb = answer_emb.unsqueeze(0).to(self.device)
        else:
            question_emb = sample['question_embedding'].unsqueeze(0).to(self.device)
            answer_emb = sample['answer_embedding'].unsqueeze(0).to(self.device)
        
        print(f"   Question embedding: shape={question_emb.shape}, norm={question_emb.norm().item():.6f}")
        print(f"   Answer embedding: shape={answer_emb.shape}, norm={answer_emb.norm().item():.6f}")
        
        # Forward pass
        outputs = trainer.forward(question_emb)
        
        # Targets
        targets = {
            'target_embedding': answer_emb,
            'target_surface': outputs['input_surface']
        }
        
        # Compute loss
        losses = trainer.compute_loss(outputs, targets)
        
        print(f"   Loss components:")
        for key, loss_tensor in losses.items():
            if torch.is_tensor(loss_tensor):
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª—É—á–∞–π –∫–æ–≥–¥–∞ —Ç–µ–Ω–∑–æ—Ä –Ω–µ —Å–∫–∞–ª—è—Ä–Ω—ã–π
                if loss_tensor.numel() == 1:
                    loss_val = loss_tensor.item()
                else:
                    loss_val = loss_tensor.mean().item()  # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –º–Ω–æ–≥–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
                    print(f"      {key}: {loss_val:.6f} (multi-element tensor)")
                    continue
                
                print(f"      {key}: {loss_val:.6f}")
                
                if loss_val == 0.0:
                    print(f"         üö® –ü–†–û–ë–õ–ï–ú–ê: {key} = 0.0!")
                    self._debug_zero_loss_component(key, outputs, targets, trainer)
                else:
                    print(f"         ‚úÖ –ù–û–†–ú–ê–õ–¨–ù–û: {key} > 0")
        
        return losses
    
    def _debug_zero_loss_component(self, component: str, outputs, targets, trainer):
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω—É–ª–µ–≤–æ–≥–æ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞"""
        print(f"         üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ {component}:")
        
        if component == 'dialogue_similarity_loss':
            final_output = outputs['final_output']
            target_embedding = targets['target_embedding']
            
            print(f"            final_output: shape={final_output.shape}, norm={final_output.norm().item():.6f}")
            print(f"            target_embedding: shape={target_embedding.shape}, norm={target_embedding.norm().item():.6f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–µ–∫—Ü–∏—é
            if hasattr(trainer.loss_function, 'embedding_to_surface'):
                projected_target = trainer.loss_function.embedding_to_surface(target_embedding)
                print(f"            projected_target: shape={projected_target.shape}, norm={projected_target.norm().item():.6f}")
                
                # –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç cosine similarity
                cos_sim = torch.nn.functional.cosine_similarity(final_output, projected_target, dim=-1)
                dialogue_loss = 1.0 - torch.mean(cos_sim)
                
                print(f"            cosine_similarity: {torch.mean(cos_sim).item():.6f}")
                print(f"            manual_dialogue_loss: {dialogue_loss.item():.6f}")
                
                if torch.mean(cos_sim).item() == 1.0:
                    print(f"            üö® –ü–†–û–ë–õ–ï–ú–ê: Perfect similarity - –≤–æ–∑–º–æ–∂–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã!")
                    print(f"            final_output[0][:5]: {final_output[0][:5]}")
                    print(f"            projected_target[0][:5]: {projected_target[0][:5]}")
    
    def _apply_proper_initialization(self, trainer):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤"""
        print("\nüîß –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –ü–†–ê–í–ò–õ–¨–ù–û–ô –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò:")
        
        def init_weights(module):
            if isinstance(module, nn.Linear):
                # Xavier/Glorot initialization –¥–ª—è Linear layers
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)  # –ù–µ–±–æ–ª—å—à–æ–π bias
                print(f"   –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω Linear: {module.weight.shape}")
                
            elif isinstance(module, nn.Conv3d):
                # Kaiming initialization –¥–ª—è Conv layers
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)
                print(f"   –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω Conv3d: {module.weight.shape}")
                
            elif isinstance(module, nn.BatchNorm3d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
                print(f"   –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω BatchNorm3d")
                
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
                print(f"   –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω LayerNorm")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∫–æ –≤—Å–µ–º –º–æ–¥—É–ª—è–º
        trainer.apply(init_weights)
        
        print("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ –≤—Å–µ–º —Å–ª–æ—è–º")
    
    def _verify_after_fix(self, trainer):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        print("\n‚úÖ –ü–†–û–í–ï–†–ö–ê –ü–û–°–õ–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Å–∞ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        print("   –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏:")
        
        sample_weights = {}
        for name, param in trainer.named_parameters():
            if 'weight' in name and param.requires_grad:
                mean_val = param.data.mean().item()
                std_val = param.data.std().item()
                sample_weights[name] = (mean_val, std_val)
                
                if len(sample_weights) <= 5:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                    print(f"      {name}: mean={mean_val:.6f}, std={std_val:.6f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º loss –µ—â–µ —Ä–∞–∑
        print("   –ü—Ä–æ–≤–µ—Ä–∫–∞ loss –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏:")
        
        dialogue_pairs = [
            {"question": "What is AI?", "answer": "AI is artificial intelligence."}
        ]
        
        dataset = create_dialogue_dataset(
            dialogue_pairs,
            teacher_model="distilbert-base-uncased",
            cache_embeddings=False,
            validation_split=0.0,
            normalize_embeddings=True
        )
        
        sample = dataset[0]
        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ dataset - —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å tuple
        if isinstance(sample, tuple):
            question_emb, answer_emb = sample
            question_emb = question_emb.unsqueeze(0).to(self.device)
            answer_emb = answer_emb.unsqueeze(0).to(self.device)
        else:
            question_emb = sample['question_embedding'].unsqueeze(0).to(self.device)
            answer_emb = sample['answer_embedding'].unsqueeze(0).to(self.device)
        
        outputs = trainer.forward(question_emb)
        targets = {
            'target_embedding': answer_emb,
            'target_surface': outputs['input_surface']
        }
        
        losses = trainer.compute_loss(outputs, targets)
        
        print("   Loss –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
        for key, loss_tensor in losses.items():
            if torch.is_tensor(loss_tensor):
                loss_val = loss_tensor.item()
                if loss_val > 0.01:
                    print(f"      ‚úÖ {key}: {loss_val:.6f} (–•–û–†–û–®–û - –±–æ–ª—å—à–µ 0!)")
                elif loss_val > 0.0:
                    print(f"      ‚ö†Ô∏è {key}: {loss_val:.6f} (–º–∞–ª–µ–Ω—å–∫–∏–π –Ω–æ –Ω–µ 0)")
                else:
                    print(f"      ‚ùå {key}: {loss_val:.6f} (–≤—Å–µ –µ—â–µ 0)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self._save_fixed_trainer(trainer)
        
    def _save_fixed_trainer(self, trainer):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        save_path = "checkpoints/fixed_initialization_trainer.pt"
        os.makedirs("checkpoints", exist_ok=True)
        
        torch.save({
            'model_state_dict': trainer.state_dict(),
            'config': trainer.config,
            'note': 'Fixed weight initialization - should have non-zero loss'
        }, save_path)
        
        print(f"\nüíæ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        print("   –≠—Ç–∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤")
        print("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ—ë –¥–ª—è overnight training")

def main():
    """–ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
    diagnostics = WeightInitializationDiagnostics()
    diagnostics.run_diagnostics()
    
    print("\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏")
    print("2. –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã - –æ–Ω–∏ –±—É–¥—É—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã")
    print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è overnight training")
    print("4. Loss –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∞—Ç—å—Å—è —Å ~5-10 –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞—Ç—å—Å—è")

if __name__ == "__main__":
    main() 