"""
Gated MLP Cell - 2024/2025 State-of-the-Art Architecture
========================================================

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞:
- Google Research gMLP (Spatial Gating Unit)
- Meta AI sparse MLP efficiency research
- Bio-inspired cortical column processing

–ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏:
- Spatial Gating Unit –∑–∞–º–µ–Ω—è–µ—Ç attention —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ
- –õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å O(n) vs O(n¬≤) —É Transformer
- –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Å–µ–¥—Å—Ç–≤–∞
- Memory component –¥–ª—è emergent behavior

Target: ~25K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –∫–ª–µ—Ç–∫—É (vs 1K –≤ –ø—Ä–æ—Å—Ç–æ–π MLP)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any
import math
import logging

logger = logging.getLogger(__name__)


class SpatialGatingUnit(nn.Module):
    """
    Spatial Gating Unit (SGU) - –∫–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è gMLP

    –ó–∞–º–µ–Ω—è–µ—Ç self-attention –º–µ—Ö–∞–Ω–∏–∑–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è 3D cellular neural networks.
    """

    def __init__(
        self, dim: int, seq_len: int = 6, init_eps: float = 1e-3  # 6 —Å–æ—Å–µ–¥–µ–π –≤ 3D
    ):
        super().__init__()

        self.dim = dim
        self.seq_len = seq_len

        # Spatial projection –¥–ª—è neighbor interactions
        self.spatial_proj = nn.Linear(seq_len, seq_len)

        # Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        self.norm = nn.LayerNorm(dim)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–ª–∏–∑–∫–∞—è –∫ identity (—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
        self.spatial_proj.weight.data.uniform_(-init_eps, init_eps)
        self.spatial_proj.bias.data.fill_(1.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, seq_len, dim] - neighbor states + own state
        Returns:
            Gated tensor —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """
        u, v = x.chunk(2, dim=-1)  # Split into gate and value

        # –ü—Ä–∏–º–µ–Ω—è–µ–º spatial gating
        v = self.norm(v)  # Normalize value part
        v = v.permute(0, 2, 1)  # [batch, dim, seq_len] –¥–ª—è spatial projection
        v = self.spatial_proj(v)  # Spatial interactions
        v = v.permute(0, 2, 1)  # Back to [batch, seq_len, dim]

        # Gate mechanism: u controls information flow
        return u * v


class GatedMLPCell(nn.Module):
    """
    Gated MLP Cell - —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è cellular networks

    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. Input processing (neighbor embedding)
    2. Spatial Gating Unit (key innovation)
    3. Memory state management (GRU)
    4. Output projection

    –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∞–Ω–∞–ª–æ–≥–∏—è:
    - Input processing = –¥–µ–Ω–¥—Ä–∏—Ç—ã (–ø—Ä–∏–µ–º —Å–∏–≥–Ω–∞–ª–æ–≤)
    - SGU = —Å–æ–º–∞ (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞)
    - Memory = –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–æ—Ç–µ–Ω—Ü–∏–∞—Ü–∏—è
    - Output = –∞–∫—Å–æ–Ω (–ø–µ—Ä–µ–¥–∞—á–∞ —Å–∏–≥–Ω–∞–ª–∞)
    """

    def __init__(
        self,
        state_size: int = 32,  # –†–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–ª–µ—Ç–∫–∏
        neighbor_count: int = 6,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π
        hidden_dim: int = 128,  # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (OPTIMIZED –¥–ª—è 25K)
        external_input_size: int = 12,  # –†–∞–∑–º–µ—Ä –≤–Ω–µ—à–Ω–µ–≥–æ –≤—Ö–æ–¥–∞
        activation: str = "gelu",  # –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è
        dropout: float = 0.1,  # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        use_memory: bool = True,  # Memory component
        memory_dim: int = 32,  # –†–∞–∑–º–µ—Ä memory state (OPTIMIZED)
        target_params: int = 25000,
    ):  # –ù–û–í–û–ï: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π target (–∏–∑ dynamic config)

        super().__init__()

        # === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
        self.state_size = state_size
        self.neighbor_count = neighbor_count
        self.hidden_dim = hidden_dim
        self.external_input_size = external_input_size
        self.use_memory = use_memory
        self.memory_dim = memory_dim
        self.target_params = target_params  # –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π target

        # === –í–´–ß–ò–°–õ–ï–ù–ù–´–ï –†–ê–ó–ú–ï–†–´ ===
        neighbor_input_size = neighbor_count * state_size  # –í—Ö–æ–¥—ã –æ—Ç —Å–æ—Å–µ–¥–µ–π
        total_input_size = neighbor_input_size + state_size + external_input_size

        # === INPUT PROCESSING LAYER ===
        self.input_norm = nn.LayerNorm(total_input_size)
        self.input_projection = nn.Linear(total_input_size, hidden_dim)

        # === SPATIAL GATING UNIT (–ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è) ===
        # –£–¥–≤–∞–∏–≤–∞–µ–º hidden_dim –¥–ª—è gate/value split
        self.pre_gating = nn.Linear(hidden_dim, hidden_dim * 2)
        self.spatial_gating = SpatialGatingUnit(
            dim=hidden_dim, seq_len=neighbor_count + 1  # +1 –¥–ª—è own state
        )

        # === FEED FORWARD NETWORK ===
        if activation == "gelu":
            self.activation = nn.GELU()
        elif activation == "swiglu":
            # SwiGLU —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ LLaMA)
            self.activation = SwiGLU(hidden_dim)
        else:
            self.activation = nn.ReLU()

        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            self.activation,
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Dropout(dropout),
        )

        # === MEMORY COMPONENT ===
        if use_memory:
            self.memory_gate = nn.GRU(
                input_size=hidden_dim, hidden_size=memory_dim, batch_first=True
            )
            self.memory_to_output = nn.Linear(memory_dim, hidden_dim)

        # === OUTPUT PROJECTION ===
        self.output_norm = nn.LayerNorm(hidden_dim)
        self.output_projection = nn.Linear(hidden_dim, state_size)

        # === RESIDUAL CONNECTIONS ===
        self.input_residual = (
            nn.Linear(total_input_size, state_size)
            if total_input_size != state_size
            else nn.Identity()
        )

        # === INTERNAL STATE ===
        self.memory_state = None  # Persistent memory –º–µ–∂–¥—É forward calls

        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —Å–æ–∑–¥–∞–Ω–∏–∏)
        if not hasattr(GatedMLPCell, "_param_count_logged"):
            self._log_parameter_count()
            GatedMLPCell._param_count_logged = True

    def _log_parameter_count(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        logger.info(f"‚úÖ GatedMLPCell –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        logger.info(f"   Total: {total_params:,} parameters")
        logger.info(f"   Trainable: {trainable_params:,} parameters")
        logger.info(f"   Target: ~{self.target_params:,} (current: {total_params:,})")

        if total_params > self.target_params * 1.2:  # 20% –¥–æ–ø—É—Å–∫
            logger.warning(
                f"‚ö†Ô∏è  Parameter count –ø—Ä–µ–≤—ã—à–∞–µ—Ç target {self.target_params:,}: {total_params:,}"
            )

    def forward(
        self,
        neighbor_states: torch.Tensor,
        own_state: torch.Tensor,
        external_input: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass gMLP –∫–ª–µ—Ç–∫–∏

        Args:
            neighbor_states: [batch, neighbor_count, state_size] - —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–æ—Å–µ–¥–µ–π
            own_state: [batch, state_size] - —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            external_input: [batch, external_input_size] - –≤–Ω–µ—à–Ω–∏–π –≤—Ö–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            new_state: [batch, state_size] - –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–µ—Ç–∫–∏
        """
        batch_size = own_state.shape[0]

        # === –≠–¢–ê–ü 1: INPUT PREPARATION ===

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ neighbor states
        if neighbor_states.numel() > 0:
            neighbor_flat = neighbor_states.view(batch_size, -1)  # Flatten neighbors
        else:
            # –ï—Å–ª–∏ —Å–æ—Å–µ–¥–µ–π –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
            neighbor_flat = torch.zeros(
                batch_size,
                self.neighbor_count * self.state_size,
                device=own_state.device,
                dtype=own_state.dtype,
            )

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ external input
        if external_input is None:
            external_input = torch.zeros(
                batch_size,
                self.external_input_size,
                device=own_state.device,
                dtype=own_state.dtype,
            )

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤
        combined_input = torch.cat(
            [
                neighbor_flat,  # –°–∏–≥–Ω–∞–ª—ã –æ—Ç —Å–æ—Å–µ–¥–µ–π
                own_state,  # –°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                external_input,  # –í–Ω–µ—à–Ω–∏–π —Å–∏–≥–Ω–∞–ª
            ],
            dim=1,
        )

        # === –≠–¢–ê–ü 2: INPUT PROCESSING ===
        x = self.input_norm(combined_input)
        x = self.input_projection(x)  # [batch, hidden_dim]

        # === –≠–¢–ê–ü 3: SPATIAL GATING UNIT ===

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è spatial gating - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ spatial positions
        x_gating = self.pre_gating(x)  # [batch, hidden_dim * 2]

        # Reshape –¥–ª—è spatial processing
        # –°–æ–∑–¥–∞–µ–º "spatial sequence": [neighbors + own_state]
        spatial_seq = x_gating.unsqueeze(1).expand(-1, self.neighbor_count + 1, -1)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º spatial gating
        x_gated = self.spatial_gating(spatial_seq)  # [batch, seq_len, hidden_dim]

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º spatial sequence
        x = x_gated.mean(dim=1)  # [batch, hidden_dim]

        # === –≠–¢–ê–ü 4: FEED FORWARD PROCESSING ===
        x_residual = x
        x = self.ffn(x)
        x = x + x_residual  # Residual connection

        # === –≠–¢–ê–ü 5: MEMORY PROCESSING ===
        if self.use_memory:
            # –û–±–Ω–æ–≤–ª—è–µ–º memory state
            x_memory = x.unsqueeze(1)  # [batch, 1, hidden_dim] –¥–ª—è GRU

            if self.memory_state is None or self.memory_state.size(1) != batch_size:
                # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è batch size - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º memory
                self.memory_state = torch.zeros(
                    1, batch_size, self.memory_dim, device=x.device, dtype=x.dtype
                )

            # GRU memory update
            memory_output, new_memory_state = self.memory_gate(
                x_memory, self.memory_state
            )
            memory_output = memory_output.squeeze(1)  # [batch, memory_dim]

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –î–µ—Ç–∞—á–∏–º memory_state –æ—Ç computational graph
            self.memory_state = new_memory_state.detach()

            # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º memory –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫
            memory_contribution = self.memory_to_output(memory_output)
            x = x + memory_contribution

        # === –≠–¢–ê–ü 6: OUTPUT PROJECTION ===
        x = self.output_norm(x)
        new_state = self.output_projection(x)

        # === RESIDUAL CONNECTION ===
        # Residual –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ input –∫ output
        input_residual = self.input_residual(combined_input)
        new_state = new_state + input_residual * 0.1  # Scaled residual

        return new_state

    def reset_memory(self):
        """–°–±—Ä–æ—Å memory state (–¥–ª—è –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)"""
        self.memory_state = None

    def get_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–µ—Ç–∫–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        total_params = sum(p.numel() for p in self.parameters())

        return {
            "architecture": "GatedMLP",
            "state_size": self.state_size,
            "neighbor_count": self.neighbor_count,
            "hidden_dim": self.hidden_dim,
            "memory_enabled": self.use_memory,
            "total_parameters": total_params,
            "target_parameters": self.target_params,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π target
            "parameter_efficiency": total_params
            / max(1, self.target_params),  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
            "memory_state_active": self.memory_state is not None,
        }


class SwiGLU(nn.Module):
    """
    SwiGLU Activation - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –∏–∑ LLaMA/GLU family
    –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —á–µ–º GELU –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
    """

    def __init__(self, dim: int):
        super().__init__()
        self.linear = nn.Linear(dim, dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x, gate = x.chunk(2, dim=-1)
        return x * F.silu(self.linear(gate))


def create_gmlp_cell_from_config(config: Dict[str, Any]) -> GatedMLPCell:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ gMLP –∫–ª–µ—Ç–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è cell_prototype

    Returns:
        GatedMLPCell: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∫–ª–µ—Ç–∫–∞
    """
    cell_config = config.get("cell_prototype", {})
    arch_config = cell_config.get("architecture", {})

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å defaults
    params = {
        "state_size": cell_config.get("state_size", 32),
        "neighbor_count": cell_config.get("num_neighbors", 6),
        "hidden_dim": arch_config.get("hidden_dim", 512),
        "external_input_size": cell_config.get("input_size", 12),
        "activation": arch_config.get("activation", "gelu"),
        "dropout": arch_config.get("dropout", 0.1),
        "use_memory": arch_config.get("use_memory", True),
        "memory_dim": arch_config.get("memory_dim", 128),
    }

    logger.info(f"üî¨ –°–æ–∑–¥–∞–Ω–∏–µ GatedMLPCell —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")

    return GatedMLPCell(**params)


def test_gmlp_cell_basic() -> bool:
    """
    –ë–∞–∑–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ gMLP –∫–ª–µ—Ç–∫–∏

    Returns:
        bool: True –µ—Å–ª–∏ –≤—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏
    """
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GatedMLPCell...")

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–µ—Ç–∫–∏
        cell = GatedMLPCell(
            state_size=32,
            neighbor_count=6,
            hidden_dim=256,  # –ú–µ–Ω—å—à–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            external_input_size=12,
        )

        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size = 4
        neighbor_states = torch.randn(batch_size, 6, 32)
        own_state = torch.randn(batch_size, 32)
        external_input = torch.randn(batch_size, 12)

        # Forward pass
        new_state = cell(neighbor_states, own_state, external_input)

        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert new_state.shape == (
            batch_size,
            32,
        ), f"Wrong output shape: {new_state.shape}"
        assert not torch.isnan(new_state).any(), "NaN values in output"
        assert not torch.isinf(new_state).any(), "Inf values in output"

        # –¢–µ—Å—Ç memory reset
        cell.reset_memory()
        assert cell.memory_state is None, "Memory not reset"

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–µ—Ç–∫–µ
        info = cell.get_info()
        logger.info(f"‚úÖ gMLP Cell —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: {info['total_parameters']} params")

        return True

    except Exception as e:
        logger.error(f"‚ùå gMLP Cell —Ç–µ—Å—Ç failed: {e}")
        return False


if __name__ == "__main__":
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –º–æ–¥—É–ª—è
    logging.basicConfig(level=logging.INFO)
    success = test_gmlp_cell_basic()
    print(f"gMLP Cell test: {'‚úÖ PASSED' if success else '‚ùå FAILED'}")
