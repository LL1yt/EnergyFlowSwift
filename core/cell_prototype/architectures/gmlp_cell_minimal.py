#!/usr/bin/env python3
"""
Minimal Gated MLP Cell - —Ü–µ–ª–µ–≤—ã–µ 10K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ë–ï–ó memory
===========================================================

–ö–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–≤—è–∑–µ–π:
- Target: ~10,000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ë–ï–ó –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏ (–ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é distributed memory)
- –†–µ—à–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ Input Residual —á–µ—Ä–µ–∑ bottleneck architecture
- Shared weights architecture –¥–ª—è –≤—Å–µ—Ö –∫–ª–µ—Ç–æ–∫ —Ä–µ—à–µ—Ç–∫–∏

–§–∏–ª–æ—Å–æ—Ñ–∏—è: Memory = topology + connection_weights, –ù–ï –ª–æ–∫–∞–ª—å–Ω—ã–π GRU
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class MinimalSpatialGatingUnit(nn.Module):
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è Spatial Gating Unit
    –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –±–µ–∑ memory overhead
    """

    def __init__(self, dim: int, seq_len: int = 27):
        super().__init__()
        self.dim = dim
        self.seq_len = seq_len

        # –¢–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.spatial_proj = nn.Linear(
            seq_len, seq_len, bias=False
        )  # –ë–µ–∑ bias –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
        self.norm = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Minimal spatial gating"""
        u, v = x.chunk(2, dim=-1)

        # Spatial transformation
        v = v.transpose(-1, -2)
        v = self.spatial_proj(v)
        v = v.transpose(-1, -2)

        # Simple gating
        out = u * torch.sigmoid(v)  # –ü—Ä–æ—Å—Ç–æ–π gating vs —Å–ª–æ–∂–Ω—ã–π
        out = self.norm(out)

        return out


class MinimalGatedMLPCell(nn.Module):
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è Gated MLP Cell –¥–ª—è shared weights architecture

    Target: ~10,000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    Philosophy: Spatial distributed memory > Local memory

    –†–µ—à–µ–Ω–∏–µ Input Residual –ø—Ä–æ–±–ª–µ–º—ã:
    - Bottleneck architecture: high-dim input ‚Üí low-dim processing ‚Üí output
    - –°–º—ã—Å–ª–æ–≤–æ–µ —Å–∂–∞—Ç–∏–µ –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ residual connection
    """

    def __init__(
        self,
        state_size: int = 36,
        neighbor_count: int = 26,
        hidden_dim: int = 32,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è bottleneck
        bottleneck_dim: int = 16,  # –ù–û–í–û–ï: –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
        external_input_size: int = 4,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π external input
        activation: str = "gelu",
        dropout: float = 0.0,  # –£–±–∏—Ä–∞–µ–º dropout –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
        target_params: int = 10000,
    ):
        super().__init__()

        # === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
        self.state_size = state_size
        self.neighbor_count = neighbor_count
        self.hidden_dim = hidden_dim
        self.bottleneck_dim = bottleneck_dim
        self.external_input_size = external_input_size
        self.target_params = target_params

        # === –í–´–ß–ò–°–õ–ï–ù–ù–´–ï –†–ê–ó–ú–ï–†–´ ===
        neighbor_input_size = neighbor_count * state_size  # 26 * 36 = 936
        total_input_size = neighbor_input_size + state_size + external_input_size  # 980

        # === BOTTLENECK INPUT PROCESSING ===
        # –†–µ—à–µ–Ω–∏–µ Input Residual –ø—Ä–æ–±–ª–µ–º—ã: —Å–∂–∏–º–∞–µ–º –≤ bottleneck
        self.input_norm = nn.LayerNorm(total_input_size)
        self.input_bottleneck = nn.Linear(
            total_input_size, bottleneck_dim, bias=False
        )  # 980*16 = 15,680
        self.bottleneck_to_hidden = nn.Linear(
            bottleneck_dim, hidden_dim
        )  # 16*32 + 32 = 544

        # === SPATIAL GATING UNIT ===
        self.pre_gating = nn.Linear(
            hidden_dim, hidden_dim * 2, bias=False
        )  # 32*64 = 2,048
        self.spatial_gating = MinimalSpatialGatingUnit(
            dim=hidden_dim, seq_len=neighbor_count + 1  # 27
        )

        # === MINIMAL FFN ===
        if activation == "gelu":
            self.activation = nn.GELU()
        else:
            self.activation = nn.ReLU()

        # –û—á–µ–Ω—å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim, bias=False),  # 32*32 = 1,024
            self.activation,
        )

        # === OUTPUT PROJECTION ===
        self.output_norm = nn.LayerNorm(hidden_dim)
        self.output_projection = nn.Linear(hidden_dim, state_size)  # 32*36 + 36 = 1,188

        # === –°–ú–´–°–õ–û–í–û–ô RESIDUAL (–≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ) ===
        # –í–º–µ—Å—Ç–æ input_residual –∏—Å–ø–æ–ª—å–∑—É–µ–º compressed residual
        self.compressed_residual = nn.Linear(
            bottleneck_dim, state_size, bias=False
        )  # 16*36 = 576

        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self._log_parameter_count()

    def _log_parameter_count(self):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        total_params = sum(p.numel() for p in self.parameters())

        logger.info(f"[MINIMAL] MinimalGatedMLPCell –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        logger.info(f"   Total: {total_params:,} parameters")
        logger.info(f"   Target: {self.target_params:,} parameters")
        logger.info(f"   Efficiency: {total_params/self.target_params:.3f}x target")

        # Breakdown –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
        component_params = {}
        for name, param in self.named_parameters():
            component = name.split(".")[0] if "." in name else name
            if component not in component_params:
                component_params[component] = 0
            component_params[component] += param.numel()

        logger.info(f"[BREAKDOWN] Parameter distribution:")
        for component, count in sorted(
            component_params.items(), key=lambda x: x[1], reverse=True
        ):
            percentage = (count / total_params) * 100
            logger.info(f"   {component}: {count:,} params ({percentage:.1f}%)")

        # –°—Ç–∞—Ç—É—Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ target
        if total_params <= self.target_params * 1.1:  # 10% tolerance
            logger.info(f"[SUCCESS] ‚úÖ Parameter count within target!")
        else:
            excess = total_params - self.target_params
            logger.warning(
                f"[OVER] ‚ö†Ô∏è Exceeds target by {excess:,} ({total_params/self.target_params:.2f}x)"
            )

    def forward(
        self,
        neighbor_states: torch.Tensor,
        own_state: torch.Tensor,
        connection_weights: torch.Tensor,
        external_input: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Minimal forward pass —Å bottleneck architecture
        """
        batch_size = own_state.shape[0]

        # === –≠–¢–ê–ü 1: INPUT PREPARATION ===
        weighted_neighbor_states = neighbor_states * connection_weights.unsqueeze(-1)

        if weighted_neighbor_states.numel() > 0:
            neighbor_flat = weighted_neighbor_states.view(batch_size, -1)
        else:
            neighbor_flat = torch.zeros(
                batch_size,
                self.neighbor_count * self.state_size,
                device=own_state.device,
                dtype=own_state.dtype,
            )

        if external_input is None:
            external_input = torch.zeros(
                batch_size,
                self.external_input_size,
                device=own_state.device,
                dtype=own_state.dtype,
            )

        # Combine inputs
        combined_input = torch.cat([neighbor_flat, own_state, external_input], dim=1)

        # === –≠–¢–ê–ü 2: BOTTLENECK PROCESSING ===
        x = self.input_norm(combined_input)

        # Bottleneck compression (—Ä–µ—à–µ–Ω–∏–µ Input Residual –ø—Ä–æ–±–ª–µ–º—ã)
        x_compressed = self.input_bottleneck(x)  # [batch, bottleneck_dim]
        x = self.bottleneck_to_hidden(x_compressed)  # [batch, hidden_dim]

        # === –≠–¢–ê–ü 3: SPATIAL GATING ===
        x_gating = self.pre_gating(x)  # [batch, hidden_dim * 2]

        # Reshape –¥–ª—è spatial processing
        spatial_seq = x_gating.unsqueeze(1).expand(-1, self.neighbor_count + 1, -1)
        x_gated = self.spatial_gating(spatial_seq)

        # Aggregate
        x = x_gated.mean(dim=1)

        # === –≠–¢–ê–ü 4: MINIMAL FFN ===
        x = x + self.ffn(x)  # Simple residual

        # === –≠–¢–ê–ü 5: OUTPUT ===
        x = self.output_norm(x)
        new_state = self.output_projection(x)

        # === –°–ú–´–°–õ–û–í–û–ô RESIDUAL ===
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º compressed representation –¥–ª—è residual
        compressed_residual = self.compressed_residual(x_compressed)
        new_state = new_state + compressed_residual * 0.1

        return new_state

    def reset_memory(self):
        """Compatibility method (no-op, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç memory)"""
        pass

    def get_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–µ—Ç–∫–µ"""
        total_params = sum(p.numel() for p in self.parameters())

        return {
            "architecture": "MinimalGatedMLP",
            "state_size": self.state_size,
            "neighbor_count": self.neighbor_count,
            "hidden_dim": self.hidden_dim,
            "bottleneck_dim": self.bottleneck_dim,
            "memory_enabled": False,  # –ü—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –ù–ï–¢ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏
            "distributed_memory": True,  # –ü–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ spatial topology
            "total_parameters": total_params,
            "target_parameters": self.target_params,
            "parameter_efficiency": total_params / max(1, self.target_params),
            "optimization_level": "10K_minimal_bottleneck",
        }


def create_minimal_gmlp_cell_from_config(config: Dict[str, Any]) -> MinimalGatedMLPCell:
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π gMLP –∫–ª–µ—Ç–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    cell_config = config.get("cell_prototype", {})
    arch_config = cell_config.get("architecture", {})

    params = {
        "state_size": cell_config.get("state_size", 36),
        "neighbor_count": cell_config.get("num_neighbors", 26),
        "hidden_dim": arch_config.get("hidden_dim", 32),
        "bottleneck_dim": arch_config.get("bottleneck_dim", 16),
        "external_input_size": cell_config.get("input_size", 4),
        "activation": arch_config.get("activation", "gelu"),
        "dropout": arch_config.get("dropout", 0.0),
        "target_params": arch_config.get("target_params", 10000),
    }

    logger.info(f"üß¨ Creating MinimalGatedMLPCell: {params}")
    return MinimalGatedMLPCell(**params)


def test_minimal_gmlp_cell() -> bool:
    """–¢–µ—Å—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∫–ª–µ—Ç–∫–∏"""
    try:
        cell = MinimalGatedMLPCell()

        # Test data
        batch_size = 4
        neighbor_states = torch.randn(batch_size, 26, 36)
        own_state = torch.randn(batch_size, 36)
        connection_weights = torch.randn(batch_size, 26)
        external_input = torch.randn(batch_size, 4)

        # Forward pass
        output = cell(neighbor_states, own_state, connection_weights, external_input)

        logger.info(f"[TEST] Forward pass successful: {output.shape}")
        logger.info(f"[TEST] Info: {cell.get_info()}")

        return True

    except Exception as e:
        logger.error(f"[TEST FAILED] {e}")
        return False


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    print("üß™ Testing Minimal GatedMLPCell...")
    success = test_minimal_gmlp_cell()
    print(f"Result: {'‚úÖ SUCCESS' if success else '‚ùå FAILED'}")
