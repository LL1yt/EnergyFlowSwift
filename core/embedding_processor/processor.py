"""
EmbeddingProcessor - –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
=======================================================

–Ø–î–†–û Phase 2.5 - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –≥–æ—Ç–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:
1. –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ (768D) ‚Üí EmbeddingReshaper.vector_to_matrix() ‚Üí 3D –º–∞—Ç—Ä–∏—Ü–∞ (8√ó8√ó12)
2. 3D –º–∞—Ç—Ä–∏—Ü–∞ ‚Üí Lattice3D.forward() ‚Üí –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 3D –º–∞—Ç—Ä–∏—Ü–∞  
3. –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 3D –º–∞—Ç—Ä–∏—Ü–∞ ‚Üí EmbeddingReshaper.matrix_to_vector() ‚Üí –≤—ã—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ (768D)

–¶–µ–ª—å: Cosine similarity >90% –≤ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Ä–µ–∂–∏–º–µ.
"""

import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Tuple, List
import numpy as np
from dataclasses import asdict
import logging
import time

# –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π
from data.embedding_reshaper import EmbeddingReshaper, validate_semantic_preservation
from core.lattice_3d import Lattice3D
from .config import EmbeddingConfig, ProcessingMode, validate_config
from .metrics import ProcessingMetrics, calculate_processing_quality

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)


class EmbeddingProcessor(nn.Module):
    """
    –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ - –Ø–î–†–û Phase 2.5
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç EmbeddingReshaper + Lattice3D –≤ –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    
    def __init__(self, config: EmbeddingConfig):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        """
        super().__init__()
        
        self.config = config
        validate_config(config)  # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ö–û–ú–ü–û–ù–ï–ù–¢–û–í ===
        
        # 1. EmbeddingReshaper –¥–ª—è 1D‚Üî3D –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
        self.reshaper = self._init_embedding_reshaper()
        
        # 2. Lattice3D –¥–ª—è 3D –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.lattice = self._init_lattice_3d()
        
        # 3. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
        self.metrics = ProcessingMetrics()
        
        # === –í–ù–£–¢–†–ï–ù–ù–ï–ï –°–û–°–¢–û–Ø–ù–ò–ï ===
        self.processing_count = 0
        self.cache = {} if config.cache_enabled else None
        self.device = torch.device(config.device)
        self.to(self.device)
        
        # –ò–Ω—Ñ–æ –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        logger.info(f"‚úÖ EmbeddingProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üìä –†–µ–∂–∏–º: {config.processing_mode.value}")
        logger.info(f"üéØ –¶–µ–ª–µ–≤–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {config.target_similarity:.1%}")
        logger.info(f"üîÑ –®–∞–≥–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è: {config.propagation_steps}")
    
    def _init_embedding_reshaper(self) -> EmbeddingReshaper:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å EmbeddingReshaper"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π API EmbeddingReshaper (–∏–∑ Phase 2.3)
        reshaper = EmbeddingReshaper(
            input_dim=self.config.input_dim,
            cube_shape=self.config.cube_shape,
            reshaping_method=self.config.reshaping_method,
            preserve_semantics=self.config.preserve_semantics,
            semantic_threshold=self.config.semantic_threshold
        )
        
        logger.info(f"‚úÖ EmbeddingReshaper –≥–æ—Ç–æ–≤: {self.config.cube_shape}")
        return reshaper
    
    def _init_lattice_3d(self) -> Lattice3D:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Lattice3D"""
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        from core.lattice_3d import LatticeConfig, BoundaryCondition, Face, PlacementStrategy
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LatticeConfig
        lattice_config = LatticeConfig(
            dimensions=self.config.lattice_size,
            boundary_conditions=BoundaryCondition.WALLS,
            parallel_processing=False,  # –ü–æ–∫–∞ –æ—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            gpu_enabled=False,  # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
            input_face=Face.FRONT,
            output_face=Face.BACK,
            placement_strategy=PlacementStrategy.PROPORTIONAL,
            enable_logging=self.config.debug_mode
        )
        
        try:
            # –°–æ–∑–¥–∞–µ–º Lattice3D –Ω–∞–ø—Ä—è–º—É—é —Å –æ–±—ä–µ–∫—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            lattice = Lattice3D(lattice_config)
            logger.info(f"‚úÖ Lattice3D –≥–æ—Ç–æ–≤: {self.config.lattice_size}")
            return lattice
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Lattice3D: {e}")
            raise
    
    def forward(self, input_embedding: torch.Tensor) -> torch.Tensor:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–∞
        
        Args:
            input_embedding: –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ [batch_size, 768] –∏–ª–∏ [768]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥ —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """
        start_time = time.time()
        original_shape = input_embedding.shape
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º batch dimension
        if input_embedding.dim() == 1:
            input_embedding = input_embedding.unsqueeze(0)
            single_input = True
        else:
            single_input = False
        
        batch_size = input_embedding.shape[0]
        
        try:
            # === –≠–¢–ê–ü 1: 1D ‚Üí 3D –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï ===
            if self.config.debug_mode:
                logger.debug(f"üîÑ –≠—Ç–∞–ø 1: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ {input_embedding.shape} ‚Üí 3D")
            
            # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è 3D –º–∞—Ç—Ä–∏—Ü
            matrices_3d = []
            
            for i in range(batch_size):
                emb_1d = input_embedding[i].cpu().numpy()  # EmbeddingReshaper —Ä–∞–±–æ—Ç–∞–µ—Ç —Å numpy
                matrix_3d = self.reshaper.vector_to_matrix(emb_1d)
                matrices_3d.append(torch.from_numpy(matrix_3d).float())
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ batch: [batch_size, depth, height, width]
            batch_3d = torch.stack(matrices_3d).to(self.device)
            
            # === –≠–¢–ê–ü 2: 3D –û–ë–†–ê–ë–û–¢–ö–ê –ß–ï–†–ï–ó LATTICE ===
            if self.config.debug_mode:
                logger.debug(f"üß† –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Lattice3D {batch_3d.shape}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤ batch –æ—Ç–¥–µ–ª—å–Ω–æ (–ø–æ–∫–∞ –Ω–µ—Ç batch support –≤ Lattice3D)
            processed_matrices = []
            
            for i in range(batch_size):
                matrix_3d = batch_3d[i]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Lattice3D (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è)
                processed_matrix = self._process_through_lattice(matrix_3d)
                processed_matrices.append(processed_matrix)
            
            processed_batch = torch.stack(processed_matrices)
            
            # === –≠–¢–ê–ü 3: 3D ‚Üí 1D –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï ===
            if self.config.debug_mode:
                logger.debug(f"üîÑ –≠—Ç–∞–ø 3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ 3D ‚Üí {self.config.output_dim}D")
            
            output_embeddings = []
            
            for i in range(batch_size):
                matrix_3d = processed_batch[i].cpu().numpy()
                emb_1d = self.reshaper.matrix_to_vector(matrix_3d)
                output_embeddings.append(torch.from_numpy(emb_1d).float())
            
            output_batch = torch.stack(output_embeddings).to(self.device)
            
            # === –≠–¢–ê–ü 4: –ö–û–ù–¢–†–û–õ–¨ –ö–ê–ß–ï–°–¢–í–ê ===
            if self.config.quality_check_enabled:
                self._update_metrics(input_embedding, output_batch, start_time)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            if single_input:
                output_batch = output_batch.squeeze(0)
            
            self.processing_count += 1
            
            return output_batch
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–∞: {e}")
            raise
    
    def _process_through_lattice(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ 3D –º–∞—Ç—Ä–∏—Ü—ã —á–µ—Ä–µ–∑ Lattice3D
        
        Args:
            matrix_3d: 3D –º–∞—Ç—Ä–∏—Ü–∞ [depth, height, width]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 3D –º–∞—Ç—Ä–∏—Ü–∞
        """
        try:
            # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.config.processing_mode == ProcessingMode.AUTOENCODER:
                # –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: —Å—Ç—Ä–µ–º–∏–º—Å—è –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é
                return self._autoencoder_processing(matrix_3d)
            elif self.config.processing_mode == ProcessingMode.GENERATOR:
                # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
                return self._generator_processing(matrix_3d)
            elif self.config.processing_mode == ProcessingMode.DIALOGUE:
                # –î–∏–∞–ª–æ–≥: –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                return self._dialogue_processing(matrix_3d)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {self.config.processing_mode}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ Lattice3D –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
            return matrix_3d
    
    def _autoencoder_processing(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)"""
        
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º identity transformation + –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        # –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –±—É–¥–µ—Ç –ø–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Lattice3D
        
        noise_level = 0.01  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —à—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        noise = torch.randn_like(matrix_3d) * noise_level
        
        return matrix_3d + noise
    
    def _generator_processing(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏)"""
        
        # –ë–æ–ª—å—à–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        transformation_strength = 0.1
        
        # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ Lattice3D)
        transformed = matrix_3d * (1.0 + torch.randn_like(matrix_3d) * transformation_strength)
        
        return transformed
    
    def _dialogue_processing(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """–î–∏–∞–ª–æ–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏)"""
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
        context_strength = 0.15
        
        # –ü—Ä–∏–º–µ—Ä (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø–æ–ª–Ω—É—é Lattice3D –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é)
        context_transform = torch.tanh(matrix_3d) * context_strength
        
        return matrix_3d + context_transform
    
    def _update_metrics(self, input_batch: torch.Tensor, output_batch: torch.Tensor, start_time: float):
        """–û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        processing_time = time.time() - start_time
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é cosine similarity –ø–æ batch
        similarities = []
        for i in range(input_batch.shape[0]):
            similarity = torch.nn.functional.cosine_similarity(
                input_batch[i], output_batch[i], dim=0
            ).item()
            similarities.append(similarity)
        
        avg_similarity = np.mean(similarities)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.metrics.update(
            similarity=avg_similarity,
            processing_time=processing_time,
            batch_size=input_batch.shape[0]
        )
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if self.config.verbose_logging:
            logger.info(f"üìä Cosine similarity: {avg_similarity:.3f} (—Ü–µ–ª—å: {self.config.target_similarity:.3f})")
            logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.3f}s")
    
    def get_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏"""
        return self.metrics.get_summary()
    
    def reset_metrics(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        self.metrics.reset()
    
    def set_mode(self, mode: ProcessingMode):
        """–ò–∑–º–µ–Ω–∏—Ç—å —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.config.processing_mode = mode
        logger.info(f"üîÑ –†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {mode.value}")
    
    def validate_quality(self, input_embedding: torch.Tensor, output_embedding: torch.Tensor) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            bool: True –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
        """
        similarity = torch.nn.functional.cosine_similarity(
            input_embedding, output_embedding, dim=0 if input_embedding.dim() == 1 else 1
        ).mean().item()
        
        return similarity >= self.config.target_similarity
    
    def __repr__(self) -> str:
        return (f"EmbeddingProcessor("
                f"mode={self.config.processing_mode.value}, "
                f"target_sim={self.config.target_similarity:.2f}, "
                f"processed={self.processing_count})") 