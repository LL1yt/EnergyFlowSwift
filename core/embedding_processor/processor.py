"""
EmbeddingProcessor - –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
=======================================================

–Ø–î–†–û Phase 2.5 - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –≥–æ—Ç–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:
1. –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ (768D) ‚Üí EmbeddingReshaper.vector_to_matrix() ‚Üí 3D –º–∞—Ç—Ä–∏—Ü–∞ (8√ó8√ó12)
2. 3D –º–∞—Ç—Ä–∏—Ü–∞ ‚Üí Lattice3D.forward() ‚Üí –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 3D –º–∞—Ç—Ä–∏—Ü–∞  
3. –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 3D –º–∞—Ç—Ä–∏—Ü–∞ ‚Üí EmbeddingReshaper.matrix_to_vector() ‚Üí –≤—ã—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ (768D)

–¶–µ–ª—å: Cosine similarity >90% –≤ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Ä–µ–∂–∏–º–µ.
"""

import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Tuple, List
import numpy as np
from dataclasses import asdict
import logging
import time

# –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π
from data.embedding_reshaper import EmbeddingReshaper, validate_semantic_preservation
from core.lattice_3d import Lattice3D
from .config import EmbeddingConfig, ProcessingMode, validate_config
from .metrics import ProcessingMetrics, calculate_processing_quality

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)


class EmbeddingProcessor(nn.Module):
    """
    –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ - –Ø–î–†–û Phase 2.5
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç EmbeddingReshaper + Lattice3D –≤ –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    
    def __init__(self, config: EmbeddingConfig):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        """
        super().__init__()
        
        self.config = config
        validate_config(config)  # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ö–û–ú–ü–û–ù–ï–ù–¢–û–í ===
        
        # 1. EmbeddingReshaper –¥–ª—è 1D‚Üî3D –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è non-surface —Ä–µ–∂–∏–º–æ–≤)
        if config.processing_mode != ProcessingMode.SURFACE_ONLY:
            self.reshaper = self._init_embedding_reshaper()
        else:
            self.reshaper = None  # –ù–µ –Ω—É–∂–µ–Ω –¥–ª—è surface-only —Ä–µ–∂–∏–º–∞
            logger.info("üìÑ EmbeddingReshaper –ø—Ä–æ–ø—É—â–µ–Ω –¥–ª—è SURFACE_ONLY —Ä–µ–∂–∏–º–∞")
        
        # 2. Lattice3D –¥–ª—è 3D –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è non-surface —Ä–µ–∂–∏–º–æ–≤)
        if config.processing_mode != ProcessingMode.SURFACE_ONLY:
            self.lattice = self._init_lattice_3d()
        else:
            self.lattice = None  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ surface-only —Ä–µ–∂–∏–º–µ
            logger.info("üé≤ Lattice3D –ø—Ä–æ–ø—É—â–µ–Ω –¥–ª—è SURFACE_ONLY —Ä–µ–∂–∏–º–∞")
        
        # 3. Learnable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è SURFACE_ONLY —Ä–µ–∂–∏–º–∞
        if config.processing_mode == ProcessingMode.SURFACE_ONLY:
            self._init_surface_learnable_params()
        else:
            # –î–ª—è non-surface —Ä–µ–∂–∏–º–æ–≤ –æ–±–Ω—É–ª—è–µ–º surface –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self.diffusion_alpha = None
            self.diffusion_beta = None
            self.expansion_weights = None
            self.extraction_weights = None
            self.activation_scale = None
            self.activation_bias = None
            self.surface_modules = None
        
        # 4. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
        self.metrics = ProcessingMetrics()
        
        # === –í–ù–£–¢–†–ï–ù–ù–ï–ï –°–û–°–¢–û–Ø–ù–ò–ï ===
        self.processing_count = 0
        self.cache = {} if config.cache_enabled else None
        self.device = torch.device(config.device)
        self.to(self.device)
        
        # –ò–Ω—Ñ–æ –æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        logger.info(f"‚úÖ EmbeddingProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üìä –†–µ–∂–∏–º: {config.processing_mode.value}")
        logger.info(f"üéØ –¶–µ–ª–µ–≤–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {config.target_similarity:.1%}")
        logger.info(f"üîÑ –®–∞–≥–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è: {config.propagation_steps}")
    
    def _init_surface_learnable_params(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è learnable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è SURFACE_ONLY —Ä–µ–∂–∏–º–∞"""
        h, w = self.config.surface_dimensions
        depth = self.config.surface_processing_depth
        
        # –°–æ–∑–¥–∞–µ–º nn.Parameter –æ–±—ä–µ–∫—Ç—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞
        self.diffusion_alpha = nn.Parameter(torch.tensor(0.7))
        self.diffusion_beta = nn.Parameter(torch.tensor(0.3))
        self.expansion_weights = nn.Parameter(torch.randn(depth, h, w) * 0.1)
        self.extraction_weights = nn.Parameter(torch.randn(h, w) * 0.1)
        self.activation_scale = nn.Parameter(torch.tensor(1.0))
        self.activation_bias = nn.Parameter(torch.tensor(0.0))
        
        # –°–æ–∑–¥–∞–µ–º nn.Module –æ–±—ä–µ–∫—Ç—ã –≤ ModuleDict
        self.surface_modules = nn.ModuleDict({
            # Emergent transformation layers
            'layer_transform': nn.Sequential(
                nn.Linear(h * w, h * w),
                nn.Tanh(),
                nn.Linear(h * w, h * w)
            )
        })
        
        logger.info(f"‚úÖ Surface learnable parameters initialized:")
        total_params = (
            sum(p.numel() for p in [
                self.diffusion_alpha, self.diffusion_beta, 
                self.expansion_weights, self.extraction_weights,
                self.activation_scale, self.activation_bias
            ]) +
            sum(p.numel() for p in self.surface_modules.parameters())
        )
        logger.info(f"   üìä Total learnable parameters: {total_params:,}")
    
    def _init_embedding_reshaper(self) -> EmbeddingReshaper:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å EmbeddingReshaper"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π API EmbeddingReshaper (–∏–∑ Phase 2.3)
        reshaper = EmbeddingReshaper(
            input_dim=self.config.input_dim,
            cube_shape=self.config.cube_shape,
            reshaping_method=self.config.reshaping_method,
            preserve_semantics=self.config.preserve_semantics,
            semantic_threshold=self.config.semantic_threshold
        )
        
        logger.info(f"‚úÖ EmbeddingReshaper –≥–æ—Ç–æ–≤: {self.config.cube_shape}")
        return reshaper
    
    def _init_lattice_3d(self) -> Lattice3D:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Lattice3D"""
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        from core.lattice_3d import LatticeConfig, BoundaryCondition, Face, PlacementStrategy
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LatticeConfig
        lattice_config = LatticeConfig(
            dimensions=self.config.lattice_size,
            boundary_conditions=BoundaryCondition.WALLS,
            parallel_processing=False,  # –ü–æ–∫–∞ –æ—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            gpu_enabled=False,  # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
            input_face=Face.FRONT,
            output_face=Face.BACK,
            placement_strategy=PlacementStrategy.PROPORTIONAL,
            enable_logging=self.config.debug_mode
        )
        
        try:
            # –°–æ–∑–¥–∞–µ–º Lattice3D –Ω–∞–ø—Ä—è–º—É—é —Å –æ–±—ä–µ–∫—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            lattice = Lattice3D(lattice_config)
            logger.info(f"‚úÖ Lattice3D –≥–æ—Ç–æ–≤: {self.config.lattice_size}")
            return lattice
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Lattice3D: {e}")
            raise
    
    def forward(self, input_embedding: torch.Tensor) -> torch.Tensor:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–∞
        
        Args:
            input_embedding: –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ [batch_size, dim] –∏–ª–∏ [dim]
                           - –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤: [batch_size, 768] –∏–ª–∏ [768]
                           - –î–ª—è SURFACE_ONLY: [batch_size, surface_size] –∏–ª–∏ [surface_size]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥ —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """
        start_time = time.time()
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º batch dimension
        if input_embedding.dim() == 1:
            input_embedding = input_embedding.unsqueeze(0)
            single_input = True
        else:
            single_input = False
        
        batch_size = input_embedding.shape[0]
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.config.processing_mode == ProcessingMode.SURFACE_ONLY:
                # SURFACE-ONLY –†–ï–ñ–ò–ú: –ø—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ surface embeddings
                output_batch = self._surface_only_processing(input_embedding)
            else:
                # –°–¢–ê–ù–î–ê–†–¢–ù–´–ô –†–ï–ñ–ò–ú: —á–µ—Ä–µ–∑ EmbeddingReshaper –∏ –ø–æ–ª–Ω—ã–π –∫—É–±
                output_batch = self._standard_processing(input_embedding)
            
            # === –≠–¢–ê–ü 4: –ö–û–ù–¢–†–û–õ–¨ –ö–ê–ß–ï–°–¢–í–ê ===
            if self.config.quality_check_enabled:
                self._update_metrics(input_embedding, output_batch, start_time)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            if single_input:
                output_batch = output_batch.squeeze(0)
            
            self.processing_count += 1
            
            return output_batch
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–∞: {e}")
            raise
    
    def _process_through_lattice(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ 3D –º–∞—Ç—Ä–∏—Ü—ã —á–µ—Ä–µ–∑ Lattice3D
        
        Args:
            matrix_3d: 3D –º–∞—Ç—Ä–∏—Ü–∞ [depth, height, width]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 3D –º–∞—Ç—Ä–∏—Ü–∞
        """
        try:
            # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.config.processing_mode == ProcessingMode.AUTOENCODER:
                # –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: —Å—Ç—Ä–µ–º–∏–º—Å—è –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é
                return self._autoencoder_processing(matrix_3d)
            elif self.config.processing_mode == ProcessingMode.GENERATOR:
                # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
                return self._generator_processing(matrix_3d)
            elif self.config.processing_mode == ProcessingMode.DIALOGUE:
                # –î–∏–∞–ª–æ–≥: –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                return self._dialogue_processing(matrix_3d)
            elif self.config.processing_mode == ProcessingMode.SURFACE_ONLY:
                # Surface-only: –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥—Ä—É–≥–æ–π pipeline
                logger.warning("‚ö†Ô∏è  _process_through_lattice –≤—ã–∑–≤–∞–Ω –¥–ª—è SURFACE_ONLY —Ä–µ–∂–∏–º–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ _surface_only_processing.")
                return matrix_3d  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {self.config.processing_mode}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ Lattice3D –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
            return matrix_3d
    
    def _autoencoder_processing(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)"""
        
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º identity transformation + –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        # –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –±—É–¥–µ—Ç –ø–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Lattice3D
        
        noise_level = 0.01  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —à—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        noise = torch.randn_like(matrix_3d) * noise_level
        
        return matrix_3d + noise
    
    def _generator_processing(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏)"""
        
        # –ë–æ–ª—å—à–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        transformation_strength = 0.1
        
        # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ Lattice3D)
        transformed = matrix_3d * (1.0 + torch.randn_like(matrix_3d) * transformation_strength)
        
        return transformed
    
    def _dialogue_processing(self, matrix_3d: torch.Tensor) -> torch.Tensor:
        """–î–∏–∞–ª–æ–≥–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏)"""
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
        context_strength = 0.15
        
        # –ü—Ä–∏–º–µ—Ä (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø–æ–ª–Ω—É—é Lattice3D –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é)
        context_transform = torch.tanh(matrix_3d) * context_strength
        
        return matrix_3d + context_transform
    
    def _update_metrics(self, input_batch: torch.Tensor, output_batch: torch.Tensor, start_time: float):
        """–û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        processing_time = time.time() - start_time
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é cosine similarity –ø–æ batch
        similarities = []
        for i in range(input_batch.shape[0]):
            similarity = torch.nn.functional.cosine_similarity(
                input_batch[i], output_batch[i], dim=0
            ).item()
            similarities.append(similarity)
        
        avg_similarity = np.mean(similarities)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.metrics.update(
            similarity=avg_similarity,
            processing_time=processing_time,
            batch_size=input_batch.shape[0]
        )
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if self.config.verbose_logging:
            logger.info(f"üìä Cosine similarity: {avg_similarity:.3f} (—Ü–µ–ª—å: {self.config.target_similarity:.3f})")
            logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.3f}s")
    
    def get_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏"""
        return self.metrics.get_summary()
    
    def reset_metrics(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        self.metrics.reset()
    
    def set_mode(self, mode: ProcessingMode):
        """–ò–∑–º–µ–Ω–∏—Ç—å —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.config.processing_mode = mode
        logger.info(f"üîÑ –†–µ–∂–∏–º –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {mode.value}")
    
    def validate_quality(self, input_embedding: torch.Tensor, output_embedding: torch.Tensor) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            bool: True –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
        """
        similarity = torch.nn.functional.cosine_similarity(
            input_embedding, output_embedding, dim=0 if input_embedding.dim() == 1 else 1
        ).mean().item()
        
        return similarity >= self.config.target_similarity
    
    def __repr__(self) -> str:
        return (f"EmbeddingProcessor("
                f"mode={self.config.processing_mode.value}, "
                f"target_sim={self.config.target_similarity:.2f}, "
                f"processed={self.processing_count})")
    
    def _surface_only_processing(self, input_embedding: torch.Tensor) -> torch.Tensor:
        """
        Surface-only –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Universal Adapter –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Args:
            input_embedding: Surface embeddings [batch_size, surface_size]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ surface embeddings —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """
        if self.config.debug_mode:
            logger.debug(f"üîÑ Surface-only processing: {input_embedding.shape}")
        
        batch_size = input_embedding.shape[0]
        surface_size = input_embedding.shape[1]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        expected_surface_size = self.config.surface_dimensions[0] * self.config.surface_dimensions[1]
        if surface_size != expected_surface_size:
            logger.warning(f"Surface size mismatch: got {surface_size}, expected {expected_surface_size}")
        
        # Reshape surface embeddings –≤ 2D surface –¥–ª—è lattice processing
        h, w = self.config.surface_dimensions
        
        # –ü—Ä–æ—Ö–æ–¥ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É –≤ batch
        processed_surfaces = []
        
        for i in range(batch_size):
            surface_emb = input_embedding[i]  # [surface_size]
            
            # Reshape –≤ 2D surface [height, width] 
            surface_2d = surface_emb.view(h, w)
            
            # Emergent processing —á–µ—Ä–µ–∑ surface-aware –º–µ—Ç–æ–¥
            processed_surface_2d = self._surface_emergent_processing(surface_2d)
            
            # Flatten –æ–±—Ä–∞—Ç–Ω–æ –≤ 1D
            processed_surface_1d = processed_surface_2d.view(-1)
            processed_surfaces.append(processed_surface_1d)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ batch
        output_batch = torch.stack(processed_surfaces).to(self.device)
        
        if self.config.debug_mode:
            logger.debug(f"üéØ Surface-only —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {output_batch.shape}")
        
        return output_batch
    
    def _standard_processing(self, input_embedding: torch.Tensor) -> torch.Tensor:
        """
        –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ EmbeddingReshaper –∏ –ø–æ–ª–Ω—ã–π –∫—É–±
        
        Args:
            input_embedding: –í—Ö–æ–¥–Ω–æ–π —ç–º–±–µ–¥–∏–Ω–≥ [batch_size, 768]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–∏–Ω–≥ [batch_size, 768]
        """
        batch_size = input_embedding.shape[0]
        
        # === –≠–¢–ê–ü 1: 1D ‚Üí 3D –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï ===
        if self.config.debug_mode:
            logger.debug(f"üîÑ –≠—Ç–∞–ø 1: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ {input_embedding.shape} ‚Üí 3D")
        
        # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è 3D –º–∞—Ç—Ä–∏—Ü
        matrices_3d = []
        
        for i in range(batch_size):
            emb_1d = input_embedding[i]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º torch —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            matrix_3d = self.reshaper.vector_to_matrix(emb_1d)  # EmbeddingReshaper –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç torch
            matrices_3d.append(matrix_3d)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ batch: [batch_size, depth, height, width]
        batch_3d = torch.stack(matrices_3d).to(self.device)
        
        # === –≠–¢–ê–ü 2: 3D –û–ë–†–ê–ë–û–¢–ö–ê –ß–ï–†–ï–ó LATTICE ===
        if self.config.debug_mode:
            logger.debug(f"üß† –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Lattice3D {batch_3d.shape}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤ batch –æ—Ç–¥–µ–ª—å–Ω–æ (–ø–æ–∫–∞ –Ω–µ—Ç batch support –≤ Lattice3D)
        processed_matrices = []
        
        for i in range(batch_size):
            matrix_3d = batch_3d[i]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Lattice3D (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è)
            processed_matrix = self._process_through_lattice(matrix_3d)
            processed_matrices.append(processed_matrix)
        
        processed_batch = torch.stack(processed_matrices)
        
        # === –≠–¢–ê–ü 3: 3D ‚Üí 1D –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï ===
        if self.config.debug_mode:
            logger.debug(f"üîÑ –≠—Ç–∞–ø 3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ 3D ‚Üí {self.config.output_dim}D")
        
        output_embeddings = []
        
        for i in range(batch_size):
            matrix_3d = processed_batch[i]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º torch —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            emb_1d = self.reshaper.matrix_to_vector(matrix_3d)  # EmbeddingReshaper –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç torch
            output_embeddings.append(emb_1d)
        
        output_batch = torch.stack(output_embeddings).to(self.device)
        
        return output_batch
    
    def _surface_emergent_processing(self, surface_2d: torch.Tensor) -> torch.Tensor:
        """
        Emergent processing –¥–ª—è surface embeddings —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏–∑ EMERGENT_ARCHITECTURE_CLARIFICATION
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç emergent internal processing:
        - Input —Ç–æ–ª—å–∫–æ –Ω–∞ surface
        - Emergent internal layers (11 layers depth)  
        - Self-organization patterns
        - Output —Ç–æ–ª—å–∫–æ —Å surface
        
        Args:
            surface_2d: 2D surface [height, width]
            
        Returns:
            torch.Tensor: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è 2D surface [height, width]
        """
        h, w = surface_2d.shape
        depth = self.config.surface_processing_depth  # 11 layers –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        if self.config.debug_mode:
            logger.debug(f"üß† Emergent processing: surface {h}√ó{w}, depth {depth}")
        
        # –°–æ–∑–¥–∞–µ–º 3D representation –¥–ª—è emergent processing
        # surface ‚Üí volume ‚Üí surface (emergent internal behavior)
        
        # 1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ surface –≤ 3D volume —á–µ—Ä–µ–∑ learned patterns
        volume_3d = self._expand_surface_to_volume(surface_2d, depth)
        
        # 2. Emergent spatial propagation —á–µ—Ä–µ–∑ internal layers
        processed_volume = self._emergent_spatial_propagation(volume_3d)
        
        # 3. Extraction —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ surface
        output_surface = self._extract_surface_from_volume(processed_volume)
        
        return output_surface
    
    def _expand_surface_to_volume(self, surface_2d: torch.Tensor, depth: int) -> torch.Tensor:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ surface –≤ 3D volume –¥–ª—è internal processing
        
        –≠–º—É–ª–∏—Ä—É–µ—Ç "surface injection" ‚Üí "internal propagation"
        """
        h, w = surface_2d.shape
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ layers –≤–º–µ—Å—Ç–æ inplace –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        layers = []
        
        # Front surface (input layer)
        layers.append(surface_2d.clone())
        
        # Propagation –≤ internal layers —á–µ—Ä–µ–∑ learned patterns (NO INPLACE)
        for layer in range(1, depth):
            # –ü—Ä–æ—Å—Ç–∞—è emergent propagation
            prev_layer = layers[layer - 1]
            
            # Spatial diffusion + learnable transformation
            diffused = self._spatial_diffusion(prev_layer)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π layer
            layers.append(diffused)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ 3D volume
        volume = torch.stack(layers, dim=0)
        
        return volume
    
    def _spatial_diffusion(self, layer_2d: torch.Tensor) -> torch.Tensor:
        """Spatial diffusion –¥–ª—è emergent propagation —Å learnable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º layer transformation —á–µ—Ä–µ–∑ learnable linear layers
        h, w = layer_2d.shape
        
        # Flatten –¥–ª—è linear layer
        flat_input = layer_2d.view(-1)
        
        # Learnable transformation
        transformed = self.surface_modules['layer_transform'](flat_input)
        
        # Reshape –æ–±—Ä–∞—Ç–Ω–æ
        result = transformed.view(h, w)
        
        # Spatial averaging —Å learnable alpha (NO INPLACE)
        if h > 2 and w > 2:
            center = layer_2d[1:-1, 1:-1]
            neighbors = (
                layer_2d[:-2, 1:-1] + layer_2d[2:, 1:-1] +   # vertical neighbors
                layer_2d[1:-1, :-2] + layer_2d[1:-1, 2:]     # horizontal neighbors
            ) / 4.0
            
            # Learnable mixing —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ (NO INPLACE)
            alpha = torch.sigmoid(self.diffusion_alpha)  # Ensure [0,1]
            mixed_center = alpha * center + (1 - alpha) * neighbors
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä –≤–º–µ—Å—Ç–æ inplace modification
            result_new = result.clone()
            result_new[1:-1, 1:-1] = mixed_center
            result = result_new
        
        # Learnable activation —Å scale –∏ bias
        scale = self.activation_scale
        bias = self.activation_bias
        result = torch.tanh(scale * result + bias)
        
        return result
    
    def _emergent_spatial_propagation(self, volume_3d: torch.Tensor) -> torch.Tensor:
        """
        Emergent spatial propagation —á–µ—Ä–µ–∑ –≤—Å–µ internal layers —Å learnable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ —á–µ—Ä–µ–∑ internal volume
        –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è emergent behavior patterns.
        """
        depth, h, w = volume_3d.shape
        result = volume_3d.clone()
        
        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ propagation —á–µ—Ä–µ–∑ layers —Å learnable weights (NO INPLACE)
        for step in range(self.config.propagation_steps):
            new_result = result.clone()  # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ step
            
            for layer in range(1, depth):
                curr_layer = result[layer]
                prev_layer = result[layer - 1]
                
                # Spatial diffusion —Ç–µ–∫—É—â–µ–≥–æ layer
                spatial_mixed = self._spatial_diffusion(curr_layer)
                
                # Learnable combination —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º layer
                expansion_weight = self.expansion_weights[layer]
                influence = torch.sigmoid(expansion_weight)  # Learnable influence map
                
                # Weighted update —Å learnable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (NO INPLACE)
                beta = torch.sigmoid(self.diffusion_beta)
                new_result[layer] = beta * spatial_mixed + (1 - beta) * (prev_layer * influence)
            
            result = new_result  # Update result –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ step
        
        return result
    
    def _extract_surface_from_volume(self, volume_3d: torch.Tensor) -> torch.Tensor:
        """
        Extraction surface –∏–∑ internal volume —Å learnable weights
        
        Args:
            volume_3d: Internal 3D volume [depth, height, width]
            
        Returns:
            torch.Tensor: Output surface [height, width]
        """
        depth, h, w = volume_3d.shape
        
        # Learnable weighted extraction –∏–∑ multiple layers
        if depth >= 3:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º learnable extraction weights
            extraction_weights = torch.softmax(self.extraction_weights, dim=0)
            
            # Weighted combination –ø–æ—Å–ª–µ–¥–Ω–∏—Ö layers
            weighted_output = torch.zeros(h, w, device=volume_3d.device)
            for i in range(min(3, depth)):
                layer_idx = -(i+1)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 layer
                if i < extraction_weights.numel():
                    weighted_output += extraction_weights.flatten()[i] * volume_3d[layer_idx]
            
            output_surface = weighted_output
        elif depth >= 1:
            # Fallback –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö depth
            output_surface = volume_3d[-1]  # Last layer
        else:
            output_surface = volume_3d[0]  # Single layer
        
        # Final learnable transformation
        scale = self.activation_scale
        bias = self.activation_bias
        output_surface = torch.tanh(scale * output_surface + bias)
        
        return output_surface 