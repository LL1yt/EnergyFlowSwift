#!/usr/bin/env python3
"""
ü§ñ Automated Long Training Script
–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
"""

import os
import sys
import time
import logging
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class AutomatedTrainer:
    """–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –¥–æ–ª–≥–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(
        self,
        mode: str = "development",
        scale: Optional[float] = None,
        max_total_time_hours: float = 8.0,
    ):
        """
        Args:
            mode: –†–µ–∂–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (development, research, etc.)
            scale: Custom scale factor
            max_total_time_hours: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ —á–∞—Å–∞—Ö
        """
        self.mode = mode
        self.scale = scale
        self.max_total_time_hours = max_total_time_hours
        self.start_time = datetime.now()

        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.training_history = []

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
        self.log_dir = Path("logs/automated_training")
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # –§–∞–π–ª –ª–æ–≥–∞ —Å–µ—Å—Å–∏–∏
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_log = self.log_dir / f"automated_session_{timestamp}.json"

        logger.info(f"ü§ñ Automated Trainer initialized")
        logger.info(f"   Mode: {mode}")
        logger.info(f"   Scale: {scale}")
        logger.info(f"   Max time: {max_total_time_hours} hours")
        logger.info(f"   Session log: {self.session_log}")

    def get_progressive_config(self, stage: int) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–¥–∏–∏ –æ–±—É—á–µ–Ω–∏—è

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        - Stage 1: –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç, –º–Ω–æ–≥–æ —ç–ø–æ—Ö (–∏–∑—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤)
        - Stage 2: –°—Ä–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç, —Å—Ä–µ–¥–Ω–∏–µ —ç–ø–æ—Ö–∏ (–∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è)
        - Stage 3: –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç, –º–∞–ª–æ —ç–ø–æ—Ö (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞)
        """
        configs = {
            1: {
                "dataset_limit": 2000,
                "epochs": 20,
                "batch_size": 32,
                "description": "Foundation Learning (small data, many epochs)",
            },
            2: {
                "dataset_limit": 5000,
                "epochs": 15,
                "batch_size": 64,
                "description": "Consolidation (medium data, medium epochs)",
            },
            3: {
                "dataset_limit": 10000,
                "epochs": 12,
                "batch_size": 64,
                "description": "Refinement (large data, fewer epochs)",
            },
            4: {
                "dataset_limit": 20000,
                "epochs": 8,
                "batch_size": 128,
                "description": "Mastery (very large data, few epochs)",
            },
            5: {
                "dataset_limit": 50000,
                "epochs": 5,
                "batch_size": 128,
                "description": "Perfection (massive data, minimal epochs)",
            },
        }

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –µ—Å–ª–∏ stage —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        return configs.get(stage, configs[5])

    def estimate_stage_time(self, config: Dict[str, Any]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å—Ç–∞–¥–∏–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö"""
        dataset_size = config["dataset_limit"]
        epochs = config["epochs"]
        batch_size = config["batch_size"]

        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Ä–µ–∂–∏–º–∞
        if self.mode == "development":
            time_per_1k_examples = 2  # –º–∏–Ω—É—Ç
        elif self.mode == "research":
            time_per_1k_examples = 5
        else:
            time_per_1k_examples = 10

        estimated_minutes = (dataset_size / 1000) * time_per_1k_examples * epochs / 10
        return estimated_minutes

    def run_training_stage(
        self, stage: int, config: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —Å—Ç–∞–¥–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üöÄ Starting Stage {stage}: {config['description']}")
        logger.info(f"   Dataset: {config['dataset_limit']:,} examples")
        logger.info(f"   Epochs: {config['epochs']}")
        logger.info(f"   Batch size: {config['batch_size']}")

        estimated_time = self.estimate_stage_time(config)
        logger.info(f"   Estimated time: {estimated_time:.1f} minutes")

        # –°—Ç—Ä–æ–∏–º –∫–æ–º–∞–Ω–¥—É
        cmd = [
            "python",
            "smart_resume_training.py",
            "--mode",
            self.mode,
            "--dataset-limit",
            str(config["dataset_limit"]),
            "--additional-epochs",
            str(config["epochs"]),
            "--batch-size",
            str(config["batch_size"]),
        ]

        if self.scale:
            cmd.extend(["--scale", str(self.scale)])

        logger.info(f"   Command: {' '.join(cmd)}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        start_time = time.time()
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=estimated_time * 60 * 2,  # –¢–∞–π–º–∞—É—Ç = 2x –æ—Ç –æ—Ü–µ–Ω–∫–∏
            )

            end_time = time.time()
            actual_time = (end_time - start_time) / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö

            if result.returncode == 0:
                logger.info(f"‚úÖ Stage {stage} completed successfully")
                logger.info(f"   Actual time: {actual_time:.1f} minutes")

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –≤—ã–≤–æ–¥–∞
                similarity = self._extract_similarity_from_output(result.stdout)

                stage_result = {
                    "stage": stage,
                    "config": config,
                    "success": True,
                    "actual_time_minutes": actual_time,
                    "estimated_time_minutes": estimated_time,
                    "final_similarity": similarity,
                    "timestamp": datetime.now().isoformat(),
                }

                self.training_history.append(stage_result)
                self._save_session_log()

                return stage_result
            else:
                logger.error(
                    f"‚ùå Stage {stage} failed with return code {result.returncode}"
                )
                logger.error(f"   Error output: {result.stderr[:500]}...")

                stage_result = {
                    "stage": stage,
                    "config": config,
                    "success": False,
                    "error": result.stderr,
                    "timestamp": datetime.now().isoformat(),
                }

                self.training_history.append(stage_result)
                self._save_session_log()

                return None

        except subprocess.TimeoutExpired:
            logger.error(
                f"‚ùå Stage {stage} timed out after {estimated_time * 2:.1f} minutes"
            )
            return None
        except Exception as e:
            logger.error(f"‚ùå Stage {stage} failed with exception: {e}")
            return None

    def _extract_similarity_from_output(self, output: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç final similarity –∏–∑ –≤—ã–≤–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å final_similarity
            for line in output.split("\n"):
                if "final_similarity:" in line:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ
                    parts = line.split("final_similarity:")
                    if len(parts) > 1:
                        similarity_str = parts[1].strip()
                        return float(similarity_str)
        except:
            pass
        return None

    def _save_session_log(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥ —Å–µ—Å—Å–∏–∏"""
        session_data = {
            "mode": self.mode,
            "scale": self.scale,
            "max_total_time_hours": self.max_total_time_hours,
            "start_time": self.start_time.isoformat(),
            "current_time": datetime.now().isoformat(),
            "elapsed_hours": (datetime.now() - self.start_time).total_seconds() / 3600,
            "training_history": self.training_history,
            "summary": self._generate_summary(),
        }

        with open(self.session_log, "w") as f:
            json.dump(session_data, f, indent=2)

    def _generate_summary(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏"""
        successful_stages = [
            h for h in self.training_history if h.get("success", False)
        ]

        if not successful_stages:
            return {"total_stages": 0, "best_similarity": None}

        total_time = sum(h["actual_time_minutes"] for h in successful_stages)
        similarities = [
            h["final_similarity"]
            for h in successful_stages
            if h["final_similarity"] is not None
        ]

        return {
            "total_stages": len(successful_stages),
            "total_time_minutes": total_time,
            "best_similarity": max(similarities) if similarities else None,
            "avg_similarity": (
                sum(similarities) / len(similarities) if similarities else None
            ),
            "similarity_trend": (
                similarities[-3:] if len(similarities) >= 3 else similarities
            ),
        }

    def should_continue(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—Ç–æ–∏—Ç –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ"""
        elapsed_hours = (datetime.now() - self.start_time).total_seconds() / 3600

        if elapsed_hours >= self.max_total_time_hours:
            logger.info(
                f"‚è∞ Time limit reached: {elapsed_hours:.1f}/{self.max_total_time_hours} hours"
            )
            return False

        return True

    def run_automated_training(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info(f"üéØ Starting automated training session")
        logger.info(f"   Max duration: {self.max_total_time_hours} hours")
        logger.info(
            f"   Target end time: {self.start_time + timedelta(hours=self.max_total_time_hours)}"
        )

        stage = 1

        while self.should_continue():
            config = self.get_progressive_config(stage)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ö–≤–∞—Ç–∏—Ç –ª–∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —ç—Ç–æ–π —Å—Ç–∞–¥–∏–∏
            estimated_time_hours = self.estimate_stage_time(config) / 60
            elapsed_hours = (datetime.now() - self.start_time).total_seconds() / 3600
            remaining_hours = self.max_total_time_hours - elapsed_hours

            if estimated_time_hours > remaining_hours:
                logger.info(f"‚è∞ Not enough time for stage {stage}")
                logger.info(
                    f"   Estimated: {estimated_time_hours:.1f}h, Remaining: {remaining_hours:.1f}h"
                )
                break

            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å—Ç–∞–¥–∏—é
            result = self.run_training_stage(stage, config)

            if result is None:
                logger.error(f"‚ùå Stage {stage} failed, stopping automated training")
                break

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            summary = self._generate_summary()
            logger.info(f"üìä Progress after stage {stage}:")
            logger.info(f"   Total stages completed: {summary['total_stages']}")
            logger.info(
                f"   Best similarity: {summary['best_similarity']:.4f}"
                if summary["best_similarity"]
                else "   Best similarity: N/A"
            )
            logger.info(
                f"   Total training time: {summary['total_time_minutes']:.1f} minutes"
            )

            stage += 1

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç–∞–¥–∏—è–º–∏
            time.sleep(10)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
        self._print_final_summary()

    def _print_final_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É"""
        summary = self._generate_summary()
        elapsed_hours = (datetime.now() - self.start_time).total_seconds() / 3600

        logger.info(f"\nüéâ Automated training session completed!")
        logger.info(f"üìä Final Summary:")
        logger.info(f"   Total duration: {elapsed_hours:.1f} hours")
        logger.info(f"   Stages completed: {summary['total_stages']}")
        logger.info(
            f"   Total training time: {summary.get('total_time_minutes', 0):.1f} minutes"
        )
        logger.info(
            f"   Best similarity achieved: {summary['best_similarity']:.4f}"
            if summary["best_similarity"]
            else "   Best similarity: N/A"
        )
        logger.info(
            f"   Average similarity: {summary['avg_similarity']:.4f}"
            if summary.get("avg_similarity")
            else "   Average similarity: N/A"
        )
        logger.info(f"   Session log saved: {self.session_log}")

        if summary.get("similarity_trend"):
            logger.info(
                f"   Recent similarity trend: {[f'{s:.3f}' for s in summary['similarity_trend']]}"
            )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description="Automated Long Training Script")
    parser.add_argument(
        "--mode",
        choices=["development", "research", "validation", "production"],
        default="development",
        help="Configuration mode",
    )
    parser.add_argument("--scale", type=float, default=None, help="Custom scale factor")
    parser.add_argument(
        "--max-hours",
        type=float,
        default=8.0,
        help="Maximum training time in hours (default: 8.0)",
    )
    parser.add_argument(
        "--test-config",
        action="store_true",
        help="Show training stages configuration and exit",
    )

    args = parser.parse_args()

    try:
        trainer = AutomatedTrainer(
            mode=args.mode, scale=args.scale, max_total_time_hours=args.max_hours
        )

        if args.test_config:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ç–∞–¥–∏–π
            logger.info(f"\nüìã Training stages configuration:")
            for stage in range(1, 6):
                config = trainer.get_progressive_config(stage)
                estimated_time = trainer.estimate_stage_time(config)
                logger.info(f"   Stage {stage}: {config['description']}")
                logger.info(
                    f"      Dataset: {config['dataset_limit']:,}, Epochs: {config['epochs']}, Batch: {config['batch_size']}"
                )
                logger.info(f"      Estimated time: {estimated_time:.1f} minutes")
                logger.info("")
            return

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        trainer.run_automated_training()

    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Automated training interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Automated training failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
