#!/usr/bin/env python3
"""
[BOT] Automated Long Training Script
–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
"""

import os
import sys
import time
import logging
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),  # –ö–æ–Ω—Å–æ–ª—å
        logging.FileHandler("logs/automated_training.log", encoding="utf-8"),  # –§–∞–π–ª
    ],
)
logger = logging.getLogger(__name__)


class AutomatedTrainer:
    """–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –¥–æ–ª–≥–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(
        self,
        mode: str = "development",
        scale: Optional[float] = None,
        max_total_time_hours: float = 8.0,
        dataset_limit_override: Optional[int] = None,
        batch_size_override: Optional[int] = None,
        timeout_multiplier: float = 2.0,
    ):
        """
        Args:
            mode: –†–µ–∂–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (development, research, etc.)
            scale: Custom scale factor
            max_total_time_hours: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ —á–∞—Å–∞—Ö
            dataset_limit_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å dataset_limit –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞–¥–∏–π (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
            batch_size_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å batch_size –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞–¥–∏–π (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
            timeout_multiplier: Multiplier for the timeout
        """
        self.mode = mode
        self.scale = scale
        self.max_total_time_hours = max_total_time_hours
        self.dataset_limit_override = dataset_limit_override
        self.batch_size_override = batch_size_override
        self.timeout_multiplier = timeout_multiplier
        self.start_time = datetime.now()

        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.training_history = []

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
        self.log_dir = Path("logs/automated_training")
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ª–æ–≥–æ–≤
        Path("logs").mkdir(exist_ok=True)

        # –§–∞–π–ª –ª–æ–≥–∞ —Å–µ—Å—Å–∏–∏
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_log = self.log_dir / f"automated_session_{timestamp}.json"

        logger.info(f"[BOT] Automated Trainer initialized")
        logger.info(f"   Mode: {mode}")
        logger.info(f"   Scale: {scale}")
        logger.info(f"   Max time: {max_total_time_hours} hours")
        if dataset_limit_override:
            logger.info(f"   Dataset limit override: {dataset_limit_override}")
        if batch_size_override:
            logger.info(f"   Batch size override: {batch_size_override}")
        logger.info(f"   Timeout multiplier: {timeout_multiplier}")
        logger.info(f"   Session log: {self.session_log}")

    def get_progressive_config(self, stage: int) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–¥–∏–∏ –æ–±—É—á–µ–Ω–∏—è

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        - Stage 1: –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç, –º–Ω–æ–≥–æ —ç–ø–æ—Ö (–∏–∑—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤)
        - Stage 2: –°—Ä–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç, —Å—Ä–µ–¥–Ω–∏–µ —ç–ø–æ—Ö–∏ (–∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è)
        - Stage 3: –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç, –º–∞–ª–æ —ç–ø–æ—Ö (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞)
        """
        configs = {
            1: {
                "dataset_limit": 2000,
                "epochs": 20,
                "batch_size": 32,
                "description": "Foundation Learning (small data, many epochs)",
            },
            2: {
                "dataset_limit": 5000,
                "epochs": 15,
                "batch_size": 64,
                "description": "Consolidation (medium data, medium epochs)",
            },
            3: {
                "dataset_limit": 10000,
                "epochs": 12,
                "batch_size": 64,
                "description": "Refinement (large data, fewer epochs)",
            },
            4: {
                "dataset_limit": 20000,
                "epochs": 8,
                "batch_size": 128,
                "description": "Mastery (very large data, few epochs)",
            },
            5: {
                "dataset_limit": 50000,
                "epochs": 5,
                "batch_size": 128,
                "description": "Perfection (massive data, minimal epochs)",
            },
        }

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –µ—Å–ª–∏ stage —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        config = configs.get(stage, configs[5])

        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –∑–∞–¥–∞–Ω—ã override
        if self.dataset_limit_override or self.batch_size_override:
            config = config.copy()  # –ù–µ –∏–∑–º–µ–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é

            if self.dataset_limit_override:
                config["dataset_limit"] = self.dataset_limit_override
                config["description"] += f" (dataset: {self.dataset_limit_override})"

            if self.batch_size_override:
                config["batch_size"] = self.batch_size_override
                config["description"] += f" (batch: {self.batch_size_override})"

        return config

    def estimate_stage_time(self, config: Dict[str, Any]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å—Ç–∞–¥–∏–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö"""
        dataset_size = config["dataset_limit"]
        epochs = config["epochs"]
        batch_size = config["batch_size"]

        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Ä–µ–∂–∏–º–∞
        if self.mode == "development":
            time_per_1k_examples = 2  # –º–∏–Ω—É—Ç
        elif self.mode == "research":
            time_per_1k_examples = 5
        else:
            time_per_1k_examples = 10

        estimated_minutes = (dataset_size / 1000) * time_per_1k_examples * epochs / 10

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        min_time_minutes = 8.0  # –º–∏–Ω–∏–º—É–º 8 –º–∏–Ω—É—Ç (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        if dataset_size <= 100:  # –î–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            min_time_minutes = 12.0  # –º–∏–Ω–∏–º—É–º 12 –º–∏–Ω—É—Ç
        elif dataset_size <= 1000:  # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            min_time_minutes = 10.0  # –º–∏–Ω–∏–º—É–º 10 –º–∏–Ω—É—Ç

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö batch size (—É—Å–∫–æ—Ä—è—é—Ç –æ–±—É—á–µ–Ω–∏–µ)
        if batch_size >= 128:
            estimated_minutes *= 0.5  # 50% —É—Å–∫–æ—Ä–µ–Ω–∏–µ
        elif batch_size >= 64:
            estimated_minutes *= 0.7  # 30% —É—Å–∫–æ—Ä–µ–Ω–∏–µ

        return max(estimated_minutes, min_time_minutes)

    def run_training_stage(
        self, stage: int, config: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —Å—Ç–∞–¥–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"[START] Starting Stage {stage}: {config['description']}")
        logger.info(f"   Dataset: {config['dataset_limit']:,} examples")
        logger.info(f"   Epochs: {config['epochs']}")
        logger.info(f"   Batch size: {config['batch_size']}")

        estimated_time = self.estimate_stage_time(config)
        logger.info(f"   Estimated time: {estimated_time:.1f} minutes")

        # –°—Ç—Ä–æ–∏–º –∫–æ–º–∞–Ω–¥—É
        cmd = [
            sys.executable,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π Python –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä
            "smart_resume_training.py",
            "--mode",
            self.mode,
            "--dataset-limit",
            str(config["dataset_limit"]),
            "--additional-epochs",
            str(config["epochs"]),
            "--batch-size",
            str(config["batch_size"]),
        ]

        if self.scale:
            cmd.extend(["--scale", str(self.scale)])

        logger.info(f"   Command: {' '.join(cmd)}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å real-time –≤—ã–≤–æ–¥–æ–º
        start_time = time.time()
        timeout_seconds = (
            estimated_time * 60 * self.timeout_multiplier
        )  # –¢–∞–π–º–∞—É—Ç = timeout_multiplier * –æ—Ç –æ—Ü–µ–Ω–∫–∏
        logger.info(
            f"   [PROGRESS] Starting subprocess with timeout: {timeout_seconds/60:.1f} minutes"
        )

        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —Å –∑–∞—Ö–≤–∞—Ç–æ–º –≤—ã–≤–æ–¥–∞ –¥–ª—è real-time –ª–æ–≥–æ–≤
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                universal_newlines=True,
                bufsize=1,  # –ü–æ—Å—Ç—Ä–æ—á–Ω–∞—è –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è
            )

            logger.info(f"   [PROGRESS] Process started with PID: {process.pid}")

            # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            stdout_lines = []
            stderr_lines = []

            # –ß–∏—Ç–∞–µ–º –≤—ã–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            import select
            import threading
            from queue import Queue, Empty

            def read_output(pipe, output_list, prefix):
                """–ß–∏—Ç–∞–µ—Ç –≤—ã–≤–æ–¥ –∏–∑ pipe –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ —Å–ø–∏—Å–æ–∫"""
                try:
                    for line in iter(pipe.readline, ""):
                        if line:
                            line = line.rstrip()
                            output_list.append(line)
                            logger.info(f"   {prefix}: {line}")
                except Exception as e:
                    logger.error(f"   [ERROR] Reading {prefix}: {e}")
                finally:
                    pipe.close()

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è stdout –∏ stderr
            stdout_thread = threading.Thread(
                target=read_output, args=(process.stdout, stdout_lines, "[SUBPROCESS]")
            )
            stderr_thread = threading.Thread(
                target=read_output,
                args=(process.stderr, stderr_lines, "[SUBPROCESS-ERR]"),
            )

            stdout_thread.daemon = True
            stderr_thread.daemon = True
            stdout_thread.start()
            stderr_thread.start()

            # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            try:
                process.wait(timeout=timeout_seconds)
                return_code = process.returncode
            except subprocess.TimeoutExpired:
                logger.error(
                    f"   [TIMEOUT] Process timed out after {timeout_seconds/60:.1f} minutes"
                )
                process.kill()
                process.wait()
                return_code = -1

            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ —á—Ç–µ–Ω–∏—è
            stdout_thread.join(timeout=5)
            stderr_thread.join(timeout=5)

            logger.info(
                f"   [PROGRESS] Process completed with return code: {return_code}"
            )

            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç result –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
            class MockResult:
                def __init__(self, returncode, stdout_list, stderr_list):
                    self.returncode = returncode
                    self.stdout = "\n".join(stdout_list)
                    self.stderr = "\n".join(stderr_list)

            result = MockResult(return_code, stdout_lines, stderr_lines)

            end_time = time.time()
            actual_time = (end_time - start_time) / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö

            if result.returncode == -1:
                # –¢–∞–π–º–∞—É—Ç
                logger.error(
                    f"[ERROR] Stage {stage} timed out after {actual_time:.1f} minutes"
                )
                return None
            elif result.returncode == 0:
                logger.info(f"[OK] Stage {stage} completed successfully")
                logger.info(f"   Actual time: {actual_time:.1f} minutes")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤—ã–≤–æ–¥–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                stdout_lines = result.stdout.strip().split("\n")
                logger.info(f"   Last few lines of output:")
                for line in stdout_lines[-5:]:
                    if line.strip():
                        logger.info(f"      {line}")

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –≤—ã–≤–æ–¥–∞
                similarity = self._extract_similarity_from_output(result.stdout)

                stage_result = {
                    "stage": stage,
                    "config": config,
                    "success": True,
                    "actual_time_minutes": actual_time,
                    "estimated_time_minutes": estimated_time,
                    "final_similarity": similarity,
                    "timestamp": datetime.now().isoformat(),
                }

                self.training_history.append(stage_result)
                self._save_session_log()

                return stage_result
            else:
                logger.error(
                    f"[ERROR] Stage {stage} failed with return code {result.returncode}"
                )
                logger.error(f"   STDOUT output:")
                logger.error(result.stdout)
                logger.error(f"   STDERR output:")
                logger.error(result.stderr)

                stage_result = {
                    "stage": stage,
                    "config": config,
                    "success": False,
                    "error": result.stderr,
                    "stdout": result.stdout,
                    "timestamp": datetime.now().isoformat(),
                }

                self.training_history.append(stage_result)
                self._save_session_log()

                return None

        except Exception as e:
            logger.error(f"[ERROR] Stage {stage} failed with exception: {e}")
            return None

    def _extract_similarity_from_output(self, output: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç final similarity –∏–∑ –≤—ã–≤–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å final_similarity
            for line in output.split("\n"):
                if "final_similarity:" in line:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ
                    parts = line.split("final_similarity:")
                    if len(parts) > 1:
                        similarity_str = parts[1].strip()
                        return float(similarity_str)
        except:
            pass
        return None

    def _save_session_log(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥ —Å–µ—Å—Å–∏–∏"""
        session_data = {
            "mode": self.mode,
            "scale": self.scale,
            "max_total_time_hours": self.max_total_time_hours,
            "start_time": self.start_time.isoformat(),
            "current_time": datetime.now().isoformat(),
            "elapsed_hours": (datetime.now() - self.start_time).total_seconds() / 3600,
            "training_history": self.training_history,
            "summary": self._generate_summary(),
        }

        with open(self.session_log, "w") as f:
            json.dump(session_data, f, indent=2)

    def _generate_summary(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏"""
        successful_stages = [
            h for h in self.training_history if h.get("success", False)
        ]

        if not successful_stages:
            return {"total_stages": 0, "best_similarity": None}

        total_time = sum(h["actual_time_minutes"] for h in successful_stages)
        similarities = [
            h["final_similarity"]
            for h in successful_stages
            if h["final_similarity"] is not None
        ]

        return {
            "total_stages": len(successful_stages),
            "total_time_minutes": total_time,
            "best_similarity": max(similarities) if similarities else None,
            "avg_similarity": (
                sum(similarities) / len(similarities) if similarities else None
            ),
            "similarity_trend": (
                similarities[-3:] if len(similarities) >= 3 else similarities
            ),
        }

    def should_continue(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—Ç–æ–∏—Ç –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ"""
        elapsed_hours = (datetime.now() - self.start_time).total_seconds() / 3600

        if elapsed_hours >= self.max_total_time_hours:
            logger.info(
                f"[TIME] Time limit reached: {elapsed_hours:.1f}/{self.max_total_time_hours} hours"
            )
            return False

        return True

    def run_automated_training(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info(f"üéØ ======== AUTOMATED TRAINING SESSION STARTED ========")
        logger.info(f"[TARGET] Starting automated training session")
        logger.info(f"   Start time: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"   Max duration: {self.max_total_time_hours} hours")
        logger.info(
            f"   Target end time: {(self.start_time + timedelta(hours=self.max_total_time_hours)).strftime('%Y-%m-%d %H:%M:%S')}"
        )
        logger.info(f"   Mode: {self.mode}")
        if self.scale:
            logger.info(f"   Scale factor: {self.scale}")
        if self.dataset_limit_override:
            logger.info(f"   Dataset limit override: {self.dataset_limit_override}")
        logger.info(f"   Session log: {self.session_log}")
        logger.info("=" * 60)

        stage = 1

        while self.should_continue():
            config = self.get_progressive_config(stage)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ö–≤–∞—Ç–∏—Ç –ª–∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —ç—Ç–æ–π —Å—Ç–∞–¥–∏–∏
            estimated_time_hours = self.estimate_stage_time(config) / 60
            elapsed_hours = (datetime.now() - self.start_time).total_seconds() / 3600
            remaining_hours = self.max_total_time_hours - elapsed_hours

            if estimated_time_hours > remaining_hours:
                logger.info(f"[TIME] Not enough time for stage {stage}")
                logger.info(
                    f"   Estimated: {estimated_time_hours:.1f}h, Remaining: {remaining_hours:.1f}h"
                )
                break

            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å—Ç–∞–¥–∏—é
            logger.info(f"üöÄ [STAGE-{stage}] ======== STARTING STAGE {stage} ========")
            stage_start_time = time.time()

            result = self.run_training_stage(stage, config)

            stage_end_time = time.time()
            stage_duration = (stage_end_time - stage_start_time) / 60

            if result is None:
                logger.error(
                    f"[ERROR] Stage {stage} failed after {stage_duration:.1f} minutes, stopping automated training"
                )
                break

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            summary = self._generate_summary()
            elapsed_total = (datetime.now() - self.start_time).total_seconds() / 3600
            remaining_time = self.max_total_time_hours - elapsed_total

            logger.info(f"‚úÖ [STAGE-{stage}] ======== STAGE {stage} COMPLETED ========")
            logger.info(f"   Stage duration: {stage_duration:.1f} minutes")
            logger.info(f"[DATA] Overall Progress:")
            logger.info(f"   Stages completed: {summary['total_stages']}")
            logger.info(
                f"   Session time: {elapsed_total:.1f}h / {self.max_total_time_hours}h"
            )
            logger.info(f"   Remaining time: {remaining_time:.1f}h")
            logger.info(
                f"   Best similarity: {summary['best_similarity']:.4f}"
                if summary["best_similarity"]
                else "   Best similarity: N/A"
            )
            logger.info(
                f"   Total training time: {summary['total_time_minutes']:.1f} minutes"
            )

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç—Ä–µ–Ω–¥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
            if summary.get("similarity_trend") and len(summary["similarity_trend"]) > 1:
                trend = summary["similarity_trend"]
                logger.info(f"   Similarity trend: {[f'{s:.3f}' for s in trend]}")
                if len(trend) >= 2:
                    improvement = trend[-1] - trend[-2]
                    logger.info(f"   Last improvement: {improvement:+.4f}")

            stage += 1

            logger.info(f"[NEXT] Preparing for stage {stage}...")

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç–∞–¥–∏—è–º–∏
            logger.info("   [PAUSE] 10 second break between stages...")
            time.sleep(10)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
        self._print_final_summary()

    def _print_final_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É"""
        summary = self._generate_summary()
        elapsed_hours = (datetime.now() - self.start_time).total_seconds() / 3600

        logger.info(f"\n[SUCCESS] Automated training session completed!")
        logger.info(f"[DATA] Final Summary:")
        logger.info(f"   Total duration: {elapsed_hours:.1f} hours")
        logger.info(f"   Stages completed: {summary['total_stages']}")
        logger.info(
            f"   Total training time: {summary.get('total_time_minutes', 0):.1f} minutes"
        )
        logger.info(
            f"   Best similarity achieved: {summary['best_similarity']:.4f}"
            if summary["best_similarity"]
            else "   Best similarity: N/A"
        )
        logger.info(
            f"   Average similarity: {summary['avg_similarity']:.4f}"
            if summary.get("avg_similarity")
            else "   Average similarity: N/A"
        )
        logger.info(f"   Session log saved: {self.session_log}")

        if summary.get("similarity_trend"):
            logger.info(
                f"   Recent similarity trend: {[f'{s:.3f}' for s in summary['similarity_trend']]}"
            )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description="Automated Long Training Script")
    parser.add_argument(
        "--mode",
        choices=["development", "research", "validation", "production"],
        default="development",
        help="Configuration mode",
    )
    parser.add_argument("--scale", type=float, default=None, help="Custom scale factor")
    parser.add_argument(
        "--max-hours",
        type=float,
        default=8.0,
        help="Maximum training time in hours (default: 8.0)",
    )
    parser.add_argument(
        "--test-config",
        action="store_true",
        help="Show training stages configuration and exit",
    )
    parser.add_argument(
        "--dataset-limit",
        type=int,
        default=None,
        help="Override dataset_limit for all stages (useful for quick testing)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Override batch_size for all stages (useful for faster training)",
    )
    parser.add_argument(
        "--timeout-multiplier",
        type=float,
        default=2.0,
        help="Timeout multiplier for the training process",
    )

    args = parser.parse_args()

    try:
        trainer = AutomatedTrainer(
            mode=args.mode,
            scale=args.scale,
            max_total_time_hours=args.max_hours,
            dataset_limit_override=args.dataset_limit,
            batch_size_override=args.batch_size,
            timeout_multiplier=args.timeout_multiplier,
        )

        if args.test_config:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ç–∞–¥–∏–π
            logger.info(f"\n[INFO] Training stages configuration:")
            for stage in range(1, 6):
                config = trainer.get_progressive_config(stage)
                estimated_time = trainer.estimate_stage_time(config)
                logger.info(f"   Stage {stage}: {config['description']}")
                logger.info(
                    f"      Dataset: {config['dataset_limit']:,}, Epochs: {config['epochs']}, Batch: {config['batch_size']}"
                )
                logger.info(f"      Estimated time: {estimated_time:.1f} minutes")
                logger.info("")
            return

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        trainer.run_automated_training()

    except KeyboardInterrupt:
        logger.info("[STOP] Automated training interrupted by user")
    except Exception as e:
        logger.error(f"[ERROR] Automated training failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
