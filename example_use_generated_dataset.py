#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏
========================================================

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç—É—é –∑–∞–≥—Ä—É–∑–∫—É –≥–æ—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ 
–∏ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ training loop –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏.
"""

import sys
import torch
from torch.utils.data import Dataset, DataLoader, TensorDataset
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from energy_flow.config import create_debug_config, set_energy_config
from energy_flow.training import EnergyTrainer


class SimpleDatasetLoader:
    """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è –≥–æ—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    
    @staticmethod
    def load_dataset(filepath: str) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print(f"üì• Loading dataset: {Path(filepath).name}")
        
        try:
            data = torch.load(filepath)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç
            required_keys = ['input_embeddings', 'target_embeddings']
            missing_keys = [key for key in required_keys if key not in data]
            if missing_keys:
                raise ValueError(f"Missing keys in dataset: {missing_keys}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            sample_count = data['input_embeddings'].shape[0]
            embedding_dim = data['input_embeddings'].shape[1]
            device = data['input_embeddings'].device
            
            print(f"‚úÖ Dataset loaded successfully:")
            print(f"   Samples: {sample_count:,}")
            print(f"   Embedding dimension: {embedding_dim}")
            print(f"   Device: {device}")
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'generation_info' in data:
                gen_info = data['generation_info']
                print(f"   Mode: {gen_info.get('mode', 'unknown')}")
                print(f"   Sources: {', '.join(gen_info.get('sources', []))}")
                
            return data
            
        except Exception as e:
            print(f"‚ùå Failed to load dataset: {e}")
            raise
    
    @staticmethod
    def create_dataloader(data: dict, batch_size: int = 32, shuffle: bool = True) -> DataLoader:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        input_embeddings = data['input_embeddings']
        target_embeddings = data['target_embeddings']
        
        # –ü—Ä–æ—Å—Ç–æ–π TensorDataset
        dataset = TensorDataset(input_embeddings, target_embeddings)
        
        # –°–æ–∑–¥–∞–µ–º generator –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º shuffle
        generator = None
        if shuffle:
            device = input_embeddings.device
            generator = torch.Generator(device=device)
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            drop_last=True,
            generator=generator
        )
        
        print(f"üì¶ DataLoader created: {len(dataloader)} batches, batch_size={batch_size}")
        
        return dataloader


def find_latest_dataset(active_dir: str = "data/energy_flow/active") -> str:
    """–ù–∞–π—Ç–∏ —Å–∞–º—ã–π –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤ –∞–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    active_path = Path(active_dir)
    
    if not active_path.exists():
        raise FileNotFoundError(f"Active directory not found: {active_dir}")
    
    # –ò—â–µ–º .pt —Ñ–∞–π–ª—ã
    dataset_files = list(active_path.glob("*.pt"))
    
    if not dataset_files:
        raise FileNotFoundError(f"No dataset files found in {active_dir}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
    latest_file = max(dataset_files, key=lambda f: f.stat().st_mtime)
    
    return str(latest_file)


def training_example_with_generated_dataset():
    """–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("üöÄ Training Example with Generated Dataset")
    print("=" * 50)
    
    try:
        # 1. –ò—â–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        print("\n1Ô∏è‚É£ Finding latest dataset...")
        try:
            dataset_file = find_latest_dataset()
            print(f"üìÅ Using: {Path(dataset_file).name}")
        except FileNotFoundError as e:
            print(f"‚ùå {e}")
            print("üí° First generate a dataset using: python generate_energy_dataset.py --mode debug")
            return
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        print("\n2Ô∏è‚É£ Loading dataset...")
        dataset_data = SimpleDatasetLoader.load_dataset(dataset_file)
        
        # 3. –°–æ–∑–¥–∞–µ–º DataLoader
        print("\n3Ô∏è‚É£ Creating DataLoader...")
        dataloader = SimpleDatasetLoader.create_dataloader(
            dataset_data, 
            batch_size=8, 
            shuffle=True
        )
        
        # 4. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        print("\n4Ô∏è‚É£ Setting up training...")
        energy_config = create_debug_config()
        set_energy_config(energy_config)
        
        trainer = EnergyTrainer(energy_config)
        print("‚úÖ EnergyTrainer initialized")
        
        # 5. –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        print("\n5Ô∏è‚É£ Running training loop...")
        
        for epoch in range(2):  # –¢–æ–ª—å–∫–æ 2 —ç–ø–æ—Ö–∏ –¥–ª—è –¥–µ–º–æ
            print(f"\nEpoch {epoch + 1}/2:")
            print("-" * 20)
            
            epoch_loss = 0.0
            batches_processed = 0
            
            for batch_idx, (input_embeddings, target_embeddings) in enumerate(dataloader):
                
                # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
                batch_size = input_embeddings.shape[0]
                input_texts = [f"input_sample_{i}" for i in range(batch_size)]
                target_texts = [f"target_sample_{i}" for i in range(batch_size)]
                
                # –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                step_metrics = trainer.train_step(
                    input_texts=input_texts,
                    target_texts=target_texts,
                    teacher_input_embeddings=input_embeddings,
                    teacher_target_embeddings=target_embeddings
                )
                
                current_loss = step_metrics.get('total_loss', 0)
                epoch_loss += current_loss
                batches_processed += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ 5-–≥–æ –±–∞—Ç—á–∞
                if batch_idx % 5 == 0:
                    print(f"  Batch {batch_idx + 1}/{len(dataloader)}: loss={current_loss:.4f}")
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–µ–º–æ
                if batch_idx >= 15:  # –ú–∞–∫—Å–∏–º—É–º 15 –±–∞—Ç—á–µ–π
                    break
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–æ—Ö–∏
            avg_loss = epoch_loss / batches_processed if batches_processed > 0 else 0
            print(f"  Epoch {epoch + 1} completed: avg_loss={avg_loss:.4f}, batches={batches_processed}")
        
        # 6. –†–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"\nüéâ Training completed successfully!")
        print(f"   Dataset: {Path(dataset_file).name}")
        print(f"   Samples: {dataset_data['input_embeddings'].shape[0]:,}")
        print(f"   Final loss: {avg_loss:.4f}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def inspect_dataset_example():
    """–ü—Ä–∏–º–µ—Ä –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("\nüîç Dataset Inspection Example")
    print("-" * 30)
    
    try:
        dataset_file = find_latest_dataset()
        data = SimpleDatasetLoader.load_dataset(dataset_file)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        print(f"\nüìä Dataset Analysis:")
        print(f"   Main keys: {list(data.keys())}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        input_emb = data['input_embeddings']
        target_emb = data['target_embeddings']
        
        print(f"\nüìà Embedding Statistics:")
        print(f"   Input norms: mean={input_emb.norm(dim=1).mean():.4f}, "
              f"std={input_emb.norm(dim=1).std():.4f}")
        print(f"   Target norms: mean={target_emb.norm(dim=1).mean():.4f}, "
              f"std={target_emb.norm(dim=1).std():.4f}")
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        if 'generation_info' in data:
            gen_info = data['generation_info']
            print(f"\nüè∑Ô∏è Generation Info:")
            print(f"   Generated: {gen_info.get('generation_timestamp', 'unknown')}")
            print(f"   Generation time: {gen_info.get('generation_time', 0):.1f}s")
            print(f"   Target pairs: {gen_info.get('target_pairs', 'unknown'):,}")
            print(f"   Actual pairs: {gen_info.get('actual_pairs', 'unknown'):,}")
        
        # –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'text_pairs' in data and data['text_pairs']:
            print(f"\nüìù Sample Text Pairs:")
            for i, (input_text, target_text) in enumerate(data['text_pairs'][:3]):
                print(f"   {i+1}. Input: '{input_text[:50]}...'")
                print(f"      Target: '{target_text[:50]}...'")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Inspection failed: {e}")
        return False


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    try:
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è
        success = training_example_with_generated_dataset()
        
        if success:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
            inspect_dataset_example()
            
            print(f"\n‚ú® All examples completed successfully!")
            print(f"\nüí° Key takeaways:")
            print(f"   - –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: torch.load(filepath)")
            print(f"   - –°–æ–∑–¥–∞–Ω–∏–µ DataLoader: TensorDataset + DataLoader")
            print(f"   - –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ trainer.train_step()")
            print(f"   - –ù–∏–∫–∞–∫–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π!")
        
    except KeyboardInterrupt:
        print("\nüõë Demo interrupted by user")
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")


if __name__ == "__main__":
    main()