#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è 3D –∫—É–±–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç 1/5 —á–∞—Å—Ç—å SNLI (Stanford Natural Language Inference) –¥–∞—Ç–∞—Å–µ—Ç–∞
"""

import torch
import numpy as np
import json
import time
from datetime import datetime
from pathlib import Path
import logging
from typing import List, Dict, Tuple, Optional
import random
from datasets import load_dataset

from simple_embedding_fallback import SimpleFallbackEmbeddingLoader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SNLIEmbeddingGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞"""

    def __init__(self, teacher_model: str = "distilbert-base-uncased"):
        self.teacher_model = teacher_model
        self.loader = SimpleFallbackEmbeddingLoader(teacher_model)

    def load_snli_dataset(self, size_fraction: float = 0.2) -> List[Dict[str, str]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç SNLI –¥–∞—Ç–∞—Å–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç premise-hypothesis –ø–∞—Ä—ã

        Args:
            size_fraction: –ö–∞–∫—É—é —á–∞—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (0.2 = 1/5)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–∞—Ä–∞–º–∏ premise-hypothesis
        """
        logger.info(
            f"[SNLI] –ó–∞–≥—Ä—É–∂–∞–µ–º SNLI –¥–∞—Ç–∞—Å–µ—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º {size_fraction*100:.0f}% –¥–∞–Ω–Ω—ã—Ö)"
        )

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º SNLI –¥–∞—Ç–∞—Å–µ—Ç
            dataset = load_dataset("snli")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ train split –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            train_data = dataset["train"]
            total_size = len(train_data)

            # –í—ã—á–∏—Å–ª—è–µ–º –Ω—É–∂–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            target_size = int(total_size * size_fraction)

            logger.info(f"[SNLI] –ü–æ–ª–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {total_size:,} –ø—Ä–∏–º–µ—Ä–æ–≤")
            logger.info(f"[SNLI] –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: {target_size:,} –ø—Ä–∏–º–µ—Ä–æ–≤")

            # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            indices = random.sample(range(total_size), target_size)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            pairs = []
            valid_labels = {
                "entailment",
                "contradiction",
                "neutral",
            }  # –ò—Å–∫–ª—é—á–∞–µ–º -1 (invalid)

            for idx in indices:
                example = train_data[idx]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
                if (
                    example["label"] != -1  # –í–∞–ª–∏–¥–Ω—ã–π label
                    and example["premise"]  # –ù–µ –ø—É—Å—Ç–æ–π premise
                    and example["hypothesis"]  # –ù–µ –ø—É—Å—Ç–æ–π hypothesis
                    and len(example["premise"].strip()) > 10  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π
                    and len(example["hypothesis"].strip()) > 10
                ):

                    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ question-answer
                    pair = {
                        "question": example["premise"],
                        "answer": example["hypothesis"],
                        "label": example["label"],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º label –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        "snli_id": idx,
                    }
                    pairs.append(pair)

            logger.info(f"[SNLI] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(pairs):,} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ labels
            label_counts = {}
            for pair in pairs:
                label = pair["label"]
                label_counts[label] = label_counts.get(label, 0) + 1

            logger.info(f"[SNLI] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ labels:")
            label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
            for label_id, count in label_counts.items():
                logger.info(
                    f"   {label_names.get(label_id, 'unknown')}: {count:,} ({count/len(pairs)*100:.1f}%)"
                )

            return pairs

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ SNLI: {e}")
            raise

    def create_embedding_dataset(
        self,
        snli_pairs: List[Dict[str, str]],
        normalize: bool = True,
        batch_size: int = 32,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ SNLI –ø–∞—Ä

        Args:
            snli_pairs: –°–ø–∏—Å–æ–∫ –ø–∞—Ä premise-hypothesis
            normalize: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

        Returns:
            Tuple (question_embeddings, answer_embeddings)
        """
        logger.info(f"[EMBED] –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(snli_pairs):,} –ø–∞—Ä")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        premises = [pair["question"] for pair in snli_pairs]  # premise –∫–∞–∫ question
        hypotheses = [pair["answer"] for pair in snli_pairs]  # hypothesis –∫–∞–∫ answer

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        premise_embeddings = []
        hypothesis_embeddings = []

        num_batches = (len(premises) + batch_size - 1) // batch_size

        for i in range(0, len(premises), batch_size):
            batch_end = min(i + batch_size, len(premises))
            batch_premises = premises[i:batch_end]
            batch_hypotheses = hypotheses[i:batch_end]

            logger.info(f"[EMBED] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {i//batch_size + 1}/{num_batches}")

            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            premise_batch_emb = self.loader.encode_texts(batch_premises)
            hypothesis_batch_emb = self.loader.encode_texts(batch_hypotheses)

            premise_embeddings.append(premise_batch_emb)
            hypothesis_embeddings.append(hypothesis_batch_emb)

            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞—Ç—á–∏
        premise_embeddings = torch.cat(premise_embeddings, dim=0)
        hypothesis_embeddings = torch.cat(hypothesis_embeddings, dim=0)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if normalize:
            premise_embeddings = torch.nn.functional.normalize(
                premise_embeddings, dim=1
            )
            hypothesis_embeddings = torch.nn.functional.normalize(
                hypothesis_embeddings, dim=1
            )
            logger.info("[EMBED] –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã")

        logger.info(f"[EMBED] –°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
        logger.info(f"   Premise embeddings: {premise_embeddings.shape}")
        logger.info(f"   Hypothesis embeddings: {hypothesis_embeddings.shape}")
        logger.info(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {premise_embeddings.shape[1]}")

        return premise_embeddings, hypothesis_embeddings

    def save_dataset(
        self,
        premise_embeddings: torch.Tensor,
        hypothesis_embeddings: torch.Tensor,
        snli_pairs: List[Dict[str, str]],
        output_dir: str = "data/embeddings",
    ):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å precomputed_embedding_loader"""

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"snli_embeddings_{len(snli_pairs)}pairs_{self.teacher_model.replace('/', '_')}_{timestamp}.pt"
        file_path = output_path / filename

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_data = {
            "question_embeddings": premise_embeddings,
            "answer_embeddings": hypothesis_embeddings,
            "teacher_model": self.teacher_model,
            "timestamp": timestamp,
            "size": len(snli_pairs),
            "dataset_info": {
                "source": "SNLI (Stanford Natural Language Inference)",
                "pairs_count": len(snli_pairs),
                "embedding_dim": premise_embeddings.shape[1],
                "data_format": "premise -> hypothesis pairs",
                "fraction_used": "1/5 of SNLI dataset",
                "labels_distribution": self._get_label_distribution(snli_pairs),
            },
            "sample_pairs": snli_pairs[:10],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        logger.info(f"[SAVE] –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç: {filename}")
        torch.save(save_data, file_path)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ
        file_size_mb = file_path.stat().st_size / 1024 / 1024
        logger.info(f"[SAVE] –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω:")
        logger.info(f"   –§–∞–π–ª: {file_path}")
        logger.info(f"   –†–∞–∑–º–µ—Ä: {file_size_mb:.1f} MB")
        logger.info(f"   –ü–∞—Ä: {len(snli_pairs):,}")
        logger.info(f"   –ú–æ–¥–µ–ª—å: {self.teacher_model}")

        return str(file_path)

    def _get_label_distribution(self, pairs: List[Dict[str, str]]) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ labels –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        label_names = {0: "entailment", 1: "neutral", 2: "contradiction"}
        distribution = {}

        for pair in pairs:
            label_name = label_names.get(pair["label"], "unknown")
            distribution[label_name] = distribution.get(label_name, 0) + 1

        return distribution


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ SNLI –¥–∞—Ç–∞—Å–µ—Ç–∞"
    )
    parser.add_argument(
        "--fraction",
        type=float,
        default=0.2,
        help="–ö–∞–∫—É—é —á–∞—Å—Ç—å SNLI –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (0.2 = 1/5)",
    )
    parser.add_argument(
        "--model",
        default="distilbert-base-uncased",
        help="Teacher –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
    )
    parser.add_argument(
        "--batch-size", type=int, default=32, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )
    parser.add_argument(
        "--output-dir", default="data/embeddings", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"
    )
    parser.add_argument(
        "--no-normalize", action="store_true", help="–ù–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"
    )

    args = parser.parse_args()

    try:
        # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        generator = SNLIEmbeddingGenerator(args.model)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º SNLI –¥–∞–Ω–Ω—ã–µ
        snli_pairs = generator.load_snli_dataset(size_fraction=args.fraction)

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        premise_embeddings, hypothesis_embeddings = generator.create_embedding_dataset(
            snli_pairs, normalize=not args.no_normalize, batch_size=args.batch_size
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        saved_file = generator.save_dataset(
            premise_embeddings, hypothesis_embeddings, snli_pairs, args.output_dir
        )

        logger.info(f"‚úÖ SNLI —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≥–æ—Ç–æ–≤—ã: {saved_file}")
        logger.info(f"   –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ run_dynamic_training.py")

        # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ precomputed_embedding_loader
        logger.info(f"\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —á–µ—Ä–µ–∑ PrecomputedEmbeddingLoader...")
        from precomputed_embedding_loader import PrecomputedEmbeddingLoader

        loader = PrecomputedEmbeddingLoader()
        dataset = loader.load_dataset(saved_file)

        sample_q, sample_a = dataset[0]
        logger.info(f"üß™ –¢–µ—Å—Ç —É—Å–ø–µ—à–µ–Ω:")
        logger.info(f"   Sample premise embedding: {sample_q.shape}")
        logger.info(f"   Sample hypothesis embedding: {sample_a.shape}")
        logger.info(f"   –ì–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")

    except KeyboardInterrupt:
        logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
