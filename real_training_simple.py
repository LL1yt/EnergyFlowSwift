#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 3D Cellular Neural Network
==================================================================

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–û–õ–¨–ö–û —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ new_rebuild.config
–°–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∞–º CLAUDE.md - –Ω–∏–∫–∞–∫–∏—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
"""

import torch
import logging
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

from new_rebuild.config import SimpleProjectConfig
from new_rebuild.core.training import EmbeddingTrainer
from new_rebuild.core.training.utils import create_training_dataloader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_experiment_tracking(experiment_name: str) -> Path:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = Path(f"experiments/{experiment_name}_{timestamp}")
    experiment_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    (experiment_dir / "checkpoints").mkdir(exist_ok=True)
    (experiment_dir / "logs").mkdir(exist_ok=True)
    (experiment_dir / "metrics").mkdir(exist_ok=True)
    
    logger.info(f"üìä Experiment tracking setup: {experiment_dir}")
    return experiment_dir


def run_training_epoch(trainer: EmbeddingTrainer, dataloader, epoch: int, experiment_dir: Path) -> Dict[str, float]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è"""
    
    logger.info(f"\nüöÄ Starting Epoch {epoch + 1}")
    epoch_start_time = time.time()
    
    # Training phase
    trainer.model.train()
    train_losses = {"total": 0.0, "reconstruction": 0.0, "similarity": 0.0, "diversity": 0.0, "emergence": 0.0}
    num_batches = 0
    
    for batch_idx, batch in enumerate(dataloader):
        batch_start_time = time.time()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –±–∞—Ç—á–∞
        embeddings = batch['embedding']  # [batch_size, 768]
        
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ trainer
        try:
            losses = trainer.train_step(embeddings)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            for key in train_losses.keys():
                if key in losses:
                    train_losses[key] += losses[key]
            
            num_batches += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ log_interval –±–∞—Ç—á–µ–π (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            log_interval = trainer.config.training_embedding.log_interval
            if (batch_idx + 1) % log_interval == 0:
                batch_time = time.time() - batch_start_time
                current_loss = losses.get('total', 0.0)
                logger.info(f"  Batch {batch_idx + 1:3d}: loss={current_loss:.6f}, time={batch_time:.2f}s")
                
        except Exception as e:
            logger.error(f"Error in batch {batch_idx}: {e}")
            continue
    
    # –£—Å—Ä–µ–¥–Ω—è–µ–º loss'—ã
    if num_batches > 0:
        for key in train_losses.keys():
            train_losses[key] /= num_batches
    
    epoch_time = time.time() - epoch_start_time
    
    logger.info(f"‚úÖ Epoch {epoch + 1} completed in {epoch_time:.2f}s")
    logger.info(f"   Average losses: {', '.join([f'{k}={v:.6f}' for k, v in train_losses.items()])}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics_file = experiment_dir / "metrics" / f"epoch_{epoch + 1}_metrics.json"
    metrics_file.parent.mkdir(exist_ok=True)
    
    metrics = {
        "epoch": epoch + 1,
        "train_losses": train_losses,
        "epoch_time": epoch_time,
        "num_batches": num_batches,
        "timestamp": datetime.now().isoformat()
    }
    
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    return train_losses


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    print("üöÄ STARTING REAL 3D CELLULAR NEURAL NETWORK TRAINING")
    print("Using CENTRAL CONFIG ONLY (new_rebuild.config)")
    print("=" * 60)
    
    # === –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–õ–¨–ö–û –¶–ï–ù–¢–†–ê–õ–¨–ù–£–Æ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ ===
    logger.info("‚öôÔ∏è Loading central configuration...")
    config = SimpleProjectConfig()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    if config.training_embedding.test_mode:
        logger.warning("‚ö†Ô∏è test_mode=True in config! Switch to real training mode in config_components.py")
        print("\n‚ùå CONFIGURATION ERROR:")
        print("test_mode=True in central config!")
        print("Edit new_rebuild/config/config_components.py:")
        print("  Change: test_mode: bool = False")
        return
    
    logger.info("‚úÖ Real training mode enabled")
    logger.info(f"üìè Lattice size: {config.lattice.dimensions}")
    logger.info(f"üéØ Target embedding dim: {config.training_embedding.target_embedding_dim}")
    logger.info(f"üìä Epochs: {config.training_embedding.num_epochs}")
    logger.info(f"üî• Batch size: {config.training_embedding.embedding_batch_size}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment_name = f"real_training_{config.lattice.dimensions[0]}x{config.lattice.dimensions[1]}x{config.lattice.dimensions[2]}"
    experiment_dir = setup_experiment_tracking(experiment_name)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
    logger.info("üìÇ Loading unified dataset...")
    if config.training_embedding.test_mode:
        max_samples = config.training_embedding.test_dataset_size
    else:
        max_samples = None  # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    
    dataloader, dataset_stats = create_training_dataloader(
        config=config,
        max_samples_per_source=max_samples,
        shuffle=True
    )
    
    logger.info(f"üìä Dataset loaded: {dataset_stats.total_samples} total samples")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞
    with open(experiment_dir / "dataset_stats.json", 'w') as f:
        stats_dict = {
            "total_samples": dataset_stats.total_samples,
            "embedding_dim": dataset_stats.embedding_dim,
            "source_distribution": dataset_stats.source_distribution,
            "type_distribution": dataset_stats.type_distribution
        }
        json.dump(stats_dict, f, indent=2)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ trainer'–∞
    logger.info("üß† Initializing EmbeddingTrainer...")
    trainer = EmbeddingTrainer(config)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    num_epochs = config.training_embedding.num_epochs
    logger.info(f"üéØ Starting training for {num_epochs} epochs...")
    
    best_loss = float('inf')
    patience_counter = 0
    early_stopping_patience = config.training_embedding.early_stopping_patience
    
    for epoch in range(num_epochs):
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_losses = run_training_epoch(trainer, dataloader, epoch, experiment_dir)
        current_loss = train_losses['total']
        
        # Early stopping check
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            best_model_path = experiment_dir / "checkpoints" / "best_model.pth"
            trainer.save_checkpoint(str(best_model_path), epoch=epoch + 1, loss=current_loss)
            logger.info(f"üíæ New best model saved: loss={current_loss:.6f}")
            
        else:
            patience_counter += 1
            logger.info(f"‚ö†Ô∏è No improvement for {patience_counter} epochs")
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ checkpoint'—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
        save_interval = config.training_embedding.save_checkpoint_every
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = experiment_dir / "checkpoints" / f"epoch_{epoch + 1}.pth"
            trainer.save_checkpoint(str(checkpoint_path), epoch=epoch + 1, loss=current_loss)
            logger.info(f"üíæ Regular checkpoint saved: epoch_{epoch + 1}.pth")
        
        # Early stopping
        if patience_counter >= early_stopping_patience:
            logger.info(f"üõë Early stopping triggered after {patience_counter} epochs without improvement")
            break
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_model_path = experiment_dir / "checkpoints" / "final_model.pth"
    trainer.save_checkpoint(str(final_model_path), epoch=epoch + 1, loss=current_loss)
    
    # –°–≤–æ–¥–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    summary = {
        "experiment_name": experiment_name,
        "total_epochs": epoch + 1,
        "best_loss": best_loss,
        "final_loss": current_loss,
        "dataset_samples": dataset_stats.total_samples,
        "config_summary": {
            "lattice_size": config.lattice.dimensions,
            "state_size": config.model.state_size,
            "batch_size": config.training_embedding.embedding_batch_size,
            "learning_rate": config.training.learning_rate,
            "test_mode": config.training_embedding.test_mode
        },
        "completion_time": datetime.now().isoformat()
    }
    
    with open(experiment_dir / "experiment_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nüéâ TRAINING COMPLETED!")
    print(f"üìä Experiment results saved to: {experiment_dir}")
    print(f"üèÜ Best loss achieved: {best_loss:.6f}")
    print(f"üìà Total samples processed: {dataset_stats.total_samples}")
    print(f"\nüöÄ Ready for analysis and next steps!")


if __name__ == "__main__":
    main()