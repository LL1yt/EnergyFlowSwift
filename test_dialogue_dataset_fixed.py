#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç DialogueDataset - Stage 1.3
=============================================

–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DialogueDataset –¥–ª—è dialogue –æ–±—É—á–µ–Ω–∏—è —Å Teacher LLM —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏.
–ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É: question_embedding ‚Üí answer_embedding —á–µ—Ä–µ–∑ Teacher LLM.

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
- –°–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫—É–±–∞: [8, 8, 12] = 768D
- –£–±—Ä–∞–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ CubeTrainer

–ê–≤—Ç–æ—Ä: 3D Cellular Neural Network Project
–í–µ—Ä—Å–∏—è: v1.0.0 (Phase 3.1 - Stage 1.3)
–î–∞—Ç–∞: 7 –∏—é–Ω—è 2025
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
import logging
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_dialogue_dataset_basic():
    """–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç DialogueDataset - Teacher LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"""
    
    print("üß™ –¢–ï–°–¢: DialogueDataset Basic Functionality")
    print("=" * 60)
    
    try:
        # –ò–º–ø–æ—Ä—Ç DialogueDataset
        from training.embedding_trainer import (
            DialogueDataset, 
            DialogueConfig,
            create_dialogue_dataset,
            DIALOGUE_DATASET_AVAILABLE
        )
        
        if not DIALOGUE_DATASET_AVAILABLE:
            print("‚ùå DialogueDataset not available - dependencies missing")
            return False
        
        print("‚úÖ DialogueDataset imported successfully")
        
        # 1. –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è DialogueDataset –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä
        print("\nüîπ –¢–µ—Å—Ç 1: DialogueDataset –∏–∑ Q&A –ø–∞—Ä")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–∞—Ä (—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã)
        dialogue_pairs = [
            {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å?",
                "answer": "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å - —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –≤ –º–æ–∑–≥–µ."
            },
            {
                "question": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
                "answer": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —É—á–∏—Ç—å—Å—è –∏ —É–ª—É—á—à–∞—Ç—å —Å–≤–æ—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö."
            },
            {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
                "answer": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω—ã—Ö –º–∞—à–∏–Ω."
            },
            {
                "question": "–†–∞—Å—Å–∫–∞–∂–∏ –æ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏",
                "answer": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö."
            },
            {
                "question": "–ö–∞–∫ –¥–µ–ª–∞?",
                "answer": "–•–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ! –†–∞–±–æ—Ç–∞—é –Ω–∞–¥ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."
            }
        ]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ dataset —á–µ—Ä–µ–∑ —É–¥–æ–±–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        dataset = create_dialogue_dataset(
            dialogue_pairs=dialogue_pairs,
            teacher_model="distilbert",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–¥–µ–∂–Ω—É—é –º–æ–¥–µ–ª—å
            validation_split=0.2,
            use_cache=True,
            normalize_embeddings=True,
            enable_quality_filter=True
        )
        
        print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω DialogueDataset: {dataset}")
        print(f"   üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä: {len(dataset.question_embeddings)}")
        print(f"   üéì Train –ø–∞—Ä—ã: {len(dataset.train_questions)}")
        print(f"   üìù Val –ø–∞—Ä—ã: {len(dataset.val_questions)}")
        print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤: {dataset.config.embedding_dim}")
        print(f"   üß† Teacher –º–æ–¥–µ–ª—å: {dataset.config.teacher_model}")
        
        # 2. –¢–µ—Å—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (question_embedding ‚Üí answer_embedding)
        print("\nüîπ –¢–µ—Å—Ç 2: Teacher LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (Q‚ÜíA)")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –æ–±—É—á–∞—é—â–µ–π –ø–∞—Ä—ã
        question_emb, answer_emb = dataset[0]
        
        print(f"   ‚úÖ Question embedding shape: {question_emb.shape}")
        print(f"   ‚úÖ Answer embedding shape: {answer_emb.shape}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –¥–ª—è 3D Cubic Core)
        assert question_emb.shape == answer_emb.shape, f"Embedding shapes must match: {question_emb.shape} vs {answer_emb.shape}"
        assert len(question_emb.shape) == 1, f"Embeddings must be 1D: {question_emb.shape}"
        assert question_emb.shape[0] == dataset.config.embedding_dim, f"Wrong embedding dimension: {question_emb.shape[0]}"
        
        print(f"   ‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞: {question_emb.shape} ‚Üí {answer_emb.shape}")
        
        # 3. –¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ Q&A
        print("\nüîπ –¢–µ—Å—Ç 3: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å Q&A")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ cosine similarity –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏
        cosine_similarities = []
        for i in range(min(5, len(dataset))):
            q_emb, a_emb = dataset[i]
            similarity = torch.cosine_similarity(q_emb.unsqueeze(0), a_emb.unsqueeze(0)).item()
            cosine_similarities.append(similarity)
            
            metadata = dataset.dialogue_metadata[i] if dataset.dialogue_metadata else {"question": "N/A", "answer": "N/A"}
            print(f"   Q: '{metadata['question'][:40]}...'")
            print(f"   A: '{metadata['answer'][:40]}...'")
            print(f"   Similarity: {similarity:.4f}")
            print()
        
        avg_similarity = np.mean(cosine_similarities)
        print(f"   ‚úÖ –°—Ä–µ–¥–Ω—è—è Q&A similarity: {avg_similarity:.4f}")
        
        # 4. –¢–µ—Å—Ç DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print("\nüîπ –¢–µ—Å—Ç 4: DataLoader –¥–ª—è dialogue training")
        
        # Train DataLoader
        train_loader = dataset.get_dataloader(batch_size=2, shuffle=True, validation=False)
        val_loader = dataset.get_dataloader(batch_size=2, shuffle=False, validation=True)
        
        print(f"   ‚úÖ Train batches: {len(train_loader)}")
        print(f"   ‚úÖ Val batches: {len(val_loader)}")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞
        for batch_questions, batch_answers in train_loader:
            print(f"   ‚úÖ Batch Q shape: {batch_questions.shape}")
            print(f"   ‚úÖ Batch A shape: {batch_answers.shape}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è 3D Cubic Core –æ–±—É—á–µ–Ω–∏—è
            assert batch_questions.shape == batch_answers.shape, "Batch shapes must match"
            assert len(batch_questions.shape) == 2, "Batch must be 2D: [batch_size, embedding_dim]"
            
            break  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
        
        # 5. –¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
        print("\nüîπ –¢–µ—Å—Ç 5: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ dataset")
        
        stats = dataset.get_statistics()
        
        print(f"   ‚úÖ –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"      Dialogue pairs: {stats['total_dialogue_pairs']}")
        print(f"      Teacher model: {stats['teacher_model']}")
        print(f"      Cache hits: {stats['cache_stats']['cache_hits']}")
        print(f"      Quality filtered: {stats['cache_stats']['quality_filtered']}")
        
        if 'embedding_quality' in stats:
            eq = stats['embedding_quality']
            print(f"   ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤:")
            print(f"      Q norm mean: {eq['question_norm_mean']:.4f}")
            print(f"      A norm mean: {eq['answer_norm_mean']:.4f}")
            print(f"      Q&A similarity: {eq['qa_similarity_mean']:.4f} ¬± {eq['qa_similarity_std']:.4f}")
        
        # 6. –¢–µ—Å—Ç –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∏–∞–ª–æ–≥–æ–≤
        print("\nüîπ –¢–µ—Å—Ç 6: –ü—Ä–∏–º–µ—Ä—ã –¥–∏–∞–ª–æ–≥–æ–≤")
        
        samples = dataset.get_sample_dialogues(n_samples=3)
        
        if 'samples' in samples:
            for i, sample in enumerate(samples['samples']):
                print(f"   –ü—Ä–∏–º–µ—Ä {i+1}:")
                print(f"      Q: '{sample['question'][:50]}...'")
                print(f"      A: '{sample['answer'][:50]}...'")
                print(f"      QA similarity: {sample['qa_similarity']:.4f}")
                print()
        
        # 7. –¢–µ—Å—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        print("\nüîπ –¢–µ—Å—Ç 7: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DialogueConfig")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        custom_config = DialogueConfig(
            teacher_model="distilbert",
            embedding_dim=768,
            max_conversations=1000,
            enable_quality_filter=True,
            validation_split=0.15,
            normalize_embeddings=True,
            cache_embeddings=True
        )
        
        print(f"   ‚úÖ Custom config —Å–æ–∑–¥–∞–Ω:")
        print(f"      Teacher model: {custom_config.teacher_model}")
        print(f"      Embedding dim: {custom_config.embedding_dim}")
        print(f"      Quality filter: {custom_config.enable_quality_filter}")
        print(f"      Validation split: {custom_config.validation_split}")
        
        # 8. –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å CubeTrainer (simulation)
        print("\nüîπ –¢–µ—Å—Ç 8: –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å CubeTrainer")
        
        try:
            from training.embedding_trainer import CubeTrainer, TrainingConfig
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –≤ dialogue —Ä–µ–∂–∏–º–µ —Å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∫—É–±–∞
            training_config = TrainingConfig(
                mode="dialogue",
                device="cpu",
                learning_rate=0.001,
                embedding_dim=dataset.config.embedding_dim,
                lattice_size=[8, 8, 12],  # 8*8*12 = 768 (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å DistilBERT 768D)
                batch_size=2
            )
            
            trainer = CubeTrainer(config=training_config)
            trainer.initialize_components()
            
            print(f"   ‚úÖ CubeTrainer —Å–æ–∑–¥–∞–Ω –≤ dialogue —Ä–µ–∂–∏–º–µ")
            print(f"   ‚úÖ Mode: {trainer.config.mode}")
            print(f"   ‚úÖ Lattice size: {trainer.config.lattice_size}")
            print(f"   ‚úÖ Embedding dim: {trainer.config.embedding_dim}")
            
            # –¢–µ—Å—Ç forward pass (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            sample_question, sample_answer = dataset[0]
            
            print(f"   üìè Sample shapes: Q={sample_question.shape}, A={sample_answer.shape}")
            
            # Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è CubeTrainer
            batch_input = sample_question.unsqueeze(0)  # [768] ‚Üí [1, 768] 
            processed_embedding = trainer.forward(batch_input)
            
            print(f"   ‚úÖ Forward pass test: {batch_input.shape} ‚Üí {processed_embedding.shape}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            assert processed_embedding.shape == batch_input.shape, f"Shape mismatch: {processed_embedding.shape} vs {batch_input.shape}"
            assert processed_embedding.shape[0] == 1, f"Batch size mismatch: {processed_embedding.shape[0]} vs 1"
            assert processed_embedding.shape[1] == sample_answer.shape[0], f"Embedding dim mismatch: {processed_embedding.shape[1]} vs {sample_answer.shape[0]}"
            
            print("   üéØ Teacher LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å CubeTrainer!")
            
        except ImportError:
            print("   ‚ö†Ô∏è  CubeTrainer not available - skipping compatibility test")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  CubeTrainer compatibility issue: {e}")
        
        print("\n" + "=" * 60)
        print("üéâ –í–°–ï –¢–ï–°–¢–´ DialogueDataset –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("üöÄ Stage 1.3 DialogueDataset –ì–û–¢–û–í –ö PRODUCTION!")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –í –¢–ï–°–¢–ï: {e}")
        import traceback
        print("Traceback:")
        traceback.print_exc()
        return False


def test_dialogue_dataset_advanced():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç DialogueDataset - —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    print("\nüß™ –¢–ï–°–¢: DialogueDataset Advanced Features")
    print("=" * 60)
    
    try:
        from training.embedding_trainer import (
            create_conversation_dataset,
            create_dialogue_dataset,
            load_dialogue_dataset_from_files,
            DialogueConfig
        )
        
        # 1. –¢–µ—Å—Ç –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
        print("\nüîπ –¢–µ—Å—Ç 1: Multi-turn conversations")
        
        # –ü—Ä–∏–º–µ—Ä –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞
        conversations = [
            [
                {"role": "user", "text": "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"},
                {"role": "assistant", "text": "–ü—Ä–∏–≤–µ—Ç! –í—Å—ë —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ. –ö–∞–∫ —É —Ç–µ–±—è –¥–µ–ª–∞?"},
                {"role": "user", "text": "–¢–æ–∂–µ –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ. –†–∞—Å—Å–∫–∞–∂–∏ –æ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö"},
                {"role": "assistant", "text": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ - —ç—Ç–æ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç–æ–π –º–æ–∑–≥–∞"}
            ],
            [
                {"role": "user", "text": "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ?"},
                {"role": "assistant", "text": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"},
                {"role": "user", "text": "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ! –ì–¥–µ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è?"},
                {"role": "assistant", "text": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏, NLP, —Ä–æ–±–æ—Ç–∏–∫–µ –∏ –º–Ω–æ–≥–∏—Ö –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö"}
            ]
        ]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ dataset –∏–∑ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ)
        conv_dataset = create_conversation_dataset(
            conversations=conversations,
            teacher_model="distilbert",
            validation_split=0.0  # –í—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö)
        )
        
        print(f"   ‚úÖ Multi-turn dataset —Å–æ–∑–¥–∞–Ω: {conv_dataset}")
        print(f"   üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ Q&A –ø–∞—Ä: {len(conv_dataset.question_embeddings)}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä
        if conv_dataset.dialogue_metadata:
            for i, metadata in enumerate(conv_dataset.dialogue_metadata[:3]):
                print(f"   –ü–∞—Ä–∞ {i+1}: Q: '{metadata['question'][:30]}...' ‚Üí A: '{metadata['answer'][:30]}...'")
        
        # 2. –¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –∫–∞—á–µ—Å—Ç–≤–∞
        print("\nüîπ –¢–µ—Å—Ç 2: Quality filtering configurations")
        
        # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        strict_config = DialogueConfig(
            teacher_model="distilbert",
            enable_quality_filter=True,
            min_question_length=10,
            min_answer_length=20,
            max_question_length=100,
            max_answer_length=200
        )
        
        # –ú—è–≥–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        lenient_config = DialogueConfig(
            teacher_model="distilbert",
            enable_quality_filter=True,
            min_question_length=3,
            min_answer_length=5,
            max_question_length=1000,
            max_answer_length=2000
        )
        
        print(f"   ‚úÖ –°—Ç—Ä–æ–≥–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: Q len {strict_config.min_question_length}-{strict_config.max_question_length}")
        print(f"   ‚úÖ –ú—è–≥–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: Q len {lenient_config.min_question_length}-{lenient_config.max_question_length}")
        
        # 3. –¢–µ—Å—Ç caching —Å–∏—Å—Ç–µ–º—ã
        print("\nüîπ –¢–µ—Å—Ç 3: Smart caching system")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–≤—É—Ö –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö datasets –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—ç—à–∞
        test_pairs = [
            {"question": "Test question 1", "answer": "Test answer 1"},
            {"question": "Test question 2", "answer": "Test answer 2"}
        ]
        
        # –ü–µ—Ä–≤—ã–π dataset (cache miss)
        dataset1 = create_dialogue_dataset(
            dialogue_pairs=test_pairs,
            teacher_model="distilbert",
            use_cache=True,
            cache_dir="cache/test_dialogue"
        )
        
        # –í—Ç–æ—Ä–æ–π dataset (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å cache hit)
        dataset2 = create_dialogue_dataset(
            dialogue_pairs=test_pairs,
            teacher_model="distilbert",
            use_cache=True,
            cache_dir="cache/test_dialogue"
        )
        
        print(f"   ‚úÖ Dataset 1 cache stats: {dataset1.cache_stats}")
        print(f"   ‚úÖ Dataset 2 cache stats: {dataset2.cache_stats}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∫—ç—à —Ä–∞–±–æ—Ç–∞–µ—Ç
        if dataset2.cache_stats['cache_hits'] > 0:
            print("   üéØ Smart caching —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
        
        print("\n" + "=" * 60)
        print("üéâ –†–ê–°–®–ò–†–ï–ù–ù–´–ï –¢–ï–°–¢–´ DialogueDataset –ü–†–û–ô–î–ï–ù–´!")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –í –†–ê–°–®–ò–†–ï–ù–ù–û–ú –¢–ï–°–¢–ï: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø DialogueDataset - Stage 1.3")
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Teacher LLM (question_embedding ‚Üí answer_embedding)")
    print("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: [8,8,12] –∫—É–± = 768D, –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∞")
    
    # –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã
    basic_success = test_dialogue_dataset_basic()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
    advanced_success = test_dialogue_dataset_advanced()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if basic_success and advanced_success:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ DialogueDataset –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("‚úÖ Stage 1.3 DialogueDataset –≥–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å CubeTrainer")
        print("‚úÖ Teacher LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (Q‚ÜíA) —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        print("‚úÖ Smart caching, quality filtering, multi-turn –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã")
        print("‚úÖ –†–∞–∑–º–µ—Ä—ã –∫—É–±–∞ [8,8,12] = 768D —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å DistilBERT")
        print("‚úÖ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è CubeTrainer –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞")
        print("\nüöÄ –ì–û–¢–û–í –ö –ü–ï–†–ï–•–û–î–£ –ö DIALOGUE TRAINING!")
    else:
        print("\n‚ùå –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–ô–î–ï–ù–´")
        print("–¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç–ª–∞–¥–∫–∞ –ø–µ—Ä–µ–¥ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º") 