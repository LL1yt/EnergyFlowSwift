#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 3D Cellular Neural Network
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç unified dataset loader –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è 8x8x8 –∫—É–±–∞
"""

import torch
import logging
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

from unified_dataset_loader import create_training_dataloader, DatasetConfig
from new_rebuild.core.training import EmbeddingTrainer
from new_rebuild.config import SimpleProjectConfig
from new_rebuild.config.config_components import (
    LatticeSettings, ModelSettings, TrainingSettings, TrainingEmbeddingSettings,
    EmbeddingSettings, DeviceSettings, LoggingSettings, CacheSettings,
    SpatialSettings, MemorySettings
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_real_training_config() -> SimpleProjectConfig:
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 8x8x8 –∫—É–±–µ"""
    
    config = SimpleProjectConfig(
        # 8x8x8 –∫—É–± –¥–ª—è –Ω–∞—á–∞–ª–∞ (512 –∫–ª–µ—Ç–æ–∫)
        lattice=LatticeSettings(
            dimensions=(8, 8, 8),
            face_placement_strategy="random"
        ),
        
        # –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è RTX 5090
        model=ModelSettings(
            state_size=64,           # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è emergent behavior
            hidden_dim=128,          # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            neighbor_count=6         # 3D —Å–æ—Å–µ–¥–∏ (6-connectivity)
        ),
        
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        training=TrainingSettings(
            learning_rate=0.001,     # Conservative start
            batch_size=16,           # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            optimizer_type="adamw",  # AdamW –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            weight_decay=0.01,
            grad_clip_norm=1.0,
            scheduler_type="cosine_annealing",
            warmup_steps=100
        ),
        
        # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è embedding training
        training_embedding=TrainingEmbeddingSettings(
            test_mode=False,                 # –†–µ–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            num_epochs=50,                   # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
            validation_interval=1,           # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
            save_checkpoint_every=5,         # Checkpoint –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
            log_interval=10,                 # –õ–æ–≥ –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
            early_stopping_patience=10,      # Early stopping —á–µ—Ä–µ–∑ 10 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
            
            # Loss weights (–Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ø–ª–∞–Ω–∞)
            reconstruction_weight=1.0,       # –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞
            similarity_weight=0.5,           # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–æ—Ö–æ–∂–µ—Å—Ç—å
            diversity_weight=0.2,            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
            emergence_weight=0.1,            # Emergent behavior
            
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            target_embedding_dim=64,         # –°–∂–∏–º–∞–µ–º 768 ‚Üí 64 –¥–ª—è –∫—É–±–∞ 8x8x8
            teacher_model="distilbert-base-uncased",
            use_teacher_forcing=True,
            lattice_steps=5                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è
        ),
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding=EmbeddingSettings(
            input_dim=768,                   # DistilBERT dimension
            output_dim=64,                   # –î–ª—è –∫—É–±–∞ 8x8x8
            use_projection=True,
            projection_layers=[768, 256, 64],
            dropout_rate=0.1
        ),
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º RTX 5090
        device=DeviceSettings(
            prefer_cuda=True,
            force_device=None,               # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            debug_mode=False,
            mixed_precision=True             # –í–∫–ª—é—á–∞–µ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        ),
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        cache=CacheSettings(
            use_gpu_cache=True,
            gpu_cache_size_mb=4096,          # 4GB cache –¥–ª—è RTX 5090
            use_connection_cache=True,
            cache_directory="cache/real_training",
            auto_cleanup=True
        ),
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        spatial=SpatialSettings(
            use_spatial_optimization=True,
            chunk_size=64,                   # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è 8x8x8
            overlap_size=8,
            use_gpu_spatial_hashing=True,
            morton_encoding=True
        ),
        
        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        memory=MemorySettings(
            max_memory_usage_gb=24,          # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –Ω–∞ RTX 5090
            memory_cleanup_interval=100,
            use_memory_mapping=True,
            cache_embedding_results=True
        ),
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logging=LoggingSettings(
            level=logging.INFO,
            enable_caller_tracking=True,
            enable_anti_duplication=True,
            log_performance_metrics=True,
            save_logs_to_file=True,
            log_directory="logs/real_training"
        )
    )
    
    logger.info("‚úÖ Real training configuration created for 8x8x8 cube")
    return config


def setup_experiment_tracking(experiment_name: str) -> Path:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = Path(f"experiments/{experiment_name}_{timestamp}")
    experiment_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    (experiment_dir / "checkpoints").mkdir(exist_ok=True)
    (experiment_dir / "logs").mkdir(exist_ok=True)
    (experiment_dir / "metrics").mkdir(exist_ok=True)
    (experiment_dir / "config").mkdir(exist_ok=True)
    
    logger.info(f"üìä Experiment tracking setup: {experiment_dir}")
    return experiment_dir


def save_experiment_config(config: SimpleProjectConfig, experiment_dir: Path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    config_file = experiment_dir / "config" / "training_config.json"
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º config –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
    config_dict = {
        "lattice": {
            "dimensions": config.lattice.dimensions,
            "face_placement_strategy": config.lattice.face_placement_strategy
        },
        "model": {
            "state_size": config.model.state_size,
            "hidden_dim": config.model.hidden_dim,
            "neighbor_count": config.model.neighbor_count
        },
        "training": {
            "learning_rate": config.training.learning_rate,
            "batch_size": config.training.batch_size,
            "optimizer_type": config.training.optimizer_type
        },
        "training_embedding": {
            "num_epochs": config.training_embedding.num_epochs,
            "reconstruction_weight": config.training_embedding.reconstruction_weight,
            "similarity_weight": config.training_embedding.similarity_weight,
            "diversity_weight": config.training_embedding.diversity_weight,
            "emergence_weight": config.training_embedding.emergence_weight,
            "target_embedding_dim": config.training_embedding.target_embedding_dim,
            "lattice_steps": config.training_embedding.lattice_steps
        }
    }
    
    with open(config_file, 'w') as f:
        json.dump(config_dict, f, indent=2)
    
    logger.info(f"üíæ Configuration saved: {config_file}")


def create_dataset_for_training() -> tuple:
    """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    dataset_config = DatasetConfig(
        use_dialogue_cache=True,
        use_prepared_embeddings=True,
        use_cache_embeddings=True,
        use_snli_generator=False,           # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        max_samples_per_source=1000,        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        shuffle_sources=True,
        embedding_dim=768,
        min_embedding_norm=0.1,
        max_embedding_norm=50.0
    )
    
    dataloader, stats = create_training_dataloader(
        config=dataset_config,
        batch_size=16,
        shuffle=True,
        num_workers=2  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    )
    
    logger.info(f"üìä Dataset created: {stats['total_samples']} samples")
    return dataloader, stats


def run_training_epoch(trainer: EmbeddingTrainer, dataloader, epoch: int, experiment_dir: Path) -> Dict[str, float]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    logger.info(f"\nüöÄ Starting Epoch {epoch + 1}")
    epoch_start_time = time.time()
    
    # Training phase
    trainer.model.train()
    train_losses = {"total": 0.0, "reconstruction": 0.0, "similarity": 0.0, "diversity": 0.0, "emergence": 0.0}
    num_batches = 0
    
    for batch_idx, batch in enumerate(dataloader):
        batch_start_time = time.time()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –±–∞—Ç—á–∞
        embeddings = batch['embedding']  # [batch_size, 768]
        
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ trainer
        try:
            losses = trainer.train_step(embeddings)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            for key in train_losses.keys():
                if key in losses:
                    train_losses[key] += losses[key]
            
            num_batches += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
            if (batch_idx + 1) % 10 == 0:
                batch_time = time.time() - batch_start_time
                current_loss = losses.get('total', 0.0)
                logger.info(f"  Batch {batch_idx + 1:3d}: loss={current_loss:.6f}, time={batch_time:.2f}s")
                
        except Exception as e:
            logger.error(f"Error in batch {batch_idx}: {e}")
            continue
    
    # –£—Å—Ä–µ–¥–Ω—è–µ–º loss'—ã
    if num_batches > 0:
        for key in train_losses.keys():
            train_losses[key] /= num_batches
    
    epoch_time = time.time() - epoch_start_time
    
    logger.info(f"‚úÖ Epoch {epoch + 1} completed in {epoch_time:.2f}s")
    logger.info(f"   Average losses: {', '.join([f'{k}={v:.6f}' for k, v in train_losses.items()])}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics_file = experiment_dir / "metrics" / f"epoch_{epoch + 1}_metrics.json"
    metrics = {
        "epoch": epoch + 1,
        "train_losses": train_losses,
        "epoch_time": epoch_time,
        "num_batches": num_batches,
        "timestamp": datetime.now().isoformat()
    }
    
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    return train_losses


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    print("üöÄ STARTING REAL 3D CELLULAR NEURAL NETWORK TRAINING")
    print("=" * 60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment_name = "first_8x8x8_real_training"
    experiment_dir = setup_experiment_tracking(experiment_name)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    logger.info("‚öôÔ∏è Creating training configuration...")
    config = create_real_training_config()
    save_experiment_config(config, experiment_dir)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    logger.info("üìÇ Loading unified dataset...")
    dataloader, dataset_stats = create_dataset_for_training()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞
    with open(experiment_dir / "config" / "dataset_stats.json", 'w') as f:
        json.dump(dataset_stats, f, indent=2)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ trainer'–∞
    logger.info("üß† Initializing EmbeddingTrainer...")
    trainer = EmbeddingTrainer(config)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    logger.info(f"üéØ Starting training for {config.training_embedding.num_epochs} epochs...")
    
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config.training_embedding.num_epochs):
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_losses = run_training_epoch(trainer, dataloader, epoch, experiment_dir)
        current_loss = train_losses['total']
        
        # Early stopping check
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            best_model_path = experiment_dir / "checkpoints" / "best_model.pth"
            trainer.save_checkpoint(str(best_model_path), epoch=epoch + 1, loss=current_loss)
            logger.info(f"üíæ New best model saved: loss={current_loss:.6f}")
            
        else:
            patience_counter += 1
            logger.info(f"‚ö†Ô∏è No improvement for {patience_counter} epochs")
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ checkpoint'—ã
        if (epoch + 1) % config.training_embedding.save_checkpoint_every == 0:
            checkpoint_path = experiment_dir / "checkpoints" / f"epoch_{epoch + 1}.pth"
            trainer.save_checkpoint(str(checkpoint_path), epoch=epoch + 1, loss=current_loss)
            logger.info(f"üíæ Regular checkpoint saved: epoch_{epoch + 1}.pth")
        
        # Early stopping
        if patience_counter >= config.training_embedding.early_stopping_patience:
            logger.info(f"üõë Early stopping triggered after {patience_counter} epochs without improvement")
            break
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_model_path = experiment_dir / "checkpoints" / "final_model.pth"
    trainer.save_checkpoint(str(final_model_path), epoch=epoch + 1, loss=current_loss)
    
    # –°–≤–æ–¥–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    summary = {
        "experiment_name": experiment_name,
        "total_epochs": epoch + 1,
        "best_loss": best_loss,
        "final_loss": current_loss,
        "dataset_samples": dataset_stats['total_samples'],
        "config_summary": {
            "lattice_size": config.lattice.dimensions,
            "state_size": config.model.state_size,
            "batch_size": config.training.batch_size,
            "learning_rate": config.training.learning_rate
        },
        "completion_time": datetime.now().isoformat()
    }
    
    with open(experiment_dir / "experiment_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nüéâ TRAINING COMPLETED!")
    print(f"üìä Experiment results saved to: {experiment_dir}")
    print(f"üèÜ Best loss achieved: {best_loss:.6f}")
    print(f"üìà Total samples processed: {dataset_stats['total_samples']}")
    print(f"\nüöÄ Ready for analysis and next steps!")


if __name__ == "__main__":
    main()