# План реализации: двунаправленная модель без внимания (Swift + Metal на Apple Silicon)

> С определённой долей вероятности (≈ 80–90%) план ниже приведёт к работоспособному MVP, который сначала проверяет “базу”, а затем пошагово оптимизируется под GPU/Metal. Возможны альтернативы и поправки в зависимости от ограничений MPSGraph/BNNS/Metal и качества датасета.

---

## Легенда
- **Цель**: студент-энкодер без внимания, который по потоку токенов выдаёт фиксированный латент `z ∈ R^768` (форма 32×24), согласованный с эмбеддингом учителя (DistilBERT). Обратный декодер по `z` генерирует правдоподобную последовательность токенов (смысл > буквальная точность).
- **Два режима входа**: (A) токены → `z`, (B) `z_teacher` → `z`. Модель умеет оба, используя кондиционирование (FiLM/Conditioned LN).
- **Лоссы**: `cosine + MSE` (дистилляция к учителю) + `CE` (языковая реконструкция), опционально — контрастивный и семантический (BERTScore/косинус к референсу).
- **Стек**: Swift 5+, SPM, Metal, MPSGraph (для автодиффа/тренировки на GPU), Accelerate/BNNS (CPU‑бэкап), XCTest, swift-log.
- **Ограничения**: точная инверсия эмбеддингов невозможна (много‑к‑одному). Меряем смысловую близость и правдоподобие текста, а не побуквенную идентичность.

---

## Древо фаз
0. [Инициализация проекта](#фаза-0-инициализация-проекта)  
1. [Данные и токенизатор](#фаза-1-данные-и-токенизатор)  
2. [Базовый энкодер (токены→z)](#фаза-2-базовый-энкодер-токеныz)  
3. [Обучение энкодера (KD)](#фаза-3-обучение-энкодера-kd)  
4. [Декодер (z→токены) + teacher-forcing](#фаза-4-декодер-zтокены--teacher-forcing)  
5. [Совместная схема, режимы и лоссы](#фаза-5-совместная-схема-режимы-и-лоссы)  
6. [Метрики и валидация](#фаза-6-метрики-и-валидация)  
7. [Инференс-API и упаковка](#фаза-7-инференс-api-и-упаковка)  
8. [Оптимизация под Metal/GPU](#фаза-8-оптимизация-под-metalgpu)  
9. [Доп. улучшения и ветки экспериментов](#фаза-9-доп-улучшения-и-ветки-экспериментов)  
10. [Риски и план обхода](#фаза-10-риски-и-план-обхода)  
11. [Приложения: структура репозитория, конфиги, формы тензоров](#приложения)

---

## Фаза 0. Инициализация проекта

**Шаги**
1. Создать SPM‑проект `StudentZ` со следующими таргетами:
   - `StudentZCore` — тензоры, графы, слои, лоссы, оптимизаторы (MPSGraph/Metal).
   - `StudentZData` — токенизатор, датасеты, батчинг.
   - `StudentZTrain` — тренинг CLI (инфраструктура экспериментов).
   - `StudentZInfer` — библиотека/демо‑приложение инференса.
   - `StudentZTests` — юнит‑ и интеграционные тесты (XCTest).
2. Подключить зависимости/фреймворки:
   - `Metal`, `MetalKit`, `MetalPerformanceShaders`, `MPSGraph`.
   - `Accelerate`/`BNNS` (CPU fallback), `os.log` или `swift-log`.
3. Настроить CI (локально достаточно `swift test` и сборка релиз/отладка).
4. Включить поддержку **FP16** и профилирование GPU (`MTLCommandBuffer addCompletedHandler`, GPU Frame Capture).
5. Определить **конфиг‑файлы** (`.yaml`/`.json`) для hyperparams.

**Критерий готовности**
- Сборка проходит; тест‑шаблон может создать/выполнить пустой MPSGraph на GPU и вернуть тензор требуемой формы.  
_Уверенность: ≈ 95%._

---

## Фаза 1. Данные и токенизатор

**Решения**
- **Токенизатор**: простой, но детерминированный (например, `whitespace + punctuation` или символьный). Фиксируем `PAD`, `BOS`, `EOS`, `UNK`.
- **Словарь**: `vocab.json` с `token → id`. Допускаем рост словаря по мере загрузки корпуса (но лучше “заморозить” после предпрогона).
- **Учительские эмбеддинги**:
  - **Практичный путь**: оффлайн‑подсчёт `z_teacher ∈ R^768` в Python (Hugging Face) → экспорт в `jsonl` (`text`, `tokens`, `teacher_vec`).
  - **Альтернатива**: конвертировать DistilBERT в Core ML и считать в Swift. Сложнее в настройке; возможно медленнее при массовой подготовке.

**Шаги**
1. Реализовать `Tokenizer`:
   - функции: `fit(corpus)`, `encode(text, maxLen)`, `decode(ids)`, сохранение/загрузка словаря.
   - модульные тесты: стабильность ID, обратимость `decode(encode(x))` для базовых примеров.
2. Формат данных `jsonl`:
   ```json
   {"text": "...", "tokens": [..], "teacher": [768 floats]}
   ```
   - Пайплайн: чистка текста, токенизация, усечение/паддинг до `L_max`.
3. Написать `DataLoader`:
   - memory‑mapped чтение, шардирование, перемешивание, нарезка батчей.
   - маски `attn_mask` (для паддингов) — пригодится в пулах/лоссах.
4. Маленький sanity‑set (например, 10–100 образцов) для оверфита.

**Критерии готовности**
- `Tokenizer` стабильно кодирует/декодирует.
- `DataLoader` выдаёт батчи `tokens: [B×L]`, `teacher: [B×768]`.
- Сверки с Python‑версией дают одинаковые токены.  
_Уверенность: ≈ 90%._

---

## Фаза 2. Базовый энкодер (токены→z)

**Архитектура (без внимания)**  
- `Embedding(V, d)`  
- N резидуальных **TCN‑блоков** (каузальные 1D‑свёртки с дилатацией):  
  `Conv1d(d→h, k, dilation) → GN/LayerNorm → GELU → Dropout → Conv1d(h→d, 1)` + skip.  
- **Агрегация** в фиксированный вектор: `GlobalMeanPool` (с маской) или learnable gated pooling.  
- `Linear(d → 768)` → `Reshape(32×24)` (форма только для интерфейса/визуализаций).
- Опционально: **FiLM/Cond‑LN** от “режима входа” (токены или teacher).

**Шаги**
1. Слои в `StudentZCore`:
   - Тензорные обёртки над `MPSGraphTensor`.
   - `Embedding`, `Conv1d` (через 2D‑conv с каналами или MPSGraph 1D), `Norm`, `GELU`, `Dropout`, `Linear`, `GlobalMaskedPool`.
2. Построение графа энкодера:
   - вход: `ids [B×L]` → `emb [B×L×d]` → TCN → `pool [B×d]` → `proj [B×768]`.
3. Юнит‑тесты форм/градиентов: сверка с CPU‑эталоном на мини‑случае.

**Критерий готовности**
- Форвард/бэквард проходит, формы корректны, слой‑за‑слоем unit‑тесты зелёные.  
_Уверенность: ≈ 85%._

---

## Фаза 3. Обучение энкодера (KD)

**Лоссы**
- `L_cos = 1 − cos(z_s, z_t)`, `L_mse = ||z_s − z_t||²`.
- (Опционально) лёгкая **контрастивка** внутри батча (позитив: аугментация, негатив: другие примеры).

**Шаги**
1. Реализовать `Losses`: cosine, MSE; (доп.) батч‑wise InfoNCE.
2. `Optimizers`: AdamW (β1=0.9, β2=0.999, ε=1e−8, weight decay 0.01).
3. Тренинг‑скрипт (`StudentZTrain`):
   - батчинг, смешанная точность (активации FP16, мастер‑веса FP32), **gradient scaling**.
   - ранний стоп по `val_cos`.
4. Sanity‑check: оверфит на 100 примерах до `cosine ≥ 0.95` (учим “повторять” учителя).

**Критерий готовности**
- На вал‑подсэмпле `cosine(z_s, z_t) ≥ 0.9` (порог уточняется эмпирически).  
_Уверенность: ≈ 80–85%._

---

## Фаза 4. Декодер (z→токены) + teacher‑forcing

**Архитектура (без внимания)**  
- Малый **авторегрессионный TCN/SSM‑подобный** декодер.
- Кондиционирование на `z` через FiLM/аддитивные смещения на каждом блоке.
- Выход: логиты `[B×L×V]`, лосс `CrossEntropy` по истинной последовательности.
- Обучение: **teacher forcing** + маска паддингов.

**Шаги**
1. Реализовать декодер‑блоки (каузальные свёртки, residual, FiLM).
2. Учебный прогон с `z = z_t` (от учителя): добиться “разумной” перефразации, потом — с `z = z_s` (студент).
3. Базовая регуляризация: label‑smoothing (например, 0.1).

**Критерии готовности**
- Потери CE снижаются на train/val; выборочные декоды читаемы и по смыслу близки.
- Без teacher forcing при инференсе — стабильное сэмплирование (top‑k/top‑p).  
_Уверенность: ≈ 70–80%._

---

## Фаза 5. Совместная схема, режимы и лоссы

**Комбинация целей**
- Итоговый лосс: `L = α·(1−cos) + β·MSE + γ·CE + δ·(контрастивный/семантический)`.
- Режимы входа:
  - **Mode A**: токены→`z_s` (градиенты текут в энкодер, CE через декодер по `z_s`).
  - **Mode B**: `z_t`→`ẑ` (MLP/identity), CE через декодер по `ẑ` (градиенты **не** текут в учителя).

**Шаги**
1. Единый тренировочный цикл с чередованием режимов A/B (например, 50/50 батчей).
2. Тюнинг весов α, β, γ, δ; валидация по **смыслам** (см. Фаза 6).
3. `Cycle‑consistency` (опционально): `z_t → x̂ → z_s(x̂)` и сближение `z_s(x̂)` с `z_t` по косинусу.

**Критерии готовности**
- Стабильная сходимость; рост метрик смысла; отсутствие “коллапса” в однотипные выходы.  
_Уверенность: ≈ 75–85%._

---

## Фаза 6. Метрики и валидация

**Метрики**
- `Cosine(z_s, z_t)` — основная (похожа ли геометрия).
- **Ретривер‑тест**: ближайшие соседи в пространстве учителя vs студента (precision@k).
- **Смысл текста**: косинус между учительскими эмбеддингами исхода и декода (`cos(z_t(x), z_t(x̂))`).
- (Опционально) BLEU/ROUGE, BERTScore (если доступно оффлайн).
- **Стабильность инференса**: перплексия/энтропия, разнообразие n‑грамм.

**Шаги**
1. Валидатор в `StudentZTrain`: считает метрики по валидации каждые N шагов.
2. Сохранённые чекпоинты и `best_by_metric` (например, best cosine/semantic).

**Критерий готовности**
- Определены “проходные” пороги метрик; выбраны 1–2 чекпоинта для инференса.  
_Уверенность: ≈ 80%._

---

## Фаза 7. Инференс‑API и упаковка

**API**
- `encode(tokens: [Int]) -> [Float](768)` (или `[32][24]` как форма).
- `decode(z: [Float], maxLen: Int, topk/temperature) -> [Int]`.
- Режим B: `decodeFromTeacher(z_teacher: [Float])` (через быструю проекцию).

**Шаги**
1. Обёртка `StudentZInfer` с инициализацией графов и весов (FP16).
2. Потокобезопасность: один/несколько command‑buffer per request.
3. Сериализация весов: `*.bin` (FP16) + JSON‑метаданные.
4. (Опционально) Конвертация в Core ML для деплоя **инференса**.

**Критерии готовности**
- Инференс на CPU‑fallback и GPU даёт одинаковые ответы (с погрешностью FP16).
- QPS/латентность удовлетворяют целям MVP.  
_Уверенность: ≈ 85%._

---

## Фаза 8. Оптимизация под Metal/GPU

**Цели**
- Минимизировать накладные расходы на сборку графа и копирование.
- Добиться стабильной FP16‑тренировки и быстрых итераций.

**Шаги**
1. **Статические формы**: фиксируйте `B`, `L_max` в графе; паддинг и маски — внутри.
2. **Сборка графа один раз**, переиспользование `MPSGraphExecutable`.
3. **Пулы буферов** (`MTLHeap`, `StorageModePrivate`), pinning/zero‑copy для входов.
4. **Слияние операций**: свёртка+норма+активация в один графовый сегмент, по возможности.
5. **Mixed Precision**: FP16 активации/градиенты, FP32 мастер‑веса; динамический **loss scaling**.
6. **Градиентный чекпоинтинг** на длинных L для экономии памяти.
7. **Микро‑батчи** (gradient accumulation), если VRAM ограничена.
8. Профилирование:
   - GPU counters/Timing, поиск узких мест (конв‑слои, softmax, эмбеддинг‑lookup).
   - Устранение лишних host↔device копий.
9. **Кэширование лук‑апов** эмбеддинга и слоёв FiLM для повторяющихся токенов.
10. **Quantize‑aware инференс** (после тренировки): FP16 (по умолчанию), затем эксперименты с INT8 пост‑тренировки (если точность держится).

**Критерии готовности**
- Сокращение времени итерации и/или памяти ≥20–40% без заметной потери метрик.  
_Уверенность: ≈ 75%._

---

## Фаза 9. Доп. улучшения и ветки экспериментов

1. **VQ‑бутылочное горлышко** над `z` (дискретизация → более стабильная декодируемость).
2. **CTC‑декодер** вместо авторегрессии (если нужен небуквальный, но быстрый вывод).  
   _Примечание:_ если в MPSGraph нет готового CTC‑лосса, можно считать DP‑лосс на CPU/Accelerate для коротких L (или реализовать кастомный Metal‑kernel позже).
3. **SSM‑блоки** (Mamba‑подобные) вместо TCN, если сильно длинные контексты.
4. **Семантический лосс**: оффлайн‑метрики (BERTScore) как RL/реранкер.
5. **Доменная адаптация**: дообучение на специфическом корпусе.
6. **Шеринг весов** между энкодером/декодером (частичный), если формы позволяют.
7. **Регуляризации**: dropout‑схемы, stochastic depth для глубокой TCN.

---

## Фаза 10. Риски и план обхода

- **CTC в Swift/Metal может быть нетривиален** — стартуйте с авторегрессии. _Вероятность сложности: ≈ 70%._  
- **Автодифф/обратный проход некоторых опов** в MPSGraph может работать не так, как ожидается (версии macOS/Xcode). Держите **CPU‑бэкап** для критичных лоссов. _≈ 30%._  
- **Нестабильность FP16** на ранних этапах — начинайте с FP32, затем включайте mixed precision и loss scaling. _≈ 50%._  
- **Данные** (качество разметки/учителя) — ключевой фактор; валидируйте пайплайн генерации `z_teacher`. _≈ 80%._

---

## Приложения

### А. Структура репозитория (рекомендация)

```
StudentZ/
  Package.swift
  Sources/
    StudentZCore/
      Tensors/
      Ops/
      Layers/{Embedding,Conv1D,Norm,FiLM,Pooling,Linear}.swift
      Losses/{Cosine,MSE,CE,InfoNCE}.swift
      Optim/{AdamW,Schedulers}.swift
      Graph/{Builders,Executables}.swift
    StudentZData/
      Tokenizer/{Tokenizer.swift,vocab.json}
      Datasets/{JsonlDataset.swift, DataLoader.swift}
      Utils/{MMAP.swift, RNG.swift}
    StudentZTrain/
      main.swift
      Configs/{baseline.json, joint.json}
      Checkpoints/
      Logs/
    StudentZInfer/
      API/{Encode.swift, Decode.swift}
      Weights/{...}
  Tests/
    StudentZTests/{TokenizerTests.swift, ShapeGradTests.swift, TrainingLoopTests.swift}
  README.md
```

### Б. Базовые конфиги (пример)

```json
{
  "vocab_size": 32000,
  "d_model": 256,
  "n_blocks": 8,
  "kernel_size": 5,
  "dilation_schedule": [1, 2, 4, 8, 16, 1, 2, 4],
  "dropout": 0.1,
  "pool": "masked_mean",
  "proj_dim": 768,
  "max_len": 256,
  "optimizer": {"type": "adamw", "lr": 3e-4, "weight_decay": 0.01},
  "loss_weights": {"cos": 1.0, "mse": 0.5, "ce": 1.0, "sem": 0.2},
  "batch_size": 64,
  "precision": "mixed"  // "fp32" для дебага
}
```

### В. Формы тензоров

- Входы: `ids [B×L]`, `mask [B×L]`  
- Эмбеддинги: `emb [B×L×d]`  
- Энкодер‑выход: `z_s [B×768]` (→ `[B×32×24]` для отображения)  
- Учитель: `z_t [B×768]`  
- Декодер‑выход: `logits [B×L×V]`

### Г. Acceptance‑чек‑лист по фазам (минимум)

- Фаза 1: `Tokenizer` и `DataLoader` проверены, токены стабильны.  
- Фаза 2–3: `cosine(z_s, z_t) ≥ 0.9` на вал‑мини‑сете.  
- Фаза 4: декодер даёт правдоподобный текст (ручной просмотр + CE↓).  
- Фаза 5–6: совмещённая тренировка стабильно сходится; `cos(z_t(x), z_t(x̂))` растёт.  
- Фаза 7: инференс‑API стабилен (CPU/GPU), контрольные примеры воспроизводимы.  
- Фаза 8: профилирование подтверждает ускорение/снижение памяти.  

---

## Заключение

Ваше рассуждение интересно в контексте разведения **семантики** (согласование с учителем) и **поверхности языка** (правдоподобный декод), и предложенный план отражает это разделение: сначала валидируем энкодер через жёсткие косинус/MSE‑критерии, затем аккуратно прикручиваем языковую часть, а после — оптимизируем под Metal. С определённой долей вероятности (≈ 85%) такой пошаговый путь даст рабочий прототип на одном Apple M‑GPU; альтернативы (CTC/VQ/SSM) внесены как опциональные ветки.
