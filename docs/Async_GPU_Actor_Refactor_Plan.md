# Async/Actor GPU Refactor Plan (EnergyFlowSwift)

Owner: EnergyFlowSwift
Status: In progress
Scope: Make GPU execution actor-isolated and async, remove nonisolated hacks, keep deterministic waits by default, and lay the groundwork for deferred GPU/CPU overlap.

## Progress snapshot

- GPUActor –≤–≤–µ–¥—ë–Ω; –≤—Å–µ Metal pipelines –∂–∏–≤—É—Ç –≤ –∞–∫—Ç—ë—Ä–µ.
- Linear/Conv1D/Elementwise –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç `GPUReadback`, `syncBatch` —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç deferred —Ä–∞–±–æ—Ç—É.
- KD/CE –º–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω—ã –Ω–∞ GPU (Metal –∞—Ç–æ–º–∏–∫–∏ + guard).
- TextBridge/DecoderTrainer –∏—Å–ø–æ–ª—å–∑—É—é—Ç deferred readbacks –ø–æ—Å–ª–µ `syncBatch`.
- –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø ‚Äî –ø—Ä–æ—Ç—è–Ω—É—Ç—å deferred conv/elementwise —á–µ—Ä–µ–∑ TCN/decoder –∏ –æ–±–Ω–æ–≤–∏—Ç—å —Ç–µ—Å—Ç—ã (—Å–º. `docs/Deferred_Readbacks_TCN.md`).

Goals
- Correct Swift Concurrency model for GPU code: no main-actor globals, no nonisolated(unsafe) statics for mutable GPU state.
- Centralize device/queue/pipeline state in a single GPU actor.
- Propagate async only where necessary, with minimal ceremony, but consistently across hot paths.
- Preserve current numerical behavior and determinism: keep waitUntilCompleted by default inside GPU calls.
- Avoid feature flags and fallbacks; optimized path becomes the only path.
- Stay aligned with project rules: simple/efficient, MPS/Metal first, no MPSGraph on hot paths.

Non-Goals (for this iteration)
- Overlapped micro-batch execution (no-wait between ops) ‚Äî can be a follow-up after functional migration.
- Changing math or training semantics.
- Introducing runtime flags for toggling implementations.

Design Overview
- Introduce a GPUActor that owns:
  - MTLDevice and MTLCommandQueue
  - Lazy-compiled Metal pipelines (LayerNorm, GELU, Elementwise, Im2Col/Col2Im, etc.)
  - MPSMatrix helper allocation utilities
  - A BufferPool replacement with actor-owned cache (label ‚Üí MTLBuffer)
  - Optional weight/buffer caches keyed by stable identifiers
- Move existing GPU helpers (ElementwiseGPU, GELUGPU, LayerNormGPU, EmbeddingGPU, Im2Col/ConvPack) into methods on GPUActor.
- Refactor MPSMatrix wrappers (GraphLinear, GraphConv1D) so that any GPU buffers and encodes flow through GPUActor. The structs retain CPU-side weights; the actor manages MTLBuffers pinned/cached as needed.
- Make forward/grad methods async across layers that touch GPU; await at callsites.
- Keep waitUntilCompleted before returning from each actor GPU op to preserve determinism.

Step-by-step Plan

Phase 0 ‚Äî Stabilize build (short-lived)
- Revert/avoid nonisolated(unsafe) where it clashes with compiler actor rules.
- Keep temporary build hacks only until phases below land; remove them as we move functions under GPUActor.

Phase 1 ‚Äî Introduce GPUActor
- New file: Sources/EFCore/GPU/GPUActor.swift
  - class GPUActor: actor
    - let device: MTLDevice
    - let commandQueue: MTLCommandQueue
    - private lazy var pipelines: structs holding MTLComputePipelineStates per domain (elementwise, LN, GELU, im2col, etc.) compiled on demand
    - private var buffers: [String: MTLBuffer] buffer pool (rounded lengths)
    - func buffer(length: Int, label: String) async -> MTLBuffer
    - Helpers to ensure pipelines (compile once)
- Provide a global entry point:
  - nonisolated(unsafe) enum GPU { static let shared = GPUActor() }
  - Note: GPU.shared is a reference to the actor; methods must be awaited.

Phase 2 ‚Äî Port Metal helpers to GPUActor
- ElementwiseGPU ‚Üí methods on GPUActor:
  - residualAdd(y: Tensor, x: Tensor) async -> Tensor
  - maskZero(y: Tensor, mask: [[Int]]) async -> Tensor
  - maskedMean(x: Tensor, mask: [[Int]]) async -> Tensor
  - maskedMeanBackward(dPooled: Tensor, mask: [[Int]], seqLen: Int) async -> Tensor
  - addBroadcast2DInto3D(y: Tensor, addBD: Tensor, L: Int) async -> Tensor
- GELUGPU ‚Üí GPUActor.geluForward(x: Tensor) async, geluBackward(x: Tensor, dY: Tensor) async
- LayerNormGPU ‚Üí GPUActor.layerNormForward(x: Tensor, gamma: Tensor, beta: Tensor, eps: Float) async; backward async
- Im2Col/ConvPack kernels ‚Üí GPUActor.im2col/col2im/fillBias + pack/unpack helpers (async)
- EmbeddingGPU ‚Üí GPUActor.embeddingForward(ids: [[Int]], weight: Tensor) async
- Delete/empty original static singletons and move code under actor; fix imports accordingly.

Phase 3 ‚Äî Refactor MPSMatrix wrappers via GPUActor
- GraphLinear
  - Remove internal MTLBuffer fields (wBufFP16, bBufFP16) from the struct or make them opaque tokens.
  - Actor owns weight buffers keyed by ObjectIdentifier(self) + version.
  - Public API:
    - mutating func forward(_ x: Tensor, on gpu: GPUActor = GPU.shared) async throws -> Tensor
    - func gradientsGPUAsync(X: Tensor, dY: Tensor, on gpu: GPUActor = GPU.shared) async throws -> (dW: Tensor, dB: Tensor)
    - func inputGradientsGPUAsync(dY: Tensor, on gpu: GPUActor = GPU.shared) async throws -> Tensor
  - Invalidate cache: bump version token so actor recreates buffers next call.
- GraphConv1D
  - Actor manages Wcol FP16 buffers and im2col intermediates.
  - public func forward(_ x: Tensor, on gpu: GPUActor = GPU.shared) async -> Tensor

Phase 4 ‚Äî Propagate async across model stack
- TCNBlock.forward ‚Üí async, awaiting LN, convs, GELU, elementwise ops.
- TCNStack.forward ‚Üí async.
- TextDecoder.forward/forwardForTraining ‚Üí async; condition-add via actor method.
- TextToCubeEncoder.encodeTokens/forwardForTraining/forwardForTrainingWithLastBlockCache ‚Üí async.
- LastTCNBackward and DecoderLastTCNBackward that use GPU ops ‚Üí async helpers or ensure GPU calls are awaited.

Phase 5 ‚Äî Trainers and EFTrain async
- CombinedTrainer.stepA/stepB ‚Üí async; await encoder/decoder calls and GPU grad ops.
- DecoderTrainer.step/stepScaled ‚Üí async.
- EFTrain.run ‚Üí async (convert to @main async entry or wrap with Task and RunLoop).
- Update tests to async XCTest (use async test functions and await calls).

Phase 6 ‚Äî Deterministic waits (default behavior)
- –í—ã–ø–æ–ª–Ω–µ–Ω–æ: `waitUntilCompleted` –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ `GPUReadback`, `syncBatch()` –¥–æ–∂–∏–¥–∞–µ—Ç—Å—è –≤—Å–µ—Ö –∫–æ–º–∞–Ω–¥.
- –î–æ–±–∞–≤–ª–µ–Ω guard (`ensureBatchSynced`) –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ä–∞–Ω–Ω–∏—Ö CPU-—Ä–∏–¥–±—ç–∫–æ–≤.

Phase 6b ‚Äî Batched waits and GPU metrics *(–≤ —Ä–∞–±–æ—Ç–µ)*
Pre-req: Phase 6 –∑–∞–≤–µ—Ä—à—ë–Ω
Goals (–æ–±–Ω–æ–≤–ª–µ–Ω–æ):
- ‚úÖ Remove per-op waits ‚Äî –≤—Å–µ —Ö–µ–ª–ø–µ—Ä—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç `GPUReadback`, `syncBatch` –∂–¥—ë—Ç –±—É—Ñ–µ—Ä—ã.
- ‚úÖ Move runtime metrics to GPU ‚Äî kdMetricsMean/crossEntropyMean –∏—Å–ø–æ–ª—å–∑—É—é—Ç Metal –∞—Ç–æ–º–∏–∫–∏.
- ‚úÖ Guard –ø—Ä–æ—Ç–∏–≤ —Ä–∞–Ω–Ω–∏—Ö `value()`.
- üöß Call-site refactor ‚Äî TCN/decoder conv & elementwise –≤—Å—ë –µ—â—ë —á–∏—Ç–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–∑—É (—Å–º. `docs/Deferred_Readbacks_TCN.md`).
- üöß Tests ‚Äî –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å GPU-—Ç–µ—Å—Ç—ã –ø–æ–¥ deferred readbacks.

Remaining Tasks:
- –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ TCNBlock/TextDecoder/Trainers –Ω–∞ deferred conv/elementwise + –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–µ `value()`.
- –û–±–Ω–æ–≤–∏—Ç—å XCTest (decoder/encoder mini-epochs, GPUKernelsAndStrideTests, Im2Col parity) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å `GPUReadback`.
- –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –º–µ–∂–¥—É `beginBatch`/`syncBatch` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ–¥ DEBUG).

Risks & mitigations
- Hidden stalls move to syncBatch: harder to locate slow ops ‚Üí add optional per-op timestamps (cmdBuf GPUStart/GPUEnd) under DEBUG.
- Buffer lifetime: ensure actor retains buffers until sync; use label-based cache with generation tokens.

API sketches (illustrative)
```swift path=null start=null
extension GPUActor {
    func beginBatch() async {}
    func syncBatch(label: String? = nil) async {
        // wait on pending command buffers, clear list
    }
    func kdMetricsMean(y: Tensor, t: Tensor) async -> (Float, Float) { /* GPU reductions */ }
    func ceMean(logits: Tensor, targets: [[Int]]) async -> Float { /* GPU softmax+NLL */ }
}

// Trainer pseudo-flow
for batch in batches {
    await gpu.beginBatch()
    // enqueue forwards/backwards (no waits)
    let (mse, cos) = await gpu.kdMetricsMean(y: out, t: zt)
    let ce = args.enableAB ? await gpu.ceMean(logits: logits, targets: targets) : 0
    await gpu.syncBatch(label: "train-batch")
    log(mse, cos, ce)
}
```

Phase 7 ‚Äî Cleanup deprecated pieces
- Remove EFCore/MPSGraph/ExecutableCache.swift and GraphContext if no longer used.
- Remove BufferPool.swift; rehome logic in GPUActor.
- Remove nonisolated(unsafe) hacks from earlier stopgaps.

Compatibility & Migration Notes
- This is a source-breaking change: forward methods become async across the stack.
- We will stage changes in small PRs, each updating call sites and tests.
- Keep numerical parity by asserting shapes and running existing unit tests after each phase.

Testing Strategy
- Convert affected unit tests to async and verify:
  - EFDecoderModeBMiniEpochTests
  - EFTrainMiniEpochTests and CombinedABAlternationTests
- LNGeLUGemmCacheTests, LNExecCacheTests ‚Äî removed alongside the legacy MPSGraph executable cache; future coverage will come from actor-based LN/GELU paths
  - GPUKernelsAndStrideTests, Im2ColCol2ImParityTests ‚Äî rewire to GPUActor APIs
- Add microbench tests (optional) to confirm no regression in throughput; still using waits.

API Sketches (illustrative)
```swift path=null start=null
actor GPUActor {
    let device: MTLDevice
    let commandQueue: MTLCommandQueue
    // lazy pipelines, buffers cache

    func residualAdd(y: Tensor, x: Tensor) async -> Tensor {
        let (cmd, yBuf, xBuf) = try await encodeResidualAdd(y: y, x: x)
        cmd.commit(); cmd.waitUntilCompleted()
        return readTensor(shape: y.shape, from: yBuf)
    }
}

struct GraphLinear {
    var inFeatures: Int
    var outFeatures: Int
    var weight: Tensor
    var bias: Tensor?
    private var version: UInt64 = 0

    mutating func invalidateCache() { version &+= 1 }

    mutating func forward(_ x: Tensor, on gpu: GPUActor = GPU.shared) async throws -> Tensor {
        try await gpu.linearForward(self, x: x, version: version)
    }
}
```

Risk & Mitigation
- Actor reentrancy: keep GPUActor methods short and non-blocking except for final waits; avoid calling back into actor from callbacks.
- Memory pressure: reuse buffers in actor buffer cache; round sizes; monitor alignment.
- Large PR risk: split work by domains (Elementwise/LN/GELU ‚Üí Conv ‚Üí Linear ‚Üí Models ‚Üí Trainers ‚Üí EFTrain).

Rollout Sequence (PR-size friendly)
1) Add GPUActor + buffer cache + minimal residualAdd (wire one call site) ‚Äî ensure build.
2) Port Elementwise entirely; update TCN/TextDecoder calls; tests.
3) Port GELU and LayerNorm; update TCN; tests.
4) Port Im2Col/Col2Im; update GraphConv1D; tests.
5) Port Embedding; update Embedding.forward; tests.
6) Refactor GraphLinear; update encoder/decoder proj paths; tests.
7) Propagate async through models/trainers; convert EFTrain to async entry; tests.
8) Phase 6b: Introduce GPU metrics + batched waits; remove per-op waits; add trainer sync points; tests for parity.
9) Cleanup MPSGraph, BufferPool; remove hacks; green build.

Performance Notes (with waits)
- Even with waitUntilCompleted, we remove CPU loops and centralize GPU resource reuse; expect net improvements vs current baseline.
- Future: after functional parity, introduce overlapped micro-batch execution by deferring waits and syncing at batch end.

Post-migration Enhancements (later)
- Pin embedding weights in GPUActor to avoid re-upload per batch when not trainable.
- Add fused LN‚ÜíGELU‚ÜíGEMM kernels where dimensions are static.
- Introduce command buffer batching per micro-batch with a single fence.

Checklist
- [ ] GPUActor.swift landed
- [ ] Elementwise ported & call sites updated
- [ ] GELU/LN ported
- [ ] Im2Col/Col2Im ported
- [ ] Embedding ported
- [x] GraphLinear refactored
- [x] GraphConv1D refactored
- [ ] Models async *(TCN/decoder awaiting deferred conv/elementwise)*
- [ ] Trainers async *(Combined/Decoder —á–∞—Å—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã)*
- [ ] EFTrain async main
- [ ] Tests async + green
- [ ] Phase 6b: GPU metrics + batched waits *(metrics done, TCN/decoder + —Ç–µ—Å—Ç—ã –≤ —Ä–∞–±–æ—Ç–µ)*
- [x] MPSGraph cache removed
- [x] Legacy BufferPool removed (GPUActor buffer cache is the single allocator)
- [ ] Nonisolated hacks removed

Timeline (rough)
- Phases 1‚Äì2: 1‚Äì2 sessions
- Phases 3‚Äì4: 1‚Äì2 sessions
- Phases 5‚Äì7: 1‚Äì2 sessions

Notes
- We will keep semantics identical (waits) to simplify verification.
- All public APIs affected will be updated in one direction (no flags).
