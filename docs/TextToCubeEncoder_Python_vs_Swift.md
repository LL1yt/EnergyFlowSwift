# TextToCubeEncoder: Python vs Swift

Документ помогает увидеть, как один и тот же Text→Cube конвейер сделан в Python (`energy_flow/text_bridge/text_to_cube_encoder.py`) и в Swift (`EnergyFlowSwift/Sources/EnergyFlow/TextBridge`). Для каждого логического блока сначала описана питоновская версия, затем свифтовая, после чего — что можно подтянуть в Swift. Формулировки максимально простые, чтобы даже новичок понял общие идеи.

## 1. Конфигурация и размеры поверхности
**Python**
- Берёт `EnergyConfig` (если не передан — `create_debug_config()`), сразу привязывает глобальную конфигурацию `set_energy_config`.
- `surface_dim = lattice_width * lattice_height`. В debug-режиме это 20×20 = 400 значений на выход.

**Swift**
- Хранит два конфига: `EnergyConfig` (ширина/высота/глубина) и `TextToCubeEncoderConfig` (размер скрытого слоя, длина последовательности, число голов и т.д.).
- `surfaceDim` берётся из `EnergyConfig`, но последний `Linear` слой настроен на `modelConfig.outputDim` (по умолчанию 768), то есть потенциально не совпадает с шириной×высотой.

**Что улучшить в Swift**
- Жёстко привязать `modelConfig.outputDim` к `energyConfig.surfaceDim`, чтобы финальный вектор точно соответствовал поверхности куба.
- Добавить режимы `DEBUG/EXPERIMENT/OPTIMIZED`, чтобы размеры решётки и максимальная длина текста синхронизировались с питоновской конфигурацией.

## 2. Токенизация
**Python**
- Использует `AutoTokenizer.from_pretrained('distilbert-base-uncased')`. Это обученный токенизатор со словарём ~30k токенов и спец-символами (`[CLS]`, `[PAD]`, `[SEP]` и др.).
- Для примера текст "energy flow emerges" превращается во что-то вроде `[101, 3626, 9240, 6415, 102]`: это стабильные ID, которые совпадают у всех запусков.

**Swift**
- `SimpleTokenizer` просто режет текст по пробелам и приводит к нижнему регистру. Словарь растёт на лету: первый раз встречаем "energy" → запоминаем ID 2, потом "flow" → 3 и т.д.
- Для того же текста получится `[2, 3, 4, 5, 0, 0, …]`: длинная пачка нулей-дополнений до `maxLength`.

**Что улучшить в Swift**
- Ввести стабильный словарь (например, выгрузить vocabulary DistilBERT и собрать `SimpleTokenizer` из файла). Тогда Swift- и Python-версии будут понимать текст одинаково.
- Добавить спец-символы (`[CLS]`, `[PAD]`, `[SEP]`) и attention mask для них — это упростит перенос весов.

## 3. Эмбеддинги и позиционное кодирование
**Python**
- `nn.Embedding(vocab_size, hidden_dim)` выдаёт вектор размера 256 (в debug). Весы инициализируются нормальным распределением.
- `positional_encoding` — обучаемый параметр формы `[512, hidden_dim]`, наполняется синусами/косинусами (как в Transformer).
- Перед трансформером просто складываем эмбеддинг токена и соответствующий позиционный вектор.

**Swift**
- `Embedding` из `PyTorchSwift` даёт вектор `hiddenDim` (по умолчанию 768). Инициализация — псевдослучайная через `Seed`.
- Позиционное кодирование статично вычисляется в `makePositionalEncoding`, хранится в `Tensor` и подрезается по текущей длине.
- Добавление PE реализовано тройным циклом по `batch × length × hidden`, что нормально для CPU-референса, но медленно.

**Что улучшить в Swift**
- Заменить циклы на векторные операции (`embs += pe.broadcasted(...)`). Это подготовит код к переносу на GPU/MPS.
- Разрешить хранить PE прямо в `Tensor` на устройстве (CPU/GPU) и переиспользовать без копирования.

## 4. Трансформерный блок
**Python**
- Строит `nn.TransformerEncoder` из двух слоёв `nn.TransformerEncoderLayer` с 8 головами, FFN = 512, активация GELU.
- Маска: `src_key_padding_mask` (булева) выключает внимание к паддингам.

**Swift**
- Использует собственный `TransformerEncoder` с `MultiHeadSelfAttention` и `LayerNorm`. Сейчас по умолчанию одна голова (`numHeads = 1`).
- Маска — массив `[[Int]]`, нули полностью обнуляют выходы в паддингах после каждого residual.
- Внутри Self-Attention и FFN много вложенных циклов; dropout пока пропущен.

**Что улучшить в Swift**
- Поддержать `numHeads > 1` по умолчанию, чтобы приблизиться к python-параметрам (8 голов при hidden=256 или 8 при 768, нужно проверить делимость).
- Реализовать dropout и возможность использовать обученные веса (когда появится экспорт из Python).
- Переписать Self-Attention в терминах матричных умножений, чтобы потом заменить backend на MPS.

## 5. Агрегация последовательности
**Python**
- После трансформера считает маскированное среднее: умножает на attention mask, суммирует и делит на число реальных токенов.
- Результат — вектор `[batch, hidden_dim]`.

**Swift**
- Функция `maskedMean` делает то же самое, но в явных циклах `for` и хранит маску как `[[Int]]`.

**Что улучшить в Swift**
- Перейти на тензорные операции: например, умножить `Tensor` на `mask` (предварительно привести к float) и вызвать встроенный `reduceSum`. Это ускорит CPU и понадобится для GPU.
- Рассмотреть добавление альтернативных пулеров (CLS-токен, максимум), если потребуется повторяемость с Python.

## 6. Проекция к поверхности куба
**Python**
- `surface_projection`: `Linear → GELU → LayerNorm → Dropout → Linear → Tanh`. Второй `Linear` выдаёт ровно `surface_dim`, `Tanh` ограничивает значения [-1, 1].
- Пример с `surface_dim = 400`: на выходе вектор длины 400, каждая компонента в диапазоне [-1, 1].

**Swift**
- `proj1 → GELU → LayerNorm → proj2` и, опционально, `tanh`. Но `proj2` выводит `modelConfig.outputDim` (стартовое значение 768).
- Финальный вектор не обязательно совпадает с размером поверхности энерго-решётки.

**Что улучшить в Swift**
- Присвоить `modelConfig.outputDim = energyConfig.surfaceDim` в конструкторе, чтобы гарантировать нужную длину.
- Всегда включить `useTanhOutput`, как это сделано в Python.
- Добавить подсчёт статистики (min/max/mean) для отладки, аналогично Python-логам.

## 7. Батчинг и работа с устройством
**Python**
- Определяет `self.device = torch.device('cuda' if available else 'cpu')`, всё переносится туда (`to(self.device, non_blocking=True)`).
- Есть `batch_process` и `_encode_text_batched`, чтобы разбивать большие списки текстов на куски.
- Оборачивает токенизацию в `torch.no_grad()` для экономии памяти.

**Swift**
- Сейчас весь расчёт на CPU. Нет специального буфера под большие списки текстов — принимается сразу `[String]`.
- Все операции выполняются в циклах через `Tensor.data`, что делает перенос на GPU сложнее.

**Что улучшить в Swift**
- Добавить опцию "mini-batch" (например, обрабатывать по 32 строки), чтобы память не росла квадратично.
- Хранить `Tensor` в формате, готовом для MPS: избегать прямых обращений к `data`, вместо этого строить новые тензоры через операции библиотеки.
- Ввести зачатки менеджера устройств (`enum Device { case cpu, mps }`) и переключение через конфиг.

## 8. Логирование и диагностика
**Python**
- Использует общий логгер `get_logger(__name__)`, уровни до `DEBUG_ENERGY`.
- Ведёт статистику (mean, std, min, max, длины последовательностей), выводит примеры коротких текстов.
- При ошибках печатает подробную информацию и возвращает тензор с градиентами (не нули без градиента).

**Swift**
- Делает несколько `logger.debug` с `prettyShape`, средним и дисперсией, но без детального списка токенов.
- При ошибках пока нет отдельного пути — бросит `precondition` или уронит программу.

**Что улучшить в Swift**
- Поддержать уровни логов, аналогичные Python (`DEBUG_ENERGY`, `DEBUG_INIT` и т.д.), связать с общей системой.
- Добавить защиту от пустых батчей (вернуть нулевой тензор правильной формы) и вспомогательные сообщения.

## 9. Мини-пример прохода
Представим энерго-конфиг DEBUG.

1. **Текст**: "energy flows fast".
2. **Python**
   - Токенизатор DistilBERT выдаёт `[101, 3626, 9240, 3435, 102]` (условно: `[CLS] energy flow fast [SEP]`).
   - Embedding + позиционное кодирование → матрица размером `[1, 5, 256]`.
   - Transformer из 2 слоёв обрабатывает последовательность, учитывая маску `[1, 1, 1, 1, 1]`.
   - Маскированное среднее → вектор `[1, 256]`.
   - Projection + Tanh → вектор `[1, 400]`, который можно reshaper'ить в `20×20`.
3. **Swift (текущая версия)**
   - `SimpleTokenizer` выдаёт условно `[2, 3, 4, 5, 0, 0, …]`, маска `[1, 1, 1, 1, 0, 0, …]`.
   - Embedding + PE → `[1, 128, 768]` (максимальная длина 128; лишние позиции заполняются нулями).
   - Transformer (1 голова) прогоняет последовательность, зануляет паддинги.
   - Masked mean → `[1, 768]`.
   - Проекция → `[1, 768]` (а должно быть 400, чтобы совпасть с поверхностью).

Чтобы свести результаты: нужно дать Swift тот же токенизатор, уменьшить `hiddenDim` до 256, сделать `outputDim = 400` и использовать те же веса/количество голов.

## 10. Шаги к ускорению и переносу Swift-версии на GPU/MPS
1. **Выравнивание параметров** — скрытый размер, число голов, длина последовательности и выходная размерность должны совпасть с Python, иначе перенос весов невозможен.
2. **Векторизация операций** — заменить ручные циклы (добавление PE, masked mean, внимание) на тензорные операции. Это ключ к запуску на MPS.
3. **Стабильный словарь** — нужен файл vocabulary и повторение токенизации HuggingFace (можно экспортировать `vocab.txt` и `tokenizer.json`). Тогда входы совпадут бит-в-бит.
4. **Интеграция dropout и LayerNorm в стиле PyTorch** — MPS-граф предполагает батчевые операции; стоит подготовить интерфейсы уже сейчас.
5. **Управление устройствами** — добавить в `Tensor` или в обёртку возможность хранить указатель на MTLBuffer. Поначалу можно имитировать, но код уже не должен лазить напрямую к `data`.
6. **Экспорт/импорт весов** — подготовить сериализацию весов из PyTorch (`state_dict`) и загрузку в Swift (например, через `.npy` или собственный бинарный формат).
7. **Тесты на идентичность** — для коротких фраз сравнивать Swift и Python выходы (L2-норма разницы). Это поможет отслеживать ошибки по мере оптимизаций.

После выполнения шагов 1–4 CPU-референс станет быстрее и чище, а шаги 5–7 откроют дорогу к полноценной реализации на Metal/MPS.

