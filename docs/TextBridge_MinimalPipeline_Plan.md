# Минимальный Text→Cube конвейер: цели и вопросы для исследования

Документ фиксирует текущее понимание задачи и набор направлений, которые нужно изучить глубже. Его можно передать в ChatGPT или другой исследовательский инструмент, чтобы найти готовые решения и свежие идеи.

## Куда хотим прийти
- **Использовать только мощный токенайзер + простое сопоставление токенов поверхности куба.** Никаких Transformer/attention в TextBridge — всё интеллектуальное преобразование происходит в кубе.
- **Обеспечить обратимость:** `текст → токены → поверхность куба → токены → текст` без потерь (или с контролируемым шумом).
- **Готовность к Apple Silicon (M-серия, 2025):** операции должны легко переноситься на MPS/Metal и масштабироваться.

## Базовые компоненты
1. **Токенайзер с обученным словарём**
   - Современные варианты: HuggingFace `swift-tokenizers`, SentencePiece-обёртки, Apple TokenizerKit.
   - Требования: стабильный словарь, возможность дообучения, поддержка спец-символов (`[PAD]`, `[CLS]`, `[SEP]`).
   - Вход/выход длиной ~64–128 токенов (логическое рассуждение, не длинный текст).

2. **Embedding lookup**
   - Таблица `vocab × hiddenDim`. Один вызов `gather` вместо классического слоя с матричным умножением.
   - HiddenDim подбираем так, чтобы помещаться на поверхность куба (например, 256 = 16×16, 576 = 24×24).

3. **Маппинг токенов на поверхность куба**
   - Берём последовательность токенов и отображаем их в матрицу фиксированного размера.
   - Нули для пустого пространства, аккуратный метод для «лишних» токенов (суммирование блоков, Hilbert-кривые, pooling).
   - Нужен обратный оператор: поверхность → последовательность токенов без потери порядка.

4. **Обратное преобразование**
   - Собираем вектор обратно из поверхности.
   - Декодируем через тот же токенайзер (`decode(ids)`), получаем текст/логическое утверждение.

## Ключевые исследовательские вопросы
- Какие Swift-токенайзеры в 2025 году дают наилучший баланс: стабильный словарь, дообучение, совместимость с Python-весами?
- Как лучше всего сжимать/распределять последовательности длиной >16 токенов в поверхность 16×16 без потери важной информации?
- Какие готовые решения уже реализуют прямой `gather`/embedding lookup на Metal или MPSGraph?
- Есть ли публикации о логико-ориентированных токенизационных схемах (не статистика, а структура рассуждений)?
- Какие подходы Apple/MPS-сообщества рекомендуют для zero-padding и reshape без лишних копий?

## Идеи для дальнейшего исследования
- Использовать пространственные кривые (Hilbert, Peano) для расположения токенов на поверхности и сохранения локальных связей.
- Хранить отдельный «контекстный» канал (например, признаки категории утверждения) в дополнительной плоскости.
- Вводить легкий checksum/CRC по поверхности, чтобы куб мог проверять целостность данных.
- Рассмотреть byte-pair или unigram модели с ограниченным словарём для более компактного размещения.
- Проверить, подходят ли MPSGraph `OneHot` + `MatMul` как замена lookup, и не будет ли это быстрее.

## Практические шаги
1. Выбрать кандидата на токенайзер, экспортировать словарь из Python и собрать минимальный Swift-прототип.
2. Реализовать `encode → reshape → decode` тест с фиксацией `surfaceDim` (например, 16×16) и убедиться, что round-trip совпадает.
3. Написать бенчмарки CPU vs MPS на M4 для операций lookup/reshape.
4. Оценить, сколько логических шагов диалога помещается в выбранный размер поверхности и надо ли расширение.
5. Подготовить список «учебных» текстов (краткие мысли/выводы) для будущего fine-tuning токенайзера.

Эти пункты сформируют основу для глубокого поиска и анализа решений, которые можно адаптировать под наш минималистичный TextBridge.

