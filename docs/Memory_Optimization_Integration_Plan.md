# Память: план интеграции улучшений (без продакшена и тестов) 768 = 32х24

Цель: уменьшить пики потребления памяти во время обучения, не уменьшая число потоков и размер батчей на первом этапе. Используем существующие модули:

- Логирование: energy_flow/utils/logging.py
- Конфигурация: energy_flow/config/energy_config.py
- Практические фиксы: energy_flow/fixes/performance_and_memory_fixes.py (источник конкретных правок)

Документ — рабочий план внедрения, без продакшена и без запуска тестов. Все изменения — через небольшие, локальные правки и фичефлаги.

## Источники и выводы

- performance_and_memory_fixes.py: содержит набор правок, которые устраняют узкие места по памяти: ограничение спавна, рефлекшн на границах, контроль числа активных «flows», более аккуратное обновление глобального шага. Важно: там также присутствует уменьшение batch_size до 32 — этот шаг переносим в «fallback», чтобы придерживаться вашей стратегии (сначала архитектурные оптимизации, потом возможное снижение батчей/потоков).
- energy_flow/utils/logging.py: уже есть расширенное централизованное логирование с категориями, в т.ч. DEBUG_MEMORY. Используем его для метрик памяти и этапной валидации прогресса.
- energy_flow/config/energy_config.py: точка управления дефолтами и фичефлагами. Здесь зафиксируем безопасные значения, не влияющие на параллелизм и батчи.

## Принципы интеграции

1. Не трогаем число потоков и размер батча на первом этапе. Любое снижение — только как fallback.
2. Ограничиваем экспоненциальный рост сущностей («flows») с помощью флагов конфигурации и логики спавна.
3. Включаем отражения на границах, чтобы уменьшить «утечку» энергии в бесконечность и случайные выбросы траекторий.
4. Подключаем детальные метрики памяти через существующий логгер (DEBUG_MEMORY) для быстрой диагностики регрессий.
5. Все правки — малыми порциями и под фичефлаги в energy_flow/config/energy_config.py.

## Конкретные шаги и минимальные правки

Шаг 1. Усилить конфигурацию (без изменения потоков/батчей)

- В energy_flow/config/energy_config.py проверьте и при необходимости выставьте безопасные дефолты:
  - movement_based_spawn=False (исключить лавинообразное появление новых объектов)
  - max_spawn_per_step=0 (жестко отключить спавн на первом этапе стабилизации)
  - max_active_flows=50000 (разумный лимит для предотвращения OOM)
  - boundary_reflection_enabled=True (включить отражение на границах)
- Примечание: любые значения держим под фичефлагами, чтобы их можно было быстро раскатить/откатить без рефакторинга.

Шаг 2. Логирование памяти и ключевых событий

- Использовать energy_flow/utils/logging.py. В точках начала/конца итерации обучения и крупных операций добавить вызовы:
  - log_memory_state(operation="train_step")
  - log_performance("forward", duration, extra_metrics)
- Категории: при необходимости включить DEBUG_MEMORY через setup_logging(debug_categories=["memory"]). Это позволит видеть GPU allocated/reserved без засорения лога остальными категориями.

Шаг 3. Границы и траектории

- Убедиться, что boundary_reflection_enabled влияет на реальную логику в energy_flow/core/flow_processor.py (performance_and_memory_fixes.py указывает, что код отражения есть, но он не всегда активировался из-за дефолтов). Активируем через конфиг.
- Если используется position_history, удостовериться, что обновление истории позиций включено (см. add_position_history_usage() в performance_and_memory_fixes.py). Это улучшает «устойчивые» траектории и снижает «блуждание», которое может провоцировать рост количества активных объектов.

Шаг 4. Контроль числа активных flows

- На входе в шаг обучения и после спавна (если он будет включен позже) проверять len(active_flows) и логировать WARN при приближении к порогу max_active_flows. Это не меняет архитектуру, но дает ранний сигнал до OOM.
- Если доступна утилита FlowMemoryManager, созданная performance_and_memory_fixes.py, пока не подключаем её глобально (по вашему требованию), но оставляем as-is как опциональную «ручку» на уровне сценария для локальных экспериментов. В план фиксации входит только «пороговые» WARN и доп. логирование.

Шаг 5. Безопасный режим спавна (поэтапное включение)

- На этапе стабилизации держим movement_based_spawn=False, max_spawn_per_step=0.
- После стабилизации памяти и подтверждения стабильного профиля — включаем спавн обратно поэтапно:
  - Этап А: max_spawn_per_step=1, movement_based_spawn=False
  - Этап B: movement_based_spawn=True при строгих лимитах (напр., max_spawn_per_step=1..2) и мониторинге DEBUG_MEMORY
- На каждом подэтапе фиксируем пиковые значения allocated/reserved и len(active_flows) в лог, чтобы увидеть, на каком уровне начинается деградация.

Шаг 6. Пороговые алерты и быстрая диагностика

- Добавить простые пороговые проверки перед крупными вычислениями:
  - Если torch.cuda.memory_reserved() > X GB → логировать WARNING и пропускать некритичные ветки/доп. вычисления в этом шаге.
  - Если len(active_flows) > 0.9 \* max_active_flows → логировать WARNING и временно блокировать спавн на следующий шаг (через флаг в рантайме).
- Эти проверки не уменьшают потоки/батч, но создают «мягкие барьеры» против OOM.

## Что НЕ делаем на первом этапе

- Не создаем новые модули логирования или памяти (используем уже существующие energy_flow/utils/logging.py и energy_flow/config/energy_config.py).
- Не уменьшаем батчи и число потоков. Это переводится в раздел «Fallback», если архитектурные меры не дают эффекта.

## Fallback (только если архитектурные меры не помогли)

- Временно уменьшить batch_size (как предлагалось в performance_and_memory_fixes.py: до 32) и/или агрессивнее ограничить max_active_flows (напр., 30–40k) до выяснения причин.
- Временно включить принудительную периодическую очистку через FlowMemoryManager на уровне сценария обучения, но как локальный эксперимент.

## План по устаревшим документам и коду

- Файл energy_flow/METRICS_PROFILING_PLAN.md выглядит устаревшим и пересекается с функционалом в energy_flow/utils/logging.py. Предложение:
  - Переместить в archive/ как deprecated c короткой пометкой «заменено централизованным логированием и метриками в energy_flow/utils/logging.py».
  - Или удалить совсем после подтверждения.
- Общий список на аудит (без удаления сейчас):
  - energy_flow/todo.md (сверить пункты с данным планом, перенести актуальные, остальное — в архив)
  - archive/\* отчеты, пересекающиеся по тематике с этим документом — оставить как исторический контекст.

## Чек‑лист интеграции (итерация 1)

- [ ] Внести дефолты в energy_flow/config/energy_config.py: disable spawn, boundary reflection on, max_active_flows=50k
- [ ] Проставить логирование памяти и времени в начале/конце train step через energy_flow/utils/logging.py
- [ ] Включить DEBUG_MEMORY на время диагностики
- [ ] Подтвердить, что код отражения границ реально активен (см. flow_processor)
- [ ] Включить обновление position_history при наличии (см. performance_and_memory_fixes.py)
- [ ] Добавить пороговые WARN по len(active_flows) и reserved GPU memory (без вмешательства в батчи/потоки)

## Чек‑лист стабилизации (итерация 2)

- [ ] Поэтапно вернуть спавн: сначала max_spawn_per_step=1 без movement_based_spawn
- [ ] Зафиксировать метрики, потом включить movement_based_spawn при жестких лимитах
- [ ] Оценить пиковые allocated/reserved и время итерации, сравнить с целью

## Решение по METRICS_PROFILING_PLAN.md

- По умолчанию: переносим в archive/ (или удаляем) после вашего подтверждения. Само логирование и метрики уже централизованы в energy_flow/utils/logging.py.

## Быстрый rollback план

- Все изменения конфигураций — под фичефлагами, можно вернуть исходные дефолты одной правкой в energy_flow/config/energy_config.py.
- Логирование — просто выключить DEBUG_MEMORY/категории или убрать добавленные вызовы логов в train step, если шум мешает.

## Примечание по валидации итогов

- На каждом шаге зафиксировать: максимум torch.cuda.memory_allocated/reserved, средний len(active_flows), время итерации. Сопоставить с целевыми метриками. Это позволит доказательно показать, что прежде чем трогать батчи/потоки, архитектурные меры действительно были исчерпаны.
