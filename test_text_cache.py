#!/usr/bin/env python3
"""
–¢–µ—Å—Ç TextCache –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
"""

import torch
import sys
import time
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ energy_flow
sys.path.append('energy_flow')

from energy_flow.config import create_debug_config
from energy_flow.text_bridge import TextToCubeEncoder, CubeToTextDecoder
from energy_flow.text_bridge.text_cache import (
    TextCache, 
    create_text_cache,
    CachedTextToCubeEncoder,
    CachedCubeToTextDecoder
)

def test_text_cache():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TextCache...")
    
    # –°–æ–∑–¥–∞–µ–º debug –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = create_debug_config()
    print(f"üìê –†–∞–∑–º–µ—Ä—ã –∫—É–±–∞: {config.lattice_width}√ó{config.lattice_height}√ó{config.lattice_depth}")
    print(f"üìè Surface dim: {config.lattice_width * config.lattice_height}")
    
    # –°–æ–∑–¥–∞–µ–º –∫—ç—à
    cache_file = "test_cache.pt"
    cache = create_text_cache(max_size=100, cache_file=cache_file, config=config)
    
    print(f"\n1Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞:")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {cache.max_size}")
    print(f"   –í–∫–ª—é—á–µ–Ω: {cache.enabled}")
    print(f"   –§–∞–π–ª –∫—ç—à–∞: {cache.cache_file}")
    print(f"   Surface dim: {cache.surface_dim}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_texts = [
        "Hello world!",
        "This is a test sentence.",
        "Machine learning is fascinating.",
        "Neural networks process information.",
        "Energy flows through the lattice."
    ]
    
    print(f"\n2Ô∏è‚É£ –¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è text ‚Üí surface:")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ surface embeddings
    surface_embeddings = []
    print(f"   üì± Device info - Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}")
    print(f"   üì± CUDA available: {torch.cuda.is_available()}")
    
    for text in test_texts:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings –Ω–∞ –±–∞–∑–µ —Ç–µ–∫—Å—Ç–∞
        torch.manual_seed(hash(text) % 2**32)
        embedding = torch.randn(cache.surface_dim) * 0.5
        embedding = torch.clamp(embedding, -1, 1)
        surface_embeddings.append(embedding)
        
        print(f"   üì± Original embedding device: {embedding.device}")
        
        # –ö—ç—à–∏—Ä—É–µ–º
        cache.put_text_to_surface(text, embedding)
        print(f"   –ö—ç—à–∏—Ä–æ–≤–∞–ª–∏: '{text}' ‚Üí {embedding.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞
    print(f"\n3Ô∏è‚É£ –¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è text ‚Üí surface:")
    for i, text in enumerate(test_texts):
        cached_embedding = cache.get_surface_from_text(text)
        if cached_embedding is not None:
            original = surface_embeddings[i]
            print(f"   üì± Original device: {original.device}, Cached device: {cached_embedding.device}")
            try:
                match = torch.allclose(cached_embedding, original, atol=1e-6)
                print(f"   '{text}': {'‚úÖ MATCH' if match else '‚ùå MISMATCH'}")
            except RuntimeError as e:
                print(f"   ‚ùå Device error for '{text}': {e}")
                # Try moving to same device for comparison
                if original.device != cached_embedding.device:
                    print(f"   üì± Attempting device alignment...")
                    if original.device.type == 'cuda':
                        cached_embedding = cached_embedding.to(original.device)
                    else:
                        cached_embedding = cached_embedding.cpu()
                    match = torch.allclose(cached_embedding, original, atol=1e-6)
                    print(f"   üì± After alignment - '{text}': {'‚úÖ MATCH' if match else '‚ùå MISMATCH'}")
        else:
            print(f"   '{text}': ‚ùå NOT FOUND")
    
    # –¢–µ—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    print(f"\n4Ô∏è‚É£ –¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è surface ‚Üí text:")
    for i, embedding in enumerate(surface_embeddings):
        text = test_texts[i]
        cache.put_surface_to_text(embedding, text)
        print(f"   –ö—ç—à–∏—Ä–æ–≤–∞–ª–∏: {embedding.shape} ‚Üí '{text}'")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
    print(f"\n5Ô∏è‚É£ –¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è surface ‚Üí text:")
    for i, embedding in enumerate(surface_embeddings):
        cached_text = cache.get_text_from_surface(embedding)
        original_text = test_texts[i]
        if cached_text is not None:
            match = cached_text == original_text
            print(f"   {embedding.shape}: {'‚úÖ MATCH' if match else '‚ùå MISMATCH'} '{cached_text}'")
        else:
            print(f"   {embedding.shape}: ‚ùå NOT FOUND")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞
    print(f"\n6Ô∏è‚É£ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞:")
    stats = cache.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # –¢–µ—Å—Ç LRU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    print(f"\n7Ô∏è‚É£ –¢–µ—Å—Ç LRU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:")
    small_cache = TextCache(max_size=3, enabled=True, config=config)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
    lru_texts = ["Text 1", "Text 2", "Text 3", "Text 4", "Text 5"]
    for i, text in enumerate(lru_texts):
        embedding = torch.randn(cache.surface_dim) * 0.5
        small_cache.put_text_to_surface(text, embedding)
        print(f"   –î–æ–±–∞–≤–∏–ª–∏ '{text}', —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(small_cache)}")
    
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(small_cache)} (–º–∞–∫—Å: {small_cache.max_size})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —É–¥–∞–ª–µ–Ω—ã
    for text in lru_texts:
        cached = small_cache.get_surface_from_text(text)
        status = "‚úÖ FOUND" if cached is not None else "‚ùå EVICTED"
        print(f"   '{text}': {status}")
    
    # –¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏
    print(f"\n8Ô∏è‚É£ –¢–µ—Å—Ç –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è:")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à
    cache.save_cache()
    print(f"   –ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {cache.cache_file}")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫—ç—à –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
    new_cache = TextCache(cache_file=cache_file, config=config)
    print(f"   –ù–æ–≤—ã–π –∫—ç—à —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä: {len(new_cache)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å
    for text in test_texts[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3
        cached = new_cache.get_surface_from_text(text)
        status = "‚úÖ LOADED" if cached is not None else "‚ùå NOT LOADED"
        print(f"   '{text}': {status}")
    
    # –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏
    print(f"\n9Ô∏è‚É£ –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏:")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
        encoder = TextToCubeEncoder(config)
        decoder = CubeToTextDecoder(config)
        
        # –°–æ–∑–¥–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
        cached_encoder = CachedTextToCubeEncoder(encoder, cache)
        cached_decoder = CachedCubeToTextDecoder(decoder, cache)
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
        integration_texts = ["Integration test 1", "Integration test 2"]
        
        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω (–±–µ–∑ –∫—ç—à–∞)
        start_time = time.time()
        embeddings1 = cached_encoder.encode_text(integration_texts)
        time1 = time.time() - start_time
        print(f"   –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω encoder: {time1:.4f}s")
        
        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω (—Å –∫—ç—à–µ–º)
        start_time = time.time()
        embeddings2 = cached_encoder.encode_text(integration_texts)
        time2 = time.time() - start_time
        print(f"   –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω encoder (–∫—ç—à): {time2:.4f}s")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
        match = torch.allclose(embeddings1, embeddings2, atol=1e-6)
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {'‚úÖ YES' if match else '‚ùå NO'}")
        print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {time1/time2 if time2 > 0 else 'N/A'}x")
        
        # –¢–µ—Å—Ç decoder –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        texts1 = cached_decoder.decode_surface(embeddings1[:2])  # –ü–µ—Ä–≤—ã–µ 2
        texts2 = cached_decoder.decode_surface(embeddings1[:2])  # –¢–µ –∂–µ (–¥–æ–ª–∂–Ω—ã –∏–∑ –∫—ç—à–∞)
        
        print(f"   Decoder —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {'‚úÖ YES' if texts1 == texts2 else '‚ùå NO'}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüîü –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    final_stats = cache.get_stats()
    for key, value in final_stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    try:
        Path(cache_file).unlink(missing_ok=True)
        print(f"\nüßπ –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –∫—ç—à–∞ —É–¥–∞–ª–µ–Ω")
    except:
        pass
    
    print("\n‚úÖ –¢–µ—Å—Ç TextCache –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return True

if __name__ == "__main__":
    try:
        test_text_cache()
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}")
        import traceback
        traceback.print_exc()