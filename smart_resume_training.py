#!/usr/bin/env python3
"""
üß† Smart Resume Training - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
–ò—â–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
"""

import os
import sys
import torch
import logging
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime
import glob
from warmup_scheduler import create_warmup_scheduler

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class SmartResumeManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —É–º–Ω–æ–≥–æ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(
        self, forced_mode: Optional[str] = None, custom_scale: Optional[float] = None
    ):
        """
        Args:
            forced_mode: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            custom_scale: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π scale factor
        """
        self.forced_mode = forced_mode
        self.custom_scale = custom_scale
        self.config_manager = None
        self.current_config = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self._initialize_config()

    def _initialize_config(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            from utils.config_manager.config_manager import (
                ConfigManager,
                ConfigManagerSettings,
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å –≤–∫–ª—é—á–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            settings = ConfigManagerSettings(
                enable_dynamic_config=True,
                dynamic_config_mode=self.forced_mode or "auto",
                auto_hardware_detection=True,
                enable_hot_reload=False,
                # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤
                config_search_paths=[],  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ = —Ç–æ–ª—å–∫–æ main_config.yaml + –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞
                # –ü–µ—Ä–µ–¥–∞–µ–º custom scale —Å—Ä–∞–∑—É –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                custom_scale_factor=self.custom_scale_factor,
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ ConfigManager
            self.config_manager = ConfigManager(settings)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.current_config = {
                "lattice": self.config_manager.get_config("lattice"),
                "embeddings": self.config_manager.get_config("embeddings"),
                "training": self.config_manager.get_config("training"),
                "gmlp": self.config_manager.get_config("gmlp"),
            }

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∂–∏–º–µ
            dynamic_info = self.config_manager.get_dynamic_config_info()
            if dynamic_info:
                logger.info(f"üéØ Current config: {dynamic_info['mode']} mode")
                logger.info(f"   Scale factor: {dynamic_info['scale_factor']}")

            lattice = self.current_config["lattice"]
            logger.info(f"üìä Target configuration:")
            logger.info(f"   Lattice: {lattice['xs']}x{lattice['ys']}x{lattice['zs']}")
            logger.info(f"   Total neurons: {lattice['total_neurons']:,}")
            logger.info(
                f"   Embedding dim: {self.current_config['embeddings']['embedding_dim']:,}"
            )

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize config: {e}")
            raise

    def find_compatible_checkpoints(
        self, checkpoints_dir: str = "checkpoints"
    ) -> List[Dict[str, Any]]:
        """
        –ò—â–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        """
        logger.info(f"üîç Searching for compatible checkpoints in {checkpoints_dir}")

        checkpoints_path = Path(checkpoints_dir)
        if not checkpoints_path.exists():
            logger.warning(f"Checkpoints directory not found: {checkpoints_dir}")
            return []

        compatible_checkpoints = []
        current_signature = self._get_config_signature(self.current_config)

        # –ò—â–µ–º –≤–æ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–∫–∞—Ö
        search_patterns = [
            checkpoints_path / "latest" / "*.pt",
            checkpoints_path / "versioned" / "*" / "*.pt",
            checkpoints_path / "*.pt",  # –ü—Ä—è–º–æ –≤ –∫–æ—Ä–Ω–µ
        ]

        for pattern in search_patterns:
            for checkpoint_file in glob.glob(str(pattern)):
                checkpoint_info = self._analyze_checkpoint(
                    checkpoint_file, current_signature
                )
                if checkpoint_info:
                    compatible_checkpoints.append(checkpoint_info)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å, –ø–æ—Ç–æ–º –≤—Ä–µ–º—è)
        compatible_checkpoints.sort(
            key=lambda x: (
                -x["compatibility_score"],  # –í—ã—Å–æ–∫–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–µ—Ä–≤–∞—è
                -x["timestamp_score"],  # –ù–æ–≤—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –ø–µ—Ä–≤—ã–µ
            )
        )

        logger.info(f"üéØ Found {len(compatible_checkpoints)} compatible checkpoints")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø 5
        for i, cp in enumerate(compatible_checkpoints[:5]):
            logger.info(
                f"   {i+1}. {cp['name']} (score: {cp['compatibility_score']:.2f})"
            )
            logger.info(f"      {cp['description']}")

        return compatible_checkpoints

    def _analyze_checkpoint(
        self, checkpoint_path: str, current_signature: Dict
    ) -> Optional[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            checkpoint_data = torch.load(checkpoint_path, map_location="cpu")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            checkpoint_config = checkpoint_data.get("config", {})
            checkpoint_metadata = checkpoint_data.get("metadata", {})

            if not checkpoint_config:
                return None

            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—É —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            checkpoint_signature = self._get_config_signature(checkpoint_config)

            # –û—Ü–µ–Ω–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            compatibility_score = self._calculate_compatibility(
                current_signature, checkpoint_signature
            )

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if compatibility_score < 0.5:
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
            timestamp_str = checkpoint_metadata.get("timestamp", "1970-01-01T00:00:00")
            try:
                timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
                timestamp_score = timestamp.timestamp()
            except:
                timestamp_score = 0

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
            checkpoint_info = {
                "path": checkpoint_path,
                "name": Path(checkpoint_path).name,
                "config": checkpoint_config,
                "metadata": checkpoint_metadata,
                "signature": checkpoint_signature,
                "compatibility_score": compatibility_score,
                "timestamp_score": timestamp_score,
                "timestamp": timestamp_str,
                "description": self._generate_checkpoint_description(
                    checkpoint_metadata
                ),
            }

            return checkpoint_info

        except Exception as e:
            logger.debug(f"Failed to analyze checkpoint {checkpoint_path}: {e}")
            return None

    def _get_config_signature(self, config: Dict) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        signature = {}

        # Lattice –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if "lattice" in config:
            lattice = config["lattice"]
            signature["lattice_dims"] = (
                lattice.get("xs", 0),
                lattice.get("ys", 0),
                lattice.get("zs", 0),
            )
            signature["total_neurons"] = lattice.get("total_neurons", 0)

        # Embedding –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if "embeddings" in config:
            embeddings = config["embeddings"]
            signature["embedding_dim"] = embeddings.get("embedding_dim", 0)

        # gMLP –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if "gmlp" in config:
            gmlp = config["gmlp"]
            signature["state_size"] = gmlp.get("state_size", 0)
            signature["hidden_dim"] = gmlp.get("hidden_dim", 0)
            signature["neighbor_count"] = gmlp.get("neighbor_count", 0)

        return signature

    def _calculate_compatibility(self, current: Dict, checkpoint: Dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (0.0 - 1.0)"""
        score = 0.0
        total_weight = 0.0

        # –í–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        weights = {
            "lattice_dims": 0.4,  # –û—á–µ–Ω—å –≤–∞–∂–Ω–æ - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            "total_neurons": 0.2,  # –í–∞–∂–Ω–æ –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
            "embedding_dim": 0.3,  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            "state_size": 0.05,  # –ú–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
            "hidden_dim": 0.03,  # –ú–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
            "neighbor_count": 0.02,  # –ú–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
        }

        for param, weight in weights.items():
            if param in current and param in checkpoint:
                current_val = current[param]
                checkpoint_val = checkpoint[param]

                if current_val == checkpoint_val:
                    score += weight  # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                elif param == "lattice_dims":
                    # –î–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤ —Ä–µ—à–µ—Ç–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–∏–∑–æ—Å—Ç—å
                    if isinstance(current_val, tuple) and isinstance(
                        checkpoint_val, tuple
                    ):
                        similarity = self._tuple_similarity(current_val, checkpoint_val)
                        score += weight * similarity
                else:
                    # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
                    if isinstance(current_val, (int, float)) and isinstance(
                        checkpoint_val, (int, float)
                    ):
                        if current_val > 0 and checkpoint_val > 0:
                            ratio = min(current_val, checkpoint_val) / max(
                                current_val, checkpoint_val
                            )
                            score += weight * ratio

                total_weight += weight

        return score / total_weight if total_weight > 0 else 0.0

    def _tuple_similarity(self, tuple1: tuple, tuple2: tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∫–æ—Ä—Ç–µ–∂–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–∑–º–µ—Ä—ã —Ä–µ—à–µ—Ç–∫–∏)"""
        if len(tuple1) != len(tuple2):
            return 0.0

        similarities = []
        for a, b in zip(tuple1, tuple2):
            if a == b:
                similarities.append(1.0)
            elif a > 0 and b > 0:
                ratio = min(a, b) / max(a, b)
                similarities.append(ratio)
            else:
                similarities.append(0.0)

        return sum(similarities) / len(similarities)

    def _generate_checkpoint_description(self, metadata: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        parts = []

        # –¢–∏–ø –æ–±—É—á–µ–Ω–∏—è
        training_type = metadata.get("training_type", "unknown")
        parts.append(training_type)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        params = metadata.get("trainable_params", metadata.get("model_params"))
        if params:
            if params < 1000:
                parts.append(f"{params} params")
            elif params < 1000000:
                parts.append(f"{params//1000}K params")
            else:
                parts.append(f"{params//1000000}M params")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if "best_similarity" in metadata:
            parts.append(f"sim={metadata['best_similarity']:.3f}")

        if "epochs" in metadata:
            parts.append(f"{metadata['epochs']} epochs")

        return " | ".join(parts) if parts else "checkpoint"

    def auto_resume_training(
        self,
        dataset_limit: Optional[int] = None,
        additional_epochs: int = 10,
        **training_kwargs,
    ) -> Optional[Dict[str, Any]]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ

        Args:
            dataset_limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            additional_epochs: –°–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–ø–æ—Ö –æ–±—É—á–∞—Ç—å
            **training_kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å
        """
        logger.info(f"üöÄ Starting smart resume training")

        # –ò—â–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
        compatible_checkpoints = self.find_compatible_checkpoints()

        if not compatible_checkpoints:
            logger.warning(
                f"‚ö†Ô∏è No compatible checkpoints found, starting fresh training"
            )
            return self._start_fresh_training(
                dataset_limit, additional_epochs, **training_kwargs
            )

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        best_checkpoint = compatible_checkpoints[0]
        logger.info(f"üéØ Selected checkpoint: {best_checkpoint['name']}")
        logger.info(f"   Compatibility: {best_checkpoint['compatibility_score']:.2f}")
        logger.info(f"   Description: {best_checkpoint['description']}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        return self._resume_from_checkpoint(
            best_checkpoint, dataset_limit, additional_epochs, **training_kwargs
        )

    def _start_fresh_training(
        self, dataset_limit: Optional[int], epochs: int, **kwargs
    ) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info(f"üÜï Starting fresh training")

        from run_dynamic_training import DynamicTrainingManager

        training_manager = DynamicTrainingManager(
            forced_mode=self.forced_mode, custom_scale=self.custom_scale
        )

        return training_manager.run_training(
            dataset_limit=dataset_limit, epochs=epochs, **kwargs
        )

    def _resume_from_checkpoint(
        self,
        checkpoint_info: Dict[str, Any],
        dataset_limit: Optional[int],
        additional_epochs: int,
        **kwargs,
    ) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        logger.info(f"‚ñ∂Ô∏è Resuming training from checkpoint: {checkpoint_info['name']}")
        logger.info(
            f"   Checkpoint similarity: {checkpoint_info['metadata'].get('best_similarity', 'unknown')}"
        )
        logger.info(f"   Checkpoint timestamp: {checkpoint_info['timestamp']}")

        try:
            # –°–æ–∑–¥–∞–µ–º trainer —Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            from run_dynamic_training import DynamicTrainingManager

            training_manager = DynamicTrainingManager(
                forced_mode=self.forced_mode, custom_scale=self.custom_scale
            )

            # –°–æ–∑–¥–∞–µ–º trainer –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            trainer = training_manager.create_trainer()
            checkpoint_data = torch.load(checkpoint_info["path"], map_location="cpu")
            trainer.load_state_dict(checkpoint_data["model_state_dict"], strict=False)

            logger.info(f"‚úÖ Checkpoint weights loaded successfully")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–π–¥–µ–Ω–Ω—ã—Ö —ç–ø–æ—Ö –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            checkpoint_metadata = checkpoint_info.get("metadata", {})
            completed_epochs = checkpoint_metadata.get("epochs", 0)

            logger.info(
                f"üéì Resuming from epoch {completed_epochs + 1}, will train {additional_epochs} more epochs"
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å resume
            return training_manager.run_training(
                dataset_limit=dataset_limit,
                epochs=additional_epochs,
                resume_trainer=trainer,
                start_epoch=completed_epochs,
                **kwargs,
            )

        except Exception as e:
            logger.error(f"‚ùå Failed to resume from checkpoint: {e}")
            logger.info(f"üÜï Falling back to fresh training")
            return self._start_fresh_training(
                dataset_limit, additional_epochs, **kwargs
            )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Smart Resume Training with Auto Checkpoint Detection"
    )
    parser.add_argument(
        "--mode",
        choices=["auto", "development", "research", "validation", "production"],
        default="development",
        help="Configuration mode",
    )
    parser.add_argument(
        "--dataset-limit", type=int, default=None, help="Limit dataset size"
    )
    parser.add_argument(
        "--additional-epochs", type=int, default=10, help="Additional epochs to train"
    )
    parser.add_argument("--scale", type=float, default=None, help="Custom scale factor")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Batch size (uses config default if not specified)",
    )
    parser.add_argument(
        "--list-only",
        action="store_true",
        help="Only list compatible checkpoints, don't train",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        resume_manager = SmartResumeManager(
            forced_mode=args.mode if args.mode != "auto" else None,
            custom_scale=args.scale,
        )

        if args.list_only:
            # –¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            compatible_checkpoints = resume_manager.find_compatible_checkpoints()

            if compatible_checkpoints:
                logger.info(f"\nüìã Compatible checkpoints found:")
                for i, cp in enumerate(compatible_checkpoints):
                    logger.info(f"   {i+1}. {cp['name']}")
                    logger.info(f"      Score: {cp['compatibility_score']:.2f}")
                    logger.info(f"      {cp['description']}")
                    logger.info(f"      Path: {cp['path']}")
                    logger.info("")
            else:
                logger.info(f"‚ùå No compatible checkpoints found")

            return

        # –ó–∞–ø—É—Å–∫ —É–º–Ω–æ–≥–æ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        results = resume_manager.auto_resume_training(
            dataset_limit=args.dataset_limit,
            additional_epochs=args.additional_epochs,
            batch_size=args.batch_size,
        )

        if results:
            logger.info("üìà Training Results:")
            for key, value in results.items():
                if key != "training_log":  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥
                    logger.info(f"   {key}: {value}")

        logger.info("‚úÖ Smart resume training completed!")

    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Training interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Smart resume failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
